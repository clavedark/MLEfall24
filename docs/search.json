[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site:\nCreated with Quarto.\nAbout me:\nProfessor of political science, PhD Florida State 1999. I’ve taught this class for a long time, but never the same way twice. I study models of political violence, collect data on protests and repression.\n\n\n\n Back to top"
  },
  {
    "objectID": "likelihood24.html#how-would-you-characterize-the-variable-measuring-deaths-by-mule-kick",
    "href": "likelihood24.html#how-would-you-characterize-the-variable-measuring-deaths-by-mule-kick",
    "title": "Likelihood",
    "section": "How would you characterize the variable measuring deaths by mule kick?",
    "text": "How would you characterize the variable measuring deaths by mule kick?\n\nVariable measures events.\nEvents are discrete, not continuous.\nAre events correlated or independent?\nVariable is bounded; cannot be below zero.\n\nSo we need to think about two different things here - the distribution of \\(y\\) based on its observed distribution, and the link between the \\(X\\) variables and \\(\\widetilde{y}\\), the latent quantity of interest.\n\nWhat distribution might describe the frequency of mule kick deaths?\nNeeds to be discrete.\nNeeds to characterize rare events - at most we see about four per period, so relatively rare.\nNeeds to have a lower bound at zero (since we can’t observe negative numbers of deaths), and upper bound at \\(+\\infty\\)\n\n\n\\[Pr(Y=y_{i})=\\frac{e^{-\\lambda}\\lambda^{y_{i}}}{y_{i}!}\\]"
  },
  {
    "objectID": "likelihood24.html#the-poisson-distribution",
    "href": "likelihood24.html#the-poisson-distribution",
    "title": "Likelihood",
    "section": "The Poisson Distribution",
    "text": "The Poisson Distribution\nThe Poisson distribution is a discrete distribution that characterizes the number of events that occur in a fixed interval of time (or sometimes, space). Below are 3 Poisson distributions with means of .5, 1.5, and 2.5.\n\n\ncode\n#using a large sample, n=1000, simulate and plot the poisson distribution for mean values of .5, 1.5, and 2.5\n\n# Load required libraries\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\n\n# Set seed for reproducibility\nset.seed(8675309)\n\n# Simulate data\nn &lt;- 10000  # n for each distribution\nlambdas &lt;- c(0.5, 1.5, 2.5)  # Means \n\ndf &lt;- data.frame(\n  lambda_0.5 = rpois(n, lambda = 0.5),\n  lambda_1.5 = rpois(n, lambda = 1.5),\n  lambda_2.5 = rpois(n, lambda = 2.5)\n)\n\n# Reshape the data for ggplot\ndf_long &lt;- df %&gt;%\n  pivot_longer(cols = everything(), \n               names_to = \"distribution\", \n               values_to = \"value\") %&gt;%\n  mutate(distribution = factor(distribution, \n                               levels = c(\"lambda_0.5\", \"lambda_1.5\", \"lambda_2.5\"),\n                               labels = c(\"λ = 0.5\", \"λ = 1.5\", \"λ = 2.5\")))\n\n# plot \nggplot(df_long, aes(x = value, fill = distribution)) +\n  geom_density(position = \"identity\", alpha = 0.5, adjust=4) +\n  facet_wrap(~ distribution, ncol = 1, scales = \"free_y\") +\n  scale_fill_manual(values = c(\"#000000\", \"#6CC24A\", \"#005A43\")) +\n  labs(title = \"Simulated Poisson Distributions\",\n       x = \"Value\",\n       y = \"Count\",\n       fill = \"Distribution\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nThinking in terms of the data, let’s write a likelihood function, the joint probability for all \\(i\\) observations in the sample, \\(n\\):\n\\[\n\\mathcal{L}(\\lambda)= \\prod_{i=1}^{n} \\left[\\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!} \\right]\n\\]\nTake the natural log of the likelihood function:\n\\[\n\\ln \\mathcal{L}(\\lambda)= \\ln \\left\\{\\prod_{i=1}^{n} \\left[\\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!} \\right] \\right\\}\n\\]\n\\[\\ln \\mathcal{L}(\\lambda)= \\sum_{i=1}^{n} \\left[-\\lambda + y_i \\ln(\\lambda) - \\ln(y_i!) \\right]\n\\]\nWhat about the \\(X\\) variables? Parameterize the model with respect to those variables such that they influence the mean, \\(\\lambda\\). So let’s make \\(\\lambda\\) a function of the \\(X\\) variables and their effects, \\(\\beta\\), using the exponential distribution as the link function:\n\\[\nE[Y|X]=\\lambda =exp(X\\beta)\n\\]\nThe exponential ensures we won’t have negative predictions.\nPutting all this together we have:\n\\[\n\\ln \\mathcal{L}(\\lambda)= \\sum_{i=1}^{n} \\left[-e^{X\\beta} + y_i \\ln(X\\beta) - \\ln(y_i!) \\right]\n\\]"
  },
  {
    "objectID": "likelihood24.html#estimation-technology-ols",
    "href": "likelihood24.html#estimation-technology-ols",
    "title": "Likelihood",
    "section": "Estimation Technology: OLS",
    "text": "Estimation Technology: OLS\nRecall that the technology of OLS is to assume a normally distributed error term, minimize the sum of those squared errors analytically using calculus."
  },
  {
    "objectID": "likelihood24.html#estimation-technology-mle",
    "href": "likelihood24.html#estimation-technology-mle",
    "title": "Likelihood",
    "section": "Estimation Technology: MLE",
    "text": "Estimation Technology: MLE\nThe technology of ML is to maximize the LLF with respect to \\(\\beta\\). We can do this in a couple of different ways:\n\nanalytic methods - solve calculus. Some/many models do not have analytical or closed form solutions.\nnumerical methods - use an algorithm to estimate starting values for \\(\\theta\\), then hill climb until the first derivative is zero, and the second derivative is negative. This is iterative, trying values, looking at the derivatives. This is what nearly all ML estimation uses - there are different algorithms for doing this. The most commonly used is the Newton Raphson method - it’s illustrated in detail in the maximization slides\n\n\nAnalytic Methods\nWith some functions, we can solve for the unknowns directly. Let’s return to the Poisson log-likelihood function and consider data on the number of civil wars in Africa over a ten year period, and the data are as follows:\n\\(y\\) = {5 0 1 1 0 3 2 3 4 1}\nThe log-likelihood function is:\n\\[\\begin{aligned}\n\\ln \\mathcal{L}(\\lambda|Y) =\\ln \\left[ \\prod\\limits_{i=1}^{N} \\frac{e^{-\\lambda}\\lambda^{y_{i}}}{y_{i}!}\\right]\\nonumber \\\\ \\nonumber \\\\\n=-N \\lambda+ \\sum(y_{i}) \\ln(\\lambda) - \\sum(\\ln(y_{i}!)) \\nonumber\n\\end{aligned}\\]\n\\(\\lambda\\) is the unknown we want to solve for. Taking the derivative with respect to \\(\\lambda\\) and setting equal to zero:\n\\[\\begin{aligned}\n\\frac{\\partial \\ln \\mathcal{L}}{\\partial \\lambda}=-N \\lambda+ \\sum(y_{i}) \\ln(\\lambda) - \\sum(\\ln(y_{i}!)) \\nonumber \\\\ \\nonumber  \\\\\n0=-N + \\frac{\\sum y_{i}}{\\lambda} \\nonumber\\\\ \\nonumber \\\\\n\\color{red}{\\widehat{\\lambda}= \\frac{\\sum y_{i}}{N}} \\nonumber\n\\end{aligned}\\]\nThis is just the sample mean of course - let’s plug in our data and solve for \\(\\lambda\\):\n\\[\\widehat{\\lambda}= \\frac{20}{10} \\] So the value of \\(\\lambda\\) the maximizes the function is 2. This is a trivial example in the sense that applications with \\(x\\) variables are sufficiently complicated that analytical methods are not usually possible, so we turn to numerical methods."
  },
  {
    "objectID": "likelihood24.html#normal-linear-llf",
    "href": "likelihood24.html#normal-linear-llf",
    "title": "Likelihood",
    "section": "Normal (linear) LLF:",
    "text": "Normal (linear) LLF:\n\\[\n= -\\frac{N}{2}(\\ln(2\\pi)) -\\frac{N}{2}(\\ln(\\sigma^{2})) -\\frac{1}{2\\sigma^{2}}\\left[\\sum\\limits_{i=1}^{n}(y_{i}-\\mu)^{2}\\right] \\nonumber\n\\]\nNotice \\(N\\) in the numerator; recall the rule of summation that \\(\\sum\\limits_{i=1}^{n}a= n\\cdot a\\).\nNow, take the derivative of the log-likelihood with respect to each of the parameters in turn (ignoring constant terms and terms that pertain exclusively to the other parameter).\n\\[\n\\frac{\\partial \\ln L}{\\partial \\mu}= \\frac{1}{\\sigma^{2}}\\sum(y_{i}-\\mu)=0 \\nonumber\\\\\n=\\sum(y_{i}-\\mu) = \\sum y_{i}- \\sum \\mu  \\nonumber\\\\\n=\\sum y_{i}- N \\mu = 0 \\nonumber \\\\\n\\mu=\\frac{\\sum y_{i}}{N} = \\widehat{y}\\nonumber\n\\]\nwe can also solve for \\(\\sigma^{2}\\), getting\n\\[\n\\frac{\\partial \\ln L}{\\partial \\sigma^{2}}= -\\frac{N}{2 \\sigma^{2}}+\\frac{1}{2 \\sigma^{4}} +\\sum(y_{i}-\\mu)=0 \\nonumber\\\\ \\nonumber\\\\\n=-\\frac{N}{2}\\sigma^{2}+\\frac{1}{2}\\sum(y_{i}-\\bar{y})^{2}= 0 \\nonumber\\\\ \\nonumber\\\\\n\\ldots\n\\widehat{\\sigma^{2}}=\\frac{\\sum(y_{i}-\\bar{y})^{2}}{N} \\nonumber\n\\]\nThis is a biased estimator of \\(\\sigma^{2}\\); \\(\\sigma^{2}\\) is underestimated because the denominator should be \\(N-1\\).\nThe same thing in matrix notation:\n\\[ln\\mathcal{L}(y | X, \\beta, \\sigma^2) = -\\frac{N}{2} ln(2\\pi) - \\frac{N}{2} ln(\\sigma^2) -\\frac{1}{2} \\left[ \\frac{(y-X\\beta)'(y-X\\beta)}{\\sigma^2} \\right] \\]\nrewriting to isolate the parameters:\n\\[\nln\\mathcal{L}(y | X, \\beta, \\sigma^2) = -\\frac{N}{2} ln(2\\pi) - \\frac{N}{2} ln(\\sigma^2) -\\frac{1}{2\\sigma^2} \\left[ yy'- 2y' X\\beta +\\beta' X' X\\beta) \\right]\n\\]\nTake derivatives of \\(\\ln \\mathcal{L}\\) w.r.t. \\(\\beta\\) and \\(\\sigma^2\\) (and skipping a lot here):\n\\[\\frac{\\partial ln \\mathcal{L}}{\\partial \\beta} = \\frac{1}{\\sigma^2} (X'y - X'X \\beta)\\]\nsetting equal to zero …\n\\[ \\frac{1}{\\sigma^2} (X'y - X'X \\beta) = 0\\] \\[X'X \\beta = X'y\\] \\[\\widehat{\\beta} = (X'X)^{-1} X' y \\]\n…going through the same thing for \\(\\sigma^2\\) gives us:\n\\[\\widehat{\\sigma^2} = \\frac{e'e}{N}\\]\nSo aside from seeing how analytic methods work, we have also seen that the BLUE OLS estimator is the ML estimator for \\(\\beta\\), and that the variance estimate in ML is biased downward (the denominator is always too large by \\(k-1\\)). This difference disappears in large samples.\nWhy do we leave OLS if these are the same? Because this is the rare case defined by normal data which both satisfies the OLS requirement for a normal disturbance, and permits MLE estimation with a normal LLF. With non-normal data, OLS and ML estimators diverge quite a lot."
  },
  {
    "objectID": "likelihood24.html#numerical-methods",
    "href": "likelihood24.html#numerical-methods",
    "title": "Likelihood",
    "section": "Numerical Methods",
    "text": "Numerical Methods\nNumerical methods are computationally intensive ways to plug in possible parameter values, generate a log likelihood, and then use calculus to evaluate whether the that value is a maximum. We use numerical methods when no analytic or “closed form” solution exists which is essentially all the time.\nDo this by evaluating:\n\nthe first derivative of the LLF - by finding the point on the function where a tangent line has a slope equal to zero, we know we’ve found an inflection point.\nthe second derivative of the LLF - if the rate of change in the function at the very next point is increasing, it’s a minimum; decreasing, it’s a maximum.\n\nthe Hessian matrix - the matrix of second derivatives - tells us the curvature of the LLF, or the rate of change.\n\nSuppose that we have the event count data reported above representing civil wars in Africa, and that we want to compute the likelihood of \\(\\lambda|Y\\). We can compute the likelihood using numerical methods; one specific technique is a grid search procedure. Just as we might try different values for \\(x\\) when graphing a function, \\(f(x)\\) in algebra, we will insert possible values for \\(\\lambda\\) into the log-likelihood function in such a way that we can identify an apparent maximum (a value for \\(\\lambda\\) for which the log-likelihood is at its largest compared to contiguous values of \\(\\lambda\\)). Put another way, we take a guess at the value of \\(\\lambda\\), compute the log-likelihood, and take another guess at \\(\\lambda\\), compute the log-likelihood and compare the two estimates of the likelihood; we repeat this process until a pattern emerges such that we can discern a maximum value.\nThe log-likelihood function for the poisson distributed data on civil wars is\n\\[\n\\ln \\mathcal{L}(\\lambda|Y)= \\ln \\left[\\frac{e^{-10\\lambda}\\lambda^{20}}{207360}\\right] \\nonumber  \\\\ \\nonumber \\\\\n= -10 \\lambda+ 20 \\ln(\\lambda) - \\ln(207360) \\nonumber\n\\]\nSuppose we make some guesses regarding the value of \\(\\lambda\\), plug them into the function and compare the resulting values of the log-likelihood - take a look at the code chunk below:\n\n\ncode\n#iterate over lambda, create data frame of lambda and log-likelihood\nlambda &lt;- seq(0.1, 3.5, by=0.1)\nllf &lt;- NULL\nfor (i in 1:length(lambda)){\n  L &lt;- -10*lambda[i] + 20*log(lambda[i]) - log(207360)\n  llf &lt;- data.frame(rbind(llf, c(lambda=lambda[i], ll=L)))\n}\n\n#highchart with reference line at maximum value of the log-likelihood\nhighchart() %&gt;% \n  hc_add_series(llf, \"line\", hcaes(x=lambda, y=ll)) %&gt;% \n  hc_title(text=\"Log-Likelihood Estimates\") %&gt;% \n  hc_subtitle(text=\"Civil Wars in Africa\") %&gt;% \n hc_xAxis(title = list(text = \"Lambda\"), plotLines = list(list(value = 2, color=\"red\"))) %&gt;%\n  hc_yAxis(title = list(text = \"log-likelihood\"), plotLines = list(list(value = max(llf$ll), color=\"red\"))) %&gt;%\n  hc_tooltip(pointFormat = \"Lambda: {point.x}&lt;br&gt;Log-Likelihood: {point.y}\") %&gt;% \n  hc_colors(\"#005A43\") \n\n\n\n\n\n\nWe can see that the largest value of the likelihood is where \\(\\lambda\\) = 2 - that’s the value that maximizes the likelihood function. And not surprisingly, notice that we have arrived at the same solution we produced in the analytic example above. This is another trivial example insofar as grid search methods are usually not sufficient for solving multivariate problems (nor for computing the variance-covariance matrix)."
  },
  {
    "objectID": "likelihood24.html#how-numerical-methods-work",
    "href": "likelihood24.html#how-numerical-methods-work",
    "title": "Likelihood",
    "section": "How numerical methods work",
    "text": "How numerical methods work\n\nChoose starting values of \\(\\beta\\) (sometimes from OLS) to estimate the log-likelihood.\nTake the derivative of the log-likelihood with respect to the parameters to find the gradient}. The gradient (or the gradient matrix, a \\(kxk\\) matrix) tells us the direction of the slope of a line tangent to the curve at the point of the log-likelihood estimate.\nIf the gradient is positive (if the matrix is positive definite), then \\(ln \\mathcal{L}\\) is increasing in \\(\\beta\\) - the slope is increasing, so increase our estimate of \\(\\beta\\) and try again.\nIf the gradient is negative (if the matrix is negative definite), the \\(ln \\mathcal{L}\\) is decreasing in \\(\\beta\\) - the slope is decreasing, so we’ve passed the maximum; choose a smaller value for \\(\\beta\\) and try again.\nAs the log-likelihood approaches the maximum, the gradient approaches zero - the slope of the line tangent to the curve at the point of the log-likelihood estimate is approaching zero, indicating we’re reaching the maximum of the function. Stop the search and evaluate the estimates of \\(\\beta\\) that produced the zero gradient.\nThroughout this process, we need to evaluate the second derivatives in order to figure out the rate at which the slope is changing; this helps us tell how close or far we are from the maximum. The second derivative describes the curvature of the LLF, or the rate of change.\nThe matrix of second derivatives (the Hessian matrix) or its approximation also provide the source of our estimates of the variance, and thus the standard errors.\n\nThe first derivative tells us the direction in which the function is changing. This is obviously important since we’re trying to find the maximum.\nThink of this as trying to figure out when you’re exactly at the top of a hill. The slope (the grade, the gradient) is positive while you’re climbing to the top, it’s zero at the top, and it’s negative on the way down the other side.\nBut is the hill flat or steep? If it’s flat, then the change in the slope between point A and point B is likely to be very small - this, of course, can make it difficult to know exactly when we’re at the top (the maximum). On the other hand, if the hill is very steep, the change in the slope between two points is pretty substantial. Put another way, the rate of change in the slope is larger (faster) the steeper the slope; it’s smaller (slower) the flatter the slope.\nThis matters to maximization because the second derivatives tell us how big (or small) a step we should take up the hill as we try to find the top. Suppose that the function is very flat; as indicated above, the change in the slope between two points would be small, so we can take larger steps in order to try to find the maximum. The second derivatives would tell us that the rate of change is very small, so we should take larger steps.\nThe software performing the estimation will choose the next value of \\(\\beta\\) a bit further away from the last value it tried. On the other hand, if the second derivatives are large so the rate of change is fast, we want to take relatively small steps so we don’t step right over the maximum. In any case, that’s the intuition for why we need to know the matrix of second derivatives."
  },
  {
    "objectID": "likelihood24.html#variance-covariance-matrix",
    "href": "likelihood24.html#variance-covariance-matrix",
    "title": "Likelihood",
    "section": "Variance-Covariance matrix",
    "text": "Variance-Covariance matrix\nEstimating the second derivatives can be a real nightmare in estimation, but it’s important not only for finding the maximum of the function (and therefore in estimating the \\(\\beta\\)s), but for computing the variance-covariance matrix as well. Here are our options:\n\nFind the Hessian. The Hessian is a \\(kxk\\) matrix of the second derivatives of the log-likelihood function with respect to \\(\\beta\\), where the second derivatives are on the main diagonal. Commonly estimated using the Newton-Raphson algorithm.\nFind the information matrix. This is the negative of the expected value of the Hessian matrix, computed using the method of scoring.\n\nOuter product approximation, where we sum the squares of the first derivatives (thus avoiding the second derivatives all together). This is computed using the Berndt, Hall, Hall, and Hausman} or BHHH algorithm."
  },
  {
    "objectID": "likelihood24.html#grid-search",
    "href": "likelihood24.html#grid-search",
    "title": "Likelihood",
    "section": "Grid Search",
    "text": "Grid Search\nGrid search is another method for maximization. The process is to iteratively try values for the parameters of interest, refining those values as the log-likelihood gets larger and larger. In general, we plug in values for the parameters, compute the likelihood, then identify the largest LL value - the parameters that produce that value are our answer.\nThis method is instructive for how numerical methods work, but not practical in most applications with more than a couple of unknowns."
  },
  {
    "objectID": "likelihood24.html#latent-variable-motivation",
    "href": "likelihood24.html#latent-variable-motivation",
    "title": "Likelihood",
    "section": "Latent Variable Motivation",
    "text": "Latent Variable Motivation\nThere are a couple of (related) ways to motivate the model. Let’s assume a latent quantity we’re interested, denoted \\(y^*\\), but our observations of \\(y\\) are limited to successes (\\(y_i=1\\)) and failures (\\(y_i=0\\)).\n\\[\ny^{*}=\\mathbf{x_i \\beta}+\\epsilon_i \\nonumber\n\\]\nfor \\(y^{*}\\), the latent variable,\n\\[\ny_{i} = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{if $y^{*}_{1}&gt;\\kappa$} \\\\\n         0, & \\mbox{if $y^{*}_{1} \\leq \\kappa$}\n         \\end{array}\n     \\right.\n\\]\nwhere \\(\\kappa\\) is an unobserved threshold.\nMake probabilities statements,\n\\[\nPr(y_i=1) = Pr(y^{*}_{1}&gt;\\kappa) \\nonumber \\\\ \\nonumber \\\\\n=Pr(\\mathbf{x_i \\beta}+\\epsilon_i&gt;\\kappa) \\nonumber \\\\ \\nonumber \\\\\n=Pr(\\epsilon_i&gt;\\kappa-\\mathbf{x_i \\beta}) \\nonumber\n\\]\nNormalizing \\(\\kappa=0\\),\n\\[\nPr(y_i=1)=Pr(\\epsilon_i&gt;-\\mathbf{x_i \\beta}) \\nonumber\n\\]\n\\[\nPr(y_i=1)=Pr(\\epsilon_i&gt;-\\mathbf{x_i \\beta}) \\nonumber \\\\ \\nonumber \\\\\n=1-F(-\\mathbf{x_i \\beta}) \\nonumber\n\\]\nassuming \\(F\\) is symmetric, with unit variance,\n\\[\n\\pi_i= Pr(y_i=1)=1-F(-\\mathbf{x_i \\beta}) = F(\\mathbf{x_i \\beta}) \\\\   \\nonumber \\\\\n1-\\pi_i=Pr(y_i=0)= 1-F(\\mathbf{x_i \\beta})\n\\]"
  },
  {
    "objectID": "likelihood24.html#binomial-likelihood-function",
    "href": "likelihood24.html#binomial-likelihood-function",
    "title": "Likelihood",
    "section": "Binomial Likelihood Function",
    "text": "Binomial Likelihood Function\nThe observed data are binary, assumed binomial (\\(\\pi\\)), \\(y_i=0,1\\). The likelihood function must have two parts, one for cases where \\(y_i=0\\), the other for cases where \\(y_i=1\\). Recalling that \\(\\pi=F(\\mathbf{x_i \\beta})\\), and \\(1-\\pi= 1-F(\\mathbf{x_i \\beta})\\),\n\\[\nPr(y_1,y_2,y_3 \\ldots y_n) =  \\prod_{y=1}F(\\mathbf{x_i \\beta}) \\prod_{y=0}[1-F(\\mathbf{x_i \\beta})]\\nonumber\n\\]\nThis is the joint probability we observe all the data, \\(Y\\), simultaneously. We can rewrite this as the likelihood of observing the data given \\(\\beta\\),\n\\[\n\\mathcal{L} (Y|\\beta) =  \\prod_{i=1}^{N} [F(\\mathbf{x_i \\beta})]^{y_i} [1-F(\\mathbf{x_i \\beta})]^{1-y_i}\\nonumber\n\\]\nAnd take the natural log\n\\[\n\\ln(\\mathcal{L} (Y|\\beta)) = \\ln( \\prod_{i=1}^{N} [F(\\mathbf{x_i \\beta})]^{y_i} [1-F(\\mathbf{x_i \\beta})]^{1-y_i})\\nonumber \\\\ \\nonumber \\\\\n= \\sum_{i=1}^{N} y_i \\ln F(\\mathbf{x_i \\beta})+ (1-y_i) \\ln[1-F(\\mathbf{x_i \\beta})] \\nonumber\n\\]\nNote the two parts of the LLF corresponding to the limited observations in the data, 0,1."
  },
  {
    "objectID": "likelihood24.html#choosing-a-link",
    "href": "likelihood24.html#choosing-a-link",
    "title": "Likelihood",
    "section": "Choosing a Link",
    "text": "Choosing a Link\nLet’s make this a probit model by assuming the link to the latent variable is standard normal, so \\(F(\\cdot)\\sim N_{i.i.d.}(0,1)\\):\n\\[\n\\ln \\mathcal{L} (Y|\\beta) = \\sum_{i=1}^{N} y_i \\ln \\Phi(\\mathbf{x_i \\beta})+ (1-y_i) \\ln[1-\\Phi(\\mathbf{x_i \\beta})] \\nonumber\n\\]\nLogit would look like this:\n\\[\n\\ln \\mathcal{L} (Y|\\beta) = \\sum_{i=1}^{N} \\left\\{ y_i \\ln \\left(\\frac{1}{1+e^{-\\mathbf{x_i \\beta}}}\\right)+ (1-y_i) \\ln \\left[1-\\left(\\frac{1}{1+e^{-\\mathbf{x_i \\beta}}}\\right)\\right] \\right\\}\\nonumber\n\\]"
  },
  {
    "objectID": "likelihood24.html#estimation",
    "href": "likelihood24.html#estimation",
    "title": "Likelihood",
    "section": "Estimation",
    "text": "Estimation\nIdeally, we’d like just to estimate by finding the values of \\(\\beta\\) that maximize the log-likelihood function, and do so analytically (i.e., using calculus). This is what we do in OLS, though with respect to minimizing the sum of the squared residuals. But because the solution is non-linear in \\(\\beta\\), there is no closed form or simple analytic solution.\nAs a result, ML models produce estimates of \\(\\beta\\) by using numerical optimization methods. These are generally iterative attempts to narrow down the range in which the maximum lies by plugging in different values of \\(\\beta\\) until the range is so small, we can safely say we’ve maximized the function using those values of \\(\\beta\\)."
  },
  {
    "objectID": "likelihood24.html#maximization",
    "href": "likelihood24.html#maximization",
    "title": "Likelihood",
    "section": "Maximization",
    "text": "Maximization\nSuppose we have binary data that look like this:\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\nOur question is what is the distribution parameter most likely responsible for having generated these observed data.\nWe need to plug a hypothetical value for the distribution parameter into the log-likelihood function, compute log-likelihoods for each observation, then do the same thing with other hypothetical values. Whichever value produces the biggest log-likelihoods is the value most likely responsible for producing the data we have.\nWhat do we mean by distribution parameter? Well, in the LLF below, we’ve referred to our unknown as \\(F(\\mathbf{x_i \\beta})\\), but we really mean we need an estimate of the parameter \\(\\Theta\\) which represents the effects of the \\(X\\)s via the functional form we’ve imposed by assuming a distribution of \\(\\epsilon\\). In this particular case (for simplicity) we don’t have any \\(X\\) variables.\n\\[\n\\ln(\\mathcal{L} (Y|\\Theta)) = \\sum_{i=1}^{N} y_i \\ln F(\\mathbf{\\Theta})+ (1-y_i) \\ln[1-F(\\mathbf{\\Theta})] \\nonumber\n\\]\nHere’s what we’ll do:\n\nchoose some hypothetical values of \\(\\Theta\\); since this is binary, and our latent variable a probability, let’s choose values from .2 to .8.\ncompute \\(N\\) log-likelihoods for each value of \\(\\Theta\\); N=20, and we have 7 values of \\(\\Theta\\).\nsum the \\(N\\) log-likelihoods for each value of \\(\\Theta\\); so we’ll end up with 7 summed log-likelihoods.\nevaluate the summed log-likelihoods, and see which is largest.\ndeclare the value of \\(\\Theta\\) that produced that largest summed log-likelihood as the parameter most likely to have generated the data.\n\nLet \\(\\Theta=.2\\):\n\\[\n\\ln(\\mathcal{L}_{i=1} (Y|\\Theta)) = 0 *\\ln (.2)+ (1-0) \\ln[1-.2] =-0.2231\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=2} (Y|\\Theta)) = 0 *\\ln (.2)+ (1-0) \\ln[1-.2]=-0.2231 \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=3} (Y|\\Theta)) = 0 *\\ln (.2)+ (1-0) \\ln[1-.2]=-0.2231 \\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=11} (Y|\\Theta)) = 1 *\\ln (.2)+ (1-1) \\ln[1-.2] =-1.609\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=12} (Y|\\Theta)) = 1 *\\ln (.2)+ (1-1) \\ln[1-.2]=-1.609 \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=13} (Y|\\Theta)) = 1 *\\ln (.2)+ (1-1) \\ln[1-.2] =-1.609\\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\sum_{i=1}^{20}=-18.32100 \\nonumber\n\\]\nLet \\(\\Theta=.3\\):\n\\[\n\\ln(\\mathcal{L}_{i=1} (Y|\\Theta)) = 0 *\\ln (.3)+ (1-0) \\ln[1-.3] =-0.3567\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=2} (Y|\\Theta)) = 0 *\\ln (.3)+ (1-0) \\ln[1-.3]=-0.3567 \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=3} (Y|\\Theta)) = 0 *\\ln (.3)+ (1-0) \\ln[1-.3]=-0.3567 \\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=11} (Y|\\Theta)) = 1 *\\ln (.3)+ (1-1) \\ln[1-.3] =-1.204\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=12} (Y|\\Theta)) = 1 *\\ln (.3)+ (1-1) \\ln[1-.3]=-1.204\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=13} (Y|\\Theta)) = 1 *\\ln (.3)+ (1-1) \\ln[1-.3] =-1.204\\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\sum_{i=1}^{20}=-15.60700 \\nonumber\n\\]\nLet \\(\\Theta=.4\\):\n\\[\n\\ln(\\mathcal{L}_{i=1} (Y|\\Theta)) = 0 *\\ln (.4)+ (1-0) \\ln[1-.5] =-0.5108\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=2} (Y|\\Theta)) = 0 *\\ln (.4)+ (1-0) \\ln[1-.5]=-0.5108 \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=3} (Y|\\Theta)) = 0 *\\ln (.4)+ (1-0) \\ln[1-.5]=-0.5108 \\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=11} (Y|\\Theta)) = 1 *\\ln (.4)+ (1-1) \\ln[1-.4] =-0.9163\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=12} (Y|\\Theta)) = 1 *\\ln (.4)+ (1-1) \\ln[1-.4]=-0.9163\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=13} (Y|\\Theta)) = 1 *\\ln (.4)+ (1-1) \\ln[1-.4] =-0.9163\\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\sum_{i=1}^{20}=-14.2711636 \\nonumber\n\\]\nLet \\(\\Theta=.5\\):\n\\[\n\\ln(\\mathcal{L}_{i=1} (Y|\\Theta)) = 0 *\\ln (.5)+ (1-0) \\ln[1-.5] =-0.6931\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=2} (Y|\\Theta)) = 0 *\\ln (.5)+ (1-0) \\ln[1-.5]=-0.6931 \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=3} (Y|\\Theta)) = 0 *\\ln (.5)+ (1-0) \\ln[1-.5]=-0.6931 \\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=11} (Y|\\Theta)) = 1 *\\ln (.5)+ (1-1) \\ln[1-.5] =-0.6931\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=12} (Y|\\Theta)) = 1 *\\ln (.5)+ (1-1) \\ln[1-.5]=-0.6931\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=13} (Y|\\Theta)) = 1 *\\ln (.5)+ (1-1) \\ln[1-.5] =-0.6931\\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\sum_{i=1}^{20}=-13.8629436 \\nonumber\n\\]\nLet’s compare what we have so far:\n\\[llf_{.2}&lt;llf_{.3}&lt;llf_{.4}&lt;llf_{.5}\\]\nYou can probably see some symmetry here due to the fact that half the data are ones, half zeros, so completing:\\~\\\n\\(llf_{.2}&lt;llf_{.3}&lt;llf_{.4}&lt;llf_{.5}&gt;llf_{.6}&gt;llf_{.7}&gt;llf_{.8}\\) \\~\\\nSo \\(\\Theta=.5\\) produces the largest log-likelihood (-13.86) and thus is the parameter most likely to have produced the observed data."
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "Binomial Models - video\nPrediction Methods - spring 2024 prediction video\nLikelihood Theory and Applications\nMaximization Methods - video\nBinary Model Extensions I\n\n\n\n Back to top"
  },
  {
    "objectID": "binarymodels24.html",
    "href": "binarymodels24.html",
    "title": "Binary Response Models",
    "section": "",
    "text": "How can we model a binary \\(y\\) variable?\nDoes OLS (the linear probability model) work sufficiently well?\nHow can we build a maximum likelihood model?"
  },
  {
    "objectID": "binarymodels24.html#questions",
    "href": "binarymodels24.html#questions",
    "title": "Binary Response Models",
    "section": "",
    "text": "How can we model a binary \\(y\\) variable?\nDoes OLS (the linear probability model) work sufficiently well?\nHow can we build a maximum likelihood model?"
  },
  {
    "objectID": "binarymodels24.html#example---democratic-peace-data",
    "href": "binarymodels24.html#example---democratic-peace-data",
    "title": "Binary Response Models",
    "section": "Example - Democratic Peace data",
    "text": "Example - Democratic Peace data\nAs a running example, I’ll use the Democratic Peace data to estimate logit and probit models. These come from Oneal and Russett (1997)’s well-known study in ISQ. The units are dyad-years; the \\(y\\) variable is the presence or absence of a militarized dispute, and the \\(x\\) variables include a measure of democracy (the lowest of the two Polity scores in the dyad), and a set of controls. The principle expectation here is that as the lowest democracy score in the dyad increases, the probability of a militarized dispute decreases.\n\nPredictions out of bounds\nThis figured plots the predictions from a logit and OLS model. Unsurprisingly, the logit predictions are probabilities, so are in the \\([0,1]\\) interval. The OLS predictions are not, and are often out of bounds.\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\nlogitpreds &lt;- predict(m1, type=\"response\")\n\nm2 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"probit\"), data=dp )\nmols &lt;-lm(dispute ~ border+deml+caprat+ally, data=dp )\nolspreds &lt;- predict(mols)\n\ndf &lt;- data.frame(logitpreds, olspreds, dispute=as.factor(dp$dispute))\n\nggplot(df, aes(x=logitpreds, y=olspreds, color=dispute)) + \n  geom_point()+\n  labs(title=\"Predictions from Logit and OLS\", x=\"Logit Predictions\", y=\"OLS Predictions\")+\n  geom_hline(yintercept=0)+\n  theme_minimal() +\n  scale_color_manual(values=c(\"#005A43\", \"#6CC24A\")) +\n  annotate(\"text\", x=.05, y=-.05, label=\"2,147 Predictions out of bounds\", color=\"red\")\n\n\n\n\n\n\n\n\n\nHere’s the distribution of predictions from the OLS model - you’ll note the modal density is around .04 (which is the sample frequency of \\(y\\).), but that a substantial and long tail are negative, so out of probability bounds.\n\n\ncode\nggplot(df, aes(x=olspreds)) + \n  geom_density(alpha=.5)+\n  labs(title=\"Density of OLS Predictions\", x=\"Predictions\", y=\"Density\")+\n  theme_minimal()+\ngeom_vline(xintercept=0, linetype=\"dashed\")\n\n\n\n\n\n\n\n\n\n\n\nHeteroskedastic Residuals\nThe residuals from the OLS model appear heteroskedastic, and the distribution is not normal. In fact, the distribution appears more binomial, clustered around zero and one. This shouldn’t be surprising since the \\(y\\) variable only takes on values of zero and one, and since we compute the residuals by \\(u = y - \\hat{y}\\).\n\n\ncode\ndf &lt;- data.frame(df, mols$residuals)\n \nggplot(df, aes(x=mols.residuals, color=dispute)) + \n  geom_density()+\n  labs(title=\"Density of OLS Residuals\", x=\"Residuals\", y=\"Density\")+\n  theme_minimal()+\n    scale_color_manual(values=c(\"#005A43\", \"#6CC24A\")) +\n  geom_vline(xintercept=0, linetype=\"dashed\")"
  },
  {
    "objectID": "binarymodels24.html#when-is-the-lpm-appropriate",
    "href": "binarymodels24.html#when-is-the-lpm-appropriate",
    "title": "Binary Response Models",
    "section": "When is the LPM Appropriate?",
    "text": "When is the LPM Appropriate?\nThe best answer is never.\n\nThere seems to be a mild trend in the discipline to rehabilitate the LPM though it’s not clear why - that is, it’s hard to find statements about the advantages of doing so in any particular setting, or about the disadvantages of estimating a logit or probit model that would lead us to prefer the LPM.\n\nOLS is a rockin’ estimator, but it’s just not well suited to limited \\(y\\) variables. Efforts to rehabilitate the LPM are like putting lipstick on a pig."
  },
  {
    "objectID": "binarymodels24.html#examples-of-limited-dvs",
    "href": "binarymodels24.html#examples-of-limited-dvs",
    "title": "Binary Response Models",
    "section": "Examples of Limited DVs",
    "text": "Examples of Limited DVs\n\nbinary variables: 0=peace, 1=war; 0=vote, 1=don’t vote.\nunordered or nominal categorical variables: type of car you prefer: Honda, Toyota, Ford, Buick; policy choices; party or candidate choices.\nordered variables that take on few values: some survey responses.\ndiscrete count variables; number of episodes of scarring torture in a country-year, 0, 1, 2, 3, …, \\(\\infty\\); the number of flawed computer chips produced in a factory in a shift; the number of times a person has been arrested; the number of self-reported extramarital affairs; number of visits to your primary care doctor.\ntime to failure; how long a civil war lasts; how long a patient survives disease; how long a leader survives in office."
  },
  {
    "objectID": "binarymodels24.html#binary-y-variables-1",
    "href": "binarymodels24.html#binary-y-variables-1",
    "title": "Binary Response Models",
    "section": "Binary \\(y\\) variables",
    "text": "Binary \\(y\\) variables\nGenerally, we think of a binary variable as being the observable manifestation of some latent, unobserved continuous variable.\nIf we could adequately observe (and measure) the underlying continuous variable, we’d use some form of OLS regression to analyze that variable. But because we have limited observation, we turn to maximum likelihood methods to estimate a model that allows to use \\(y\\), but generate estimates of \\(y^*\\), the variable we wish we could measure."
  },
  {
    "objectID": "binarymodels24.html#a-nonlinear-model-for-binary-data",
    "href": "binarymodels24.html#a-nonlinear-model-for-binary-data",
    "title": "Binary Response Models",
    "section": "A nonlinear model for binary data",
    "text": "A nonlinear model for binary data\nSo \\(y\\) is binary, and we’ve established the linear model is not appropriate. The observed variable, \\(y\\), appears to be binomial (iid):\n\\[ y \\sim f_{binomial}(\\pi_i)\\]\n\\[ y = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{} \\pi_{i}\\\\\n         0, & \\mbox{} 1-\\pi_{i}\n         \\end{array}\n     \\right. \\]\n\\[ \\pi_i = F(x_i\\widehat{\\beta}) \\] \\[1- \\pi_i=1-F(x_i\\widehat{\\beta})\\]"
  },
  {
    "objectID": "binarymodels24.html#binomial-likelihood",
    "href": "binarymodels24.html#binomial-likelihood",
    "title": "Binary Response Models",
    "section": "Binomial Likelihood",
    "text": "Binomial Likelihood\nWrite the binomial density:\n\\[ Pr(y=1| \\pi) = \\pi_i^{y_i} (1-\\pi_i)^{1-y_i} \\]\nWrite the joint probability as a likelihood:\n\\[\\mathcal{L} (\\pi |\\ y) = \\prod \\limits_{i=1}^{n} \\left[ \\pi_i^{y_i} (1-\\pi_i)^{1-y_i}\\right]\\]\nTake the log of that likelihood:\n\\[\\ln \\mathcal{L} (\\pi| \\ y) = \\sum \\limits_{i=1}^{n}  \\left[ y_i \\ln ( \\pi_i) +  (1-y_i) \\ln(1-\\pi_i)\\right]\\]"
  },
  {
    "objectID": "binarymodels24.html#parameterize-the-model",
    "href": "binarymodels24.html#parameterize-the-model",
    "title": "Binary Response Models",
    "section": "Parameterize the model",
    "text": "Parameterize the model\nParameterize \\(\\pi_i\\) - make \\(\\pi_i\\) a function of some variables and their slope effects, \\(x\\beta\\) - this is the systematic component of the model:\n\\[\\pi_i= F(x \\beta)\\]\nThis is the binomial log-likelihood function.\n\\[\\ln \\mathcal{L} (\\pi| \\ y) = \\sum \\limits_{i=1}^{n}  \\left[ y_i \\ln (F(x_i\\widehat{\\beta})) +  (1-y_i) \\ln(1-F(x_i\\widehat{\\beta}))\\right]\\]\nBut we need to fill in \\(F\\), the link function."
  },
  {
    "objectID": "binarymodels24.html#link-function",
    "href": "binarymodels24.html#link-function",
    "title": "Binary Response Models",
    "section": "Link Function",
    "text": "Link Function\nWe parameterized \\(\\pi_i\\):\n\\[\\pi_i= F(x \\beta)\\]\nand now need to choose an appropriate link function for \\(F\\) such that:\n\nour prediction of \\(\\widehat{\\pi_i}\\) is bounded [0,1].\n\\(x_i \\widehat{\\beta}\\) can range over the interval \\([-\\infty, +\\infty]\\) and map onto the [0,1] interval.\n\nThere’s a large number of sigmoid shaped probability functions that will satisfy these needs.\n\n\n\n\n\n\nImportant Concept\n\n\n\nThe link function maps or transforms the linear prediction on the sigmoid probability space, and obeys the bounds of 0,1.\n\n\nThe most commonly used link functions are the standard normal (probit)}\n\\[Pr(y_i=1 | X) = \\Phi(x_i\\widehat{\\beta}) \\]\nand the logistic (logit) CDFs.\n\\[Pr(y_i=1 | X) = \\frac{1}{1+exp^{-(x_i\\widehat{\\beta})}} \\]\nHere are the logistic and Normal CDFs:\n\n\ncode\nz &lt;- seq(-5,5,.1)\nl &lt;- seq(0,1,.01)\ns1 &lt;- 1/(1+exp(-z))\ns2 &lt;- pnorm(z)\n\ndf &lt;- data.frame(z=z, l=l, s1=s1, s2=s2)\n\nggplot(df, aes(x=z, y=l)) + \n  geom_line(aes(x=z, y=l), color=\"black\", linetype=\"dashed\" )+\n  geom_line(aes(x=z, y=s1), color=\"#005A43\")+\n  geom_line(aes(x=z, y=s2), color=\"#6CC24A\")+\n  labs(title=\"Logistic and Normal CDFs\", x=expression(x*beta), y=\"Pr(y=1)\")+\n  theme_minimal() +\n  annotate(\"text\", x=1.3, y=.7, label=\"logistic\", color=\"black\")+\n  annotate(\"text\", x=-.2, y=.15, label=\"normal\", color=\"black\")\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nNote the sigmoid functions approach the limits at decreasing rates; the fastest rate of change is at \\(y=.5\\), a point around which the curves are symmetric. The point \\(y=.5\\) is the transition point below which we’d predict a zero, above which we’d predict a one if we were interested in classifying cases into zeros and ones. Classification is a common use for models like these, say distinguishing spam from non-spam emails, or predicting the presence or absence of a disease. More on this later."
  },
  {
    "objectID": "binarymodels24.html#probit-and-logit-llfs",
    "href": "binarymodels24.html#probit-and-logit-llfs",
    "title": "Binary Response Models",
    "section": "Probit and Logit LLFs",
    "text": "Probit and Logit LLFs\nProbit - link between \\(x\\hat{\\beta}\\) and \\(Pr(y=1)\\) is standard normal CDF: \\[\n\\ln \\mathcal{L} (Y|\\beta) = \\sum_{i=1}^{N} y_i \\ln \\Phi(\\mathbf{x_i \\beta})+ (1-y_i) \\ln[1-\\Phi(\\mathbf{x_i \\beta})] \\nonumber\n\\]\nLogit (logistic CDF):\n\\[\n\\ln \\mathcal{L} (Y|\\beta) = \\sum_{i=1}^{N} \\left\\{ y_i \\ln \\left(\\frac{1}{1+e^{-\\mathbf{x_i \\beta}}}\\right)+ (1-y_i) \\ln \\left[1-\\left(\\frac{1}{1+e^{-\\mathbf{x_i \\beta}}}\\right)\\right] \\right\\}\\nonumber\n\\]"
  },
  {
    "objectID": "binarymodels24.html#predicted-probabilities",
    "href": "binarymodels24.html#predicted-probabilities",
    "title": "Binary Response Models",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\nIn the nonlinear model, the most basic quantity is\n\\[F(x\\widehat{\\beta})\\]\nwhere \\(F\\) is the link function, mapping the linear prediction onto the probability space.\nFor the logit, the predicted probability is\n\\[Pr(y=1) = \\frac{1}{1+exp(-x\\widehat{\\beta})}\\]\nFor the probit, the predicted probability is\n\\[Pr(y=1) = \\Phi(x\\widehat{\\beta})\\]\nAgain, simply using the link function to map the linear prediction onto the probability space."
  },
  {
    "objectID": "binarymodels24.html#marginal-effects",
    "href": "binarymodels24.html#marginal-effects",
    "title": "Binary Response Models",
    "section": "Marginal Effects",
    "text": "Marginal Effects\nIn the linear model, the marginal effect of \\(x\\) is \\(\\widehat{\\beta}\\). That is, the effect of a one unit change in \\(x\\) on \\(y\\) is \\(\\widehat{\\beta}\\).\n\\[\n\\frac{\\partial \\widehat{y}}{\\partial x_k}= \\frac{\\partial x \\widehat{\\beta}}{\\partial x_k} \\nonumber \\\\  \\nonumber \\\\\n= \\widehat{\\beta} \\nonumber\n\\]\nThe marginal effect is constant with respect to \\(x_k\\). Take a look:\n\n\ncode\nx &lt;- seq(0,10,.1)\ny &lt;- 2*x\ndf &lt;- data.frame(x=x, y=y)\n\nggplot(df, aes(x=x, y=y)) + \n  geom_line()+\n  labs(title=\"Marginal Effect of x on y\", x=\"x\", y=\"y\")+\n  theme_minimal() +\n  annotate(\"text\", x=5, y=15, label=\"y = 2x\", color=\"black\")+\n  geom_segment(aes(x = 5, xend = 5, y = 0, yend = 10), color = \"red\")+\n  geom_segment(aes(x = 10, xend = 10, y = 0, yend = 20), color = \"red\")\n\n\n\n\n\n\n\n\n\nThe effect of \\(x\\) on \\(y\\) is 2 - it’s the same at \\(x=5\\) and at \\(x=10\\).\nIn the nonlinear model, the marginal effect of \\(x_k\\) depends on where \\(x\\widehat{\\beta}\\) lies with respect to the probability distribution \\(F(\\cdot)\\).\n\\[\n\\frac{\\partial Pr(y=1)}{\\partial x_k}= \\frac{\\partial F(x\\widehat{\\beta})}{\\partial x_k} \\nonumber \\\\  \\nonumber \\\\\n=  \\frac{\\partial F(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} \\cdot \\frac{\\partial (x\\widehat{\\beta})}{\\partial x_k}  \\nonumber\n\\]\nBoth of these terms simplify …\nRemember that\n\\[\n\\frac{\\partial (x\\widehat{\\beta})}{\\partial x} = \\widehat{\\beta} \\nonumber\n\\]\nand \\[\n\\frac{\\partial F(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} = f(x\\widehat{\\beta}) \\nonumber\n\\]\nwhere the derivative of the CDF is the PDF.\nPutting these together gives us:\n\\[\n\\frac{\\partial F(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} = f(x\\widehat{\\beta}) \\widehat{\\beta} \\nonumber\n\\]\nThis is \\(\\widehat{\\beta}\\) weighted by or measured at the ordinate on the PDF - the ordinate is the height of the PDF associated with a value of the \\(x\\) axis.\n\n\n\n\n\n\nImportant Concept\n\n\n\nThe effect of \\(x\\) on \\(Pr(y=1)\\) is not constant; it will be large for some values of \\(x\\) and small for others. This makes sense if we think about the sigmoid functions - the slope of the curve is steepest at \\(y=.5\\), and flattens as we move away from that point toward either limit. Take another look at Figure 1\n\n\n\nLogit Marginal Effects\n\n\nRecall \\(\\Lambda\\) is the logistic CDF = \\[1/(1+exp(-x_i\\widehat{\\beta}))\\].\n\\(\\lambda\\) is the logit PDF \\[1/(1+exp(-x_i\\widehat{\\beta}))^2\\]\nAlso, remember that\n\\[\\frac{e^{x_i\\widehat{\\beta}}}{1+e^{x_i\\widehat{\\beta}}} = \\frac{1}{1+e^{-x_i\\widehat{\\beta}}}\\]\n\\[\n\\begin{align}\n\\frac{\\partial \\Lambda(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} = \\lambda(x\\widehat{\\beta}) \\widehat{\\beta} \\\\\n= \\frac{e^{x_i\\widehat{\\beta}}}{(1+e^{x_i\\widehat{\\beta}})^2} \\widehat{\\beta}  \\\\\n=\\frac{e^{x_i\\widehat{\\beta}}}{1+e^{x_i\\widehat{\\beta}}} \\frac{1}{1+e^{x_i\\widehat{\\beta}}} \\widehat{\\beta}  \\\\\n=\\Lambda(x_i\\widehat{\\beta}) \\frac{1+e^{x_i\\widehat{\\beta}}-e^{x_i\\widehat{\\beta}}}{1+e^{x_i\\widehat{\\beta}}} \\widehat{\\beta}   \\\\\n=\\Lambda(x_i\\widehat{\\beta}) 1-\\frac{e^{x_i\\widehat{\\beta}}}{1+e^{x_i\\widehat{\\beta}}} \\widehat{\\beta}  \\\\\n=\\Lambda(x_i\\widehat{\\beta}) (1-\\Lambda(x_i\\widehat{\\beta})) \\widehat{\\beta}  \n\\end{align}\n\\]\nSo this last line indicates the marginal effect of \\(x\\) is the probability of a one times the probability of a zero times \\(\\widehat{\\beta}\\).\nThis is useful because the largest value this can take on is .25 \\((Pr(y_i=1)=0.5 \\cdot Pr(y_i=0)=0.5= 0.25)\\) - therefore, the maximum marginal effect any \\(x\\) can have is \\(0.25 \\widehat{\\beta}\\).\nLooking at the democratic peace model below, the coefficient on democracy is -.071, so the largest effect democracy can have on the probability of a militarized dispute is \\(0.25 \\cdot -.071 = -.01775\\).\n\ncode\nlibrary(stargazer)\n\nstargazer(m1,m2, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\nlogistic\n\n\nprobit\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n0.587*** (0.037)\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n-0.031*** (0.003)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n-0.001*** (0.0001)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n-0.350*** (0.038)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n-1.903*** (0.032)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n7,032.985\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\nIn the probit model, the marginal effect is:\n\\[\n\\frac{\\partial \\Phi(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} = \\phi(x\\widehat{\\beta}) \\widehat{\\beta} \\nonumber\n\\]\nThe ordinate at the maximum of the standard normal PDF is 0.3989 - rounding to 0.4, we can say that the maximum marginal effect of any \\(\\widehat{\\beta}\\) in the probit model is \\(0.4\\widehat{\\beta}\\).\nThe ordinate is at the maximum where \\(z=0\\); recall this is the standard normal, so \\(x_i\\widehat{\\beta}=z\\). When \\(z=0\\),\n\\[Pr(z)=\\frac{1}{\\sqrt{2 \\pi}} \\exp \\left[\\frac{-(z)^{2}}{2}\\right] \\nonumber \\\\ \\nonumber\\\\\n=\\frac{1}{\\sqrt{2 \\pi}} \\nonumber\\\\\n\\approx .4 \\nonumber \\]\nSo the maximum marginal effect of any \\(x\\) in the probit model is \\(0.4\\widehat{\\beta}\\)."
  },
  {
    "objectID": "binarymodels24.html#logit-odds-interpretation",
    "href": "binarymodels24.html#logit-odds-interpretation",
    "title": "Binary Response Models",
    "section": "Logit Odds Interpretation",
    "text": "Logit Odds Interpretation\nThe odds are given by the probability an event occurs divided by the probability it does not:\n\\[\n\\Omega(X) = \\frac{Pr(y=1)}{1-Pr(y=1)} \\nonumber\n= \\frac{\\Lambda(X\\widehat{\\beta})}{(1-\\Lambda(X\\widehat{\\beta}))} \\nonumber\n\\]"
  },
  {
    "objectID": "binarymodels24.html#logit-log-odds",
    "href": "binarymodels24.html#logit-log-odds",
    "title": "Binary Response Models",
    "section": "Logit Log-odds",
    "text": "Logit Log-odds\nLogging …\n\\[\\ln \\Omega(X) = \\ln \\left(\\frac{\\Lambda(X\\widehat{\\beta})}{(1-\\Lambda(X\\widehat{\\beta}))}\\right) =X\\widehat{\\beta} \\]\n\\[\n\\frac{\\partial \\ln \\Omega}{\\partial X} = \\widehat{\\beta} \\nonumber\n\\]\nWhich shows the change in the log-odds given a change in \\(X\\) is constant (and therefore linear). This quantity is sometimes called “the logit.”"
  },
  {
    "objectID": "binarymodels24.html#logit-odds-ratios",
    "href": "binarymodels24.html#logit-odds-ratios",
    "title": "Binary Response Models",
    "section": "Logit Odds Ratios",
    "text": "Logit Odds Ratios\nOdds ratios are very useful:\n\\[\n\\frac{ \\Omega x_k + 1}{\\Omega x_k} =exp(\\widehat{\\beta_k}) \\nonumber\n\\]\ncomparing the difference in odds between two values of \\(x_k\\); note the change in value does not have to be 1.\n\\[\n\\frac{ \\Omega x_k + \\iota}{\\Omega x_k} =exp(\\widehat{\\beta_k}* \\iota) \\nonumber\n\\]\nNot only is it simple to exponentiate \\(\\widehat{\\beta_k}\\), but the interpretation is that \\(x\\) increases/decreases \\(Pr(y=1)\\) by that factor, \\(exp(\\widehat{\\beta_k})\\), and more usefully, that:\n\\[\n100*(exp(\\widehat{\\beta_k})-1) \\nonumber\n\\]\nis the percentage change in the odds given a one unit change in \\(x_k\\).\nSo a logit coefficient of .226\n\\[\n100*(exp(.226)-1) =25.36 \\nonumber\n\\]\nProduces a 25.36% increase in the odds of \\(y\\) occurring."
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "mlesyllabus24.html#seminar-description",
    "href": "mlesyllabus24.html#seminar-description",
    "title": "MLE Syllabus",
    "section": "Seminar Description",
    "text": "Seminar Description\nThis 4 credit hour seminar is a survey of maximum likelihood methods and their applications to empirical political questions. It presumes students have a detailed and intuitive knowledge of least squares, probability theory, basic skills in scalar and matrix algebra, and a basic understanding of calculus. The course will deal mainly in understanding the principles of maximum likelihood estimation, under what conditions we move away from least squares, and what particular models are appropriate given observed data. The seminar will focus on application and interpretation of ML models and linking theory to statistical models. The course emphasizes coding and data viz in R and Stata.\nThe class meets one time per week for three hours. My office hours are designed to be homework help hours where I’ll work in the grad lab with any of you who are working on the exercises. The most productive pathway for this class is for you to get in the habit of working together, and those office hours are a good time for this."
  },
  {
    "objectID": "mlesyllabus24.html#course-purpose",
    "href": "mlesyllabus24.html#course-purpose",
    "title": "MLE Syllabus",
    "section": "Course Purpose",
    "text": "Course Purpose\nThis seminar fulfills the advanced quantitative methods requirement in the Ph.D. curriculum. The method of maximum likelihood underlies a majority of quantitative models in Political Science; this class teaches students to be astute consumers of such models, and how to implement and interpret ML models. These are crucial skills for dissertations in Political Science, and for producing publishable quantitative research."
  },
  {
    "objectID": "mlesyllabus24.html#learning-objectives",
    "href": "mlesyllabus24.html#learning-objectives",
    "title": "MLE Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nStudents will encounter an array of maximum likelihood models in this course. By the end of the course, students will have mastered the theory of maximum likelihood sufficient to write and program likelihood functions in ; they will be able to choose, estimate, and interpret appropriate models, model specifications, and model evaluation tools given their data; and they will be able to produce sophisticated quantities of interest (e.g. predicted probabilities, expected values, confidence intervals) via a variety of techniques including simulation and end point transformation. Students will also be able to present model findings verbally and graphically."
  },
  {
    "objectID": "mlesyllabus24.html#class-meetings-office-hours-assignments",
    "href": "mlesyllabus24.html#class-meetings-office-hours-assignments",
    "title": "MLE Syllabus",
    "section": "Class Meetings, Office Hours, Assignments",
    "text": "Class Meetings, Office Hours, Assignments\nThe course will meet this fall entirely in-person in the Social Science Experiment Lab on Wednesdays 9:40am-12:40pm.\nOffice hours are Mondays 1:30pm-3:30pm. I’ll likely hold these in the grad work room to help with your assignments. For an appointment, email me and we’ll sort out a time.\nAll assignments should be turned in on Brightspace - please submit ::\n\nPDFs generated from LaTeX or R Markdown (Quarto).\nannotated R scripts.\nwhere necessary, data.\n\nAssignments should be instantly replicable - running the code file should produce all models, tables, plots, etc."
  },
  {
    "objectID": "mlesyllabus24.html#reading",
    "href": "mlesyllabus24.html#reading",
    "title": "MLE Syllabus",
    "section": "Reading",
    "text": "Reading\nThe reading material for the course is important because it often demonstrates application of various MLE models; seeing how folks apply these and how they motivate their applications is really informative, something you cannot miss. We often won’t directly discuss the readings, but don’t let that imply they’re not important. If I get the sense we’re not keeping up with reading, expect the syllabus to change to incorporate quizzes or other accountability measures.\nReading for the course will consist of several books and articles (listed by week below). The books listed below also have Amazon links - you’ll find most of these cheaper used online.\n\nRequired\n\nBox-Steffensmeier, Janet and Jones, Brad. 2004. Event History Modeling. Cambridge. ISBN 0521546737\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Sage Publications Inc. ISBN 0803973748\nWard, Michael D. and John S. Ahlquist. 2018 Maximum Likelihood for Social Science. Cambridge. ISBN 978-1316636824.\n\n\n\nRecommended\nUseful, but not required (though some required reading in the first one):\n\nGary King. 1998. Unifying Political Methodology. University of Michigan Press. ISBN 0472085549\nJ. Scott Long. 2014. Regression Models for Categorical Dependent Variables Using Stata. 3rd Ed. Stata Press. ISBN 1597181110 (this book is good for practical/applied examples even if R is your primary language)\n\nGary King’s book is regarded as seminal in developing ML applications in political science. Scott Long’s is a similarlyaccessible treatment of a host of ML models and applications (and the Stata book is a great applied companion). Together, these two books are probably the most important on the syllabus as they are both accessible, but comprehensive and technical enough to be useful. Ward and Ahlquist’s book is a new overview of applied ML in a political science setting. Box-Steffensmeier and Jones is a thorough and accessible treatment of hazard models in a variety of empirical settings.\n\n\nAdditional Resources\nOther useful books include:\n\nCameron, A. Colin and Trivedi, Pravin K. 1998. Regression Analysis of Count Data. Cambridge. ISBN 0521635675\nMaddala, Gregory. 1983. Limited Dependent and Qualitative Variables in Econometrics. Cambridge. ISBN 0521338255\nPaul D Allison - Event History Analysis : Regression for Longitudinal Event Data. Sage Publications Inc. ISBN 0803920555\nTim Futing Liao - Interpreting Probability Models : Logit, Probit, and Other Generalized Linear Models. Sage Publications Inc. ISBN 0803949995\nJohn H Aldrich and Forrest D Nelson - Linear Probability, Logit, and Probit Models. Sage Publications Inc. ISBN 0803921330\nFred C Pampel - Logistic Regression : A Primer. Sage Publications Inc. ISBN 0761920102\nVani Kant Borooah - Logit and Probit : Ordered and Multinomial Models. Sage Publications Inc. ISBN 0761922423\nScott R Eliason - Maximum Likelihood Estimation : Logic and Practice. Sage Publications Inc. ISBN 0803941072\nRichard Breen - Regression Models : Censored, Sample Selected, or Truncated Data. Sage Publications Inc. ISBN 0803957106\nKrishnan Namboodiri - Matrix Algebra : An Introduction. ISBN 0803920520"
  },
  {
    "objectID": "mlesyllabus24.html#course-requirements-and-grades",
    "href": "mlesyllabus24.html#course-requirements-and-grades",
    "title": "MLE Syllabus",
    "section": "Course Requirements and Grades",
    "text": "Course Requirements and Grades\nThe seminar requires the following:\n\nProblem sets - 60% total\nMechanism papers - 40%\n\nPlease note that all written assignments must be submitted as PDFs either compiled in LaTeX or in R markdown (Quarto).\nYou’ll complete a series of problem sets, mostly applied. How many will depend on how things move along during the term. Regarding the problem sets - the work you turn in for the problem sets should clearly be your own, but I urge you to work together - doing so is a great way to learn and to overcome problems.\nThe mechanism papers are a series of three short papers you’ll write during the semester aimed at learning to identify and describe causal mechanisms, then at producing a causal mechanism. More on these early in the term.\nA word about completeness - attempt everything. To receive a passing grade in the course, you must finish all elements of the course, so all problem sets, all exams, papers, etc. To complete an element, you must at least attempt all parts of the element - so if a problem set has 10 problems, you must attempt all 10 or the assignment is incomplete, you’ve not completed every element of the course, and you cannot pass. I realize there may be problems you have trouble with and even get wrong, but you must try - the bottom line is don’t turn in incomplete work.\nGrades will be assigned on the following scale:\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\nRange\nGrade\nRange\n\n\n\n\nA\n94-100%\nC+\n77-79%\n\n\nA-\n90–93%\nC\n73-76%\n\n\nB+\n87–89%\nC-\n70-72%\n\n\nB\n83-86%\nD\n60-69%\n\n\nB-\n80-82%\nF\n&lt;60%"
  },
  {
    "objectID": "mlesyllabus24.html#course-policies",
    "href": "mlesyllabus24.html#course-policies",
    "title": "MLE Syllabus",
    "section": "Course Policies",
    "text": "Course Policies\n\nAttendance\nAttendance is expected, and is essential if you’re to succeed in this class.\n\n\nAcademic Integrity\nIdeas are the currency in academic exchange, so acknowledging where ideas come from is important. Acknowledging the sources of ideas also helps us identify an idea’s lineage which can be important for understanding how that line of thought has developed, and toward promoting future growth. As graduate students, you should have a good understanding of academic honesty and best practices. Here are details of Binghamton’s honesty policy."
  },
  {
    "objectID": "mlesyllabus24.html#course-schedule",
    "href": "mlesyllabus24.html#course-schedule",
    "title": "MLE Syllabus",
    "section": "Course Schedule",
    "text": "Course Schedule\nWeek 1, Aug 21 – Binary \\(y\\) Variables I - probit/logit, QI\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 1, 2, 4\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 3.    \n\nWeek 2, Aug 28 – Likelihood Theory and ML Estimation\n\nGary King. 1998. Unifying Political Methodology. Chapter 1-4\nJ. Scott Long. 1997. *Regression Models for Categorical and Limited Dependent Variables}. Chapters 1-2.\n\nWeek 3, Sept 4 – Binary \\(y\\) Variables II - symmetry, fit, diagnostics, prediction\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 3, 5, 6, 7\n\n\n\nNagler (1994)\nKing and Zeng (2001)\nFranklin and Kosaki (1989)\nC. Zorn (2005)\n\nWeek 4, Sept 11 – Binary \\(y\\) Variables III (discrete hazards)\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 11 \nBeck, Katz, and Tucker (1998)\nCarter and Signorino (2010)\n\nWeek 5, Sept 18 – Binary \\(y\\) Variables IV - variance, order\n\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 5.\nFranklin (1991)\nAlvarez and Brehm (1995)\nClark and Nordstrom (2005)\n\nWeek 6, Sept 25 – Assumptions and Specification - interactions, functional form, measurement of \\(y\\)\n\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 5.\nClark, Nordstrom, and Reed (2008)\nClarke and Stone (2008)\nBerry, Golder, and Milton (2012)\nBrambor, Clark, and Golder (2006)\n\nWeek 7, Oct 2 – No class, Yom Kippur\nWeek 8, Oct 9 – Choice Models I (Unordered \\(y\\) Variables) - MNL, MNP, CL (IIA)\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 9\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 6.\nAlvarez and Nagler (1998)\nLacy and Burden (1999)\nC. J. W. Zorn (1996)\n\nWeek 9, Oct 16– Choice Models II (Unordered Dependent Variables continued, and systems of eqs, ordered)\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 8\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 5.\nFranklin and Kosaki (1989)\n\nWeek 10, Oct 23 – Event Count Models I - poisson, dispersion\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 10\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 8.1, 8.2.\nGowa (1998)\nFordham (1998)\n\nWeek 11, Oct 30 – Event Count Models II - negative binomial, zero-altered\n\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 8.3-8.7.\nC. J. W. Zorn (1998)\nClark (2003)\n\nWeek 12, Nov 6 – Continuous Time Hazard Models I - parametric, semi-parametric models\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 11\nJanet Box-Steffensmeier and Brad Jones. 2004. Event History Modeling. Chs. 1-4\nJ. M. Box-Steffensmeier, Arnold, and Zorn (1997)\n\nWeek 13, Nov 13 – Continuous Time Hazard Models II - parametric models, special topics\n\nJanet Box-Steffensmeier and Brad Jones. 2004. Event History Modeling. Chs. 5-11\nC. J. W. Zorn (2000)\nBennett and Stam (1996)\nJ. Box-Steffensmeier, Reiter, and Zorn (2003)\n\nWeek 14, Nov 20 – Censored/Truncated Variables, Samples - selection models - J. Scott Long. 1997. *Regression Models for Categorical and Limited Dependent Variables}. Chapter 7\n\nReed (2000)\nSignorino (1999)\nTimpone (1998)\n\nWeek 15, Nov 27 – no class, Thanksgiving\nWeek 16, Dec 4 – Review"
  },
  {
    "objectID": "prediction24.html",
    "href": "prediction24.html",
    "title": "Prediction Methods for MLE Models",
    "section": "",
    "text": "Most MLE models are nonlinear, so their coefficients are not their marginal effects. As a result, most MLE models require a transformation of the linear prediction to generate quantities of interest. The methods outlined here apply to most MLE applications; the immediate interest and examples here use binary response models. These slides will form a foundation for prediction in other types of models we encounter.\n\n\nProbit and logit coefficients are directly interpretable in the senses that\n\nWe can interpret direction.\nWe can interpret statistical difference from zero.\nWe can say the largest marginal effect of \\(x \\approx 0.4\\cdot\\widehat{\\beta}\\) for the probit model.\nWe can say the largest marginal effect of \\(x \\approx 0.25\\cdot\\widehat{\\beta}\\) for the logit model.\nWe can say that \\(exp(\\widehat{\\beta_k})-1\\) is the percentage change in the odds that \\(y=1\\), for the logit model.\n\nIt’s still the case that we often want other quantities of interest like probabilities, and that requires the straightforward transformations of the linear prediction, \\(F(x_i\\widehat{\\beta})\\).\nLet’s look briefly at the intuition of the “maximum marginal effect” in the logit model.\n\n\ncode\nz &lt;- seq(-5,5,.1)\nncdf &lt;- pnorm(z)\nnpdf &lt;- dnorm(z)\nlcdf &lt;- plogis(z)\nlpdf &lt;- dlogis(z)\n\ndf &lt;- data.frame(ncdf=ncdf, npdf=npdf, lcdf=lcdf, lpdf=lpdf, z=z)\n\nggplot(df, aes(x=z, y=ncdf), color=\"black\") +\n  geom_line() +\n  geom_line(aes(x=z, y=lcdf), color=\"green\") +\n  geom_line(aes(x=z, y=npdf), color=\"black\") +\n  geom_line(aes(x=z, y=lpdf), color=\"green\") +\n  geom_hline(yintercept = .3989, linetype=\"dashed\") +\n  geom_hline(yintercept = .25, linetype=\"dashed\") +\n  labs(x=\"z\", y=\"Pr(y=1)\") +\n  theme_minimal() +\n  annotate(\"text\", x = 2.5, y = .36, label = \"Normal\", color = \"black\") +\n  annotate(\"text\", x = 2.5, y = .22, label = \"Logistic\", color = \"black\") \n\n\n\n\n\n\n\n\n\nThe highest points on the PDFs indicate the maximum marginal effect of \\(x\\) on \\(Pr(y=1)\\) in the logit and probit models."
  },
  {
    "objectID": "prediction24.html#binary-response-models",
    "href": "prediction24.html#binary-response-models",
    "title": "Prediction Methods for MLE Models",
    "section": "",
    "text": "Probit and logit coefficients are directly interpretable in the senses that\n\nWe can interpret direction.\nWe can interpret statistical difference from zero.\nWe can say the largest marginal effect of \\(x \\approx 0.4\\cdot\\widehat{\\beta}\\) for the probit model.\nWe can say the largest marginal effect of \\(x \\approx 0.25\\cdot\\widehat{\\beta}\\) for the logit model.\nWe can say that \\(exp(\\widehat{\\beta_k})-1\\) is the percentage change in the odds that \\(y=1\\), for the logit model.\n\nIt’s still the case that we often want other quantities of interest like probabilities, and that requires the straightforward transformations of the linear prediction, \\(F(x_i\\widehat{\\beta})\\).\nLet’s look briefly at the intuition of the “maximum marginal effect” in the logit model.\n\n\ncode\nz &lt;- seq(-5,5,.1)\nncdf &lt;- pnorm(z)\nnpdf &lt;- dnorm(z)\nlcdf &lt;- plogis(z)\nlpdf &lt;- dlogis(z)\n\ndf &lt;- data.frame(ncdf=ncdf, npdf=npdf, lcdf=lcdf, lpdf=lpdf, z=z)\n\nggplot(df, aes(x=z, y=ncdf), color=\"black\") +\n  geom_line() +\n  geom_line(aes(x=z, y=lcdf), color=\"green\") +\n  geom_line(aes(x=z, y=npdf), color=\"black\") +\n  geom_line(aes(x=z, y=lpdf), color=\"green\") +\n  geom_hline(yintercept = .3989, linetype=\"dashed\") +\n  geom_hline(yintercept = .25, linetype=\"dashed\") +\n  labs(x=\"z\", y=\"Pr(y=1)\") +\n  theme_minimal() +\n  annotate(\"text\", x = 2.5, y = .36, label = \"Normal\", color = \"black\") +\n  annotate(\"text\", x = 2.5, y = .22, label = \"Logistic\", color = \"black\") \n\n\n\n\n\n\n\n\n\nThe highest points on the PDFs indicate the maximum marginal effect of \\(x\\) on \\(Pr(y=1)\\) in the logit and probit models."
  },
  {
    "objectID": "prediction24.html#at-means-predictions",
    "href": "prediction24.html#at-means-predictions",
    "title": "Prediction Methods for MLE Models",
    "section": "At-Means Predictions",
    "text": "At-Means Predictions\nAt-means predictions are what they sound like - effects with independent variables set at central tendencies. These are sometimes called “adjusted predictions.”\n\nestimate model.\ncreate out of sample data.\nvary \\(x\\) of interest; set all other \\(x\\) variables to appropriate central tendencies - hence the “at Means.”\ngenerate QIs in out of sample data."
  },
  {
    "objectID": "prediction24.html#average-effects",
    "href": "prediction24.html#average-effects",
    "title": "Prediction Methods for MLE Models",
    "section": "Average Effects",
    "text": "Average Effects\nAverage Marginal Effects are in-sample but create a counterfactual for a variable of interest, assuming the entire sample looks like that case.\nFor instance, suppose a model of wages with covariates for education and gender. We might ask the question what would the predictions look like if the entire sample were male, but otherwise looked as it does? Alternatively, what would the predictions look like if the entire sample were female, but all other variables the same as they appear in the estimation data?\nTo answer these, we’d change the gender variable to male, generate \\(x{\\widehat{\\beta}}\\) for the entire sample, and take the average, then repeat with the gender variable set to female.\nTo generate Average Effects,\n\nestimate model.\nin estimation data, set variable of interest to a particular value for the entire estimation sample.\ngenerate QIs (expected values, standard errors).\ntake average of QIs, and save.\nrepeat for all values of variable of interest, and plot."
  },
  {
    "objectID": "prediction24.html#at-means-predictions-logit",
    "href": "prediction24.html#at-means-predictions-logit",
    "title": "Prediction Methods for MLE Models",
    "section": "At-means predictions (logit)",
    "text": "At-means predictions (logit)\nHere’s an example of at-means predictions for a logit model of the democratic peace. FIrst, let’s look at the model estimates:\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n\nstargazer(m1, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\nAs with any nonlinear model, we need to compute a linear prediction, \\(x\\widehat{\\beta}\\), and then transform that to a probability. For at-means predictions, we’ll vary democracy across its range, holding the remaining variables at appropriate central tendency (e.g, mode for dummy variables, median for categorical or skewed variables, etc.) Take a look at the code:\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\nlogitpreds &lt;- predict(m1, type=\"response\")\n\nm2 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"probit\"), data=dp )\n\n#new data frame for MEM prediction\nmem &lt;- data.frame(deml= c(seq(-10,10,1)), \n                  border=0, caprat=median(dp$caprat), ally=0)\n\n# type=\"link\" produces the linear predictions; transform by hand below w/EPT\nmem  &lt;-data.frame(mem, predict(m1, type=\"link\", newdata=mem, se=TRUE))\n\nmem &lt;- cbind(mem,lb=plogis(mem$fit-1.96*mem$se.fit),\n             ub=plogis(mem$fit+1.96*mem$se.fit), \n             p=plogis(mem$fit))\n\nggplot(mem, aes(x=deml, y=p)) +\n  geom_line() +\n  geom_ribbon(data=mem, aes(x=deml, ymin=lb, ymax=ub),fill = \"grey30\", alpha = .4, ) +\n  labs(x=\"Polity Score\", y=\"Pr(Dispute) (95% confidence interval)\")"
  },
  {
    "objectID": "prediction24.html#average-effects-logit",
    "href": "prediction24.html#average-effects-logit",
    "title": "Prediction Methods for MLE Models",
    "section": "Average effects (logit)",
    "text": "Average effects (logit)\nAverage effects are often a better choice because they represent the data more completely than central tendency can (in the at-mean effects). Here are average effects (using the logit estimates) across the range of polity, and for pairs of states that share borders and those that do not.\n\n\ncode\n#avg effects\n\n#identify the estimation sample\ndp$used &lt;- TRUE\ndp$used[na.action(m1)] &lt;- FALSE\ndpesample &lt;- dp %&gt;%  filter(used==\"TRUE\")\n\npolity &lt;- 0\nmedxbd0 &lt;- 0\nubxbd0 &lt;- 0\nlbxbd0 &lt;- 0\n# medse &lt;- 0\n# medxbd1 &lt;- 0\n# ubxbd1 &lt;- 0\n# lbxbd1 &lt;- 0\n\nfor(i in seq(1,21,1)){\n  dpesample$border&lt;- 0\n  dpesample$deml &lt;- i-11\n  polity[i] &lt;- i-11\n  allpreds &lt;- data.frame(predict(m1, type= \"response\", se.fit=TRUE, newdata = dpesample))  \n  medxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)\n  ubxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)+1.96*(median(allpreds$se.fit, na.rm=TRUE))\n  lbxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)-1.96*(median(allpreds$se.fit, na.rm=TRUE))\n}\n  \nnoborder &lt;- data.frame(polity, medxbd0, ubxbd0, lbxbd0)\n  \nfor(i in seq(1,21,1)){\n  dpesample$border&lt;- 1\n  dpesample$deml &lt;- i-11\n  polity[i] &lt;- i-11\n  allpreds &lt;- data.frame(predict(m1, type= \"response\", se.fit=TRUE, newdata = dpesample))  \n  medxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)\n  ubxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)+1.96*(median(allpreds$se.fit, na.rm=TRUE))\n  lbxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)-1.96*(median(allpreds$se.fit, na.rm=TRUE))\n}\n  \nborder &lt;- data.frame(polity, medxbd0, ubxbd0, lbxbd0)\n  \n\n\nggplot() +\n  geom_ribbon(data=noborder, aes(x=polity, ymin=lbxbd0, ymax=ubxbd0),fill = \"grey30\", alpha = .4, ) +\n  geom_line(data=noborder, aes(x=polity, y=medxbd0))+\n  geom_ribbon(data=border, aes(x=polity, ymin=lbxbd0, ymax=ubxbd0),fill = \"grey30\", alpha = .4, ) +\n  geom_line(data=border, aes(x=polity, y=medxbd0))+\n  labs ( colour = NULL, x = \"Polity Score\", y =  \"Pr(Dispute)\" )+\n  theme_minimal()+\n  ggtitle(\"Average Effects\")"
  },
  {
    "objectID": "prediction24.html#simulation",
    "href": "prediction24.html#simulation",
    "title": "Prediction Methods for MLE Models",
    "section": "Simulation",
    "text": "Simulation\nSimulation is an especially good approach for nonlinear models:\n\nestimate the model.\n\\(m\\) times (say, 1000 times), simulate the distribution of \\(\\widehat{\\beta}\\).\ngenerate the \\(m\\) linear predictions, \\(x\\widehat{\\beta}\\).\ntransform by the appropriate link function (logistic, standard normal CDF).\nidentify the 2.5, 50, and 97.5 percentiles.\nplot against \\(x\\).\n\n\n\ncode\n#draws from multivariate normal using logit model estimates\nsimL &lt;- data.frame(MASS::mvrnorm(1000, coef(m1), vcov(m1)))\n#draws from multivariate normal using probit model estimates\nsimP &lt;- data.frame(MASS::mvrnorm(1000, coef(m2), vcov(m2)))\n\n#Logit predictions\nlogitprobs &lt;- data.frame(dem= numeric(0) , lb=numeric(0), med= numeric(0), ub=numeric(0))\nfor (i in seq(1,21,1)) {\nsimpreds &lt;- quantile(simL$X.Intercept.+ simL$border*0+simL$deml*(i-11)+simL$caprat*median(dp$caprat)+simL$ally*0, probs=c(.025, .5, .975))\nlogitprobs[i,] &lt;- data.frame(dem=i, lb=plogis(simpreds[1]), med=plogis(simpreds[2]), ub=plogis(simpreds[3]))\n}\n\n#Probit predictions\nprobitprobs &lt;- data.frame(dem= numeric(0) , lb=numeric(0), med= numeric(0), ub=numeric(0))\nfor (i in seq(1,21,1)) {\nsimpreds &lt;- quantile(simP$X.Intercept.+ simP$border*0+simP$deml*(i-11)+simP$caprat*median(dp$caprat)+simP$ally*0, probs=c(.025, .5, .975))\nprobitprobs[i,] &lt;- data.frame(dem=i, lb=pnorm(simpreds[1]), med=pnorm(simpreds[2]), ub=pnorm(simpreds[3]))\n}\n\nlogit &lt;- ggplot() +\n  geom_ribbon(data=logitprobs, aes(x=dem, ymin=lb, ymax=ub),fill = \"grey30\", alpha = .4, ) +\n  geom_line(data=logitprobs, aes(x=dem, y=med))+\n  labs ( colour = NULL, x = \"Polity Score\", y =  \"Pr(Dispute)\" )+\n  theme_minimal()+\n  ggtitle(\"Logit Predictions\")\n\nprobit &lt;- ggplot() +\n  geom_ribbon(data=probitprobs, aes(x=dem, ymin=lb, ymax=ub),fill = \"grey30\", alpha = .4, ) +\n  geom_line(data=probitprobs, aes(x=dem, y=med))+\n  labs ( colour = NULL, x = \"Polity Score\", y =  \"Pr(Dispute)\" )+\n  theme_minimal()+\n  ggtitle(\"Probit Predictions\")\n\nlogit+probit"
  },
  {
    "objectID": "prediction24.html#simulating-combinations-of-binary-variables",
    "href": "prediction24.html#simulating-combinations-of-binary-variables",
    "title": "Prediction Methods for MLE Models",
    "section": "Simulating combinations of binary variables",
    "text": "Simulating combinations of binary variables\nLet’s look at the differences among the four combinations of the binary variables, border and ally.\n\n\ncode\n## simulating for binary combinations ----\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n\n#draws from multivariate normal using logit model estimates\nsimL &lt;- data.frame(MASS::mvrnorm(1000, coef(m1), vcov(m1)))\n\n\nlogitprobs &lt;- data.frame(b0a0= numeric(0) , b1a0=numeric(0), b0a1= numeric(0), b1a1=numeric(0))\n\n  b0a0 &lt;- plogis(simL$X.Intercept.+ simL$border*0+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*0)\nb1a0 &lt;- plogis(simL$X.Intercept.+ simL$border*1+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*0)\nb0a1 &lt;- plogis(simL$X.Intercept.+ simL$border*0+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*1)\nb1a1 &lt;- plogis(simL$X.Intercept.+ simL$border*1+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*1)\n\nlogitprobs &lt;- data.frame(b0a0, b1a0, b0a1, b1a1)\n\nggplot() +\n  geom_density(data=logitprobs, aes(x=b0a0), fill=\"grey30\", alpha = .4, ) +\n  geom_density(data=logitprobs, aes(x=b1a0), fill=\"grey30\", alpha = .4, ) +\n  geom_density(data=logitprobs, aes(x=b0a1), fill=\"grey30\", alpha = .4, ) +\n  geom_density(data=logitprobs, aes(x=b1a1), fill=\"grey30\", alpha = .4, ) +\n  labs ( colour = NULL, x = \"Pr(Dispute)\", y =  \"Density\" ) +\n  theme_minimal()+\n  annotate(\"text\", x = .07, y = 150, label = \"No border, not allies\", color = \"black\") +\n  annotate(\"text\", x = .13, y = 70, label = \"Border, not allies\", color = \"black\") +\n  annotate(\"text\", x = .04, y = 200, label = \"No border, allies\", color = \"black\") +\n  annotate(\"text\", x = .09, y = 50, label = \"Border, allies\", color = \"black\") +\n  ggtitle(\"Logit Predictions\")"
  },
  {
    "objectID": "prediction24.html#ml-standard-errors-of-linear-predictions",
    "href": "prediction24.html#ml-standard-errors-of-linear-predictions",
    "title": "Prediction Methods for MLE Models",
    "section": "ML Standard Errors of Linear Predictions",
    "text": "ML Standard Errors of Linear Predictions\nOne commonly used measure of uncertainty is the standard error of the linear prediction, \\(X\\widehat{\\beta}\\).\nConsider the linear prediction\n\\[X \\widehat{\\beta} \\]\nunder maximum likelihood theory:\n\\[var(X \\widehat{\\beta}) = \\mathbf{X V X'} \\]\nan \\(N x N\\) matrix, where \\(V\\) is the var-cov matrix of \\({\\widehat{\\beta}}\\). The main diagonal contains the variances of the \\(N\\) predictions. The standard errors are:\n\\[se(X \\widehat{\\beta}) = \\sqrt{diag(\\mathbf{X V X'})} \\]\nwhich is an \\(N x 1\\) vector. So now we have a column vector of standard errors for the linear prediction, \\(X\\widehat{\\beta}\\). Like the linear predictions, these are not transformed into probabilities, so when we compute confidence intervals, we need to map the upper and lower bounds onto the probability space.\n\\[F(X \\widehat{\\beta} - c*s.e.) \\leq F(X \\widehat{\\beta}) \\leq F(X \\widehat{\\beta} + c* s.e.)\\]"
  },
  {
    "objectID": "prediction24.html#delta-method-standard-errors",
    "href": "prediction24.html#delta-method-standard-errors",
    "title": "Prediction Methods for MLE Models",
    "section": "Delta Method standard errors",
    "text": "Delta Method standard errors\nThe maximum likelihood method is appropriate for monotonic functions of \\(X \\widehat{\\beta}\\), e.g. logit, probit. In other models (e.g., multinomial logit), the function is not monotonic in \\(X \\widehat{\\beta}\\) so we use the Delta Method - this creates a linear approximation of the function. Greene (2012) (693ff) gives this as a general derivation of the variance:\n\\[Var[F(X \\widehat{\\beta})] = f(\\mathbf{x'\\widehat{\\beta}})^2 \\mathbf{x' V x} \\]\nwhere this would generate variances for whatever \\(F(X \\widehat{\\beta})\\) is, perhaps a predicted probability.\n\nDelta method standard errors for Logit\nFor the logit, the delta standard errors are given by:\n\\[F(X \\widehat{\\beta}) * (1-F(X \\widehat{\\beta}) * \\mathbf(X V X')\\]\n\\[ = f(X \\widehat{\\beta})  *  \\mathbf{\\sqrt{X V X'}}\\]\nor\n\\[ p * (1-p) * stdp\\]\nwhere \\(stdp\\) is the standard error of the linear prediction."
  },
  {
    "objectID": "prediction24.html#ses-of-predictions-for-linear-combinations",
    "href": "prediction24.html#ses-of-predictions-for-linear-combinations",
    "title": "Prediction Methods for MLE Models",
    "section": "SEs of Predictions for linear combinations",
    "text": "SEs of Predictions for linear combinations\nA common circumstance that requires joint hypothesis tests is the case of polynomials (which are themselves interactions):\n\\[y = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_1 + \\widehat{\\beta}_2 x_{1}^2  + \\varepsilon \\]\nThe question is whether \\(\\widehat{\\beta}_1 = \\widehat{\\beta}_2  = 0\\) - the marginal effect is:\n\\[ \\widehat{\\beta}_1 + 2 \\widehat{\\beta}_2x_1\\]\nand requires the standard error for \\(\\widehat{\\beta}_1+\\widehat{\\beta}_2\\), which is:\n\\[ \\sqrt{var(\\widehat{\\beta}_1) + 4x_{1}^{2}var(\\widehat{\\beta}_2) +  4x_1 cov(\\widehat{\\beta}_1, \\widehat{\\beta}_2)  }\\]"
  },
  {
    "objectID": "prediction24.html#cis---end-point-transformation",
    "href": "prediction24.html#cis---end-point-transformation",
    "title": "Prediction Methods for MLE Models",
    "section": "CIs - End Point Transformation",
    "text": "CIs - End Point Transformation\nGenerate upper and lower bounds using either ML or Delta standard errors, such that\n\\[F(X \\widehat{\\beta} - c*s.e.) \\leq F(X \\widehat{\\beta}) \\leq F(X \\widehat{\\beta} + c* s.e.)\\]\n\nestimate the model, generate the linear prediction, and the standard error of the linear prediction using the either ML or Delta.\ngenerate linear boundary predictions, \\(x{\\widehat{\\beta}} \\pm c * \\text{st. err.}\\) where \\(c\\) is a critical value on the normal, eg. \\(z=1.96\\).\ntransform the linear prediction and the upper and lower boundary predictions by \\(F(\\cdot)\\).\nWith ML standard errors, EPT boundaries will obey distributional boundaries (ie, won’t fall outside 0-1 interval for probabilities); the linear end point predictions are symmetric, though they will not be symmetric in nonlinear models.\nWith delta standard errors, bounds may not obey distributional boundaries."
  },
  {
    "objectID": "prediction24.html#simulating-confidence-intervals-i",
    "href": "prediction24.html#simulating-confidence-intervals-i",
    "title": "Prediction Methods for MLE Models",
    "section": "Simulating confidence intervals, I",
    "text": "Simulating confidence intervals, I\n\ndraw a sample with replacement of size \\(\\tilde{N}\\) from the estimation sample.\nestimate the model parameters in that bootstrap sample.\nusing the bootstrap estimates, generate quantities of interest (e.g. \\(x\\widehat{\\beta}\\)) repeat \\(j\\) times.\ncollect all these bootstrap QIs and use either percentiles or standard deviations to measure uncertainty."
  },
  {
    "objectID": "prediction24.html#uncertainty-simulating-confidence-intervals-ii",
    "href": "prediction24.html#uncertainty-simulating-confidence-intervals-ii",
    "title": "Prediction Methods for MLE Models",
    "section": "Uncertainty: Simulating confidence intervals, II",
    "text": "Uncertainty: Simulating confidence intervals, II\n\nestimate the model.\ngenerate a large sample distribution of parameters (e.g. using drawnorm).\ngenerate quantities of interest for the distribution of parameters.\nuse either percentiles or standard deviations of the QI to measure uncertainty."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Maximum Likelihood, Fall 2024",
    "section": "",
    "text": "This is the course website for PLSC 606J, Maximum Likelihood Estimation, Fall 2024. The course is an advanced course in data science focusing on maximum likelihood methods and their applications in political science.\n\nSyllabus\nSlides\nCode\n\n\n\n\n Back to top"
  },
  {
    "objectID": "mlesyllabus24.html#resources",
    "href": "mlesyllabus24.html#resources",
    "title": "MLE Syllabus",
    "section": " Resources",
    "text": "Resources\nThere are lots of good, free  resources online. Here are a few:\n\nModern Statistics with R\nR for Data Science\nR Markdown: The Definitive Guide\nR Graphics Cookbook\nAdvanced R\nR Markdown Cookbook\nData Science:A First Introduction\nThe Big Book of R"
  },
  {
    "objectID": "llfmax.html",
    "href": "llfmax.html",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "The goal here is to walk through the process of writing a likelihood function based on the data we observe, then programming that likelihood function and optimizing it with respect to the data.\n\n\nLet’s recall the motivation for turning to ML. The data we have are not suitable for OLS because our observation of the data is limited. For instance, we observe a binary variable \\([0,1]\\) rather than the underlying continuous probability beneath it. If we could observe and measure that probability, we’d do so and perhaps use OLS to estimate a model. But since we only observe \\([0,1]\\), our observation is limited.\nOLS is poorly suited to model this variable especially if one of our goals is to generate predictions of that underlying probability. That is, one of our goals might be to estimate a measure of the \\(y\\) variable we wish we could measure directly.\nMLE is better suited to this task - it asks us what the data generating process is that produced the observed data, and to build a model appropriate to the limited observation of that \\(y\\) variable.\nTo do so, we need to:\n\ndescribe the observed distribution of \\(y\\)\nconsider what we wish we could measure - this points to the key quantities of interest we want to derive from the model.\ndescribe \\(y\\) according to a probability distribution; write that distribution\nwrite a log-likelihood function appropriate to that probability distribution.\nparameterize the log-likelihood function such that we have a link distribution to map the linear prediction, \\(x\\beta\\), onto the probability space of \\(y\\).\n\n\n\n\nLet’s start with a \\(y\\) variable as follows:\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                y\n                Freq\n              \n        \n        \n        \n                \n                  0\n                  707\n                \n                \n                  1\n                  293\n                \n        \n      \n    \n\n\n\nThe variable, \\(y\\), takes on values of zero and one - it appears binomial; let’s write this in terms of the binomial distribution parameter \\(\\pi\\), so it takes on the value of one with probability \\(\\pi\\) and zero with probability \\(1-\\pi\\).\n\\[ y_i = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{} \\pi_{i}\\\\\n         0, & \\mbox{} 1-\\pi_{i}\n         \\end{array}\n     \\right.\\]\nThe likelihood of a single observation is:\n\\[ \\mathcal{L}(\\pi_i|y) = \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\]\nThis is a statement of the likelihood that any particular value of \\(\\pi\\) generated an observation. We want the likelihood any value of \\(\\pi\\) generated the entire dataset - that is, we want the joint likelihood of all the observations.\n\\[ \\mathcal{L}(\\pi|y) = \\prod_{i=1}^{n} \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\] recalling that a joint probability is the product of individual probabilities.\nLet’s take the log of this likelihood function - the natural log makes computation easier. Even for computers, this reduces the computational intensity especially with respect to extremely small decimals.\n\\[ \\ln \\mathcal{L}(\\pi|y) = \\sum_{i=1}^{n} y_i \\ln(\\pi_i) + (1-y_i) \\ln(1-\\pi_i) \\]\nWe can take this a step further and parameterize \\(\\pi\\) as a function of \\(X\\beta\\) where \\(X\\) is the matrix of predictors and \\(\\beta\\) is the vector of coefficients such that\n\\[ \\pi_i = F({X_i\\beta}) \\]\nSo our estimate of the binomial probability is a function of the linear predictor \\(X\\beta\\). We map \\(X\\beta\\) onto the probability space \\(\\pi\\) using a link function, \\(F\\). The most common link functions are the logistic (logit model), and the standard normal (probit model). Let’s write the logit link:\n\\[ \\pi_i = \\frac{1}{1 + e^{-(X_i\\beta)}} \\]\nand now let’s write this all in the log-likelihood function:\n\\[ \\ln \\mathcal{L}(\\beta|y) = \\sum_{i=1}^{n} y_i \\ln(\\frac{1}{1 + e^{-(X_i\\beta)}}) + (1-y_i) \\ln(1-\\frac{1}{1 + e^{-(X_i\\beta)}}) \\]\nThis is the logit log-likelihood function for a binary \\(y\\) variable.\n\n\n\nHow do we solve this function for the data \\(y\\)? Put differently, what is the value of \\(\\pi\\) that most likely generated the data, \\(y\\)? There are a few ways to do this - one instructive method is to use a grid search, where we calculate the log-likelihood for a range of values of \\(\\pi\\) and find the value that maximizes the log-likelihood. We can repeat this to the desired level of specificity, progressively narrowing the grid. In this example, we’ll just do one search out to 3 decimal places. The steps are these:\n\ngenerate the range of candidate values of our parameter \\(\\pi\\) to plug into the log-likelihood function.\nplug each value into the log-likelihood function to compute the log-likelihood for that value.\nidentify which value of \\(\\pi\\) maximizes the log-likelihood.\n\nHere’s code to do this:\n\n\ncode\n# generate a vector of values of pi \n\npi_trials &lt;- seq(0, 1, by = 0.001)\n\n# write the log-likelihood function\n\nlog_likelihood &lt;- function(pi_trials, y) {\n  sum(y * log(pi_trials) + (1 - y) * log(1 - pi_trials))\n}\n\n# Calculate log-likelihood for each pi value\n\nll_values &lt;- sapply(pi_trials, log_likelihood, y = y)\n\n# Find the pi value that maximizes the log-likelihood\n\npi_hat &lt;- pi_trials[which.max(ll_values)]\nprint(pi_hat)\n\n\n[1] 0.293\n\n\nYou’ll note we recover the sample mean of \\(y\\). Let’s plot the log-likelihood against the values of \\(\\pi\\) to visualize the maximum.\n\n\ncode\n# Plot log-likelihood against pi\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(data.frame(pi = pi_trials, ll = ll_values), \"line\", hcaes(x = pi, y = ll)) %&gt;%\n  hc_title(text = \"Grid search - maximizing the log-likelihood\") %&gt;%\n  hc_xAxis(title = list(text = \"Pi\"), \n           plotLines = list(\n    list(color =\"red\", value = pi_hat ))) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_legend(enabled = FALSE) %&gt;%\n  hc_colors(bucolors)\n\n\n\n\n\n\n\n\n\nGrid searches are not sufficient for multivariate models, especially where we also need measures of uncertainty. Here, we turn to numerical optimization.\nNumerical optimization is a field aimed at finding “best” outcomes or answers depending on some set of criteria - often, the “best” is the maximum or minimum of some function - that’s the case in maximum likelihood.\n\n\nThere are lots of ways to optimize a function - in statistical software, most of them derive from Newton’s iterative method. A very common application of this is the Newton-Raphson method. Newton-Raphson is an iterative process that starts with an initial guess for the parameter \\(\\pi\\) (or the vector of unknowns, \\(\\beta\\)) and updates it in the direction of the maximum until it converges to the maximum. At each step, it computes the first and second derivatives of the likelihood function, then uses these to update the parameter estimate(s). The update rule is:\n\\[ \\pi_{new} = \\pi_{old} - H(\\pi)^{-1}*g(\\pi) \\]\nSo the new estimate of the parameter is the old estimate minus the first derivative (gradient) of the log-likelihood function pre-multiplied by the Hessian (second derivative) of the log-likelihood function. Here, \\(g(\\pi)\\) is the gradient of the log-likelihood function and \\(H(\\pi)\\) is the Hessian of the log-likelihood function. The algorithm iterates this process until the change in \\(\\pi\\) is sufficiently small, below a specified threshold. Since \\(g\\) and \\(H\\) are matrices, you should see how this straightforwardly applied to a multivariate regression.\nLet’s write the Newton-Raphson algorithm ourselves - take a look at the code chunk below. You’ll see it produces the same result as the grid search; the sample frequency of \\(y\\).\n\n\n\n\n\ncode\n# use same data generated above; same LLF as above, aiming to recover pi\n\n#write the gradient - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(pi, y) {\n  sum(y / pi - (1 - y) / (1 - pi))\n}\n\n#write the Hessian - matrix of partial second derivatives of the log-likelihood function.\n\nhessian &lt;- function(pi, y) {\n  sum(-y / pi^2 - (1 - y) / (1 - pi)^2)\n}\n\n#declare a starting value for pi\n\npi &lt;- 0.5\n\n#set convergence criteria - when new pi is within 1e-6 of old pi, stop iterating\n\ntol &lt;- 1e-6\n\n#initialize iteration counter, set maximum iterations; how many times will we do this before we stop if we don't converge prior? \n\niter &lt;- 0\nmax_iter &lt;- 100\n\n#iterate the Newton-Raphson algorithm\n\nfor (iter in 1:max_iter) {\n  # Compute gradient  \n  grad &lt;- gradient(pi, y)\n  \n  # Compute Hessian\n  hess &lt;- hessian(pi, y)\n  \n  # Update pi\n  pi_new &lt;- pi - (grad / hess)\n  \n  pi &lt;- pi_new\n  \n  # Check for convergence\n  if (abs(pi_new - pi) &lt; tol) {\n    break\n  }\n  \n  iter &lt;- iter + 1\n}\n\n#after convergence, print the result\n\nprint(pi)\n\n\n[1] 0.293\n\n\nSo what’s gone on here? We have 1000 observations of a binary variable; maximum likelihood is answering the question “what is the value of the parameter \\(\\pi\\) that makes the observed data most likely?” To get that answer, we have tried candidate values of \\(\\pi\\) to see which one maximizes the log-likelihood function.\nWe’ve maximized the function two ways - using a grid search process, and using the Newton-Raphson algorithm. The latter is more efficient and is the basis for most optimization algorithms in statistical software, and easy to adapt to estimate multiple parameters.\nHere’s one more approach also using the Newton-Raphson method. It calls the maxLik package to do the optimization instead of our having to write the algorithm ourselves.\n\n\ncode\nlibrary(maxLik)\n# Write the binomial log-likelihood function\nlog_likelihood &lt;- function(pi, y) {\n  sum(y * log(pi) + (1 - y) * log(1 - pi))\n}\n\n#maximize the function using maxLik\n\nm1m &lt;- maxLik(y=y, log_likelihood, start = c(0.5), method = \"NR\") #Newton-Raphson method, starting value of 0.5\n\nsummary(m1m)\n\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 3 iterations\nReturn code 2: successive function values within tolerance limit (tol)\nLog-Likelihood: -604.816 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(&gt; t)    \n[1,]  0.29300    0.01441   20.34  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\nAgain, the value of \\(\\pi\\) that maximizes the log-likelihood is the same as the sample frequency of \\(y\\), which is 0.293.\n\n\n\nLet’s make this more realistic in terms of resembling the models we want to run by adding an intercept term and multiple predictors to the model - the likelihood now has to account for those \\(X\\) variables and maximize with respect to them.\n\ngenerate some data on \\(y\\), and \\(X\\)\nwrite the likelihood\nwrite the Newton-Raphson algorithm\napply to the data\n\n\n\ncode\n# Generate binary y, and two X variables for regression; coefficients are -1, 0.5, -0.5\n\nset.seed(8675309)\nn &lt;- 10000\nX &lt;- matrix(rnorm(n * 2), ncol = 2)\ntrue_beta &lt;- c(-1, 0.5, -0.5)\nz &lt;- cbind(1, X) %*% true_beta\nprob &lt;- 1 / (1 + exp(-z))\ny &lt;- rbinom(n, 1, prob)\n\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson iterative algorithm\n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # Add intercept term to X if not already present\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#write the gradient function - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- plogis(X %*% beta)\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  k &lt;- ncol(X)\n  \n# set starting values for k columns of data (betas)\n  beta &lt;- rep(0, k) \n  \n# Store gradients and log-likelihoods for analysis\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n# iterate NR \n  for (iter in 1:max_iter) {\n    # compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # compute Hessian\n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # check for convergence\n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n    \n    # compute standard errors for this model by taking the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  }\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#html table comparing glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\", caption=\"Comparing GLM and Dave Estimates\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  column_spec(3, border_right = T) %&gt;%\n  column_spec(4, border_right = T) %&gt;%\n  collapse_rows(columns = 1:3, valign = \"top\") \n\n\n\n\nTable 1: Comparing GLM and Dave Estimates\n\n\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-0.9531826\n-0.9531826\n0.0236756\n0.0236748\n\n\nX1\n0.4938831\n0.4938831\n0.0238303\n0.0238295\n\n\nX2\n-0.4719703\n-0.4719703\n0.0240294\n0.0240286\n\n\n\n\n\n\n\n\n\n\n\nTable 1 compares the coefficients and standard errors from the Newton-Raphson algorithm to the coefficients and standard errors from the glm function. The estimates are the same, the standard errors very close.\nAlright, lots of fun here. Now, let’s use the program above to estimate a logit model using the democratic peace data.\n\n\n\nTable 2 compares the coefficients and standard errors from the program we wrote to those produced by the glm function. The regression here is a logit model predicting the onset of a militarized dispute as a function of the logged capabilities ratio, an indicator of whether the pair of states share a border, and the lowest democracy (polity) score in the pair.\n\n\ncode\ndp &lt;- read.csv(\"/users/dave/documents/teaching/606J-mle/2022/exercises/ex1/dp.csv\", header=TRUE)\n\n# for the log_likelihood function below, define y as the variable \"dispute\" in the dp data frame, and x1, x2, and x3 as the variables \"lncaprat\", \"border\", and \"deml\" in the dp data frame, respectively.\ndp$lncaprat &lt;- log(dp$caprat)\nX &lt;- as.matrix(dp[, c(\"lncaprat\", \"border\", \"deml\")])\ny &lt;- as.vector(dp$dispute)\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson \n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # check for constant; add one if necessary\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#write the gradient function -  partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- 1 / (1 + exp(-X %*% beta))\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  p &lt;- ncol(X)\n  \n  # set starting values for beta\n  beta &lt;- rep(0, p)\n  \n  # Store gradients and log-likelihoods\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n  for (iter in 1:max_iter) {\n    # Compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # Compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # Compute Hessian - 2nd partial derivatives \n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # Update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # Check for convergence - end if change in beta is less than tolerance value \n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n  }\n\n#compute standard errors for this model; take the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n# logit$coefficients\n# logit$st.errors\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#compare glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\", caption=\"Comparing GLM and Dave Estimates, Democratic Peace Model\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:3, valign = \"top\")\n\n\n\n\nTable 2: Comparing GLM and Dave Estimates, Democratic Peace Model\n\n\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-3.2407604\n-3.2407605\n0.1034544\n0.1034630\n\n\nXlncaprat\n-0.2028768\n-0.2028768\n0.0237847\n0.0237860\n\n\nXborder\n0.7827135\n0.7827135\n0.0852298\n0.0852337\n\n\nXdeml\n-0.0783467\n-0.0783467\n0.0068065\n0.0068078\n\n\n\n\n\n\n\n\n\n\n\nRecapping what we’ve done:\n\nWe wrote a log-likelihood function for a logistic regression model with multiple predictors.\nTo maximize the log-likelihood, we implemented a Newton-Raphson algorithm in the following steps:\n\nDefined the gradient and Hessian functions. The gradient is the vector of first partial derivatives of the log-likelihood, and the Hessian is the matrix of second partial derivatives.\nInitialized the beta coefficients and stored gradients and log-likelihoods.\nIterated the algorithm to update beta values.\nChecked for convergence based on the change in beta.\n\nOnce the model converged, we computed standard errors for the estimated coefficients by taking the square root of the main diagonal elements of the inverse negative Hessian.\n\n\n\n\nHere, you can see how the gradients change over each iteration.\n\n\ncode\n#plot gradient convergence\ngradients &lt;- do.call(cbind, logit$gradient_history)\ngradients &lt;- as.data.frame(gradients) %&gt;%\n  mutate(coef=c(\"Intercept\", \"lncaprat\", \"border\", \"deml\")) \n\ngradients &lt;- gradients %&gt;% \n  pivot_longer(-coef, names_to = \"iteration\", values_to = \"value\")\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(gradients, \"line\", hcaes(x=iteration, y=value, group=coef, color=coef)) %&gt;%\n  hc_title(text = \"Gradient Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Gradient\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\n\n\nAnd here, we can see how the log-likelihood changes over iterations:\n\n\ncode\n#plot ll convergence\n\nlls &lt;- as.data.frame(logit$ll_history) %&gt;%\n  mutate(iteration=1:nrow(.))\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(lls, \"line\", hcaes(x=iteration, y=`logit$ll_history`)) %&gt;%\n  hc_title(text = \"Log-Likelihood Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\n\n\nHere are two ways to recover the final log-likelihood from the glm model. The first uses the logLik function from the stats package. The second calculates the log-likelihood by hand - generating the predictions and plugging those into the log-likelihood function, then summing.\n\\[ LL = \\sum_{i=1}^{n} y_i \\ln(p) + (1-y_i) \\ln(1-p) \\]\n\n\ncode\n# use the logLik function from the stats package  \n\nllest &lt;- logLik(glm_fit)\n\n# or calculate it by hand - generate the predictions and plug those into the log-likelihood function, then sum.\n\npred &lt;- predict(glm_fit, type = \"response\")\nllestbyhand &lt;- sum(dp$dispute * log(pred) + (1 - dp$dispute) * log(1 - pred))\n\n\ndata.frame(llest, llestbyhand) %&gt;% \n  kable(\"html\", caption=\"Recovering the Log-Likelihood\" ) %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:2, valign = \"top\")\n\n\n\n\nRecovering the Log-Likelihood\n\n\nllest\nllestbyhand\n\n\n\n\n-3565.219\n-3565.219"
  },
  {
    "objectID": "llfmax.html#motivating-likelihood",
    "href": "llfmax.html#motivating-likelihood",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "Let’s recall the motivation for turning to ML. The data we have are not suitable for OLS because our observation of the data is limited. For instance, we observe a binary variable \\([0,1]\\) rather than the underlying continuous probability beneath it. If we could observe and measure that probability, we’d do so and perhaps use OLS to estimate a model. But since we only observe \\([0,1]\\), our observation is limited.\nOLS is poorly suited to model this variable especially if one of our goals is to generate predictions of that underlying probability. That is, one of our goals might be to estimate a measure of the \\(y\\) variable we wish we could measure directly.\nMLE is better suited to this task - it asks us what the data generating process is that produced the observed data, and to build a model appropriate to the limited observation of that \\(y\\) variable.\nTo do so, we need to:\n\ndescribe the observed distribution of \\(y\\)\nconsider what we wish we could measure - this points to the key quantities of interest we want to derive from the model.\ndescribe \\(y\\) according to a probability distribution; write that distribution\nwrite a log-likelihood function appropriate to that probability distribution.\nparameterize the log-likelihood function such that we have a link distribution to map the linear prediction, \\(x\\beta\\), onto the probability space of \\(y\\)."
  },
  {
    "objectID": "llfmax.html#binary-y-variable",
    "href": "llfmax.html#binary-y-variable",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "Let’s start with a \\(y\\) variable as follows:\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                y\n                Freq\n              \n        \n        \n        \n                \n                  0\n                  707\n                \n                \n                  1\n                  293\n                \n        \n      \n    \n\n\n\nThe variable, \\(y\\), takes on values of zero and one - it appears binomial; let’s write this in terms of the binomial distribution parameter \\(\\pi\\), so it takes on the value of one with probability \\(\\pi\\) and zero with probability \\(1-\\pi\\).\n\\[ y_i = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{} \\pi_{i}\\\\\n         0, & \\mbox{} 1-\\pi_{i}\n         \\end{array}\n     \\right.\\]\nThe likelihood of a single observation is:\n\\[ \\mathcal{L}(\\pi_i|y) = \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\]\nThis is a statement of the likelihood that any particular value of \\(\\pi\\) generated an observation. We want the likelihood any value of \\(\\pi\\) generated the entire dataset - that is, we want the joint likelihood of all the observations.\n\\[ \\mathcal{L}(\\pi|y) = \\prod_{i=1}^{n} \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\] recalling that a joint probability is the product of individual probabilities.\nLet’s take the log of this likelihood function - the natural log makes computation easier. Even for computers, this reduces the computational intensity especially with respect to extremely small decimals.\n\\[ \\ln \\mathcal{L}(\\pi|y) = \\sum_{i=1}^{n} y_i \\ln(\\pi_i) + (1-y_i) \\ln(1-\\pi_i) \\]\nWe can take this a step further and parameterize \\(\\pi\\) as a function of \\(X\\beta\\) where \\(X\\) is the matrix of predictors and \\(\\beta\\) is the vector of coefficients such that\n\\[ \\pi_i = F({X_i\\beta}) \\]\nSo our estimate of the binomial probability is a function of the linear predictor \\(X\\beta\\). We map \\(X\\beta\\) onto the probability space \\(\\pi\\) using a link function, \\(F\\). The most common link functions are the logistic (logit model), and the standard normal (probit model). Let’s write the logit link:\n\\[ \\pi_i = \\frac{1}{1 + e^{-(X_i\\beta)}} \\]\nand now let’s write this all in the log-likelihood function:\n\\[ \\ln \\mathcal{L}(\\beta|y) = \\sum_{i=1}^{n} y_i \\ln(\\frac{1}{1 + e^{-(X_i\\beta)}}) + (1-y_i) \\ln(1-\\frac{1}{1 + e^{-(X_i\\beta)}}) \\]\nThis is the logit log-likelihood function for a binary \\(y\\) variable."
  },
  {
    "objectID": "llfmax.html#maximizing-the-likelihood",
    "href": "llfmax.html#maximizing-the-likelihood",
    "title": "Writing the Likelihood",
    "section": "",
    "text": "How do we solve this function for the data \\(y\\)? Put differently, what is the value of \\(\\pi\\) that most likely generated the data, \\(y\\)? There are a few ways to do this - one instructive method is to use a grid search, where we calculate the log-likelihood for a range of values of \\(\\pi\\) and find the value that maximizes the log-likelihood. We can repeat this to the desired level of specificity, progressively narrowing the grid. In this example, we’ll just do one search out to 3 decimal places. The steps are these:\n\ngenerate the range of candidate values of our parameter \\(\\pi\\) to plug into the log-likelihood function.\nplug each value into the log-likelihood function to compute the log-likelihood for that value.\nidentify which value of \\(\\pi\\) maximizes the log-likelihood.\n\nHere’s code to do this:\n\n\ncode\n# generate a vector of values of pi \n\npi_trials &lt;- seq(0, 1, by = 0.001)\n\n# write the log-likelihood function\n\nlog_likelihood &lt;- function(pi_trials, y) {\n  sum(y * log(pi_trials) + (1 - y) * log(1 - pi_trials))\n}\n\n# Calculate log-likelihood for each pi value\n\nll_values &lt;- sapply(pi_trials, log_likelihood, y = y)\n\n# Find the pi value that maximizes the log-likelihood\n\npi_hat &lt;- pi_trials[which.max(ll_values)]\nprint(pi_hat)\n\n\n[1] 0.293\n\n\nYou’ll note we recover the sample mean of \\(y\\). Let’s plot the log-likelihood against the values of \\(\\pi\\) to visualize the maximum.\n\n\ncode\n# Plot log-likelihood against pi\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(data.frame(pi = pi_trials, ll = ll_values), \"line\", hcaes(x = pi, y = ll)) %&gt;%\n  hc_title(text = \"What value of Pi maximizes the log-likelihood?\") %&gt;%\n  hc_xAxis(title = list(text = \"Pi\"), \n           plotLines = list(\n    list(color =\"red\", value = pi_hat ))) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_legend(enabled = FALSE) %&gt;%\n  hc_colors(bucolors)"
  },
  {
    "objectID": "llfmax.html#optimization",
    "href": "llfmax.html#optimization",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "Grid searches are not sufficient for multivariate models, especially where we also need measures of uncertainty. Here, we turn to numerical optimization.\nNumerical optimization is a field aimed at finding “best” outcomes or answers depending on some set of criteria - often, the “best” is the maximum or minimum of some function - that’s the case in maximum likelihood.\n\n\nThere are lots of ways to optimize a function - in statistical software, most of them derive from Newton’s iterative method. A very common application of this is the Newton-Raphson method. Newton-Raphson is an iterative process that starts with an initial guess for the parameter \\(\\pi\\) (or the vector of unknowns, \\(\\beta\\)) and updates it in the direction of the maximum until it converges to the maximum. At each step, it computes the first and second derivatives of the likelihood function, then uses these to update the parameter estimate(s). The update rule is:\n\\[ \\pi_{new} = \\pi_{old} - H(\\pi)^{-1}*g(\\pi) \\]\nSo the new estimate of the parameter is the old estimate minus the first derivative (gradient) of the log-likelihood function pre-multiplied by the Hessian (second derivative) of the log-likelihood function. Here, \\(g(\\pi)\\) is the gradient of the log-likelihood function and \\(H(\\pi)\\) is the Hessian of the log-likelihood function. The algorithm iterates this process until the change in \\(\\pi\\) is sufficiently small, below a specified threshold. Since \\(g\\) and \\(H\\) are matrices, you should see how this straightforwardly applied to a multivariate regression.\nLet’s write the Newton-Raphson algorithm ourselves - take a look at the code chunk below. You’ll see it produces the same result as the grid search; the sample frequency of \\(y\\).\n\n\n\n\n\ncode\n# use same data generated above; same LLF as above, aiming to recover pi\n\n#write the gradient - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(pi, y) {\n  sum(y / pi - (1 - y) / (1 - pi))\n}\n\n#write the Hessian - matrix of partial second derivatives of the log-likelihood function.\n\nhessian &lt;- function(pi, y) {\n  sum(-y / pi^2 - (1 - y) / (1 - pi)^2)\n}\n\n#declare a starting value for pi\n\npi &lt;- 0.5\n\n#set convergence criteria - when new pi is within 1e-6 of old pi, stop iterating\n\ntol &lt;- 1e-6\n\n#initialize iteration counter, set maximum iterations; how many times will we do this before we stop if we don't converge prior? \n\niter &lt;- 0\nmax_iter &lt;- 100\n\n#iterate the Newton-Raphson algorithm\n\nfor (iter in 1:max_iter) {\n  # Compute gradient  \n  grad &lt;- gradient(pi, y)\n  \n  # Compute Hessian\n  hess &lt;- hessian(pi, y)\n  \n  # Update pi\n  pi_new &lt;- pi - (grad / hess)\n  \n  pi &lt;- pi_new\n  \n  # Check for convergence\n  if (abs(pi_new - pi) &lt; tol) {\n    break\n  }\n  \n  iter &lt;- iter + 1\n}\n\n#after convergence, print the result\n\nprint(pi)\n\n\n[1] 0.293\n\n\nSo what’s gone on here? We have 1000 observations of a binary variable; maximum likelihood is answering the question “what is the value of the parameter \\(\\pi\\) that makes the observed data most likely?” To get that answer, we have tried candidate values of \\(\\pi\\) to see which one maximizes the log-likelihood function.\nWe’ve maximized the function two ways - using a grid search process, and using the Newton-Raphson algorithm. The latter is more efficient and is the basis for most optimization algorithms in statistical software, and easy to adapt to estimate multiple parameters.\nHere’s one more approach also using the Newton-Raphson method. It calls the maxLik package to do the optimization instead of our having to write the algorithm ourselves.\n\n\ncode\nlibrary(maxLik)\n# Write the binomial log-likelihood function\nlog_likelihood &lt;- function(pi, y) {\n  sum(y * log(pi) + (1 - y) * log(1 - pi))\n}\n\n#maximize the function using maxLik\n\nm1m &lt;- maxLik(y=y, log_likelihood, start = c(0.5), method = \"NR\") #Newton-Raphson method, starting value of 0.5\n\nsummary(m1m)\n\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 3 iterations\nReturn code 2: successive function values within tolerance limit (tol)\nLog-Likelihood: -604.816 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(&gt; t)    \n[1,]  0.29300    0.01441   20.34  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\nAgain, the value of \\(\\pi\\) that maximizes the log-likelihood is the same as the sample frequency of \\(y\\), which is 0.293.\n\n\n\nLet’s make this more realistic in terms of resembling the models we want to run by adding an intercept term and multiple predictors to the model - the likelihood now has to account for those \\(X\\) variables and maximize with respect to them.\n\ngenerate some data on \\(y\\), and \\(X\\)\nwrite the likelihood\nwrite the Newton-Raphson algorithm\napply to the data\n\n\n\ncode\n# Generate binary y, and two X variables for regression; coefficients are -1, 0.5, -0.5\n\nset.seed(8675309)\nn &lt;- 10000\nX &lt;- matrix(rnorm(n * 2), ncol = 2)\ntrue_beta &lt;- c(-1, 0.5, -0.5)\nz &lt;- cbind(1, X) %*% true_beta\nprob &lt;- 1 / (1 + exp(-z))\ny &lt;- rbinom(n, 1, prob)\n\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson iterative algorithm\n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # Add intercept term to X if not already present\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#write the gradient function - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- plogis(X %*% beta)\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  k &lt;- ncol(X)\n  \n# set starting values for k columns of data (betas)\n  beta &lt;- rep(0, k) \n  \n# Store gradients and log-likelihoods for analysis\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n# iterate NR \n  for (iter in 1:max_iter) {\n    # compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # compute Hessian\n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # check for convergence\n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n    \n    # compute standard errors for this model by taking the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  }\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#html table comparing glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\", caption=\"Comparing GLM and Dave Estimates\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  column_spec(3, border_right = T) %&gt;%\n  column_spec(4, border_right = T) %&gt;%\n  collapse_rows(columns = 1:3, valign = \"top\") \n\n\n\n\nTable 1: Comparing GLM and Dave Estimates\n\n\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-0.9531826\n-0.9531826\n0.0236756\n0.0236748\n\n\nX1\n0.4938831\n0.4938831\n0.0238303\n0.0238295\n\n\nX2\n-0.4719703\n-0.4719703\n0.0240294\n0.0240286\n\n\n\n\n\n\n\n\n\n\n\nTable 1 compares the coefficients and standard errors from the Newton-Raphson algorithm to the coefficients and standard errors from the glm function. The estimates are the same, the standard errors very close.\nAlright, lots of fun here. Now, let’s use the program above to estimate a logit model using the democratic peace data.\n\n\n\nTable 2 compares the coefficients and standard errors from the program we wrote to those produced by the glm function. The regression here is a logit model predicting the onset of a militarized dispute as a function of the logged capabilities ratio, an indicator of whether the pair of states share a border, and the lowest democracy (polity) score in the pair.\n\n\ncode\ndp &lt;- read.csv(\"/users/dave/documents/teaching/606J-mle/2022/exercises/ex1/dp.csv\", header=TRUE)\n\n# for the log_likelihood function below, define y as the variable \"dispute\" in the dp data frame, and x1, x2, and x3 as the variables \"lncaprat\", \"border\", and \"deml\" in the dp data frame, respectively.\ndp$lncaprat &lt;- log(dp$caprat)\nX &lt;- as.matrix(dp[, c(\"lncaprat\", \"border\", \"deml\")])\ny &lt;- as.vector(dp$dispute)\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson \n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # check for constant; add one if necessary\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#write the gradient function -  partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- 1 / (1 + exp(-X %*% beta))\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  p &lt;- ncol(X)\n  \n  # set starting values for beta\n  beta &lt;- rep(0, p)\n  \n  # Store gradients and log-likelihoods\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n  for (iter in 1:max_iter) {\n    # Compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # Compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # Compute Hessian - 2nd partial derivatives \n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # Update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # Check for convergence - end if change in beta is less than tolerance value \n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n  }\n\n#compute standard errors for this model; take the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n# logit$coefficients\n# logit$st.errors\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#compare glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\", caption=\"Comparing GLM and Dave Estimates, Democratic Peace Model\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:3, valign = \"top\")\n\n\n\n\nTable 2: Comparing GLM and Dave Estimates, Democratic Peace Model\n\n\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-3.2407604\n-3.2407605\n0.1034544\n0.1034630\n\n\nXlncaprat\n-0.2028768\n-0.2028768\n0.0237847\n0.0237860\n\n\nXborder\n0.7827135\n0.7827135\n0.0852298\n0.0852337\n\n\nXdeml\n-0.0783467\n-0.0783467\n0.0068065\n0.0068078\n\n\n\n\n\n\n\n\n\n\n\nRecapping what we’ve done:\n\nWe wrote a log-likelihood function for a logistic regression model with multiple predictors.\nTo maximize the log-likelihood, we implemented a Newton-Raphson algorithm in the following steps:\n\nDefined the gradient and Hessian functions. The gradient is the vector of first partial derivatives of the log-likelihood, and the Hessian is the matrix of second partial derivatives.\nInitialized the beta coefficients and stored gradients and log-likelihoods.\nIterated the algorithm to update beta values.\nChecked for convergence based on the change in beta.\n\nOnce the model converged, we computed standard errors for the estimated coefficients by taking the square root of the main diagonal elements of the inverse negative Hessian.\n\n\n\n\nHere, you can see how the gradients change over each iteration.\n\n\ncode\n#plot gradient convergence\ngradients &lt;- do.call(cbind, logit$gradient_history)\ngradients &lt;- as.data.frame(gradients) %&gt;%\n  mutate(coef=c(\"Intercept\", \"lncaprat\", \"border\", \"deml\")) \n\ngradients &lt;- gradients %&gt;% \n  pivot_longer(-coef, names_to = \"iteration\", values_to = \"value\")\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(gradients, \"line\", hcaes(x=iteration, y=value, group=coef, color=coef)) %&gt;%\n  hc_title(text = \"Gradient Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Gradient\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\n\n\nAnd here, we can see how the log-likelihood changes over iterations:\n\n\ncode\n#plot ll convergence\n\nlls &lt;- as.data.frame(logit$ll_history) %&gt;%\n  mutate(iteration=1:nrow(.))\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(lls, \"line\", hcaes(x=iteration, y=`logit$ll_history`)) %&gt;%\n  hc_title(text = \"Log-Likelihood Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\n\n\nHere are two ways to recover the final log-likelihood from the glm model. The first uses the logLik function from the stats package. The second calculates the log-likelihood by hand - generating the predictions and plugging those into the log-likelihood function, then summing.\n\\[ LL = \\sum_{i=1}^{n} y_i \\ln(p) + (1-y_i) \\ln(1-p) \\]\n\n\ncode\n# use the logLik function from the stats package  \n\nllest &lt;- logLik(glm_fit)\n\n# or calculate it by hand - generate the predictions and plug those into the log-likelihood function, then sum.\n\npred &lt;- predict(glm_fit, type = \"response\")\nllestbyhand &lt;- sum(dp$dispute * log(pred) + (1 - dp$dispute) * log(1 - pred))\n\n\ndata.frame(llest, llestbyhand) %&gt;% \n  kable(\"html\", caption=\"Recovering the Log-Likelihood\" ) %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:2, valign = \"top\")\n\n\n\n\nRecovering the Log-Likelihood\n\n\nllest\nllestbyhand\n\n\n\n\n-3565.219\n-3565.219"
  },
  {
    "objectID": "llfmax.html#maximizing-the-likelihood---grid-search",
    "href": "llfmax.html#maximizing-the-likelihood---grid-search",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "How do we solve this function for the data \\(y\\)? Put differently, what is the value of \\(\\pi\\) that most likely generated the data, \\(y\\)? There are a few ways to do this - one instructive method is to use a grid search, where we calculate the log-likelihood for a range of values of \\(\\pi\\) and find the value that maximizes the log-likelihood. We can repeat this to the desired level of specificity, progressively narrowing the grid. In this example, we’ll just do one search out to 3 decimal places. The steps are these:\n\ngenerate the range of candidate values of our parameter \\(\\pi\\) to plug into the log-likelihood function.\nplug each value into the log-likelihood function to compute the log-likelihood for that value.\nidentify which value of \\(\\pi\\) maximizes the log-likelihood.\n\nHere’s code to do this:\n\n\ncode\n# generate a vector of values of pi \n\npi_trials &lt;- seq(0, 1, by = 0.001)\n\n# write the log-likelihood function\n\nlog_likelihood &lt;- function(pi_trials, y) {\n  sum(y * log(pi_trials) + (1 - y) * log(1 - pi_trials))\n}\n\n# Calculate log-likelihood for each pi value\n\nll_values &lt;- sapply(pi_trials, log_likelihood, y = y)\n\n# Find the pi value that maximizes the log-likelihood\n\npi_hat &lt;- pi_trials[which.max(ll_values)]\nprint(pi_hat)\n\n\n[1] 0.293\n\n\nYou’ll note we recover the sample mean of \\(y\\). Let’s plot the log-likelihood against the values of \\(\\pi\\) to visualize the maximum.\n\n\ncode\n# Plot log-likelihood against pi\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(data.frame(pi = pi_trials, ll = ll_values), \"line\", hcaes(x = pi, y = ll)) %&gt;%\n  hc_title(text = \"Grid search - maximizing the log-likelihood\") %&gt;%\n  hc_xAxis(title = list(text = \"Pi\"), \n           plotLines = list(\n    list(color =\"red\", value = pi_hat ))) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_legend(enabled = FALSE) %&gt;%\n  hc_colors(bucolors)"
  },
  {
    "objectID": "likelihood24.html",
    "href": "likelihood24.html",
    "title": "Likelihood",
    "section": "",
    "text": "Why do we turn to maximum likelihood instead of OLS, especially given the simplicity and robustness of OLS?\nOur data can’t meet the OLS assumptions; \\(y\\) is often limited* such that we observe \\(y\\) but really want to measure \\(y^*\\).\n\\(y^*\\) is often a continuous (unlimited) variable we wish we could measure - if we could, we’d use OLS to estimate its correlates.\nOLS asks us to make the data satisfy the model. MLE asks us to build a model based on the data. Since our data often don’t satisfy the OLS assumptions, MLE offers a flexible alternative.\n\n\n\n\n\nML asks us to think of the data as given and to imagine the model that might best represent the Data Generating Process.\n\nIn OLS, the model is fixed - the assumptions about \\(\\epsilon\\) are given.\nIn ML, the data are fixed - we construct a model that reflects the nature of the data.\nWhat we assume about the unobservables is informed by what we know about the observables, the \\(y\\) variable.\nA useful way to describe \\(y\\) is to characterize its observed or empirical distribution.\nOnce we know the distribution of \\(y\\), we can begin building a model based on that distribution.\n\n\n\n\n\nWe are limited in what we can observe and/or measure in \\(y\\). E.g., we observe an individual voting or not; we cannot observe the chances an individual votes.\n\nThe \\(y\\) variable has both observed and latent qualities; label the latent variable \\(\\widetilde{y}\\).\nOften, we are more interested in this latent variable. We are more interested in the latent chance of voting than the observed behavior.\n\\(\\widetilde{y}\\) is the variable we wish we could measure.\nThe latent and observed variables are often distributed differently. Observed voting \\(y=(0,1)\\); latent variable \\(Pr(y=1)\\).\nLinking \\(X\\) variables to the latent \\(\\widetilde{y}\\) requires rescaling; this is often because changes in \\(\\widetilde{y}\\) given \\(X\\) are nonlinear and in different units. In the example above, \\(y\\) is a binary realizing of voting, so is binomial. \\(\\widetilde{y}\\) is the probability of voting, so is continuous, bounded between 0 and 1.\nBesides, \\(\\widetilde{y}\\) is unobserved - so we have to generate it from the model.\nTo link \\(X\\) with \\(\\widetilde{y}\\) or \\(y\\), we assume their relationship follows some distribution; we call this the link distribution.\n\n\n\n\n\nOur data are often limited, and not suitable for OLS. OLS asks us to transform data to make it meet the model assumptions (e.g., Generalized Least Squares); MLE builds the model based on the data."
  },
  {
    "objectID": "likelihood24.html#ml-ols",
    "href": "likelihood24.html#ml-ols",
    "title": "Likelihood",
    "section": "",
    "text": "Why do we turn to maximum likelihood instead of OLS, especially given the simplicity and robustness of OLS?\nOur data can’t meet the OLS assumptions; \\(y\\) is often limited* such that we observe \\(y\\) but really want to measure \\(y^*\\).\n\\(y^*\\) is often a continuous (unlimited) variable we wish we could measure - if we could, we’d use OLS to estimate its correlates.\nOLS asks us to make the data satisfy the model. MLE asks us to build a model based on the data. Since our data often don’t satisfy the OLS assumptions, MLE offers a flexible alternative."
  },
  {
    "objectID": "likelihood24.html#ml",
    "href": "likelihood24.html#ml",
    "title": "Likelihood",
    "section": "",
    "text": "ML asks us to think of the data as given and to imagine the model that might best represent the Data Generating Process.\n\nIn OLS, the model is fixed - the assumptions about \\(\\epsilon\\) are given.\nIn ML, the data are fixed - we construct a model that reflects the nature of the data.\nWhat we assume about the unobservables is informed by what we know about the observables, the \\(y\\) variable.\nA useful way to describe \\(y\\) is to characterize its observed or empirical distribution.\nOnce we know the distribution of \\(y\\), we can begin building a model based on that distribution."
  },
  {
    "objectID": "likelihood24.html#limited-dependent-variables",
    "href": "likelihood24.html#limited-dependent-variables",
    "title": "Likelihood",
    "section": "",
    "text": "We are limited in what we can observe and/or measure in \\(y\\). E.g., we observe an individual voting or not; we cannot observe the chances an individual votes.\n\nThe \\(y\\) variable has both observed and latent qualities; label the latent variable \\(\\widetilde{y}\\).\nOften, we are more interested in this latent variable. We are more interested in the latent chance of voting than the observed behavior.\n\\(\\widetilde{y}\\) is the variable we wish we could measure.\nThe latent and observed variables are often distributed differently. Observed voting \\(y=(0,1)\\); latent variable \\(Pr(y=1)\\).\nLinking \\(X\\) variables to the latent \\(\\widetilde{y}\\) requires rescaling; this is often because changes in \\(\\widetilde{y}\\) given \\(X\\) are nonlinear and in different units. In the example above, \\(y\\) is a binary realizing of voting, so is binomial. \\(\\widetilde{y}\\) is the probability of voting, so is continuous, bounded between 0 and 1.\nBesides, \\(\\widetilde{y}\\) is unobserved - so we have to generate it from the model.\nTo link \\(X\\) with \\(\\widetilde{y}\\) or \\(y\\), we assume their relationship follows some distribution; we call this the link distribution."
  },
  {
    "objectID": "likelihood24.html#the-big-point",
    "href": "likelihood24.html#the-big-point",
    "title": "Likelihood",
    "section": "",
    "text": "Our data are often limited, and not suitable for OLS. OLS asks us to transform data to make it meet the model assumptions (e.g., Generalized Least Squares); MLE builds the model based on the data."
  },
  {
    "objectID": "likelihood24.html#for-example",
    "href": "likelihood24.html#for-example",
    "title": "Likelihood",
    "section": "For example …",
    "text": "For example …\n\n\\(Y = (0,1)\\), does an individual vote or not. This is binary, discrete, let’s say binomial.\n\\(\\tilde{y}\\) is the latent probability an individual votes. Wish we could measure this, use OLS.\nWrite a function based on \\(Y\\): \\(\\pi^y (1-\\pi)^{(1-y)}\\) , the binomial PDF (without the n-tuple).\n\\(\\pi\\) is given by some \\(X\\) variables we think affect voting - so \\(\\pi = x\\beta\\).\nSo substitute for \\(\\pi\\) …\\((x\\beta)^y (1-x\\beta)^{(1-y)}\\)\nNow, link the linear prediction to \\(\\tilde{Y}\\) - map \\(x\\beta\\) onto the Pr(vote). How about using the Standard Normal CDF \\(\\Phi\\)?\n\n\\(\\Phi(x\\beta)^y  \\Phi(1-x\\beta)^{(1-y)}\\) - this is now the core of the Probit."
  },
  {
    "objectID": "likelihood24.html#likelihood",
    "href": "likelihood24.html#likelihood",
    "title": "Likelihood",
    "section": "Likelihood",
    "text": "Likelihood\nKing (pp. 9, 14) puts it this way:\n\\[Y\\sim f(y|\\theta,\\alpha)\\]\nand\n\\[\\theta = G(X,\\beta)\\]\nso our data, \\(Y\\) has a probability distribution given by parameters \\(\\theta, \\alpha\\), and \\(\\theta\\) is a function of some variables, \\(X\\) and their parameters, \\(\\beta\\).\n\nAll of this comprises the model in King’s lingo, so a basic probability statement appears as:\n\\[Pr(y|\\mathcal{M}) \\equiv Pr(\\mathrm{data|model})\\]\nThis is a conditional probability resting on two conditions:\n\nThe data are random and unknown.\nThe model is known.\n\nUh oh. The model is known? The data aren’t? This works in many probability settings. What is the probability of rolling 3 threes in 6 rolls of a six-sided die? What is the probability you draw an ace from a standard deck of cards? In cases like these, the model is known even before we observe events (data).\n\nIn cases like these, the model’s parameters are known and fixed. In our applications, the model and its parameters are the unknowns, but the events or data are known, fixed, and given.\nSuppose I flip a coin to decide whether I roll a 20-sided die, or a 6-sided die. You do not observe any of this - I only tell you I rolled a 4 - which die did I roll? This is the problem we try to deal with in ML - what is the data generating process that most likely produced the observed data. Unlike the simple die roll, the model isn’t known. Instead, there are many possible models that could have produced the data."
  },
  {
    "objectID": "likelihood24.html#inverse-probability",
    "href": "likelihood24.html#inverse-probability",
    "title": "Likelihood",
    "section": "Inverse Probability",
    "text": "Inverse Probability\nGiven these conditions, the more sensible model would be:\n\\[Pr(\\mathcal{M}|y)\\]\nThis is the inverse probability model - but it requires knowledge (or strong assumptions) regarding an important element of the (unknown) model, \\(\\theta\\). Even Bayes can’t really do this."
  },
  {
    "objectID": "likelihood24.html#likelihood-1",
    "href": "likelihood24.html#likelihood-1",
    "title": "Likelihood",
    "section": "Likelihood",
    "text": "Likelihood\nLikelihood estimates the model, \\(\\mathcal{M}\\) given the data, but assumes that \\(\\theta\\) can take on different values, representing different (competing) hypothetical models.\nThe set of \\(\\theta\\)’s are the competing models or data generating processes that could have produced the observed data set.\n\nKeeping with King’s notation, the likelihood axiom is:\n\\[\\mathcal{L}(\\tilde{\\theta}|y,\\mathcal{M}*)\\equiv L(\\tilde{\\theta}|y) \\] \\[=k(y)Pr(y|\\tilde{\\theta})\\] \\[\\propto Pr(y|\\tilde{\\theta})\\]\nwhere \\(\\tilde{\\theta}\\) represents the hypothetical value of \\(\\theta\\) (rather than its true value).\nThe term \\(k(y)\\) (known as the “constant of proportionality”) is a constant across all the hypothetical values of \\(\\tilde{\\theta}\\) (and thus drops out) but is the key to the likelihood axiom; it represents the functional form through which the data (\\(y\\)) shape \\(\\tilde{\\theta}\\) and thus allow us to estimate the likelihood as a measure of relative (instead of absolute) uncertainty; our uncertainty in this case is relative to the other possible functions of \\(y\\) and the hypothetical values of \\(\\tilde{\\theta}\\).\nAs King (p. 61) puts it , \\(k(y)\\) “measures the relative likelihood of a specified hypothetical model \\(\\tilde{\\beta}\\) producing the data we observed.”\nIt turns out that the likelihood of observing the data is proportional to the probability of observing the data."
  },
  {
    "objectID": "likelihood24.html#take-away-points",
    "href": "likelihood24.html#take-away-points",
    "title": "Likelihood",
    "section": "Take Away Points",
    "text": "Take Away Points\n\nWe want to know the probability (the model) of observing some data; if we find the model parameters with the highest likelihood of generating the observed data, we also know the probability because the two are proportional.\nLikelihoods are always negative; they do not have a scale or specific meaning; they do not transform into probabilities or anything familiar.\nWe find the parameter values that produce the largest likelihood values; those parameters that maximize the likelihood then can be translated into sensible quantities - they can be mapped onto \\(\\tilde{y}\\), giving us a measure of the variable we wish we had.\nIn OLS, we compute parameter estimates that minimize the sum (squared) distance of all the observed points to the predicted points - we minimize the errors in this fashion.\nThe technology of MLE is trial and error - choose some values for \\(\\tilde{\\theta}\\), compute the likelihood, then repeat. Compare all the likelihoods - the values of \\(\\tilde{\\theta}\\) that produced the highest likelihood value are the ones that most likely generated the observed data."
  },
  {
    "objectID": "likelihood24.html#writing-the-likelihood",
    "href": "likelihood24.html#writing-the-likelihood",
    "title": "Likelihood",
    "section": "Writing the Likelihood",
    "text": "Writing the Likelihood\nSince we’ve determined \\(y\\) is normal, write the Normal PDF.\n\\[Pr(Y=y_{i})=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{\\left[\\frac{-(y_{i}-\\mu_{i})^{2}}{2\\sigma^{2}}\\right]}\\]\nWe’re interested in the joint probability of the observations, the probability the data result from a particular Data Generating Process. Assuming the observations in the data are independent of one another, the joint density is equal to the product of the marginal probabilities:\n\\[Pr(A~ \\mathrm{and}~ B)=Pr(A)\\cdot Pr(B)\\]\nso the joint probability of \\(y\\), written in terms of the likelihood, is given by\n\\[\\mathcal{L} (\\mu, \\sigma^{2}| y )= \\prod\\limits_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{\\left[\\frac{-(y_{i}-\\mu_{i})^{2}}{2\\sigma^{2}}\\right]}\\]\nAdding is easier than multiplying; since we can transform the likelihood function by any monotonic form, we can take its natural log to replace the products with summations:\n\\[\\ln \\mathcal{L} (\\mu, \\sigma^{2}|y) = \\ln \\prod\\limits_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{\\left[\\frac{-(y_{i}-\\mu_{i})^{2}}{2\\sigma^{2}}\\right]}\\]\n\\[= \\sum \\ln \\left[\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{\\left[\\frac{-(y_{i}-\\mu_{i})^{2}}{2\\sigma^{2}}\\right]} \\right]\\]\n\\[=\\sum\\left( -\\frac{1}{2}(\\ln(2\\pi))-\\frac{1}{2}(\\ln(\\sigma^{2}))-\\frac{1}{2\\sigma^{2}}\\left[\\sum\\limits_{i=1}^{n}(y_{i}-\\mu)^{2}\\right] \\right)\\]"
  },
  {
    "objectID": "likelihood24.html#the-linear-model",
    "href": "likelihood24.html#the-linear-model",
    "title": "Likelihood",
    "section": "The Linear model",
    "text": "The Linear model\nIt should be pretty evident this is the linear model.\n\nwe started with data that looked normal; continuous, unbounded, infinitely differentiable.\nassuming normality, we wrote a LLF in terms of the distribution parameters \\(\\mu, \\sigma^2\\).\nbut we want \\(\\mu\\) itself to vary with some \\(X\\) variables; \\(y\\) isn’t just characterized by a grand mean, but by a set of of conditional means given by \\(X\\).\nso we need to parameterize the model - write the distribution parameter as a function of some variables.\ngenerally, this is to declare \\(\\theta = F(x\\beta)\\).\nin the linear/normal case, to declare \\(\\mu=F(x\\beta)\\). Of course, \\(F\\) is linear, we just write \\(\\mu=x\\beta\\).\nin the LLF, substitute \\(x\\beta\\) for \\(\\mu\\), and now we’re taking the difference between \\(y\\) and \\(x\\beta\\), squaring those differences, and weighting them by the variance.\n\n\nTo put it all together \\(\\ldots\\)\n\\[\\ln \\mathcal{L}(\\mu, \\sigma^{2}|y) = \\ln \\prod\\limits_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} exp \\left[\\frac{-(y-x\\beta)^{2}}{2\\sigma^{2}}\\right] \\] \\[= \\sum \\ln \\left\\{\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} exp \\left[\\frac{-(y-x\\beta)^{2}}{2\\sigma^{2}}\\right]\\right\\}\\]\n\\[= \\sum\\left(-\\frac{1}{2}(\\ln(2\\pi)) -\\frac{1}{2}(\\ln(\\sigma^{2})) -\\frac{1}{2\\sigma^{2}}\\left[\\sum\\limits_{i=1}^{n}(y-x\\beta)^{2}\\right] \\right)\\]\nDoes this look familiar? A lot like deriving OLS, eh?"
  },
  {
    "objectID": "likelihood24.html#linear-regression-in-ml",
    "href": "likelihood24.html#linear-regression-in-ml",
    "title": "Likelihood",
    "section": "Linear regression in ML",
    "text": "Linear regression in ML\nThis is the Normal (linear) log-likelihood function. It presumes some data, \\(\\mathbf{y}\\) and some unknowns \\(\\mathbf{\\beta, \\sigma^2}\\). You should note the kernel of the function is the sum of the squared differences of \\(y\\) and \\(x\\beta\\).\n\\[\\ln\\mathcal{L} =\\sum\\left(-\\frac{1}{2}(\\ln(2\\pi)) -\\frac{1}{2}(\\ln(\\sigma^{2})) -\\frac{1}{2\\sigma^{2}}\\color{red}{\\left[\\sum\\limits_{i=1}^{n}(y-x\\beta)^{2}\\right]} \\right)\\]\nIt turns out if we take the derivative of this function with respect to \\(\\beta\\) and \\(\\sigma^2\\), the result is the ML estimator, and the OLS estimator. They’re the same."
  },
  {
    "objectID": "likelihood24.html#linearity",
    "href": "likelihood24.html#linearity",
    "title": "Likelihood",
    "section": "Linearity",
    "text": "Linearity\nThis model is linear - our estimates, \\(x\\beta\\) map directly onto \\(y\\) - there is no latent variable, \\(\\tilde{y}\\) to map onto - so \\(x\\beta = \\widehat{y}\\).\nNote that this is not because \\(y\\) is normal. Rather, the fact that \\(y\\) is continuous and contains a lot of information and is not limited makes it more likely \\(y\\) is normal.\nThough ML is most often used in cases where \\(y\\) is limited (so OLS is inappropriate), it’s important to see that we can estimate the linear model using either technology (OLS, ML) and get the same estimates."
  },
  {
    "objectID": "likelihood24.html#footnotes",
    "href": "likelihood24.html#footnotes",
    "title": "Likelihood",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee von Bortkiewicz, Ladislaus (1898). Das Gesetz der Kleinen Zahlen. Leipzig: Teubner.↩︎"
  },
  {
    "objectID": "likelihood24.html#numerical-method-intuition",
    "href": "likelihood24.html#numerical-method-intuition",
    "title": "Likelihood",
    "section": "Numerical method intuition",
    "text": "Numerical method intuition\n\nChoose starting values of \\(\\beta\\) (sometimes from OLS) to estimate the log-likelihood.\nTake the derivative of the log-likelihood with respect to the parameters to find the gradient}. The gradient (or the gradient matrix, a \\(kxk\\) matrix) tells us the direction of the slope of a line tangent to the curve at the point of the log-likelihood estimate.\nIf the gradient is positive (if the matrix is positive definite), then \\(ln \\mathcal{L}\\) is increasing in \\(\\beta\\) - the slope is increasing, so increase our estimate of \\(\\beta\\) and try again.\nIf the gradient is negative (if the matrix is negative definite), the \\(ln \\mathcal{L}\\) is decreasing in \\(\\beta\\) - the slope is decreasing, so we’ve passed the maximum; choose a smaller value for \\(\\beta\\) and try again.\nAs the log-likelihood approaches the maximum, the gradient approaches zero - the slope of the line tangent to the curve at the point of the log-likelihood estimate is approaching zero, indicating we’re reaching the maximum of the function. Stop the search and evaluate the estimates of \\(\\beta\\) that produced the zero gradient.\nThroughout this process, we need to evaluate the second derivatives in order to figure out the rate at which the slope is changing; this helps us tell how close or far we are from the maximum. The second derivative describes the curvature of the LLF, or the rate of change.\nThe matrix of second derivatives (the Hessian matrix) or its approximation also provide the source of our estimates of the variance, and thus the standard errors.\n\n\nThe first derivative tells us the direction in which the function is changing. This is obviously important since we’re trying to find the maximum.\nThink of this as trying to figure out when you’re exactly at the top of a hill. The slope (the grade, the gradient) is positive while you’re climbing to the top, it’s zero at the top, and it’s negative on the way down the other side.\nBut is the hill flat or steep? If it’s flat, then the change in the slope between point A and point B is likely to be very small - this, of course, can make it difficult to know exactly when we’re at the top (the maximum). On the other hand, if the hill is very steep, the change in the slope between two points is pretty substantial. Put another way, the rate of change in the slope is larger (faster) the steeper the slope; it’s smaller (slower) the flatter the slope.\nThis matters to maximization because the second derivatives tell us how big (or small) a step we should take up the hill as we try to find the top. Suppose that the function is very flat; as indicated above, the change in the slope between two points would be small, so we can take larger steps in order to try to find the maximum. The second derivatives would tell us that the rate of change is very small, so we should take larger steps.\nThe software performing the estimation will choose the next value of \\(\\beta\\) a bit further away from the last value it tried. On the other hand, if the second derivatives are large so the rate of change is fast, we want to take relatively small steps so we don’t step right over the maximum. In any case, that’s the intuition for why we need to know the matrix of second derivatives."
  },
  {
    "objectID": "llfmax24.html",
    "href": "llfmax24.html",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "The goal here is to walk through the process of writing a likelihood function based on the data we observe, then programming that likelihood function and optimizing it with respect to the data.\n\n\nLet’s recall the motivation for turning to ML. The data we have are not suitable for OLS because our observation of the data is limited. For instance, we observe a binary variable \\([0,1]\\) rather than the underlying continuous probability beneath it. If we could observe and measure that probability, we’d do so and perhaps use OLS to estimate a model. But since we only observe \\([0,1]\\), our observation is limited.\nOLS is poorly suited to model this variable especially if one of our goals is to generate predictions of that underlying probability. That is, one of our goals might be to estimate a measure of the \\(y\\) variable we wish we could measure directly.\nMLE is better suited to this task - it asks us what the data generating process is that produced the observed data, and to build a model appropriate to the limited observation of that \\(y\\) variable.\nTo do so, we need to:\n\ndescribe the observed distribution of \\(y\\)\nconsider what we wish we could measure - this points to the key quantities of interest we want to derive from the model.\ndescribe \\(y\\) according to a probability distribution; write that distribution\nwrite a log-likelihood function appropriate to that probability distribution.\nparameterize the log-likelihood function such that we have a link distribution to map the linear prediction, \\(x\\beta\\), onto the probability space of \\(y\\).\n\n\n\n\nLet’s start with a \\(y\\) variable as follows:\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                y\n                Freq\n              \n        \n        \n        \n                \n                  0\n                  707\n                \n                \n                  1\n                  293\n                \n        \n      \n    \n\n\n\nThe variable, \\(y\\), takes on values of zero and one - it appears binomial; let’s write this in terms of the binomial distribution parameter \\(\\pi\\), so it takes on the value of one with probability \\(\\pi\\) and zero with probability \\(1-\\pi\\).\n\\[ y_i = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{} \\pi_{i}\\\\\n         0, & \\mbox{} 1-\\pi_{i}\n         \\end{array}\n     \\right.\\]\nThe likelihood of a single observation is:\n\\[ \\mathcal{L}(\\pi_i|y) = \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\]\nThis is a statement of the likelihood that any particular value of \\(\\pi\\) generated an observation. We want the likelihood any value of \\(\\pi\\) generated the entire dataset - that is, we want the joint likelihood of all the observations.\n\\[ \\mathcal{L}(\\pi|y) = \\prod_{i=1}^{n} \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\] recalling that a joint probability is the product of individual probabilities.\nLet’s take the log of this likelihood function - the natural log makes computation easier. Even for computers, this reduces the computational intensity especially with respect to extremely small decimals.\n\\[ \\ln \\mathcal{L}(\\pi|y) = \\sum_{i=1}^{n} y_i \\ln(\\pi_i) + (1-y_i) \\ln(1-\\pi_i) \\]\nWe can take this a step further and parameterize \\(\\pi\\) as a function of \\(X\\beta\\) where \\(X\\) is the matrix of predictors and \\(\\beta\\) is the vector of coefficients such that\n\\[ \\pi_i = F({X_i\\beta}) \\]\nSo our estimate of the binomial probability is a function of the linear predictor \\(X\\beta\\). We map \\(X\\beta\\) onto the probability space \\(\\pi\\) using a link function, \\(F\\). The most common link functions are the logistic (logit model), and the standard normal (probit model). Let’s write the logit link:\n\\[ \\pi_i = \\frac{1}{1 + e^{-(X_i\\beta)}} \\]\nand now let’s write this all in the log-likelihood function:\n\\[ \\ln \\mathcal{L}(\\beta|y) = \\sum_{i=1}^{n} y_i \\ln(\\frac{1}{1 + e^{-(X_i\\beta)}}) + (1-y_i) \\ln(1-\\frac{1}{1 + e^{-(X_i\\beta)}}) \\]\nThis is the logit log-likelihood function for a binary \\(y\\) variable.\n\n\n\nHow do we solve this function for the data \\(y\\)? Put differently, what is the value of \\(\\pi\\) that most likely generated the data, \\(y\\)? There are a few ways to do this - one instructive method is to use a grid search, where we calculate the log-likelihood for a range of values of \\(\\pi\\) and find the value that maximizes the log-likelihood. We can repeat this to the desired level of specificity, progressively narrowing the grid. In this example, we’ll just do one search out to 3 decimal places. The steps are these:\n\ngenerate the range of candidate values of our parameter \\(\\pi\\) to plug into the log-likelihood function.\nplug each value into the log-likelihood function to compute the log-likelihood for that value.\nidentify which value of \\(\\pi\\) maximizes the log-likelihood.\n\nHere’s code to do this:\n\n\ncode\n# generate a vector of values of pi \n\npi_trials &lt;- seq(0, 1, by = 0.001)\n\n# write the log-likelihood function\n\nlog_likelihood &lt;- function(pi_trials, y) {\n  sum(y * log(pi_trials) + (1 - y) * log(1 - pi_trials))\n}\n\n# Calculate log-likelihood for each pi value\n\nll_values &lt;- sapply(pi_trials, log_likelihood, y = y)\n\n# Find the pi value that maximizes the log-likelihood\n\npi_hat &lt;- pi_trials[which.max(ll_values)]\nprint(pi_hat)\n\n\n[1] 0.293\n\n\nYou’ll note we recover the sample mean of \\(y\\). Let’s plot the log-likelihood against the values of \\(\\pi\\) to visualize the maximum.\n\n\ncode\n# Plot log-likelihood against pi\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(data.frame(pi = pi_trials, ll = ll_values), \"line\", hcaes(x = pi, y = ll)) %&gt;%\n  hc_title(text = \"Grid search - maximizing the log-likelihood\") %&gt;%\n  hc_xAxis(title = list(text = \"Pi\"), \n           plotLines = list(\n    list(color =\"red\", value = pi_hat ))) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_legend(enabled = FALSE) %&gt;%\n  hc_colors(bucolors)\n\n\n\n\n\n\n\n\n\nGrid searches are not sufficient for multivariate models, especially where we also need measures of uncertainty. Here, we turn to numerical optimization.\nNumerical optimization is a field aimed at finding “best” outcomes or answers depending on some set of criteria - often, the “best” is the maximum or minimum of some function - that’s the case in maximum likelihood.\n\n\nThere are lots of ways to optimize a function - in statistical software, most of them derive from Newton’s iterative method. A very common application of this is the Newton-Raphson method. Newton-Raphson is an iterative process that starts with an initial guess for the parameter \\(\\pi\\) (or the vector of unknowns, \\(\\beta\\)) and updates it in the direction of the maximum until it converges to the maximum. At each step, it computes the first and second derivatives of the likelihood function, then uses these to update the parameter estimate(s). The update rule is:\n\\[ \\pi_{new} = \\pi_{old} - H(\\pi)^{-1}*g(\\pi) \\]\nSo the new estimate of the parameter is the old estimate minus the first derivative (gradient) of the log-likelihood function pre-multiplied by the Hessian (second derivative) of the log-likelihood function. Here, \\(g(\\pi)\\) is the gradient of the log-likelihood function and \\(H(\\pi)\\) is the Hessian of the log-likelihood function. The algorithm iterates this process until the change in \\(\\pi\\) is sufficiently small, below a specified threshold. Since \\(g\\) and \\(H\\) are matrices, you should see how this straightforwardly applied to a multivariate regression.\nLet’s write the Newton-Raphson algorithm ourselves - take a look at the code chunk below. You’ll see it produces the same result as the grid search; the sample frequency of \\(y\\).\n\n\n\n\n\ncode\n# use same data generated above; same LLF as above, aiming to recover pi\n\n#write the gradient - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(pi, y) {\n  sum(y / pi - (1 - y) / (1 - pi))\n}\n\n#write the Hessian - matrix of partial second derivatives of the log-likelihood function.\n\nhessian &lt;- function(pi, y) {\n  sum(-y / pi^2 - (1 - y) / (1 - pi)^2)\n}\n\n#declare a starting value for pi\n\npi &lt;- 0.5\n\n#set convergence criteria - when new pi is within 1e-6 of old pi, stop iterating\n\ntol &lt;- 1e-6\n\n#initialize iteration counter, set maximum iterations; how many times will we do this before we stop if we don't converge prior? \n\niter &lt;- 0\nmax_iter &lt;- 100\n\n#iterate the Newton-Raphson algorithm\n\nfor (iter in 1:max_iter) {\n  # Compute gradient  \n  grad &lt;- gradient(pi, y)\n  \n  # Compute Hessian\n  hess &lt;- hessian(pi, y)\n  \n  # Update pi\n  pi_new &lt;- pi - (grad / hess)\n  \n  pi &lt;- pi_new\n  \n  # Check for convergence\n  if (abs(pi_new - pi) &lt; tol) {\n    break\n  }\n  \n  iter &lt;- iter + 1\n}\n\n#after convergence, print the result\n\nprint(pi)\n\n\n[1] 0.293\n\n\nSo what’s gone on here? We have 1000 observations of a binary variable; maximum likelihood is answering the question “what is the value of the parameter \\(\\pi\\) that makes the observed data most likely?” To get that answer, we have tried candidate values of \\(\\pi\\) to see which one maximizes the log-likelihood function.\nWe’ve maximized the function two ways - using a grid search process, and using the Newton-Raphson algorithm. The latter is more efficient and is the basis for most optimization algorithms in statistical software, and easy to adapt to estimate multiple parameters.\nHere’s one more approach also using the Newton-Raphson method. It calls the maxLik package to do the optimization instead of our having to write the algorithm ourselves.\n\n\ncode\nlibrary(maxLik)\n# Write the binomial log-likelihood function\nlog_likelihood &lt;- function(pi, y) {\n  sum(y * log(pi) + (1 - y) * log(1 - pi))\n}\n\n#maximize the function using maxLik\n\nm1m &lt;- maxLik(y=y, log_likelihood, start = c(0.5), method = \"NR\") #Newton-Raphson method, starting value of 0.5\n\nsummary(m1m)\n\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 3 iterations\nReturn code 2: successive function values within tolerance limit (tol)\nLog-Likelihood: -604.816 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(&gt; t)    \n[1,]  0.29300    0.01441   20.34  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\nAgain, the value of \\(\\pi\\) that maximizes the log-likelihood is the same as the sample frequency of \\(y\\), which is 0.293.\n\n\n\nLet’s make this more realistic in terms of resembling the models we want to run by adding an intercept term and multiple predictors to the model - the likelihood now has to account for those \\(X\\) variables and maximize with respect to them.\n\ngenerate some data on \\(y\\), and \\(X\\)\nwrite the likelihood\nwrite the Newton-Raphson algorithm\napply to the data\n\n\n\ncode\n# Generate binary y, and two X variables for regression; coefficients are -1, 0.5, -0.5\n\nset.seed(8675309)\nn &lt;- 10000\nX &lt;- matrix(rnorm(n * 2), ncol = 2)\ntrue_beta &lt;- c(-1, 0.5, -0.5)\nz &lt;- cbind(1, X) %*% true_beta\nprob &lt;- 1 / (1 + exp(-z))\ny &lt;- rbinom(n, 1, prob)\n\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson iterative algorithm\n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # Add intercept term to X if not already present\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#write the gradient function - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- plogis(X %*% beta)\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  k &lt;- ncol(X)\n  \n# set starting values for k columns of data (betas)\n  beta &lt;- rep(0, k) \n  \n# Store gradients and log-likelihoods for analysis\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n# iterate NR \n  for (iter in 1:max_iter) {\n    # compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # compute Hessian\n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # check for convergence\n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n    \n    # compute standard errors for this model by taking the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  }\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#html table comparing glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\", caption=\"Comparing GLM and Dave Estimates\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  column_spec(3, border_right = T) %&gt;%\n  column_spec(4, border_right = T) %&gt;%\n  collapse_rows(columns = 1:3, valign = \"top\") \n\n\n\n\nTable 1: Comparing GLM and Dave Estimates\n\n\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-0.9531826\n-0.9531826\n0.0236756\n0.0236748\n\n\nX1\n0.4938831\n0.4938831\n0.0238303\n0.0238295\n\n\nX2\n-0.4719703\n-0.4719703\n0.0240294\n0.0240286\n\n\n\n\n\n\n\n\n\n\n\nTable 1 compares the coefficients and standard errors from the Newton-Raphson algorithm to the coefficients and standard errors from the glm function. The estimates are the same, the standard errors very close.\nAlright, lots of fun here. Now, let’s use the program above to estimate a logit model using the democratic peace data.\n\n\n\nTable 2 compares the coefficients and standard errors from the program we wrote to those produced by the glm function. The regression here is a logit model predicting the onset of a militarized dispute as a function of the logged capabilities ratio, an indicator of whether the pair of states share a border, and the lowest democracy (polity) score in the pair.\n\n\ncode\ndp &lt;- read.csv(\"/users/dave/documents/teaching/606J-mle/2022/exercises/ex1/dp.csv\", header=TRUE)\n\n# for the log_likelihood function below, define y as the variable \"dispute\" in the dp data frame, and x1, x2, and x3 as the variables \"lncaprat\", \"border\", and \"deml\" in the dp data frame, respectively.\ndp$lncaprat &lt;- log(dp$caprat)\nX &lt;- as.matrix(dp[, c(\"lncaprat\", \"border\", \"deml\")])\ny &lt;- as.vector(dp$dispute)\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson \n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # check for constant; add one if necessary\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#write the gradient function -  partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- 1 / (1 + exp(-X %*% beta))\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  p &lt;- ncol(X)\n  \n  # set starting values for beta\n  beta &lt;- rep(0, p)\n  \n  # Store gradients and log-likelihoods\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n  for (iter in 1:max_iter) {\n    # Compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # Compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # Compute Hessian - 2nd partial derivatives \n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # Update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # Check for convergence - end if change in beta is less than tolerance value \n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n  }\n\n#compute standard errors for this model; take the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n# logit$coefficients\n# logit$st.errors\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#compare glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\", caption=\"Comparing GLM and Dave Estimates, Democratic Peace Model\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:3, valign = \"top\")\n\n\n\n\nTable 2: Comparing GLM and Dave Estimates, Democratic Peace Model\n\n\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-3.2407604\n-3.2407605\n0.1034544\n0.1034630\n\n\nXlncaprat\n-0.2028768\n-0.2028768\n0.0237847\n0.0237860\n\n\nXborder\n0.7827135\n0.7827135\n0.0852298\n0.0852337\n\n\nXdeml\n-0.0783467\n-0.0783467\n0.0068065\n0.0068078\n\n\n\n\n\n\n\n\n\n\n\nRecapping what we’ve done:\n\nWe wrote a log-likelihood function for a logistic regression model with multiple predictors.\nTo maximize the log-likelihood, we implemented a Newton-Raphson algorithm in the following steps:\n\nDefined the gradient and Hessian functions. The gradient is the vector of first partial derivatives of the log-likelihood, and the Hessian is the matrix of second partial derivatives.\nInitialized the beta coefficients and stored gradients and log-likelihoods.\nIterated the algorithm to update beta values.\nChecked for convergence based on the change in beta.\n\nOnce the model converged, we computed standard errors for the estimated coefficients by taking the square root of the main diagonal elements of the inverse negative Hessian.\n\n\n\n\nHere, you can see how the gradients change over each iteration.\n\n\ncode\n#plot gradient convergence\ngradients &lt;- do.call(cbind, logit$gradient_history)\ngradients &lt;- as.data.frame(gradients) %&gt;%\n  mutate(coef=c(\"Intercept\", \"lncaprat\", \"border\", \"deml\")) \n\ngradients &lt;- gradients %&gt;% \n  pivot_longer(-coef, names_to = \"iteration\", values_to = \"value\")\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(gradients, \"line\", hcaes(x=iteration, y=value, group=coef, color=coef)) %&gt;%\n  hc_title(text = \"Gradient Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Gradient\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\n\n\nAnd here, we can see how the log-likelihood changes over iterations:\n\n\ncode\n#plot ll convergence\n\nlls &lt;- as.data.frame(logit$ll_history) %&gt;%\n  mutate(iteration=1:nrow(.))\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(lls, \"line\", hcaes(x=iteration, y=`logit$ll_history`)) %&gt;%\n  hc_title(text = \"Log-Likelihood Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\n\n\nHere are two ways to recover the final log-likelihood from the glm model. The first uses the logLik function from the stats package. The second calculates the log-likelihood by hand - generating the predictions and plugging those into the log-likelihood function, then summing.\n\\[ LL = \\sum_{i=1}^{n} y_i \\ln(p) + (1-y_i) \\ln(1-p) \\]\n\n\ncode\n# use the logLik function from the stats package  \n\nllest &lt;- logLik(glm_fit)\n\n# or calculate it by hand - generate the predictions and plug those into the log-likelihood function, then sum.\n\npred &lt;- predict(glm_fit, type = \"response\")\nllestbyhand &lt;- sum(dp$dispute * log(pred) + (1 - dp$dispute) * log(1 - pred))\n\n\ndata.frame(llest, llestbyhand) %&gt;% \n  kable(\"html\", caption=\"Recovering the Log-Likelihood\" ) %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:2, valign = \"top\")\n\n\n\n\nRecovering the Log-Likelihood\n\n\nllest\nllestbyhand\n\n\n\n\n-3565.219\n-3565.219"
  },
  {
    "objectID": "llfmax24.html#motivating-likelihood",
    "href": "llfmax24.html#motivating-likelihood",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "Let’s recall the motivation for turning to ML. The data we have are not suitable for OLS because our observation of the data is limited. For instance, we observe a binary variable \\([0,1]\\) rather than the underlying continuous probability beneath it. If we could observe and measure that probability, we’d do so and perhaps use OLS to estimate a model. But since we only observe \\([0,1]\\), our observation is limited.\nOLS is poorly suited to model this variable especially if one of our goals is to generate predictions of that underlying probability. That is, one of our goals might be to estimate a measure of the \\(y\\) variable we wish we could measure directly.\nMLE is better suited to this task - it asks us what the data generating process is that produced the observed data, and to build a model appropriate to the limited observation of that \\(y\\) variable.\nTo do so, we need to:\n\ndescribe the observed distribution of \\(y\\)\nconsider what we wish we could measure - this points to the key quantities of interest we want to derive from the model.\ndescribe \\(y\\) according to a probability distribution; write that distribution\nwrite a log-likelihood function appropriate to that probability distribution.\nparameterize the log-likelihood function such that we have a link distribution to map the linear prediction, \\(x\\beta\\), onto the probability space of \\(y\\)."
  },
  {
    "objectID": "llfmax24.html#binary-y-variable",
    "href": "llfmax24.html#binary-y-variable",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "Let’s start with a \\(y\\) variable as follows:\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                y\n                Freq\n              \n        \n        \n        \n                \n                  0\n                  707\n                \n                \n                  1\n                  293\n                \n        \n      \n    \n\n\n\nThe variable, \\(y\\), takes on values of zero and one - it appears binomial; let’s write this in terms of the binomial distribution parameter \\(\\pi\\), so it takes on the value of one with probability \\(\\pi\\) and zero with probability \\(1-\\pi\\).\n\\[ y_i = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{} \\pi_{i}\\\\\n         0, & \\mbox{} 1-\\pi_{i}\n         \\end{array}\n     \\right.\\]\nThe likelihood of a single observation is:\n\\[ \\mathcal{L}(\\pi_i|y) = \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\]\nThis is a statement of the likelihood that any particular value of \\(\\pi\\) generated an observation. We want the likelihood any value of \\(\\pi\\) generated the entire dataset - that is, we want the joint likelihood of all the observations.\n\\[ \\mathcal{L}(\\pi|y) = \\prod_{i=1}^{n} \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\] recalling that a joint probability is the product of individual probabilities.\nLet’s take the log of this likelihood function - the natural log makes computation easier. Even for computers, this reduces the computational intensity especially with respect to extremely small decimals.\n\\[ \\ln \\mathcal{L}(\\pi|y) = \\sum_{i=1}^{n} y_i \\ln(\\pi_i) + (1-y_i) \\ln(1-\\pi_i) \\]\nWe can take this a step further and parameterize \\(\\pi\\) as a function of \\(X\\beta\\) where \\(X\\) is the matrix of predictors and \\(\\beta\\) is the vector of coefficients such that\n\\[ \\pi_i = F({X_i\\beta}) \\]\nSo our estimate of the binomial probability is a function of the linear predictor \\(X\\beta\\). We map \\(X\\beta\\) onto the probability space \\(\\pi\\) using a link function, \\(F\\). The most common link functions are the logistic (logit model), and the standard normal (probit model). Let’s write the logit link:\n\\[ \\pi_i = \\frac{1}{1 + e^{-(X_i\\beta)}} \\]\nand now let’s write this all in the log-likelihood function:\n\\[ \\ln \\mathcal{L}(\\beta|y) = \\sum_{i=1}^{n} y_i \\ln(\\frac{1}{1 + e^{-(X_i\\beta)}}) + (1-y_i) \\ln(1-\\frac{1}{1 + e^{-(X_i\\beta)}}) \\]\nThis is the logit log-likelihood function for a binary \\(y\\) variable."
  },
  {
    "objectID": "llfmax24.html#maximizing-the-likelihood---grid-search",
    "href": "llfmax24.html#maximizing-the-likelihood---grid-search",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "How do we solve this function for the data \\(y\\)? Put differently, what is the value of \\(\\pi\\) that most likely generated the data, \\(y\\)? There are a few ways to do this - one instructive method is to use a grid search, where we calculate the log-likelihood for a range of values of \\(\\pi\\) and find the value that maximizes the log-likelihood. We can repeat this to the desired level of specificity, progressively narrowing the grid. In this example, we’ll just do one search out to 3 decimal places. The steps are these:\n\ngenerate the range of candidate values of our parameter \\(\\pi\\) to plug into the log-likelihood function.\nplug each value into the log-likelihood function to compute the log-likelihood for that value.\nidentify which value of \\(\\pi\\) maximizes the log-likelihood.\n\nHere’s code to do this:\n\n\ncode\n# generate a vector of values of pi \n\npi_trials &lt;- seq(0, 1, by = 0.001)\n\n# write the log-likelihood function\n\nlog_likelihood &lt;- function(pi_trials, y) {\n  sum(y * log(pi_trials) + (1 - y) * log(1 - pi_trials))\n}\n\n# Calculate log-likelihood for each pi value\n\nll_values &lt;- sapply(pi_trials, log_likelihood, y = y)\n\n# Find the pi value that maximizes the log-likelihood\n\npi_hat &lt;- pi_trials[which.max(ll_values)]\nprint(pi_hat)\n\n\n[1] 0.293\n\n\nYou’ll note we recover the sample mean of \\(y\\). Let’s plot the log-likelihood against the values of \\(\\pi\\) to visualize the maximum.\n\n\ncode\n# Plot log-likelihood against pi\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(data.frame(pi = pi_trials, ll = ll_values), \"line\", hcaes(x = pi, y = ll)) %&gt;%\n  hc_title(text = \"Grid search - maximizing the log-likelihood\") %&gt;%\n  hc_xAxis(title = list(text = \"Pi\"), \n           plotLines = list(\n    list(color =\"red\", value = pi_hat ))) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_legend(enabled = FALSE) %&gt;%\n  hc_colors(bucolors)"
  },
  {
    "objectID": "llfmax24.html#optimization",
    "href": "llfmax24.html#optimization",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "Grid searches are not sufficient for multivariate models, especially where we also need measures of uncertainty. Here, we turn to numerical optimization.\nNumerical optimization is a field aimed at finding “best” outcomes or answers depending on some set of criteria - often, the “best” is the maximum or minimum of some function - that’s the case in maximum likelihood.\n\n\nThere are lots of ways to optimize a function - in statistical software, most of them derive from Newton’s iterative method. A very common application of this is the Newton-Raphson method. Newton-Raphson is an iterative process that starts with an initial guess for the parameter \\(\\pi\\) (or the vector of unknowns, \\(\\beta\\)) and updates it in the direction of the maximum until it converges to the maximum. At each step, it computes the first and second derivatives of the likelihood function, then uses these to update the parameter estimate(s). The update rule is:\n\\[ \\pi_{new} = \\pi_{old} - H(\\pi)^{-1}*g(\\pi) \\]\nSo the new estimate of the parameter is the old estimate minus the first derivative (gradient) of the log-likelihood function pre-multiplied by the Hessian (second derivative) of the log-likelihood function. Here, \\(g(\\pi)\\) is the gradient of the log-likelihood function and \\(H(\\pi)\\) is the Hessian of the log-likelihood function. The algorithm iterates this process until the change in \\(\\pi\\) is sufficiently small, below a specified threshold. Since \\(g\\) and \\(H\\) are matrices, you should see how this straightforwardly applied to a multivariate regression.\nLet’s write the Newton-Raphson algorithm ourselves - take a look at the code chunk below. You’ll see it produces the same result as the grid search; the sample frequency of \\(y\\).\n\n\n\n\n\ncode\n# use same data generated above; same LLF as above, aiming to recover pi\n\n#write the gradient - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(pi, y) {\n  sum(y / pi - (1 - y) / (1 - pi))\n}\n\n#write the Hessian - matrix of partial second derivatives of the log-likelihood function.\n\nhessian &lt;- function(pi, y) {\n  sum(-y / pi^2 - (1 - y) / (1 - pi)^2)\n}\n\n#declare a starting value for pi\n\npi &lt;- 0.5\n\n#set convergence criteria - when new pi is within 1e-6 of old pi, stop iterating\n\ntol &lt;- 1e-6\n\n#initialize iteration counter, set maximum iterations; how many times will we do this before we stop if we don't converge prior? \n\niter &lt;- 0\nmax_iter &lt;- 100\n\n#iterate the Newton-Raphson algorithm\n\nfor (iter in 1:max_iter) {\n  # Compute gradient  \n  grad &lt;- gradient(pi, y)\n  \n  # Compute Hessian\n  hess &lt;- hessian(pi, y)\n  \n  # Update pi\n  pi_new &lt;- pi - (grad / hess)\n  \n  pi &lt;- pi_new\n  \n  # Check for convergence\n  if (abs(pi_new - pi) &lt; tol) {\n    break\n  }\n  \n  iter &lt;- iter + 1\n}\n\n#after convergence, print the result\n\nprint(pi)\n\n\n[1] 0.293\n\n\nSo what’s gone on here? We have 1000 observations of a binary variable; maximum likelihood is answering the question “what is the value of the parameter \\(\\pi\\) that makes the observed data most likely?” To get that answer, we have tried candidate values of \\(\\pi\\) to see which one maximizes the log-likelihood function.\nWe’ve maximized the function two ways - using a grid search process, and using the Newton-Raphson algorithm. The latter is more efficient and is the basis for most optimization algorithms in statistical software, and easy to adapt to estimate multiple parameters.\nHere’s one more approach also using the Newton-Raphson method. It calls the maxLik package to do the optimization instead of our having to write the algorithm ourselves.\n\n\ncode\nlibrary(maxLik)\n# Write the binomial log-likelihood function\nlog_likelihood &lt;- function(pi, y) {\n  sum(y * log(pi) + (1 - y) * log(1 - pi))\n}\n\n#maximize the function using maxLik\n\nm1m &lt;- maxLik(y=y, log_likelihood, start = c(0.5), method = \"NR\") #Newton-Raphson method, starting value of 0.5\n\nsummary(m1m)\n\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 3 iterations\nReturn code 2: successive function values within tolerance limit (tol)\nLog-Likelihood: -604.816 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(&gt; t)    \n[1,]  0.29300    0.01441   20.34  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\nAgain, the value of \\(\\pi\\) that maximizes the log-likelihood is the same as the sample frequency of \\(y\\), which is 0.293.\n\n\n\nLet’s make this more realistic in terms of resembling the models we want to run by adding an intercept term and multiple predictors to the model - the likelihood now has to account for those \\(X\\) variables and maximize with respect to them.\n\ngenerate some data on \\(y\\), and \\(X\\)\nwrite the likelihood\nwrite the Newton-Raphson algorithm\napply to the data\n\n\n\ncode\n# Generate binary y, and two X variables for regression; coefficients are -1, 0.5, -0.5\n\nset.seed(8675309)\nn &lt;- 10000\nX &lt;- matrix(rnorm(n * 2), ncol = 2)\ntrue_beta &lt;- c(-1, 0.5, -0.5)\nz &lt;- cbind(1, X) %*% true_beta\nprob &lt;- 1 / (1 + exp(-z))\ny &lt;- rbinom(n, 1, prob)\n\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson iterative algorithm\n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # Add intercept term to X if not already present\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#write the gradient function - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- plogis(X %*% beta)\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  k &lt;- ncol(X)\n  \n# set starting values for k columns of data (betas)\n  beta &lt;- rep(0, k) \n  \n# Store gradients and log-likelihoods for analysis\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n# iterate NR \n  for (iter in 1:max_iter) {\n    # compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # compute Hessian\n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # check for convergence\n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n    \n    # compute standard errors for this model by taking the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  }\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#html table comparing glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\", caption=\"Comparing GLM and Dave Estimates\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  column_spec(3, border_right = T) %&gt;%\n  column_spec(4, border_right = T) %&gt;%\n  collapse_rows(columns = 1:3, valign = \"top\") \n\n\n\n\nTable 1: Comparing GLM and Dave Estimates\n\n\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-0.9531826\n-0.9531826\n0.0236756\n0.0236748\n\n\nX1\n0.4938831\n0.4938831\n0.0238303\n0.0238295\n\n\nX2\n-0.4719703\n-0.4719703\n0.0240294\n0.0240286\n\n\n\n\n\n\n\n\n\n\n\nTable 1 compares the coefficients and standard errors from the Newton-Raphson algorithm to the coefficients and standard errors from the glm function. The estimates are the same, the standard errors very close.\nAlright, lots of fun here. Now, let’s use the program above to estimate a logit model using the democratic peace data.\n\n\n\nTable 2 compares the coefficients and standard errors from the program we wrote to those produced by the glm function. The regression here is a logit model predicting the onset of a militarized dispute as a function of the logged capabilities ratio, an indicator of whether the pair of states share a border, and the lowest democracy (polity) score in the pair.\n\n\ncode\ndp &lt;- read.csv(\"/users/dave/documents/teaching/606J-mle/2022/exercises/ex1/dp.csv\", header=TRUE)\n\n# for the log_likelihood function below, define y as the variable \"dispute\" in the dp data frame, and x1, x2, and x3 as the variables \"lncaprat\", \"border\", and \"deml\" in the dp data frame, respectively.\ndp$lncaprat &lt;- log(dp$caprat)\nX &lt;- as.matrix(dp[, c(\"lncaprat\", \"border\", \"deml\")])\ny &lt;- as.vector(dp$dispute)\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson \n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # check for constant; add one if necessary\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#write the gradient function -  partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- 1 / (1 + exp(-X %*% beta))\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  p &lt;- ncol(X)\n  \n  # set starting values for beta\n  beta &lt;- rep(0, p)\n  \n  # Store gradients and log-likelihoods\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n  for (iter in 1:max_iter) {\n    # Compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # Compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # Compute Hessian - 2nd partial derivatives \n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # Update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # Check for convergence - end if change in beta is less than tolerance value \n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n  }\n\n#compute standard errors for this model; take the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n# logit$coefficients\n# logit$st.errors\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#compare glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\", caption=\"Comparing GLM and Dave Estimates, Democratic Peace Model\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:3, valign = \"top\")\n\n\n\n\nTable 2: Comparing GLM and Dave Estimates, Democratic Peace Model\n\n\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-3.2407604\n-3.2407605\n0.1034544\n0.1034630\n\n\nXlncaprat\n-0.2028768\n-0.2028768\n0.0237847\n0.0237860\n\n\nXborder\n0.7827135\n0.7827135\n0.0852298\n0.0852337\n\n\nXdeml\n-0.0783467\n-0.0783467\n0.0068065\n0.0068078\n\n\n\n\n\n\n\n\n\n\n\nRecapping what we’ve done:\n\nWe wrote a log-likelihood function for a logistic regression model with multiple predictors.\nTo maximize the log-likelihood, we implemented a Newton-Raphson algorithm in the following steps:\n\nDefined the gradient and Hessian functions. The gradient is the vector of first partial derivatives of the log-likelihood, and the Hessian is the matrix of second partial derivatives.\nInitialized the beta coefficients and stored gradients and log-likelihoods.\nIterated the algorithm to update beta values.\nChecked for convergence based on the change in beta.\n\nOnce the model converged, we computed standard errors for the estimated coefficients by taking the square root of the main diagonal elements of the inverse negative Hessian.\n\n\n\n\nHere, you can see how the gradients change over each iteration.\n\n\ncode\n#plot gradient convergence\ngradients &lt;- do.call(cbind, logit$gradient_history)\ngradients &lt;- as.data.frame(gradients) %&gt;%\n  mutate(coef=c(\"Intercept\", \"lncaprat\", \"border\", \"deml\")) \n\ngradients &lt;- gradients %&gt;% \n  pivot_longer(-coef, names_to = \"iteration\", values_to = \"value\")\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(gradients, \"line\", hcaes(x=iteration, y=value, group=coef, color=coef)) %&gt;%\n  hc_title(text = \"Gradient Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Gradient\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\n\n\nAnd here, we can see how the log-likelihood changes over iterations:\n\n\ncode\n#plot ll convergence\n\nlls &lt;- as.data.frame(logit$ll_history) %&gt;%\n  mutate(iteration=1:nrow(.))\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(lls, \"line\", hcaes(x=iteration, y=`logit$ll_history`)) %&gt;%\n  hc_title(text = \"Log-Likelihood Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\n\n\nHere are two ways to recover the final log-likelihood from the glm model. The first uses the logLik function from the stats package. The second calculates the log-likelihood by hand - generating the predictions and plugging those into the log-likelihood function, then summing.\n\\[ LL = \\sum_{i=1}^{n} y_i \\ln(p) + (1-y_i) \\ln(1-p) \\]\n\n\ncode\n# use the logLik function from the stats package  \n\nllest &lt;- logLik(glm_fit)\n\n# or calculate it by hand - generate the predictions and plug those into the log-likelihood function, then sum.\n\npred &lt;- predict(glm_fit, type = \"response\")\nllestbyhand &lt;- sum(dp$dispute * log(pred) + (1 - dp$dispute) * log(1 - pred))\n\n\ndata.frame(llest, llestbyhand) %&gt;% \n  kable(\"html\", caption=\"Recovering the Log-Likelihood\" ) %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:2, valign = \"top\")\n\n\n\n\nRecovering the Log-Likelihood\n\n\nllest\nllestbyhand\n\n\n\n\n-3565.219\n-3565.219"
  },
  {
    "objectID": "binaryextensions124.html",
    "href": "binaryextensions124.html",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "",
    "text": "symmetry - what are the implications of using symmetric links, especially given data on \\(y\\)?\nclassification\nmodel evaluation and fit\nrareness - what happens when there are few events?\n\nWe’re going to start with symmetry, then thinking about prediction and classification and fit."
  },
  {
    "objectID": "binaryextensions124.html#symmetry-and-asymmetry",
    "href": "binaryextensions124.html#symmetry-and-asymmetry",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Symmetry and Asymmetry",
    "text": "Symmetry and Asymmetry\nCompare three CDFs: the logistic, the clog-log, and a skewed logit function. The logistic is symmetric, the clog-log and skewed logit are not. You should notice the probability associated with x=0 for each function - the logistic is .5, the clog-log is about .64, and the skewed logit is about than .71.\nFor skewed binary \\(y\\) variables, it could be that one of these CDFs is a more appropriate link function than the symmetric logistic or normal. Implementing these merely requires substituting the appropriate CDF into the log-likelihood function.\n\n\ncode\n# Load required libraries\nlibrary(highcharter)\nlibrary(dplyr)\n\n# Binghamton University colors\nbinghamton_green &lt;- \"#005A43\"\nbinghamton_gray &lt;- \"#707070\"\nbinghamton_yellow &lt;- \"#FFC726\"\n\n# Generate data\nx &lt;- seq(-5, 5, length.out = 1000)\nlogistic_cdf &lt;- plogis(x)\ncloglog_cdf &lt;- 1 - exp(-exp(x))\n\n# Skewed logit function (shape parameter = 0.5)\n\nskewed_logit_cdf &lt;- 1 / (1 + exp(-x)) ^ 0.5 \n\n# Create data frame\ndf &lt;- data.frame(x = x, logistic = logistic_cdf, cloglog = cloglog_cdf, skewed_logit = skewed_logit_cdf)\n\n# Create the highchart\nhc &lt;- highchart() %&gt;%\n  hc_chart(type = \"line\") %&gt;%\n  hc_title(text = \"Comparison of CDFs: Logistic, Clog-log, and Skewed Logit\") %&gt;%\n  hc_xAxis(\n    title = list(text = \"x\"),\n    plotLines = list(\n      list(\n        color = \"#999\",\n        width = 2,\n        value = 0,\n        zIndex = 3,\n        label = list(text = \"x = 0\")\n      )\n    )\n  ) %&gt;%\n  hc_yAxis(\n    title = list(text = \"Cumulative Probability\"),\n    plotLines = list(\n      list(\n        color = \"#999\",\n        width = 2,\n        value = 0.5,\n        zIndex = 3,\n        label = list(text = \"y = 0.5\")\n      )\n    )\n  ) %&gt;%\n  hc_tooltip(\n    shared = TRUE,\n    formatter = JS(\"function() {\n      return 'x: ' + this.x.toFixed(4) + '&lt;br&gt;' +\n             'Logistic: ' + this.points[0].y.toFixed(4) + '&lt;br&gt;' +\n             'Clog-log: ' + this.points[1].y.toFixed(4) + '&lt;br&gt;' +\n             'Skewed Logit: ' + this.points[2].y.toFixed(4);\n    }\")\n  ) %&gt;%\n  hc_plotOptions(series = list(marker = list(enabled = FALSE))) %&gt;%\n  \n  # Add logistic CDF\n  hc_add_series(\n    data = df,\n    type = \"line\",\n    name = \"Logistic\",\n    color = binghamton_green,\n    hcaes(x = x, y = logistic)\n  ) %&gt;%\n  \n  # Add clog-log CDF\n  hc_add_series(\n    data = df,\n    type = \"line\",\n    name = \"Clog-log\",\n    color = binghamton_gray,\n    hcaes(x = x, y = cloglog)\n  ) %&gt;%\n  \n  # Add skewed logit CDF\n  hc_add_series(\n    data = df,\n    type = \"line\",\n    name = \"Skewed Logit\",\n    color = binghamton_yellow,\n    hcaes(x = x, y = skewed_logit)\n  )\n\n# Display the chart\nhc"
  },
  {
    "objectID": "binaryextensions124.html#skewed-logit",
    "href": "binaryextensions124.html#skewed-logit",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Skewed Logit",
    "text": "Skewed Logit\nNagler (1994) proposes the skewed logit (scobit) model - it’s a binary response model, the usual LLF, with a different link (the Burr-10):\n\\[ Pr(y=1) = \\frac{1}{(1+e^{-x\\beta})^\\alpha}\\]\nNote that if \\(\\alpha=1\\) this is the logistic CDF. If it is less than 1, the fastest rate of change is at \\(Pr(y =1 &lt; .5)\\); when greater than 1, the fastest rate of change, is at \\(Pr(y=1 &gt; .5)\\)\nNagler’s logic is that symmetric links require the assumption that individuals in the model are most sensitive to the effects of the \\(X\\) variables at or around \\(Pr(y=1) = .5\\). Looking at the (symmetric) logit curve above, you can see that’s where the derivative with respect to changes in \\(x\\) is greatest. If \\(y\\) is about half ones, half zeros, this may make sense - but often, we have \\(y\\) variables that are not symmetrically distributed like this. It makes sense in such cases not to assume the fastest rate of change, and the transition point from zero to one, is at \\(Pr(y=1) = .5\\).\nThe scobit model allows us to estimate the \\(\\alpha\\) parameter, which tells us where the fastest rate of change is in the CDF - that is, the transition point is an empirical question, not an assumption.\nThe model appears rarely in the political science literature; a Google Scholar search indicates most of its use is transportation analysis. A cursory survey also indicates the scobit estimates are often not that different from logit estimates. Estimation sometimes is funky insofar as we cannot always tell if changes in the likelihood are due to changes in \\(\\beta\\) or in \\(\\alpha\\)."
  },
  {
    "objectID": "binaryextensions124.html#skewed-logit-cdfs",
    "href": "binaryextensions124.html#skewed-logit-cdfs",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Skewed Logit CDFs",
    "text": "Skewed Logit CDFs\n\\includegraphics&lt;1&gt;[scale=.80]{/Users/dave/Documents/teaching/606J-mle/2022/slides/L3_binaryextensions/scobit22.pdf}"
  },
  {
    "objectID": "binaryextensions124.html#symmetry-1",
    "href": "binaryextensions124.html#symmetry-1",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Symmetry",
    "text": "Symmetry\nThe big point here is not that we should or should not use the scobit, but that we need to be very aware that the assumption in models with symmetric links is that the biggest effect of an \\(x\\) variable is at \\(Pr(y=1) = 0.5\\) which is where \\(x\\beta=0\\)."
  },
  {
    "objectID": "binaryextensions124.html#why-does-symmetry-matter",
    "href": "binaryextensions124.html#why-does-symmetry-matter",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Why does symmetry matter?",
    "text": "Why does symmetry matter?\n\nsymmetry determines where the greatest effect of \\(x\\) is.\nsymmetry ensures rates of change above and below \\(x=0\\) are the same as they approach the limits.\nsymmetry implies the theoretical threshold, \\(\\tau\\), separating observed zeros and ones is \\(\\tau=0.5\\).\nif we want to use the model to generate predicted values of \\(y\\) (rather than \\(y^*\\)), we need some threshold for classification.\n\nSome (very few) questions naturally link to a clear threshold like 0.5 …election outcomes? But …are we measuring the correct outcome variable?"
  },
  {
    "objectID": "binaryextensions124.html#confusion-matrix",
    "href": "binaryextensions124.html#confusion-matrix",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nThe “confusion matrix” (I didn’t make this up) illustrates that intersection and identifies where our classification is “confused.”\n\n\n\n\ncode\nlibrary(kableExtra)\n# opts &lt;- options(knitr.kable.NA = \"\")\ndf &lt;- data.frame(\"Predicted Positive\" = c(\"True Positive\", \"False Positive\"), \"Predicted Negative\" = c(\"False Positive\", \"True Negative\"), \"Rate\" = c(\"TPR=TP/P\", \"FPR=FP/N\"))\nrow.names(df) &lt;- c(\"Observed Positive\", \"Observed Negative\")\n\ntbl &lt;- kbl(df, align = rep('c', 4)) |&gt; \n  column_spec(2:4, border_left = T) |&gt; \n  column_spec(1,  bold=T) |&gt;\n  column_spec(1:4, width = \"2cm\", color = 'white', background = '#005A43', include_thead = TRUE) |&gt;\n  column_spec(2:4, background = \"inherit\", color=\"inherit\") |&gt;\n  row_spec(1, extra_css = \"border-bottom: 1px solid\") |&gt;\n  kable_paper(\"hover\", bootstrap_options = c(\"condensed\", \"responsive\"), full_width = F, font_size = 20)\ntbl\n\n\n\n\n\n\nPredicted.Positive\nPredicted.Negative\nRate\n\n\n\n\nObserved Positive\nTrue Positive\nFalse Positive\nTPR=TP/P\n\n\nObserved Negative\nFalse Positive\nTrue Negative\nFPR=FP/N\n\n\n\n\n\n\n\n\n\n\n\nTrue Positive Rate: correctly classify positive outcomes. This is often called “sensitivity.”\nFalse Positive Rate: we incorrectly classify negative outcomes (\\(y=0\\)) as positive (\\(y=1\\)). This is often called “1-specificity.” Specificity is the True Negative Rate, or the probability of correctly classifying a negative outcome (\\(y=0\\))."
  },
  {
    "objectID": "binaryextensions124.html#using-the-confusion-matrix-to-measure-model-fit",
    "href": "binaryextensions124.html#using-the-confusion-matrix-to-measure-model-fit",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Using the Confusion Matrix to Measure Model Fit",
    "text": "Using the Confusion Matrix to Measure Model Fit\nSo here’s the deal:\n\nestimate the model.\ngenerate the predicted probability \\(y=1\\) for each observation.\nassume a threshold separating zeros and ones; usually \\(\\tau=0.5\\).\nif \\(Pr(y=1 \\geq 0.5)\\), predict a positive outcome (predict \\(y=1\\)).\nif \\(Pr(y=0 &lt; 0.5)\\), predict a negative outcome (predict \\(y=0\\)).\nusing the observed and predicted outcomes, generate a confusion table, and compute measures of fit like “percent correctly predicted” (PCP) and “proportional reduction of error” (PRE).\n\n\nPercent Correctly Predicted (PCP)\nOne thing we can do is sum the main diagonal and divide by the estimation sample: \\((TP+TN)/N\\). This gives us the Percent Correctly Predicted (PCP).\n\n\nProportional Reduction of Error (PRE)}\nA second thing is to compute the Proportional Reduction of Error (PRE) - the difference between the correct predictions of the model versus a null model usually the unconditional frequency of \\(y\\) - in other words, we use the Percent in the Modal Category (PCM) of the \\(y\\) variable.\n\\[\\frac{correct_{\\text{informed}} - correct_{\\text{null}}}{N - correct_{\\text{null}}} \\]\nor\n\\[\\frac{PCP- PMC}{1-PMC} \\]\n\n\nExample: NAFTA vote, 1993\nSo here’s a model predicting members of the US House votes for or against the NAFTA treaty in 1993: 1 is a “yes” vote; the \\(x\\) variables are party (Democrat) and an ideology score.\n\n\ncode\nnafta &lt;- read.csv(\"NAFTA.csv\")\n\nnaftamodel &lt;- glm(vote ~ democrat + cope93, data=nafta, family=binomial(link=\"logit\"))\n\nstargazer(naftamodel, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: NAFTA vote\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Party (Democrat=1)\", \"Ideology (COPE score)\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: NAFTA vote\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParty (Democrat=1)\n\n\n1.990*** (0.504)\n\n\n\n\nIdeology (COPE score)\n\n\n-0.054*** (0.007)\n\n\n\n\nConstant\n\n\n2.380*** (0.272)\n\n\n\n\n\n\n\n\nObservations\n\n\n434\n\n\n\n\nAkaike Inf. Crit.\n\n\n465.904\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\nAnd here’s the confusion matrix from that model assuming a threshold of \\(\\tau=.5\\) - it compares the observed vote frequency against how we classify our predictions based on a predicted probability greater than or less than 0.5. This is generated using the caret package in R.\n\n\ncode\n# Load required libraries\nlibrary(caret)\n\n# Assuming you have a fitted GLM object called 'glm_model'\n# and test data 'test_data' with actual outcomes in 'test_data$actual'\n\ntest_data &lt;- nafta %&gt;% dplyr::select(vote, democrat, cope93) %&gt;% mutate(actual = vote)\n# Make predictions on the test data\npredictions &lt;- predict(naftamodel, newdata = test_data, type = \"response\")\n\n# Create confusion matrix\n# You may need to adjust the threshold (default is 0.5)\npredicted_classes &lt;- ifelse(predictions &gt; 0.5, 1, 0)\nconf_matrix &lt;- confusionMatrix(factor(predicted_classes), factor(test_data$actual))\n\n# Print confusion matrix\nprint(conf_matrix)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 160  77\n         1  40 157\n                                         \n               Accuracy : 0.7304         \n                 95% CI : (0.686, 0.7716)\n    No Information Rate : 0.5392         \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      \n                                         \n                  Kappa : 0.4647         \n                                         \n Mcnemar's Test P-Value : 0.0008741      \n                                         \n            Sensitivity : 0.8000         \n            Specificity : 0.6709         \n         Pos Pred Value : 0.6751         \n         Neg Pred Value : 0.7970         \n             Prevalence : 0.4608         \n         Detection Rate : 0.3687         \n   Detection Prevalence : 0.5461         \n      Balanced Accuracy : 0.7355         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\nYou should see the main diagonal presents the number of correct predictions - the off-diagonal elements are the incorrect predictions. If we sum the main diagonal and divide by \\(N\\), we get the Percent Correctly Predicted (PCP). In this case, the PCP is 0.735 - 73.5% of the votes are correctly predicted.\nThis all depends on the threshold (.5) - in the case of a Congressional vote, especially a relatively close vote like this one, the threshold might not be crazy. But it might be in other cases, and arbitrarily choosing a value for \\(\\tau\\) is problematic. So another approach is to compute the ROC curve.\nWhat makes this work relatively well in the NAFTA context? As you’ll see below, it works less well in the democratic peace models.\n\n\nExample: Democratic Peace\nHere’s a basic democratic peace model:\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\ndpm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n\nstargazer(dpm1, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\nLet’s generate the confusion matrix for the democratic peace model, threshold set at \\(\\tau =0.5\\):\n\n\ncode\nlibrary(kableExtra)\n# opts &lt;- options(knitr.kable.NA = \"\")\ndf &lt;- data.frame(\"Predicted Positive\" = c(\"0\", \"0\"), \"Predicted Negative\" = c(\"897\", \"19,245\"), \"Rate\" = c(\"TPR=0/897=0\", \"FPR=0/19,245=0\"))\nrow.names(df) &lt;- c(\"Observed Positive\", \"Observed Negative\")\n\ntbl &lt;- kbl(df, align = rep('c', 4)) |&gt; \n  column_spec(2:4, border_left = T) |&gt; \n  column_spec(1,  bold=T) |&gt;\n  column_spec(1:4, width = \"2cm\", color = 'white', background = '#005A43', include_thead = TRUE) |&gt;\n  column_spec(2:4, background = \"inherit\", color=\"inherit\") |&gt;\n  row_spec(1, extra_css = \"border-bottom: 1px solid\") |&gt;\n  kable_paper(\"hover\", bootstrap_options = c(\"condensed\", \"responsive\"), full_width = F, font_size = 20)\ntbl\n\n\n\n\n\n\nPredicted.Positive\nPredicted.Negative\nRate\n\n\n\n\nObserved Positive\n0\n897\nTPR=0/897=0\n\n\nObserved Negative\n0\n19,245\nFPR=0/19,245=0\n\n\n\n\n\n\n\nThe problem is \\(\\tau\\); at \\(\\tau=.5\\) we get none of the ones correct. Here’s the democratic peace at \\(\\tau = 0.1\\):\n\n\ncode\nlibrary(kableExtra)\n# opts &lt;- options(knitr.kable.NA = \"\")\ndf &lt;- data.frame(\"Predicted Positive\" = c(\"303\", \"1,764\"), \"Predicted Negative\" = c(\"594\", \"17,481\"), \"Rate\" = c(\"TPR=303/897=.34\", \"FPR=1,764/19,245=0.092\"))\nrow.names(df) &lt;- c(\"Observed Positive\", \"Observed Negative\")\n\ntbl &lt;- kbl(df, align = rep('c', 4)) |&gt; \n  column_spec(2:4, border_left = T) |&gt; \n  column_spec(1,  bold=T) |&gt;\n  column_spec(1:4, width = \"2cm\", color = 'white', background = '#005A43', include_thead = TRUE) |&gt;\n  column_spec(2:4, background = \"inherit\", color=\"inherit\") |&gt;\n  row_spec(1, extra_css = \"border-bottom: 1px solid\") |&gt;\n  kable_paper(\"hover\", bootstrap_options = c(\"condensed\", \"responsive\"), full_width = F, font_size = 20)\ntbl\n\n\n\n\n\n\nPredicted.Positive\nPredicted.Negative\nRate\n\n\n\n\nObserved Positive\n303\n594\nTPR=303/897=.34\n\n\nObserved Negative\n1,764\n17,481\nFPR=1,764/19,245=0.092\n\n\n\n\n\n\n\nAt \\(\\tau = 0.1\\), we get a TPR = 0.34 and FPR = 0.092. The PCP = (303+17481)/20142 = 0.88 …which is lower, but we get about 1/3 of the ones correct."
  },
  {
    "objectID": "binaryextensions124.html#percent-correctly-predicted-pcp",
    "href": "binaryextensions124.html#percent-correctly-predicted-pcp",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Percent Correctly Predicted (PCP)",
    "text": "Percent Correctly Predicted (PCP)\nOne thing we can do is sum the main diagonal and divide by the estimation sample: \\((TP+TN)/N\\). This gives us the Percent Correctly Predicted (PCP)."
  },
  {
    "objectID": "binaryextensions124.html#proportional-reduction-of-error-pre",
    "href": "binaryextensions124.html#proportional-reduction-of-error-pre",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Proportional Reduction of Error (PRE)}",
    "text": "Proportional Reduction of Error (PRE)}\nA second thing is to compute the Proportional Reduction of Error (PRE) - the difference between the correct predictions of the model versus a null model usually the unconditional frequency of \\(y\\) - in other words, we use the Percent in the Modal Category (PCM).\n\\[\\frac{correct_{\\text{informed}} - correct_{\\text{null}}}{N - correct_{\\text{null}}} \\]\nor\n\\[\\frac{PCP- PMC}{1-PMC} \\]"
  },
  {
    "objectID": "binaryextensions124.html#example-democratic-peace",
    "href": "binaryextensions124.html#example-democratic-peace",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Example: Democratic Peace",
    "text": "Example: Democratic Peace\nHere’s a basic democratic peace model:\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\ndpm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n\nstargazer(dpm1, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\nLet’s generate the confusion matrix for the democratic peace model, threshold set at \\(\\tau =0.5\\):\n\n\ncode\nlibrary(kableExtra)\n# opts &lt;- options(knitr.kable.NA = \"\")\ndf &lt;- data.frame(\"Predicted Positive\" = c(\"0\", \"0\"), \"Predicted Negative\" = c(\"897\", \"19,245\"), \"Rate\" = c(\"TPR=0/897=0\", \"FPR=0/19,245=0\"))\nrow.names(df) &lt;- c(\"Observed Positive\", \"Observed Negative\")\n\ntbl &lt;- kbl(df, align = rep('c', 4)) |&gt; \n  column_spec(2:4, border_left = T) |&gt; \n  column_spec(1,  bold=T) |&gt;\n  column_spec(1:4, width = \"2cm\", color = 'white', background = '#005A43', include_thead = TRUE) |&gt;\n  column_spec(2:4, background = \"inherit\", color=\"inherit\") |&gt;\n  row_spec(1, extra_css = \"border-bottom: 1px solid\") |&gt;\n  kable_paper(\"hover\", bootstrap_options = c(\"condensed\", \"responsive\"), full_width = F, font_size = 20)\ntbl\n\n\n\n\n\n\nPredicted.Positive\nPredicted.Negative\nRate\n\n\n\n\nObserved Positive\n0\n897\nTPR=0/897=0\n\n\nObserved Negative\n0\n19,245\nFPR=0/19,245=0\n\n\n\n\n\n\n\nThe problem is \\(\\tau\\); at \\(\\tau=.5\\) we get none of the ones correct. Here’s the democratic peace at \\(\\tau = 0.1\\):\n\n\ncode\nlibrary(kableExtra)\n# opts &lt;- options(knitr.kable.NA = \"\")\ndf &lt;- data.frame(\"Predicted Positive\" = c(\"303\", \"1,764\"), \"Predicted Negative\" = c(\"594\", \"17,481\"), \"Rate\" = c(\"TPR=303/897=.34\", \"FPR=1,764/19,245=0.092\"))\nrow.names(df) &lt;- c(\"Observed Positive\", \"Observed Negative\")\n\ntbl &lt;- kbl(df, align = rep('c', 4)) |&gt; \n  column_spec(2:4, border_left = T) |&gt; \n  column_spec(1,  bold=T) |&gt;\n  column_spec(1:4, width = \"2cm\", color = 'white', background = '#005A43', include_thead = TRUE) |&gt;\n  column_spec(2:4, background = \"inherit\", color=\"inherit\") |&gt;\n  row_spec(1, extra_css = \"border-bottom: 1px solid\") |&gt;\n  kable_paper(\"hover\", bootstrap_options = c(\"condensed\", \"responsive\"), full_width = F, font_size = 20)\ntbl\n\n\n\n\n\n\nPredicted.Positive\nPredicted.Negative\nRate\n\n\n\n\nObserved Positive\n303\n594\nTPR=303/897=.34\n\n\nObserved Negative\n1,764\n17,481\nFPR=1,764/19,245=0.092\n\n\n\n\n\n\n\nAt \\(\\tau = 0.1\\), we get a TPR = 0.34 and FPR = 0.092. The PCP = (303+17481)/20142 = 0.88 …which is lower, but we get about 1/3 of the ones correct."
  },
  {
    "objectID": "binaryextensions124.html#receiver-operator-characteristic-roc-curves",
    "href": "binaryextensions124.html#receiver-operator-characteristic-roc-curves",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Receiver-Operator Characteristic (ROC) Curves",
    "text": "Receiver-Operator Characteristic (ROC) Curves\nThe problem is choosing the threshold - imagine that we compute a confusion matrix for all possible thresholds, \\(\\tau=.01, .02, .03 \\ldots 1\\), then compute TPR and FPR, and plot them against one another. This is an ROC curve.\nROCs originate in efforts to distinguish signal from noise in radar returns - the British built a radar system before WWII to detect German air attacks; they had the problem of distinguishing planes (signal) from flocks of geese (noise). As the turned up the sensitivity of the radar, they more often correctly detected planes, but they also lacked specificity and detected more geese too. So there was a tradeoff between sensitivity (correctly identifying positive signals as positive) and specificity (incorrectly identify negative signals as positive).\nROCs measure these two dimensions and graph them against one another:\n\nsensitivity - true positive rate at every possible latent threshold between zero and one.\n1- specificity - false positive rate at every possible latent threshold between zero and one. This is 1 minus the True Negative Rate\n\nHere’s the ROC for the NAFTA model - we’ll use the pROC package in R to compute the ROC and plot it:\n\n\ncode\nlibrary(pROC)\n\n# NAFTA\n\nnaftaroc &lt;- roc(test_data$actual, predictions, plot=TRUE, grid=TRUE, partial.auc.correct=TRUE,\n         print.auc=TRUE)"
  },
  {
    "objectID": "binaryextensions124.html#roc-intepretation",
    "href": "binaryextensions124.html#roc-intepretation",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "ROC Intepretation",
    "text": "ROC Intepretation\n\nthe diagonal is a model guessing randomly zero or one - no better than a coin toss.\nabove that line, the model is improving our classification over random guesses.\nthe top left corner would indicate a model that classifies perfectly - 100% sensitivity (TPR), and 0% FPR.\nbelow the diagonal line, the model is classifying worse than a coin toss would.\nwe can compute the Area Under the Curve (AUC) as a percentage - AUC is often reported to indicate model fit. In the NAFTA model, the AUC is .843. AUC closer to one indicates better fit; closer to .5 indicates worse fit, similar to random guessing.\nwe can plot ROCs from different models on the same space and compare their fits.\nThe x-axis is 1-specificity, or the False Positive Rate."
  },
  {
    "objectID": "binaryextensions124.html#correctly-predicted",
    "href": "binaryextensions124.html#correctly-predicted",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Correctly Predicted",
    "text": "Correctly Predicted"
  },
  {
    "objectID": "binaryextensions124.html#roc-democratic-peace",
    "href": "binaryextensions124.html#roc-democratic-peace",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "ROC Democratic Peace",
    "text": "ROC Democratic Peace\nHere’s we compare fit for two models, one including “borders,” the other excluding it. Here are the two models :\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\ndpm1 &lt;-glm(dispute ~ border+deml+caprat+ally+border, family=binomial(link=\"logit\"), data=dp )\ndpm2 &lt;-glm(dispute ~ deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n  \nstargazer(list(dpm1,dpm2), type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n-0.078*** (0.007)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n-0.005*** (0.0005)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n-0.374*** (0.076)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n-2.979*** (0.064)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n7,262.635\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\nAnd compute the ROC for each model - here Claude.ai and I have written a function to compute the ROC and AUC for each model, and then plot them on the same space.\n\n\ncode\n# part written by Claude.ai\n# compute ROC curve\ncompute_roc &lt;- function(actual, predictions) {\n  # Sort actual and predictions in descending order of predicted probabilities\n  sorted_data &lt;- data.frame(actual, predictions)\n  sorted_data &lt;- sorted_data[order(sorted_data$predictions, decreasing = TRUE), ]\n\n  # Initialize variables\n  n_positive &lt;- sum(actual)\n  n_negative &lt;- length(actual) - n_positive\n  tp &lt;- 0\n  fp &lt;- 0\n  tpr &lt;- c()\n  fpr &lt;- c()\n\n  # Iterate over sorted data\n  for (i in 1:nrow(sorted_data)) {\n    if (sorted_data$actual[i] == 1) {\n      tp &lt;- tp + 1\n    } else {\n      fp &lt;- fp + 1\n    }\n\n    # Calculate true positive rate (TPR) and false positive rate (FPR)\n    tpr &lt;- c(tpr, tp / n_positive)\n    fpr &lt;- c(fpr, fp / n_negative)\n  }\n\n  # Create ROC curve\n  roc_curve &lt;- data.frame(fpr, tpr)\n  return(roc_curve)\n}\n\n# Function to compute AUC\ncompute_auc &lt;- function(fpr, tpr) {\n  # Sort FPR and TPR\n  ord &lt;- order(fpr)\n  fpr &lt;- fpr[ord]\n  tpr &lt;- tpr[ord]\n  \n  # Compute AUC using trapezoidal rule\n  auc &lt;- sum((fpr[-1] - fpr[-length(fpr)]) * (tpr[-1] + tpr[-length(tpr)]) / 2)\n  return(auc)\n}\n\n# Democratic Peace models\n\ntest_dataB &lt;- dp %&gt;% dplyr::select(border,deml,caprat,ally) %&gt;% \n  mutate(actual = dp$dispute)\n\ntest_dataNB &lt;- dp %&gt;% dplyr::select(deml,caprat,ally) %&gt;% \n  mutate(actual = dp$dispute)\n\n# Make predictions on the test data\npredictionsB &lt;- predict(dpm1, newdata = test_dataB, type = \"response\")\npredictionsNB &lt;- predict(dpm2, newdata = test_dataNB, type = \"response\")\n\n\nroc_curveB &lt;- compute_roc(test_dataB$actual, predictionsB)\n#AUC\nauc_border &lt;- compute_auc(roc_curveB$fpr, roc_curveB$tpr)\nroc_curveNB &lt;- compute_roc(test_dataNB$actual, predictionsNB)\nauc_noborder &lt;- compute_auc(roc_curveNB$fpr, roc_curveNB$tpr)\n\n# ROC Plot, pasting auc_border and auc_noborder on plot\n\nggplot() +\n  geom_line(data = roc_curveB, aes(x = fpr, y = tpr, color = \"Borders\")) +\n  geom_line(data = roc_curveNB, aes(x = fpr, y = tpr, color = \"No Borders\")) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"solid\", color = \"red\") +\n  labs(title = \"ROC Curve: Democratic Peace\", x = \"False Positive Rate\", y = \"True Positive Rate\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"Borders\" = \"#005A43\", \"No Borders\" = \"#6CC24A\"))+\n  annotate(\"text\", x = 0.75, y = 0.5, label = paste(\"AUC Model 1: \", round(auc_border, 2)), color = \"#005A43\") +\n  annotate(\"text\", x = 0.75, y = 0.4, label = paste(\"AUC Model 2: \", round(auc_noborder, 2)), color = \"#6CC24A\")\n\n\n\n\n\n\n\n\n\ncode\n#bucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nAlso, note the measure Area Under the Curve (AUC) for each model - the AUC is often reported to indicate model fit. The AUC for the model including borders is 0.75, while the AUC for the model excluding borders is 0.72. A model with an AUC of 0.5 is no better than a coin toss, while a model with an AUC of 1 is perfect."
  },
  {
    "objectID": "binaryextensions124.html#single-coefficient-estimates",
    "href": "binaryextensions124.html#single-coefficient-estimates",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Single Coefficient Estimates",
    "text": "Single Coefficient Estimates\nOne property of MLEs is they are asymptotically multivariate normal; inference is straightforward because the variances are also normal so the ratio of \\(\\beta /se\\) is a z-score."
  },
  {
    "objectID": "binaryextensions124.html#model-evaluation",
    "href": "binaryextensions124.html#model-evaluation",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nMost commonly, we evaluate model fit using one of the “trinity” of tests:\n\nlog-likelihood ratio tests (LLR)\nWald tests\nLagrange Multiplier tests (LM)\n\nThe first two are the most common, and it’s not clear one is better than the other.\n\nLog-Likelihood Ratio Test\nThe LLR test requires estimating two models - a null or constrained model, (\\(M_0\\)), and informed (unconstrained) model (\\(M_1\\)) - it compares the heights of the log-likelihood functions of the two models:\n\\[ \\chi^2 = -2 (ln\\mathcal{L}(M_0) - ln\\mathcal{L}(M_1))  \\]\nThe log-likelihoods here are literally the values of the \\(ln\\mathcal{L}\\) at the estimated maxima of the functions. Their difference is distributed \\(\\chi^2\\) with \\(k_1-k_0\\) degrees of freedom.\nThe LLR is simple to compute (you can do it in your head), but requires estimating two nested models. Recall, two models are nested iff the regressors in the constrained model are a strict subset of those in the unconstrained model, and the samples are identical.\n\n\nWald \\(\\chi^2\\)\nThe Wald test is similar, but only requires the unconstrained or informed model. During maximization, it examines the distance between \\(M_0\\) and \\(M_1\\), and weights that distance by the rate of change between the two (second derivative). If the distance is large and the rate of change is fast, the informed model improves a good bit on the uninformed one. You can imagine other combinations of distance and curvature. Long (p. 88) has a great illustration of this.\n\n\nIn Practice\nThese two tests are asymptotically equivalent. In practice, it makes little difference in most cases which you choose, provided the models are nested.\nStata reports LLR for most models, but reports Wald if you request robust standard errors.\n\n\nLimits\nThe limits of these tests is they apply only to nested models - models where the regressors in the constrained model are a strict subset of those in the unconstrained model and where the samples are identical.\n\n\nInformation Criterion Tests\nAlternatively, we can use information criterion tests - the two most common are the Akaike and Bayesian Information Criteria tests (AIC, BIC). These are both formulated to penalize likelihoods for the number of parameters estimated; this in effect rewards better specification (good variables, but few) and penalizes “garbage can” approaches (including lots of poor predictors).\nIC tests are useful for either nested or nonnested models. This is a significant though under-appreciated virtue of such tests.\n\n\nAkaike and Bayesian Information Criterion tests\n\\[AIC =  -2 ln(\\mathcal{L}) + 2k \\]\n\\[BIC =  -2 ln(\\mathcal{L}) + ln(N) k \\]\nwhere \\(k\\) is the number of parameters.\n\nProcess\nAIC: Estimate model 1; generate the AIC. Estimate model 2; generate the AIC. The model with the smaller AIC is the preferred model (see Long 1997: 110).\nBIC: Estimate model 1; generate the BIC. Estimate model 2; generate the BIC. Compute \\(BIC_1 - BIC_2\\) - the smaller BIC value is the preferable model. The strength of the test statistic is given by Rafferty (1996): absolute value of this difference 0-2 = weak; 2-6= Positive; 6-10= Strong; greater than 10 = Very Strong (see Long 1997: 112)."
  },
  {
    "objectID": "binaryextensions124.html#rare-events-logit",
    "href": "binaryextensions124.html#rare-events-logit",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Rare Events Logit",
    "text": "Rare Events Logit\nWhat they propose is:\n\nSelect all the cases with events (failures).\nRandomly choose a sample of the non-event (censored) cases (they say 2-5 times the size of the failure group).\nThis smaller sample makes data collection possible (compared to the gigantic number of zeros in some event data).\nEstimate a logit on the new, smaller sample, and adjust the estimates for the sample.\n\nThe Rare Events Logit doesn’t appear in the literature much, though it’s not uncommon for reviewers to ask for it. In my experience inferences from this model don’t vary much from the usual logit. The model does present a major opportunity for data collection efforts."
  },
  {
    "objectID": "binaryextensions124.html#example-nafta-vote-1993",
    "href": "binaryextensions124.html#example-nafta-vote-1993",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Example: NAFTA vote, 1993",
    "text": "Example: NAFTA vote, 1993\nSo here’s a model predicting members of the US House votes for or against the NAFTA treaty in 1993: 1 is a “yes” vote; the \\(x\\) variables are party (Democrat) and an ideology score.\n\n\ncode\nnafta &lt;- read.csv(\"NAFTA.csv\")\n\nnaftamodel &lt;- glm(vote ~ democrat + cope93, data=nafta, family=binomial(link=\"logit\"))\n\nstargazer(naftamodel, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: NAFTA vote\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Party (Democrat=1)\", \"Ideology (COPE score)\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: NAFTA vote\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParty (Democrat=1)\n\n\n1.990*** (0.504)\n\n\n\n\nIdeology (COPE score)\n\n\n-0.054*** (0.007)\n\n\n\n\nConstant\n\n\n2.380*** (0.272)\n\n\n\n\n\n\n\n\nObservations\n\n\n434\n\n\n\n\nAkaike Inf. Crit.\n\n\n465.904\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\nAnd here’s the confusion matrix from that model - it compares the observed vote frequency against how we classify our predictions based on a predicted probability greater than or less than 0.5.\n\n\ncode\n# Load required libraries\nlibrary(pROC)\nlibrary(caret)\n\n# Assuming you have a fitted GLM object called 'glm_model'\n# and test data 'test_data' with actual outcomes in 'test_data$actual'\n\ntest_data &lt;- nafta %&gt;% dplyr::select(vote, democrat, cope93) %&gt;% mutate(actual = vote)\n# Make predictions on the test data\npredictions &lt;- predict(naftamodel, newdata = test_data, type = \"response\")\n\n# Create confusion matrix\n# You may need to adjust the threshold (default is 0.5)\npredicted_classes &lt;- ifelse(predictions &gt; 0.5, 1, 0)\nconf_matrix &lt;- confusionMatrix(factor(predicted_classes), factor(test_data$actual))\n\n# Print confusion matrix\nprint(conf_matrix)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 160  77\n         1  40 157\n                                         \n               Accuracy : 0.7304         \n                 95% CI : (0.686, 0.7716)\n    No Information Rate : 0.5392         \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      \n                                         \n                  Kappa : 0.4647         \n                                         \n Mcnemar's Test P-Value : 0.0008741      \n                                         \n            Sensitivity : 0.8000         \n            Specificity : 0.6709         \n         Pos Pred Value : 0.6751         \n         Neg Pred Value : 0.7970         \n             Prevalence : 0.4608         \n         Detection Rate : 0.3687         \n   Detection Prevalence : 0.5461         \n      Balanced Accuracy : 0.7355         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\ncode\n# # Create ROC curve\n# roc_obj &lt;- roc(test_data$actual, predictions)\n# \n# # Plot ROC curve\n# plot(roc_obj, main = \"ROC Curve\", col = \"blue\", lwd = 2)\n# abline(a = 0, b = 1, lty = 2, col = \"red\")\n# \n# # Compute AUC\n# auc_value &lt;- auc(roc_obj)\n# print(paste(\"AUC:\", round(auc_value, 4)))\n\n\n\n\ncode\n# Function to compute ROC curve\ncompute_roc &lt;- function(actual, predictions) {\n  # Sort actual and predictions in descending order of predicted probabilities\n  sorted_data &lt;- data.frame(actual, predictions)\n  sorted_data &lt;- sorted_data[order(sorted_data$predictions, decreasing = TRUE), ]\n\n  # Initialize variables\n  n_positive &lt;- sum(actual)\n  n_negative &lt;- length(actual) - n_positive\n  tp &lt;- 0\n  fp &lt;- 0\n  tpr &lt;- c()\n  fpr &lt;- c()\n\n  # Iterate over sorted data\n  for (i in 1:nrow(sorted_data)) {\n    if (sorted_data$actual[i] == 1) {\n      tp &lt;- tp + 1\n    } else {\n      fp &lt;- fp + 1\n    }\n\n    # Calculate true positive rate (TPR) and false positive rate (FPR)\n    tpr &lt;- c(tpr, tp / n_positive)\n    fpr &lt;- c(fpr, fp / n_negative)\n  }\n\n  # Create ROC curve\n  roc_curve &lt;- data.frame(fpr, tpr)\n  return(roc_curve)\n}\n\n# Example usage\n\ntest_data &lt;- nafta %&gt;% dplyr::select(vote, democrat, cope93) %&gt;% mutate(actual = vote)\n# Make predictions on the test data\npredictions &lt;- predict(naftamodel, newdata = test_data, type = \"response\")\n\n\nroc_curve &lt;- compute_roc(test_data$actual, predictions)\n\n# Plot ROC curve\nplot(roc_curve$fpr, roc_curve$tpr, type = \"l\", xlab = \"False Positive Rate\", ylab = \"True Positive Rate\")\n\n# Add diagonal line\nabline(a = 0, b = 1, lty = 2, col = \"red\")\n\n# Add AUC to plot\n\nauc_value &lt;- auc(roc_curve$fpr, roc_curve$tpr)\ntext(0.5, 0.5, paste(\"AUC =\", round(auc_value, 2)), pos = 4)\n\n\n\n\n\n\n\n\n\ncode\n#use ggplot to plot the ROC curve\n\nroc_curve &lt;- data.frame(fpr = roc_curve$fpr, tpr = roc_curve$tpr)\n\nggplot(roc_curve, aes(x = fpr, y = tpr)) +\n  geom_line() +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"ROC Curve\", x = \"False Positive Rate\", y = \"True Positive Rate\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nYou’ll note these examples using data on the US House vote on the NAFTA treaty make some sense - these measures of goodness of fit tell us how much our covariates improve classification.\nWhat makes this work in the NAFTA context? Because the next example using the democratic peace data, does not work so well."
  },
  {
    "objectID": "binaryextensions124.html#footnotes",
    "href": "binaryextensions124.html#footnotes",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nsee what I did there?↩︎"
  },
  {
    "objectID": "discretehazards24.html",
    "href": "discretehazards24.html",
    "title": "Discrete Time Hazard Models",
    "section": "",
    "text": "consider what “memory” might look like in a binary time series setting.\nintroduce concepts underlying hazard models.\nunderstand the discrete time hazard model."
  },
  {
    "objectID": "discretehazards24.html#memoryless",
    "href": "discretehazards24.html#memoryless",
    "title": "Discrete Time Hazard Models",
    "section": "Memoryless",
    "text": "Memoryless\nBy construction, this model lacks any memory. \\(Pr(Y_{i,t}=1)\\) is a function of \\(X_{i,t}\\), but is independent of anything that happened prior to \\(t\\).\nThis means dyads that have been at peace for 2 years and for 22 years are treated as the same, varying only on the \\(X\\) variables.\nNote this is by choice - we’re assuming \\(y_t\\) has no bearing on \\(y_{t+1}\\), so there is no persistence or memory from period to period. We’ll encounter a range of models that are explicitly aimed at understanding this question: How does having survived up until now affect the chances of failing now?"
  },
  {
    "objectID": "discretehazards24.html#time",
    "href": "discretehazards24.html#time",
    "title": "Discrete Time Hazard Models",
    "section": "Time",
    "text": "Time\nWhat would data for mortality (or any sort of spells) look like? There are two basic types:\nSurvival:\n\\[Age = 1, 2, 3, 4, 5, \\ldots 87, 88, 89\\]\nor\nFailure:\n\\[Death = 0, 0, 0, 0, 0, \\ldots 0, 0, 1\\]\nNote that these convey the two parts of the hazard - how many periods the individual survives, and at what period the individual fails.\nThese also represent two ways to think about time: continuously:\n\\[Age = 1, 2, 3, 4, 5, \\ldots 87, 88, 89\\]\nor discretely:\n\\[Death = 0, 0, 0, 0, 0, \\ldots 0, 0, 1\\]\nIn truth, we almost always measure time discretely in the sense that failure can only happen in certain intervals (days, weeks, years, etc.), even though time is continuous and failure can happen in much smaller increments than these (e.g. minutes, seconds). Still, most treatments consider two types of hazard models:\n\nContinuous time models using data like \\(Age = 1, 2, 3, 4, 5, \\ldots 87, 88, 89\\) as the \\(y\\) variable.\nDiscrete time models using data like \\(Death = 0, 0, 0, 0, 0, \\ldots 0, 0, 1\\) as the \\(y\\) variable. These are usually estimated using a binomial LLF (e.g. the logit model)."
  },
  {
    "objectID": "discretehazards24.html#hazard-models",
    "href": "discretehazards24.html#hazard-models",
    "title": "Discrete Time Hazard Models",
    "section": "Hazard models",
    "text": "Hazard models\nA key feature of any hazard model is that the model accounts for both time until failure, and the realization of failure. In continuous or discrete time models, both of these are part of the estimation.\n\n\n\n\n\n\nNote\n\n\n\nAn aside on naming - models like these are interchangeably called “hazard models,” “survival models,” “duration models,” or “event history models.” They all refer to the same basic idea - modeling the time until an event occurs. They can sometimes indicate whether the quantity of interest is the hazard or survival - note that these are opposites in the sense that the hazard is the probability of failure at a particular point in time, while survival is the probability of surviving up to that point in time. “Event history” often refers to discrete time models."
  },
  {
    "objectID": "discretehazards24.html#binary-time-series-cross-section-data",
    "href": "discretehazards24.html#binary-time-series-cross-section-data",
    "title": "Discrete Time Hazard Models",
    "section": "Binary Time Series Cross Section data",
    "text": "Binary Time Series Cross Section data\nIt’s common to have binary \\(y\\) variables observed for cross sections over time - these are Binary Time Series Cross Section (BTSCS) data. This is the form the democratic peace data takes, and is a common form of data in the social sciences. BTSCS data are grouped duration data, and failure is measured in discrete time.\nHere’s an example of BTSCS data thinking of disputes in dyads over time.\n\n\n\n\nBTSCS data\n\n\n\n\n\n\n\n\n\nState\nYear\nDispute\nCensored\n\n\n\nUS-Cuba\n1960\n0\n0\n\n\n\nUS-Cuba\n1961\n1\n0\n\n\n\nUS-Cuba\n1962\n0\n0\n\n\n\nUS-Cuba\n1963\n0\n0\n\n\n\nUS-Cuba\n1964\n0\n0\n\n\n\nUS-Cuba\n1965\n0\n0\n\n\n\nUS-Cuba\n1966\n0\n0\n\n\n\nUS-Cuba\n1967\n1\n0\n\n\n\nUS-Cuba\n1968\n0\n0\n\n\n\nUS-Cuba\n1969\n0\n0\n\n\n\nUS-Cuba\n1970\n0\n1\n\n\n\nUS-UK\n1960\n0\n0\n\n\n\nUS-UK\n1961\n0\n0\n\n\n\nUS-UK\n1962\n0\n0\n\n\n\nUS-UK\n1963\n0\n0\n\n\n\nUS-UK\n1964\n0\n0\n\n\n\nUS-UK\n1965\n0\n0\n\n\n\nUS-UK\n1966\n0\n0\n\n\n\nUS-UK\n1967\n0\n0"
  },
  {
    "objectID": "discretehazards24.html#terminology",
    "href": "discretehazards24.html#terminology",
    "title": "Discrete Time Hazard Models",
    "section": "Terminology",
    "text": "Terminology\nLet’s begin thinking about terminology:\n\nwe observe at each point \\(t\\) whether a unit fails or not. Failure means experiencing the event of interest. In mortality studies, this is is death; in the democratic peace data, the event is a militarized dispute.\neach unit is at risk until it exits the data either because the period of observation ends, or because it fails and can only fail once. In mortality studies, an individual can only fail once; in the democratic peace data, a dyad can fail multiple times.\na unit survives some spell up to the point at which it fails. We can count these time periods to measure survival time.\nthe period a unit survives is called a spell; spells end at failure.\nwe have no idea what happened to these units prior to 1960; the units are left-censored.\nwe have no idea what happens to these units after 1970; the units are right censored. Any unit that does not experience the failure event during the period of study is right-censored."
  },
  {
    "objectID": "discretehazards24.html#spells",
    "href": "discretehazards24.html#spells",
    "title": "Discrete Time Hazard Models",
    "section": "Spells",
    "text": "Spells\nHere are different spells:\n\n\ncode\nlibrary(tidyverse)\nlibrary(highcharter)\n\n# Binghamton University colors\nbinghamton_colors &lt;- c(\"#005A43\", \"#8C2132\", \"#FFD100\", \"#000000\", \"#636466\")\n\n# Create dataframes for each case with updated labels\ncases &lt;- list(\n  list(x = c(3, 6), y = c(1, 1), name = \"uncensored\"),\n  list(x = c(-0.5, 2.5), y = c(2, 2), name = \"left censored\"),\n  list(x = c(2.8, 8), y = c(3, 3), name = \"fails at last period\"),\n  list(x = c(3.5, 10), y = c(4, 4), name = \"right censored\"),\n  list(x = c(5.5, 7), y = c(5, 5), name = \"uncensored\")\n)\n\n# Create the plot\nhc &lt;- highchart() %&gt;%\n  hc_chart(type = \"line\") %&gt;%\n  hc_xAxis(\n    title = list(text = \"time\"),\n    plotLines = list(\n      list(value = 2, width = 2, color = \"black\"),\n      list(value = 8, width = 2, color = \"black\")\n    ),\n    min = 0,\n    max = 10\n  ) %&gt;%\n  hc_yAxis(\n    title = list(text = \"case\"),\n    min = 0,\n    max = 5,\n    tickInterval = 1\n  ) %&gt;%\n  hc_plotOptions(\n    series = list(\n      lineWidth = 3,\n      marker = list(enabled = FALSE)\n    )\n  ) %&gt;%\n  hc_legend(enabled = FALSE)\n\n# Add each case as a separate series with Binghamton colors\nfor (i in seq_along(cases)) {\n  hc &lt;- hc %&gt;% hc_add_series(\n    data = list_parse2(data.frame(x = cases[[i]]$x, y = cases[[i]]$y)),\n    name = cases[[i]]$name,\n    color = binghamton_colors[i]\n  )\n}\n\n# Display the plot\nhc\n\n\n\n\n\n\n\nsome units survive through the end of the study; these units are right-censored. That is, they do not fail during the period of observation.\nfailure is only observed per year; so failure is grouped by year; these are grouped duration data. We could, for instance, graph the density of failures at each point in time, effectively grouping them by failure time.\nthe chances of failing at \\(t\\), given survival til \\(t\\) is the hazard of failure; at any point in time, this is called the hazard rate, denoted \\(h(t)\\).\nin the model above, \\(h(t)\\) does not depend on what happened at \\(t-1\\), so \\(h(t)\\) is constant over time or is time invariant, or is duration independent."
  },
  {
    "objectID": "discretehazards24.html#survival-spells",
    "href": "discretehazards24.html#survival-spells",
    "title": "Discrete Time Hazard Models",
    "section": "Survival Spells",
    "text": "Survival Spells\nWe can measure survival spells; time elapsed until failure or censoring. These are the same data as above, just re-formed so the units are different. Note the summed survival time is equal to the total time at risk. So for the US-Cuba dyad, the total time at risk is 11 years. Also, notice that the US-Cuba dyad is censored in 1970. It survives 3 years since its last dispute, but the end of that spell is our observation period, not another dispute.\n\n\n\n\nSpell data\n\n\n\n\n\n\n\n\n\n\nState\nYear\nDispute\nfail\ncensored\nsurvival\n\n\nUS-Cuba\n1961\n1\n1\n0\n2\n\n\nUS-Cuba\n1967\n1\n1\n0\n6\n\n\nUS-Cuba\n1970\n0\n0\n1\n3\n\n\nUS-UK\n1970\n0\n0\n1\n11"
  },
  {
    "objectID": "discretehazards24.html#why-not-use-ols",
    "href": "discretehazards24.html#why-not-use-ols",
    "title": "Discrete Time Hazard Models",
    "section": "Why not use OLS?",
    "text": "Why not use OLS?\nIt looks almost as if we could estimate a linear regression of the survival variable. Time is continuous, after all. Problems with doing so include:\n\nwe can’t have negative survival time.\nfailing at \\(t = 8\\) is conditional on having survived until \\(t=8\\); can’t include this in a linear model.\nsome observations never fail during the period of observation (are censored).\nthe outcome variable of interest is latent - it’s the hazard rate."
  },
  {
    "objectID": "discretehazards24.html#discrete-time-hazards",
    "href": "discretehazards24.html#discrete-time-hazards",
    "title": "Discrete Time Hazard Models",
    "section": "Discrete Time Hazards",
    "text": "Discrete Time Hazards\nUntil the late 1990s, studies using BTSCS data ignored memory. Put differently, the conventional way to model these data was using a binomial model like the logit. Beck, Katz, and Tucker’s (1998) paper pointed out the problems with doing this, and suggested an easy fix.\n\nThe problem\nThe standard logit/probit model in these data assumes the errors are i.i.d. - that the disturbances are uncorrelated. A somewhat more interesting observation is that the model assumes no relationship between the outcome at \\(t\\) and the outcome at \\(t-1, t-2 \\ldots t-k\\). So the observations on \\(y\\) arise independently of one another …almost as if each observation is an independent Bernoulli trial. The model is misspecified, and likely the parameter estimates are biased.\n\n\nThe solution\nInclude “survival time” as a right hand side variable. Doing so explicitly models the effect of surviving up til \\(t\\) on the probability of failing at \\(t\\).\nBKT suggest including a function of survival time so the effect of time isn’t constrained to be monotonic. They suggested using cubic splines of survival time; Carter and Signorino (2010) later show polynomials for survival time are just as good and easier to compute/understand.\n\n\nThe result\nThis fundamentally changed models on BTSCS data - the state of the art is to include survival time, thereby measuring “memory” in the \\(y\\) series. While most BTSCS models since Beck, Katz, and Tucker (1998) include survival time, relatively few interpret it; that’s okay insofar as the effect of survival might not be of theoretical interest. Most incorrectly interpret the predictions as probabilities - they are hazards."
  },
  {
    "objectID": "discretehazards24.html#survival-time",
    "href": "discretehazards24.html#survival-time",
    "title": "Discrete Time Hazard Models",
    "section": "Survival time",
    "text": "Survival time\nSurvival time: the time up to failure, in the interval \\(t_0, t_{\\infty}\\) such that \\(t \\in \\{1,2,3 \\ldots t_{\\infty} \\}\\)"
  },
  {
    "objectID": "discretehazards24.html#failure",
    "href": "discretehazards24.html#failure",
    "title": "Discrete Time Hazard Models",
    "section": "Failure",
    "text": "Failure\nThe probability of the failure event:\n\\[\\begin{aligned}\nf(t) = Pr(t_i=t) \\nonumber\n\\end{aligned}\\]\nThis is the density."
  },
  {
    "objectID": "discretehazards24.html#cumulative-function",
    "href": "discretehazards24.html#cumulative-function",
    "title": "Discrete Time Hazard Models",
    "section": "Cumulative Function",
    "text": "Cumulative Function\nWrite the cumulative probability of failure up to \\(t_i\\).\n\\[\\begin{aligned}\nF(t) = \\sum_{i=1}^{\\infty} f(t_i) \\nonumber\n\\end{aligned}\\]\nNow, consider the probability of surviving up until \\(t\\) - this is equal to 1 minus the CDF, so\n\\[S(t) = 1-F(t) = P(t_{i} \\geq t)\\]\nMost importantly, the conditional probability given by the probability of failing at \\(t_i\\) given survival up until \\(t_i\\):\n\\[\\begin{aligned}\nh(t)=Pr(t=t_i | t \\geq t_i) \\nonumber \\\\\n= \\frac{f(t)}{S(t)} \\nonumber\n\\end{aligned}\\]\nThis is the hazard rate."
  },
  {
    "objectID": "discretehazards24.html#hazard-rate",
    "href": "discretehazards24.html#hazard-rate",
    "title": "Discrete Time Hazard Models",
    "section": "Hazard Rate",
    "text": "Hazard Rate\n\\[\\begin{aligned}\nh(t)=Pr(t=t_i | t \\geq t_i) \\nonumber \\\\\n= \\frac{f(t)}{S(t)} \\nonumber\n\\end{aligned}\\]\nThe hazard rate is conceptually important because it explicitly relates the past to the present, thereby incorporating memory into the statistical model. The hazard is different from \\(Pr(y_t=1)\\) because it conditions on what has happened prior to \\(t\\)."
  },
  {
    "objectID": "discretehazards24.html#discrete-time-ht",
    "href": "discretehazards24.html#discrete-time-ht",
    "title": "Discrete Time Hazard Models",
    "section": "Discrete time h(t)",
    "text": "Discrete time h(t)\nWhat we have derived is the discrete time hazard function - time is measured in discrete units (e.g. years, not parts of years like months or days); some processes only make sense in discrete terms - e.g. a member of the US House can only be turned out by voters every two years, not before."
  },
  {
    "objectID": "discretehazards24.html#density",
    "href": "discretehazards24.html#density",
    "title": "Discrete Time Hazard Models",
    "section": "Density",
    "text": "Density\nSince the probability of survival at some value of \\(t\\) is the probability of survival at \\(t\\) given survival up to \\(t\\), the conditional probability of survival is 1 minus the hazard rate:\n\\[Pr(t_j&gt;t | t_j\\geq t) = 1 - h(t)\\]\nWe can rewrite the survivor function as a product of the probabilities of surviving up to \\(t\\):\n\\[S(t) = \\prod_{j=0}^{t} \\{1-h(t-j)\\}\\]\nWe can rewrite the density \\(f(t)\\):\n\\[f(t) = h(t)S(t)\\]"
  },
  {
    "objectID": "discretehazards24.html#estimation",
    "href": "discretehazards24.html#estimation",
    "title": "Discrete Time Hazard Models",
    "section": "Estimation",
    "text": "Estimation\nLet’s build a likelihood - as you might have guessed, it needs to involve \\(f(t)\\) and \\(S(t)\\) (failure and survival times) so we can estimate \\(h(t)\\).\n\\[ \\mathcal{L} = \\prod_{t_i\\leq t} f(t_i) \\prod_{t_i\\geq t} S(t_i) \\]\nthen, think of censoring where \\(y_{i,t}\\) indicates when, and whether a subject ever fails; if zero, censored, if one, uncensored (fails during our period of observation).\n\\[ \\mathcal{L} = \\prod_{t_i\\leq t} f(t_i)^{y_{i,t}} \\prod_{t_i\\geq t} S(t_i)^{1- y_{i,t}} \\]\nThis should be looking familiar.\nNow, substituting:\n\\[ \\mathcal{L} = \\prod_{i=1}^{N} \\Bigg\\{ h(t) \\prod_{j=1}^{t-1}   [1-h(t-i)] \\Bigg\\} ^{y_{i,t}} \\Bigg\\{ \\prod_{j=1}^{t}   [1-h(t-i)] \\Bigg\\}^{1- y_{i,t}}\\]\nAnd substitute an appropriate link density for \\(f(t)\\) and \\(S(t)\\), e.g., exponential,\n\\[f(t) = \\lambda(t) exp^{\\lambda(t)}\\] \\[S(t) = exp^{-\\lambda(t)}\\] \\[h(t) = \\lambda\\]\nWeibull: \\[f(t) = \\lambda p (\\lambda(t))^{p-1} exp^{-(\\lambda t)^p}\\] \\[S(t) = exp^{-(\\lambda t)^p}\\] \\[h(t) = \\lambda p (\\lambda t)^{p-1}\\]\netc …"
  },
  {
    "objectID": "discretehazards24.html#constant-ht---no-memory",
    "href": "discretehazards24.html#constant-ht---no-memory",
    "title": "Discrete Time Hazard Models",
    "section": "Constant \\(h(t)\\) - no memory",
    "text": "Constant \\(h(t)\\) - no memory\nRevisiting …\nThis is the case where\n\\[h_0(t) = \\frac{1}{1+ exp(\\beta_0)}\\]\nthe baseline hazard is the constant. Even with \\(x\\) variables, there is still no accounting for time - the \\(x\\) effects are only shifts around this baseline hazard.\n\\[h_0(t) = \\frac{1}{1+ exp(\\beta_0+ x'\\beta)}\\]\nthis is still a constant baseline hazard with the effects of \\(x\\) deviating around it."
  },
  {
    "objectID": "discretehazards24.html#non-constant-ht",
    "href": "discretehazards24.html#non-constant-ht",
    "title": "Discrete Time Hazard Models",
    "section": "Non-constant \\(h(t)\\)",
    "text": "Non-constant \\(h(t)\\)\nSo how to deal with this, incorporating memory: thinking in terms of hazards rather than probabilities (i.e., conditional rather than unconditional probabilities), what if we measure survival time?\n\nThe binary \\(y\\) variable is an indicator of failure at \\(t\\); the model estimates \\(f(t)\\), which we’ve said is not especially informative since subjects might fail before \\(t\\).\nThink of the number of periods up to failure as the cumulative survival time, \\(S(t)\\).\n\nSee how we’re starting to construct the hazard rate by its parts."
  },
  {
    "objectID": "discretehazards24.html#measuring-survival-time",
    "href": "discretehazards24.html#measuring-survival-time",
    "title": "Discrete Time Hazard Models",
    "section": "Measuring survival time",
    "text": "Measuring survival time\n\ncount periods of survival up to failure. This is a counter of survival time. generate a binary variable for each survival period.\nEither include those survival dummies in the logit, or include the survival counter itself with polynomials, e.g. \\(t^2, t^3, \\ldots\\).\ninterpret those coefficients as baseline hazards for groups that survive to \\(t_i\\).\nwith all \\(x\\) variables set to zero, the probability of failure is now given by the constant and the appropriate dummy or counter coefficient.\nNote the quantity of interest is not constant across time: it’s a conditional probability; the probability of failing at \\(t\\) given the estimated probability of survival through \\(t-1\\) - \\(h(t)|S(t)\\)."
  },
  {
    "objectID": "discretehazards24.html#understanding-substantive-variables-in-the-hazard-context",
    "href": "discretehazards24.html#understanding-substantive-variables-in-the-hazard-context",
    "title": "Discrete Time Hazard Models",
    "section": "Understanding substantive variables in the hazard context",
    "text": "Understanding substantive variables in the hazard context\nThe survival variables now permit the baseline hazard to vary. The effects of \\(x\\) variables can be thought of as deviations from those baseline hazards. For example, think about the models presented above, but with democracy. The estimates on democracy will shift the baseline hazard up or down.\n\n\ncode\nsurvivaldp &lt;- survivaldp %&gt;% mutate(stime2=stime^2, stime3=stime^3)\n\ndpm4 &lt;-glm(dispute ~ stime+stime2+stime3+deml, family=binomial(link=\"logit\"), data=survivaldp )\n\nstargazer(dpm4, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Survival Time\", \"Survival Time2\", \"Survival Time3\", \"Democracy\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Time\n\n\n-0.823*** (0.035)\n\n\n\n\nSurvival Time2\n\n\n0.051*** (0.004)\n\n\n\n\nSurvival Time3\n\n\n-0.001*** (0.0001)\n\n\n\n\nDemocracy\n\n\n-0.065*** (0.007)\n\n\n\n\nConstant\n\n\n-1.298*** (0.065)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n5,650.709\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\n\ncode\ndpm4 &lt;-glm(dispute ~ stime+stime2+stime3+deml+caprat, family=binomial(link=\"logit\"), data=survivaldp )\n\n# copy estimation data for avg effects\ndppred &lt;- survivaldp\n\n# df for output\ndf &lt;- data.frame(time=seq(1,34,1))\nfor (d in seq(-10,10,10)) {\n    df[paste0(\"p\", d+10)] &lt;- NA\n  df[paste0(\"Se\", d+10)] &lt;- NA\n}\n\n# predictions\nfor (d in seq(-10,10,10)) {\n  dppred$deml &lt;- d\n for (t in seq(1,34,1)) {  \n  dppred$stime &lt;- t\n  dppred$stime2 &lt;- t^2 \n  dppred$stime3 &lt;- t^3\n  pred &lt;- predict(dpm4, newdata=dppred, type=\"response\", se=TRUE)\n  df[t, paste0(\"p\", d+10)] &lt;- mean(pred$fit, na.rm=TRUE)\n  df[t, paste0(\"Se\", d+10)] &lt;- mean(pred$se.fit, na.rm=TRUE)\n  df$time[t] &lt;- t\n}\n}\n\n# plot\nggplot(df, aes(x=time, y=p0)) +\n  geom_line(color=\"#005A43\") +\n  geom_ribbon(aes(ymin=p0-1.96*Se0, ymax=p0+1.96*Se0), fill=\"#6CC24A\", alpha=0.4) +\n  geom_line(aes(y=p10), color=\"#005A43\") +\n  geom_ribbon(aes(ymin=p10-1.96*Se10, ymax=p10+1.96*Se10), fill=\"#A7DA92\", alpha=0.4) +\n  geom_line(aes(y=p20), color=\"#005A43\") +\n  geom_ribbon(aes(ymin=p20-1.96*Se20, ymax=p20+1.96*Se20), fill=\"#005A43\", alpha=0.4) +\n  labs(x=\"Time since last dispute\", y=\"Pr(Dispute)\") +\n  theme_minimal()"
  },
  {
    "objectID": "discretehazards24.html#summary",
    "href": "discretehazards24.html#summary",
    "title": "Discrete Time Hazard Models",
    "section": "Summary",
    "text": "Summary\n\nWe relaxed the assumption of temporal independence.\nDid so by conceiving of the QI as a hazard rather than a probability.\nBuilt a model that estimates \\(h(t)\\) such that we don’t have to assume \\(h(t)\\) is constant.\nHave permitted memory in the model such that the past can shape the present."
  },
  {
    "objectID": "discretehazards24.html#temporal-independence",
    "href": "discretehazards24.html#temporal-independence",
    "title": "Discrete Time Hazard Models",
    "section": "Temporal Independence",
    "text": "Temporal Independence\nIn this model, \\(Pr(y_i=1) = F(x_i\\beta + \\beta_0)\\), \\(x_i \\beta\\) induces deviation from the constant or baseline level; but there is no temporal variation, temporal persistence, or memory. What happened last year has no bearing on what we observe this year. Repeating, it is as if these are Bernoulli trials.\n\nAn alternative - transitions\nWhat happens if we lag \\(y\\) as we might with a continuous variable (say, in OLS), such that the estimation model is\n\\[y_t = \\beta_0 + \\beta_1(x_1) + \\ldots + \\gamma(y_{t-1}) + \\varepsilon\\]\nWith binary time series data, lagging \\(y\\) would measure changes of state - these are a class known as transition models (there are a variety of these).\nIn general, these are interactive models where\n\\[Pr(y_i=1) = F(x_{i,t}\\beta + y_{i,t-1}*x_{i,t} \\gamma)\\]\nand \\(\\gamma\\) measures the difference in effect when \\(y_{i,t-1} = 0\\) (this is just \\(\\beta\\)), and when \\(y_{i,t-1}  = 1\\); denote this \\(\\alpha\\). So \\(\\gamma = \\beta - \\alpha\\). That difference indicates the conditional probability of state transitions from the state where \\(y\\) takes on one value, to the state where it takes on the other value.\nTransition models are useful, but measure something fundamentally different from the latent hazard rate, or the chances of failure given a history of survival. Put differently, lagging \\(y\\) in a binary variable model does not measure memory or persistence; it does not measure the extent to which the observed value today depends on the value yesterday; it does not measure how the latent probability of failure today depends on surviving through yesterday."
  },
  {
    "objectID": "discretehazards24.html#data",
    "href": "discretehazards24.html#data",
    "title": "Discrete Time Hazard Models",
    "section": "Data",
    "text": "Data\nThe democratic peace data is panel data - composed of cross sections observed over time. The \\(y\\) variable is binary, and measures a rare event - conflict. So the \\(y\\) variable for any particular panel is usually a string of zeros, occasionally punctuated by a one.\nWhat we’d really like to know is if/how strings of zeros affect the chances of a one at any given point in time. This is a question of hazards."
  },
  {
    "objectID": "discretehazards24.html#discrete-time---binary-time-series-cross-section-data",
    "href": "discretehazards24.html#discrete-time---binary-time-series-cross-section-data",
    "title": "Discrete Time Hazard Models",
    "section": "Discrete Time - Binary Time Series Cross Section data",
    "text": "Discrete Time - Binary Time Series Cross Section data\nTime is continuous insofar as time units are infinitely divisible, but in practice, we measure time in discrete units like days, months, years, etc. In the democratic peace data above, we will have two dyads “fail” (have disputes) at year 3; but it’s nearly certain one of those dyads started its dispute before the other one. We group the data by these time intervals (years, in this case). So we are measuring time discretely (i.e, in years), but the underlying process is continuous. Moreover, the fact these can be grouped by failure time makes them grouped duration data.\nIt’s common to have binary \\(y\\) variables observed for cross sections over time - these are Binary Time Series Cross Section (BTSCS) data. This is the form the democratic peace data takes, and is a common form of data in the social sciences. BTSCS data are grouped duration data, and failure is measured in discrete time.\nHere’s an example of BTSCS data thinking of disputes in dyads over time.\n\n\n\n\nBTSCS data\n\n\n\n\n\n\n\n\n\nState\nYear\nDispute\nCensored\n\n\n\nUS-Cuba\n1960\n0\n0\n\n\n\nUS-Cuba\n1961\n1\n0\n\n\n\nUS-Cuba\n1962\n0\n0\n\n\n\nUS-Cuba\n1963\n0\n0\n\n\n\nUS-Cuba\n1964\n0\n0\n\n\n\nUS-Cuba\n1965\n0\n0\n\n\n\nUS-Cuba\n1966\n0\n0\n\n\n\nUS-Cuba\n1967\n1\n0\n\n\n\nUS-Cuba\n1968\n0\n0\n\n\n\nUS-Cuba\n1969\n0\n0\n\n\n\nUS-Cuba\n1970\n0\n1\n\n\n\nUS-UK\n1960\n0\n0\n\n\n\nUS-UK\n1961\n0\n0\n\n\n\nUS-UK\n1962\n0\n0\n\n\n\nUS-UK\n1963\n0\n0\n\n\n\nUS-UK\n1964\n0\n0\n\n\n\nUS-UK\n1965\n0\n0\n\n\n\nUS-UK\n1966\n0\n0\n\n\n\nUS-UK\n1967\n0\n0"
  },
  {
    "objectID": "discretehazards24.html#illustration",
    "href": "discretehazards24.html#illustration",
    "title": "Discrete Time Hazard Models",
    "section": "Illustration",
    "text": "Illustration\nHere are different spells:\n\n\ncode\nlibrary(tidyverse)\nlibrary(highcharter)\n\n# Binghamton University colors\nbinghamton_colors &lt;- c(\"#005A43\", \"#8C2132\", \"#FFD100\", \"#000000\", \"#636466\")\n\n# Create dataframes for each case with updated labels\ncases &lt;- list(\n  list(x = c(3, 6), y = c(1, 1), name = \"uncensored\"),\n  list(x = c(-0.5, 2.5), y = c(2, 2), name = \"left censored\"),\n  list(x = c(2.8, 8), y = c(3, 3), name = \"fails at last period\"),\n  list(x = c(3.5, 10), y = c(4, 4), name = \"right censored\"),\n  list(x = c(5.5, 7), y = c(5, 5), name = \"uncensored\")\n)\n\n# Create the plot\nhc &lt;- highchart() %&gt;%\n  hc_chart(type = \"line\") %&gt;%\n  hc_xAxis(\n    title = list(text = \"time\"),\n    plotLines = list(\n      list(value = 2, width = 2, color = \"black\"),\n      list(value = 8, width = 2, color = \"black\")\n    ),\n    min = 0,\n    max = 10\n  ) %&gt;%\n  hc_yAxis(\n    title = list(text = \"case\"),\n    min = 0,\n    max = 5,\n    tickInterval = 1\n  ) %&gt;%\n  hc_plotOptions(\n    series = list(\n      lineWidth = 3,\n      marker = list(enabled = FALSE)\n    )\n  ) %&gt;%\n  hc_legend(enabled = FALSE)\n\n# Add each case as a separate series with Binghamton colors\nfor (i in seq_along(cases)) {\n  hc &lt;- hc %&gt;% hc_add_series(\n    data = list_parse2(data.frame(x = cases[[i]]$x, y = cases[[i]]$y)),\n    name = cases[[i]]$name,\n    color = binghamton_colors[i]\n  )\n}\n\n# Display the plot\nhc\n\n\n\n\n\n\n\nsome units survive through the end of the study; these units are right censored. That is, they do not fail during the period of observation.\nfailure is only observed per year; so failure is grouped by year; these are grouped duration data. We could, for instance, graph the density of failures at each point in time, effectively grouping them by failure time.\nthe probability of failing at \\(t\\), given survival til \\(t\\) is the hazard of failure; at any point in time, this is called the hazard rate, denoted \\(h(t)\\).\nin the democratic peace model above, \\(h(t)\\) does not depend on what happened at \\(t-1\\), so \\(h(t)\\) is constant over time or is time invariant, or is duration independent."
  },
  {
    "objectID": "discretehazards24.html#the-problem",
    "href": "discretehazards24.html#the-problem",
    "title": "Discrete Time Hazard Models",
    "section": "The problem",
    "text": "The problem\nThe standard logit/probit model in these data assumes the errors are i.i.d. - that the disturbances are uncorrelated. A somewhat more interesting observation is that the model assumes no relationship between the outcome at \\(t\\) and the outcome at \\(t-1, t-2 \\ldots t-k\\). So the observations on \\(y\\) arise independently of one another …almost as if each observation is an independent Bernoulli trial. If this isn’t true, the model is misspecified, and likely the parameter estimates are biased.\nIn the context of the democratic peace data, this means the probability of a dispute at any point in time is unrelated to how long that particular dyad has been at peace. Whether it’s been at peace for 1 year or 10 years has no bearing on the chances of conflict now. On its face, this is a heroic assumption."
  },
  {
    "objectID": "discretehazards24.html#the-solution",
    "href": "discretehazards24.html#the-solution",
    "title": "Discrete Time Hazard Models",
    "section": "The solution",
    "text": "The solution\nAt its root, this is a model specification issue - we think time since last dispute is probably related to the chances of a dispute today, but no such measure is in the model. BKT suggest including “survival time” as a right hand side variable. Doing so explicitly models the effect of surviving up til \\(t\\) on the probability of failing at \\(t\\).\nBKT suggest including nonlinear functions of survival time so the effect of time isn’t constrained to be monotonic. They suggested using cubic splines of survival time; Carter and Signorino (2010) later show polynomials for survival time are just as good and easier to compute/understand."
  },
  {
    "objectID": "discretehazards24.html#the-result",
    "href": "discretehazards24.html#the-result",
    "title": "Discrete Time Hazard Models",
    "section": "The result",
    "text": "The result\nThis fundamentally changed models on BTSCS data - the state of the art since then is to include survival time, thereby measuring “memory” in the \\(y\\) series. While most BTSCS models since Beck, Katz, and Tucker (1998) include survival time, relatively few interpret it; that’s okay insofar as the effect of survival might not be of theoretical interest. Most incorrectly interpret the predictions as probabilities - they are now conditional probabilities, $pr(fail | survival), so hazards.\n\nConstant \\(h(t)\\) - no memory\nRevisiting …in this model, \\(Pr(y_i=1) = F(x_i\\beta + \\beta_0)\\), \\(x_i \\beta\\) induces deviation from the constant or baseline level; but there is no temporal variation, temporal persistence, or memory. What happened last year has no bearing on what we observe this year. Repeating, it is as if these are Bernoulli trials.\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\ndpm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n\nstargazer(dpm1, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\n\ncode\n# at mean data over 30 observations\n\natmean &lt;- data.frame(stime=seq(1,34,1), deml=median(dp$deml), border=0, caprat=median(dp$caprat), ally=0)\n\npredictions &lt;- data.frame(atmean, predict(dpm1, newdata=atmean, type=\"response\", se=TRUE)) %&gt;% mutate(fit=round(fit, 2) )\n\n\nggplot(predictions, aes(x=stime, y=fit)) +\n  geom_line(color=\"#005A43\", size=1) +\n  labs(x=\"Time since last dispute\", y=\"Pr(Dispute)\") +\n  geom_ribbon(aes(ymin=fit-1.96*se.fit, ymax=fit+1.96*se.fit), fill=\"grey90\", alpha=0.4) +\n  theme_minimal() +\n  annotate(\"text\", x = 15, y = 0.041, label = \"Effect of Democracy\", color = \"red\", size = 3) +\n  theme(panel.grid.major = element_line(color = \"black\",\n                                        size = 0.25,\n                                        linetype = 1)) +\n  theme(panel.grid.minor = element_line(color = \"black\",\n                                        size = 0.15,\n                                        linetype = 1))\n\n\n\n\n\n\n\n\n\nThis is the case where\n\\[h_0(t) = \\frac{1}{1+ exp(\\beta_0)}\\]\nthe baseline hazard is the constant. Even with \\(x\\) variables, there is still no accounting for time - the \\(x\\) effects are only shifts around this baseline hazard.\n\\[h_0(t) = \\frac{1}{1+ exp(\\beta_0+ x'\\beta)}\\]\nthis is still a constant baseline hazard with the effects of \\(x\\) deviating around it.\n\n\nMeasuring survival time\n\ncount periods of survival up to failure. This is a counter of survival time. generate a binary variable for each survival period.\nEither include those survival dummies in the logit, or include the survival counter itself with polynomials, e.g. \\(t^2, t^3, \\ldots\\).\ninterpret those coefficients as baseline hazards for groups that survive to \\(t_i\\).\nwith all \\(x\\) variables set to zero, the probability of failure is now given by the constant and the appropriate dummy or counter coefficient.\nNote the quantity of interest is not constant across time: it’s a conditional probability; the probability of failing at \\(t\\) given the estimated probability of survival through \\(t-1\\) - \\(h(t)|S(t)\\).\n\n\n\nMonotonic hazard\nSo how to deal with this, incorporating memory: thinking in terms of hazards rather than probabilities (i.e., conditional rather than unconditional probabilities), what if we measure survival time?\n\nThe binary \\(y\\) variable is an indicator of failure at \\(t\\); the model estimates \\(f(t)\\), which we’ve said is not especially informative since subjects might fail before \\(t\\).\nThink of the number of periods up to failure as the cumulative survival time, \\(S(t)\\).\n\nSee how we’re starting to construct the hazard rate by its parts.\nHere’s an example in the democratic peace data:\n\\(dispute = \\beta_0+ \\beta_1(survival)\\)\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\nprocess_data &lt;- function(data) {\n  data %&gt;%\n    arrange(dyad, year) %&gt;%\n    group_by(dyad) %&gt;%\n    mutate(\n      dispute_lag = lag(dispute, default = 1),  # Treat first year as following a dispute\n      reset_group = cumsum(dispute_lag == 1)\n    ) %&gt;%\n    group_by(dyad, reset_group) %&gt;%\n    mutate(\n      stime = row_number() - 1  # Start counting from 0\n    ) %&gt;%\n    ungroup() %&gt;%\n    dplyr::select(-dispute_lag, -reset_group)\n}\n\nsurvivaldp &lt;- process_data(dp)\n\ndpm2 &lt;-glm(dispute ~ stime, family=binomial(link=\"logit\"), data=survivaldp )\n\nstargazer(dpm2, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Survival Time\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Time\n\n\n-0.265*** (0.010)\n\n\n\n\nConstant\n\n\n-1.513*** (0.045)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n6,208.082\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\nAnd here are predicted probabilities from the model.\n\n\ncode\natmean &lt;- data.frame(stime=seq(1,34,1))\n\npredictions &lt;- data.frame(atmean, predict(dpm2, newdata=atmean, type=\"response\", se=TRUE)) \n\nggplot(predictions, aes(x=stime, y=fit)) +\n  geom_line() +\n  labs(x=\"Time since last dispute\", y=\"Pr(Dispute)\") +\n  geom_ribbon(aes(ymin=fit-1.96*se.fit, ymax=fit+1.96*se.fit), fill=\"#005A43\", alpha=0.4) +\n  theme_minimal()+\n  theme(panel.grid.major = element_line(color = \"black\",\n                                        size = 0.25,\n                                        linetype = 1)) +\n  theme(panel.grid.minor = element_line(color = \"black\",\n                                        size = 0.15,\n                                        linetype = 1))\n\n\n\n\n\n\n\n\n\nWhere the baseline hazard is no longer constant - it can increase or decrease monotonically:\n\\[h_0(t) = \\frac{1}{1+ exp(\\beta_0 + \\gamma_0(t))}\\]\nthe baseline hazard is the constant plus the effect of survival time - the \\(x\\) effects are shifts around this baseline hazard.\n\\[h_0(t) = \\frac{1}{1+ exp(\\beta_0 + \\gamma_0(t)) + x'\\beta)}\\]\nThe baseline hazard is no longer constrained to be constant, though it can be if \\(\\gamma_0=0\\).\n\nThis model accounts for “memory” - the QI is now the hazard.\nThe hazard is not constrained to be constant, but is constrained to be monotonic.\nTo relax this, we can\n\ninclude dummy variables - these are discrete time indicators based on the counter.\ninclude cubic splines or lowess estimates - these are smoothed time functions based on the counter.\ninclude polynomials of the time counter.\n\n\n\n\nNon-monotonic hazard\n\n\ncode\nsurvivaldp &lt;- survivaldp %&gt;% mutate(stime2=stime^2, stime3=stime^3)\n\ndpm3 &lt;-glm(dispute ~ stime+stime2+stime3, family=binomial(link=\"logit\"), data=survivaldp )\n\nstargazer(dpm3, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Survival Time\", \"Survival Time2\", \"Survival Time3\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Time\n\n\n-0.830*** (0.035)\n\n\n\n\nSurvival Time2\n\n\n0.052*** (0.004)\n\n\n\n\nSurvival Time3\n\n\n-0.001*** (0.0001)\n\n\n\n\nConstant\n\n\n-0.950*** (0.048)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n5,742.615\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\n\ncode\natmean &lt;- data.frame(stime=seq(1,34,1), stime2=seq(1,34,1)^2, stime3=seq(1,34,1)^3)\n\npredictions &lt;- data.frame(atmean, predict(dpm3, newdata=atmean, type=\"response\", se=TRUE)) \n\nggplot(predictions, aes(x=stime, y=fit)) +\n  geom_line() +\n  labs(x=\"Time since last dispute\", y=\"Pr(Dispute)\") +\n  geom_ribbon(aes(ymin=fit-1.96*se.fit, ymax=fit+1.96*se.fit), fill=\"#005A43\", alpha=0.4) +\n  theme_minimal()+\n  theme(panel.grid.major = element_line(color = \"black\",\n                                        size = 0.25,\n                                        linetype = 1)) +\n  theme(panel.grid.minor = element_line(color = \"black\",\n                                        size = 0.15,\n                                        linetype = 1))\n\n\n\n\n\n\n\n\n\nThis last is a close approximation of a Cox proportional hazards model. The hazard is non monotonic; it nests the exponential (constant hazard), and the monotonic (Weibull) hazard, and is very general. Besides, it’s very easy to estimate and interpret.\n\n\nUnderstanding substantive variables in the hazard context\nThe survival variables now permit the baseline hazard to vary. The effects of \\(x\\) variables can be thought of as deviations from those baseline hazards. For example, think about the models presented above, but with democracy. The estimates on democracy will shift the baseline hazard up or down.\n\n\ncode\nsurvivaldp &lt;- survivaldp %&gt;% mutate(stime2=stime^2, stime3=stime^3)\n\ndpm4 &lt;-glm(dispute ~ stime+stime2+stime3+deml, family=binomial(link=\"logit\"), data=survivaldp )\n\nstargazer(dpm4, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Survival Time\", \"Survival Time2\", \"Survival Time3\", \"Democracy\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Time\n\n\n-0.823*** (0.035)\n\n\n\n\nSurvival Time2\n\n\n0.051*** (0.004)\n\n\n\n\nSurvival Time3\n\n\n-0.001*** (0.0001)\n\n\n\n\nDemocracy\n\n\n-0.065*** (0.007)\n\n\n\n\nConstant\n\n\n-1.298*** (0.065)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n5,650.709\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\n\ncode\ndpm4 &lt;-glm(dispute ~ stime+stime2+stime3+deml+caprat, family=binomial(link=\"logit\"), data=survivaldp )\n\n# copy estimation data for avg effects\ndppred &lt;- survivaldp\n\n# df for output\ndf &lt;- data.frame(time=seq(1,34,1))\nfor (d in seq(-10,10,10)) {\n    df[paste0(\"p\", d+10)] &lt;- NA\n  df[paste0(\"Se\", d+10)] &lt;- NA\n}\n\n# predictions\nfor (d in seq(-10,10,10)) {\n  dppred$deml &lt;- d\n for (t in seq(1,34,1)) {  \n  dppred$stime &lt;- t\n  dppred$stime2 &lt;- t^2 \n  dppred$stime3 &lt;- t^3\n  pred &lt;- predict(dpm4, newdata=dppred, type=\"response\", se=TRUE)\n  df[t, paste0(\"p\", d+10)] &lt;- mean(pred$fit, na.rm=TRUE)\n  df[t, paste0(\"Se\", d+10)] &lt;- mean(pred$se.fit, na.rm=TRUE)\n  df$time[t] &lt;- t\n}\n}\n\n# plot\nggplot(df, aes(x=time, y=p0)) +\n  geom_line(color=\"#005A43\") +\n  geom_ribbon(aes(ymin=p0-1.96*Se0, ymax=p0+1.96*Se0), fill=\"#6CC24A\", alpha=0.4) +\n  geom_line(aes(y=p10), color=\"#005A43\") +\n  geom_ribbon(aes(ymin=p10-1.96*Se10, ymax=p10+1.96*Se10), fill=\"#A7DA92\", alpha=0.4) +\n  geom_line(aes(y=p20), color=\"#005A43\") +\n  geom_ribbon(aes(ymin=p20-1.96*Se20, ymax=p20+1.96*Se20), fill=\"#005A43\", alpha=0.4) +\n  labs(x=\"Time since last dispute\", y=\"Pr(Dispute)\") +\n  theme_minimal()+\n  theme(panel.grid.major = element_line(color = \"black\",\n                                        size = 0.25,\n                                        linetype = 1)) +\n  theme(panel.grid.minor = element_line(color = \"black\",\n                                        size = 0.15,\n                                        linetype = 1))"
  },
  {
    "objectID": "discretehazards24.html#an-alternative---transitions",
    "href": "discretehazards24.html#an-alternative---transitions",
    "title": "Discrete Time Hazard Models",
    "section": "An alternative - transitions",
    "text": "An alternative - transitions\nWhat happens if we lag \\(y\\) as we might with a continuous variable (say, in OLS), such that the estimation model is\n\\[y_t = \\beta_0 + \\beta_1(x_1) + \\ldots + \\gamma(y_{t-1}) + \\varepsilon\\]\nWith binary time series data, lagging \\(y\\) would measure changes of state - these are a class known as transition models (there are a variety of these).\nIn general, these are interactive models where\n\\[Pr(y_i=1) = F(x_{i,t}\\beta + y_{i,t-1}*x_{i,t} \\gamma)\\]\nand \\(\\gamma\\) measures the difference in effect when \\(y_{i,t-1} = 0\\) (this is just \\(\\beta\\)), and when \\(y_{i,t-1}  = 1\\); denote this \\(\\alpha\\). So \\(\\gamma = \\beta - \\alpha\\). That difference indicates the conditional probability of state transitions from the state where \\(y\\) takes on one value, to the state where it takes on the other value.\nTransition models are useful, but measure something fundamentally different from the latent hazard rate, or the chances of failure given a history of survival. Put differently, lagging \\(y\\) in a binary variable model does not measure memory or persistence; it does not measure the extent to which the observed value today depends on the value yesterday; it does not measure how the latent probability of failure today depends on surviving through yesterday."
  }
]