[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site:\nCreated with Quarto.\nAbout me:\nProfessor of political science, PhD Florida State 1999. I’ve taught this class for a long time, but never the same way twice. I study models of political violence, collect data on protests and repression.\n\n\n\n Back to top"
  },
  {
    "objectID": "likelihood24.html#how-would-you-characterize-the-variable-measuring-deaths-by-mule-kick",
    "href": "likelihood24.html#how-would-you-characterize-the-variable-measuring-deaths-by-mule-kick",
    "title": "Likelihood",
    "section": "How would you characterize the variable measuring deaths by mule kick?",
    "text": "How would you characterize the variable measuring deaths by mule kick?\n\nVariable measures events.\nEvents are discrete, not continuous.\nAre events correlated or independent?\nVariable is bounded; cannot be below zero.\n\nSo we need to think about two different things here - the distribution of \\(y\\) based on its observed distribution, and the link between the \\(X\\) variables and \\(\\widetilde{y}\\), the latent quantity of interest.\n\nWhat distribution might describe the frequency of mule kick deaths?\nNeeds to be discrete.\nNeeds to characterize rare events - at most we see about four per period, so relatively rare.\nNeeds to have a lower bound at zero (since we can’t observe negative numbers of deaths), and upper bound at \\(+\\infty\\)\n\n\n\\[Pr(Y=y_{i})=\\frac{e^{-\\lambda}\\lambda^{y_{i}}}{y_{i}!}\\]"
  },
  {
    "objectID": "likelihood24.html#the-poisson-distribution",
    "href": "likelihood24.html#the-poisson-distribution",
    "title": "Likelihood",
    "section": "The Poisson Distribution",
    "text": "The Poisson Distribution\nThe Poisson distribution is a discrete distribution that characterizes the number of events that occur in a fixed interval of time (or sometimes, space). Below are 3 Poisson distributions with means of .5, 1.5, and 2.5.\n\n\ncode\n#using a large sample, n=1000, simulate and plot the poisson distribution for mean values of .5, 1.5, and 2.5\n\n# Load required libraries\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\n\n# Set seed for reproducibility\nset.seed(8675309)\n\n# Simulate data\nn &lt;- 10000  # n for each distribution\nlambdas &lt;- c(0.5, 1.5, 2.5)  # Means \n\ndf &lt;- data.frame(\n  lambda_0.5 = rpois(n, lambda = 0.5),\n  lambda_1.5 = rpois(n, lambda = 1.5),\n  lambda_2.5 = rpois(n, lambda = 2.5)\n)\n\n# Reshape the data for ggplot\ndf_long &lt;- df %&gt;%\n  pivot_longer(cols = everything(), \n               names_to = \"distribution\", \n               values_to = \"value\") %&gt;%\n  mutate(distribution = factor(distribution, \n                               levels = c(\"lambda_0.5\", \"lambda_1.5\", \"lambda_2.5\"),\n                               labels = c(\"λ = 0.5\", \"λ = 1.5\", \"λ = 2.5\")))\n\n# plot \nggplot(df_long, aes(x = value, fill = distribution)) +\n  geom_density(position = \"identity\", alpha = 0.5, adjust=4) +\n  facet_wrap(~ distribution, ncol = 1, scales = \"free_y\") +\n  scale_fill_manual(values = c(\"#000000\", \"#6CC24A\", \"#005A43\")) +\n  labs(title = \"Simulated Poisson Distributions\",\n       x = \"Value\",\n       y = \"Count\",\n       fill = \"Distribution\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nThinking in terms of the data, let’s write a likelihood function, the joint probability for all \\(i\\) observations in the sample, \\(n\\):\n\\[\n\\mathcal{L}(\\lambda)= \\prod_{i=1}^{n} \\left[\\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!} \\right]\n\\]\nTake the natural log of the likelihood function:\n\\[\n\\ln \\mathcal{L}(\\lambda)= \\ln \\left\\{\\prod_{i=1}^{n} \\left[\\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!} \\right] \\right\\}\n\\]\n\\[\\ln \\mathcal{L}(\\lambda)= \\sum_{i=1}^{n} \\left[-\\lambda + y_i \\ln(\\lambda) - \\ln(y_i!) \\right]\n\\]\nWhat about the \\(X\\) variables? Parameterize the model with respect to those variables such that they influence the mean, \\(\\lambda\\). So let’s make \\(\\lambda\\) a function of the \\(X\\) variables and their effects, \\(\\beta\\), using the exponential distribution as the link function:\n\\[\nE[Y|X]=\\lambda =exp(X\\beta)\n\\]\nThe exponential ensures we won’t have negative predictions.\nPutting all this together we have:\n\\[\n\\ln \\mathcal{L}(\\lambda)= \\sum_{i=1}^{n} \\left[-e^{X\\beta} + y_i \\ln(X\\beta) - \\ln(y_i!) \\right]\n\\]"
  },
  {
    "objectID": "likelihood24.html#estimation-technology-ols",
    "href": "likelihood24.html#estimation-technology-ols",
    "title": "Likelihood",
    "section": "Estimation Technology: OLS",
    "text": "Estimation Technology: OLS\nRecall that the technology of OLS is to assume a normally distributed error term, minimize the sum of those squared errors analytically using calculus."
  },
  {
    "objectID": "likelihood24.html#estimation-technology-mle",
    "href": "likelihood24.html#estimation-technology-mle",
    "title": "Likelihood",
    "section": "Estimation Technology: MLE",
    "text": "Estimation Technology: MLE\nThe technology of ML is to maximize the LLF with respect to \\(\\beta\\). We can do this in a couple of different ways:\n\nanalytic methods - solve calculus. Some/many models do not have analytical or closed form solutions.\nnumerical methods - use an algorithm to estimate starting values for \\(\\theta\\), then hill climb until the first derivative is zero, and the second derivative is negative. This is iterative, trying values, looking at the derivatives. This is what nearly all ML estimation uses - there are different algorithms for doing this. The most commonly used is the Newton Raphson method - it’s illustrated in detail in the maximization slides\n\n\nAnalytic Methods\nWith some functions, we can solve for the unknowns directly. Let’s return to the Poisson log-likelihood function and consider data on the number of civil wars in Africa over a ten year period, and the data are as follows:\n\\(y\\) = {5 0 1 1 0 3 2 3 4 1}\nThe log-likelihood function is:\n\\[\\begin{aligned}\n\\ln \\mathcal{L}(\\lambda|Y) =\\ln \\left[ \\prod\\limits_{i=1}^{N} \\frac{e^{-\\lambda}\\lambda^{y_{i}}}{y_{i}!}\\right]\\nonumber \\\\ \\nonumber \\\\\n=-N \\lambda+ \\sum(y_{i}) \\ln(\\lambda) - \\sum(\\ln(y_{i}!)) \\nonumber\n\\end{aligned}\\]\n\\(\\lambda\\) is the unknown we want to solve for. Taking the derivative with respect to \\(\\lambda\\) and setting equal to zero:\n\\[\\begin{aligned}\n\\frac{\\partial \\ln \\mathcal{L}}{\\partial \\lambda}=-N \\lambda+ \\sum(y_{i}) \\ln(\\lambda) - \\sum(\\ln(y_{i}!)) \\nonumber \\\\ \\nonumber  \\\\\n0=-N + \\frac{\\sum y_{i}}{\\lambda} \\nonumber\\\\ \\nonumber \\\\\n\\color{red}{\\widehat{\\lambda}= \\frac{\\sum y_{i}}{N}} \\nonumber\n\\end{aligned}\\]\nThis is just the sample mean of course - let’s plug in our data and solve for \\(\\lambda\\):\n\\[\\widehat{\\lambda}= \\frac{20}{10} \\] So the value of \\(\\lambda\\) the maximizes the function is 2. This is a trivial example in the sense that applications with \\(x\\) variables are sufficiently complicated that analytical methods are not usually possible, so we turn to numerical methods."
  },
  {
    "objectID": "likelihood24.html#normal-linear-llf",
    "href": "likelihood24.html#normal-linear-llf",
    "title": "Likelihood",
    "section": "Normal (linear) LLF:",
    "text": "Normal (linear) LLF:\n\\[\n= -\\frac{N}{2}(\\ln(2\\pi)) -\\frac{N}{2}(\\ln(\\sigma^{2})) -\\frac{1}{2\\sigma^{2}}\\left[\\sum\\limits_{i=1}^{n}(y_{i}-\\mu)^{2}\\right] \\nonumber\n\\]\nNotice \\(N\\) in the numerator; recall the rule of summation that \\(\\sum\\limits_{i=1}^{n}a= n\\cdot a\\).\nNow, take the derivative of the log-likelihood with respect to each of the parameters in turn (ignoring constant terms and terms that pertain exclusively to the other parameter).\n\\[\n\\frac{\\partial \\ln L}{\\partial \\mu}= \\frac{1}{\\sigma^{2}}\\sum(y_{i}-\\mu)=0 \\nonumber\\\\\n=\\sum(y_{i}-\\mu) = \\sum y_{i}- \\sum \\mu  \\nonumber\\\\\n=\\sum y_{i}- N \\mu = 0 \\nonumber \\\\\n\\mu=\\frac{\\sum y_{i}}{N} = \\widehat{y}\\nonumber\n\\]\nwe can also solve for \\(\\sigma^{2}\\), getting\n\\[\n\\frac{\\partial \\ln L}{\\partial \\sigma^{2}}= -\\frac{N}{2 \\sigma^{2}}+\\frac{1}{2 \\sigma^{4}} +\\sum(y_{i}-\\mu)=0 \\nonumber\\\\ \\nonumber\\\\\n=-\\frac{N}{2}\\sigma^{2}+\\frac{1}{2}\\sum(y_{i}-\\bar{y})^{2}= 0 \\nonumber\\\\ \\nonumber\\\\\n\\ldots\n\\widehat{\\sigma^{2}}=\\frac{\\sum(y_{i}-\\bar{y})^{2}}{N} \\nonumber\n\\]\nThis is a biased estimator of \\(\\sigma^{2}\\); \\(\\sigma^{2}\\) is underestimated because the denominator should be \\(N-1\\).\nThe same thing in matrix notation:\n\\[ln\\mathcal{L}(y | X, \\beta, \\sigma^2) = -\\frac{N}{2} ln(2\\pi) - \\frac{N}{2} ln(\\sigma^2) -\\frac{1}{2} \\left[ \\frac{(y-X\\beta)'(y-X\\beta)}{\\sigma^2} \\right] \\]\nrewriting to isolate the parameters:\n\\[\nln\\mathcal{L}(y | X, \\beta, \\sigma^2) = -\\frac{N}{2} ln(2\\pi) - \\frac{N}{2} ln(\\sigma^2) -\\frac{1}{2\\sigma^2} \\left[ yy'- 2y' X\\beta +\\beta' X' X\\beta) \\right]\n\\]\nTake derivatives of \\(\\ln \\mathcal{L}\\) w.r.t. \\(\\beta\\) and \\(\\sigma^2\\) (and skipping a lot here):\n\\[\\frac{\\partial ln \\mathcal{L}}{\\partial \\beta} = \\frac{1}{\\sigma^2} (X'y - X'X \\beta)\\]\nsetting equal to zero …\n\\[ \\frac{1}{\\sigma^2} (X'y - X'X \\beta) = 0\\] \\[X'X \\beta = X'y\\] \\[\\widehat{\\beta} = (X'X)^{-1} X' y \\]\n…going through the same thing for \\(\\sigma^2\\) gives us:\n\\[\\widehat{\\sigma^2} = \\frac{e'e}{N}\\]\nSo aside from seeing how analytic methods work, we have also seen that the BLUE OLS estimator is the ML estimator for \\(\\beta\\), and that the variance estimate in ML is biased downward (the denominator is always too large by \\(k-1\\)). This difference disappears in large samples.\nWhy do we leave OLS if these are the same? Because this is the rare case defined by normal data which both satisfies the OLS requirement for a normal disturbance, and permits MLE estimation with a normal LLF. With non-normal data, OLS and ML estimators diverge quite a lot."
  },
  {
    "objectID": "likelihood24.html#numerical-methods",
    "href": "likelihood24.html#numerical-methods",
    "title": "Likelihood",
    "section": "Numerical Methods",
    "text": "Numerical Methods\nNumerical methods are computationally intensive ways to plug in possible parameter values, generate a log likelihood, and then use calculus to evaluate whether the that value is a maximum. We use numerical methods when no analytic or “closed form” solution exists which is essentially all the time.\nDo this by evaluating:\n\nthe first derivative of the LLF - by finding the point on the function where a tangent line has a slope equal to zero, we know we’ve found an inflection point.\nthe second derivative of the LLF - if the rate of change in the function at the very next point is increasing, it’s a minimum; decreasing, it’s a maximum.\n\nthe Hessian matrix - the matrix of second derivatives - tells us the curvature of the LLF, or the rate of change.\n\nSuppose that we have the event count data reported above representing civil wars in Africa, and that we want to compute the likelihood of \\(\\lambda|Y\\). We can compute the likelihood using numerical methods; one specific technique is a grid search procedure. Just as we might try different values for \\(x\\) when graphing a function, \\(f(x)\\) in algebra, we will insert possible values for \\(\\lambda\\) into the log-likelihood function in such a way that we can identify an apparent maximum (a value for \\(\\lambda\\) for which the log-likelihood is at its largest compared to contiguous values of \\(\\lambda\\)). Put another way, we take a guess at the value of \\(\\lambda\\), compute the log-likelihood, and take another guess at \\(\\lambda\\), compute the log-likelihood and compare the two estimates of the likelihood; we repeat this process until a pattern emerges such that we can discern a maximum value.\nThe log-likelihood function for the poisson distributed data on civil wars is\n\\[\n\\ln \\mathcal{L}(\\lambda|Y)= \\ln \\left[\\frac{e^{-10\\lambda}\\lambda^{20}}{207360}\\right] \\nonumber  \\\\ \\nonumber \\\\\n= -10 \\lambda+ 20 \\ln(\\lambda) - \\ln(207360) \\nonumber\n\\]\nSuppose we make some guesses regarding the value of \\(\\lambda\\), plug them into the function and compare the resulting values of the log-likelihood - take a look at the code chunk below:\n\n\ncode\n#iterate over lambda, create data frame of lambda and log-likelihood\nlambda &lt;- seq(0.1, 3.5, by=0.1)\nllf &lt;- NULL\nfor (i in 1:length(lambda)){\n  L &lt;- -10*lambda[i] + 20*log(lambda[i]) - log(207360)\n  llf &lt;- data.frame(rbind(llf, c(lambda=lambda[i], ll=L)))\n}\n\n#highchart with reference line at maximum value of the log-likelihood\nhighchart() %&gt;% \n  hc_add_series(llf, \"line\", hcaes(x=lambda, y=ll)) %&gt;% \n  hc_title(text=\"Log-Likelihood Estimates\") %&gt;% \n  hc_subtitle(text=\"Civil Wars in Africa\") %&gt;% \n hc_xAxis(title = list(text = \"Lambda\"), plotLines = list(list(value = 2, color=\"red\"))) %&gt;%\n  hc_yAxis(title = list(text = \"log-likelihood\"), plotLines = list(list(value = max(llf$ll), color=\"red\"))) %&gt;%\n  hc_tooltip(pointFormat = \"Lambda: {point.x}&lt;br&gt;Log-Likelihood: {point.y}\") %&gt;% \n  hc_colors(\"#005A43\") \n\n\n\n\n\n\nWe can see that the largest value of the likelihood is where \\(\\lambda\\) = 2 - that’s the value that maximizes the likelihood function. And not surprisingly, notice that we have arrived at the same solution we produced in the analytic example above. This is another trivial example insofar as grid search methods are usually not sufficient for solving multivariate problems (nor for computing the variance-covariance matrix)."
  },
  {
    "objectID": "likelihood24.html#how-numerical-methods-work",
    "href": "likelihood24.html#how-numerical-methods-work",
    "title": "Likelihood",
    "section": "How numerical methods work",
    "text": "How numerical methods work\n\nChoose starting values of \\(\\beta\\) (sometimes from OLS) to estimate the log-likelihood.\nTake the derivative of the log-likelihood with respect to the parameters to find the gradient}. The gradient (or the gradient matrix, a \\(kxk\\) matrix) tells us the direction of the slope of a line tangent to the curve at the point of the log-likelihood estimate.\nIf the gradient is positive (if the matrix is positive definite), then \\(ln \\mathcal{L}\\) is increasing in \\(\\beta\\) - the slope is increasing, so increase our estimate of \\(\\beta\\) and try again.\nIf the gradient is negative (if the matrix is negative definite), the \\(ln \\mathcal{L}\\) is decreasing in \\(\\beta\\) - the slope is decreasing, so we’ve passed the maximum; choose a smaller value for \\(\\beta\\) and try again.\nAs the log-likelihood approaches the maximum, the gradient approaches zero - the slope of the line tangent to the curve at the point of the log-likelihood estimate is approaching zero, indicating we’re reaching the maximum of the function. Stop the search and evaluate the estimates of \\(\\beta\\) that produced the zero gradient.\nThroughout this process, we need to evaluate the second derivatives in order to figure out the rate at which the slope is changing; this helps us tell how close or far we are from the maximum. The second derivative describes the curvature of the LLF, or the rate of change.\nThe matrix of second derivatives (the Hessian matrix) or its approximation also provide the source of our estimates of the variance, and thus the standard errors.\n\nThe first derivative tells us the direction in which the function is changing. This is obviously important since we’re trying to find the maximum.\nThink of this as trying to figure out when you’re exactly at the top of a hill. The slope (the grade, the gradient) is positive while you’re climbing to the top, it’s zero at the top, and it’s negative on the way down the other side.\nBut is the hill flat or steep? If it’s flat, then the change in the slope between point A and point B is likely to be very small - this, of course, can make it difficult to know exactly when we’re at the top (the maximum). On the other hand, if the hill is very steep, the change in the slope between two points is pretty substantial. Put another way, the rate of change in the slope is larger (faster) the steeper the slope; it’s smaller (slower) the flatter the slope.\nThis matters to maximization because the second derivatives tell us how big (or small) a step we should take up the hill as we try to find the top. Suppose that the function is very flat; as indicated above, the change in the slope between two points would be small, so we can take larger steps in order to try to find the maximum. The second derivatives would tell us that the rate of change is very small, so we should take larger steps.\nThe software performing the estimation will choose the next value of \\(\\beta\\) a bit further away from the last value it tried. On the other hand, if the second derivatives are large so the rate of change is fast, we want to take relatively small steps so we don’t step right over the maximum. In any case, that’s the intuition for why we need to know the matrix of second derivatives."
  },
  {
    "objectID": "likelihood24.html#variance-covariance-matrix",
    "href": "likelihood24.html#variance-covariance-matrix",
    "title": "Likelihood",
    "section": "Variance-Covariance matrix",
    "text": "Variance-Covariance matrix\nEstimating the second derivatives can be a real nightmare in estimation, but it’s important not only for finding the maximum of the function (and therefore in estimating the \\(\\beta\\)s), but for computing the variance-covariance matrix as well. Here are our options:\n\nFind the Hessian. The Hessian is a \\(kxk\\) matrix of the second derivatives of the log-likelihood function with respect to \\(\\beta\\), where the second derivatives are on the main diagonal. Commonly estimated using the Newton-Raphson algorithm.\nFind the information matrix. This is the negative of the expected value of the Hessian matrix, computed using the method of scoring.\n\nOuter product approximation, where we sum the squares of the first derivatives (thus avoiding the second derivatives all together). This is computed using the Berndt, Hall, Hall, and Hausman} or BHHH algorithm."
  },
  {
    "objectID": "likelihood24.html#grid-search",
    "href": "likelihood24.html#grid-search",
    "title": "Likelihood",
    "section": "Grid Search",
    "text": "Grid Search\nGrid search is another method for maximization. The process is to iteratively try values for the parameters of interest, refining those values as the log-likelihood gets larger and larger. In general, we plug in values for the parameters, compute the likelihood, then identify the largest LL value - the parameters that produce that value are our answer.\nThis method is instructive for how numerical methods work, but not practical in most applications with more than a couple of unknowns."
  },
  {
    "objectID": "likelihood24.html#latent-variable-motivation",
    "href": "likelihood24.html#latent-variable-motivation",
    "title": "Likelihood",
    "section": "Latent Variable Motivation",
    "text": "Latent Variable Motivation\nThere are a couple of (related) ways to motivate the model. Let’s assume a latent quantity we’re interested, denoted \\(y^*\\), but our observations of \\(y\\) are limited to successes (\\(y_i=1\\)) and failures (\\(y_i=0\\)).\n\\[\ny^{*}=\\mathbf{x_i \\beta}+\\epsilon_i \\nonumber\n\\]\nfor \\(y^{*}\\), the latent variable,\n\\[\ny_{i} = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{if $y^{*}_{1}&gt;\\kappa$} \\\\\n         0, & \\mbox{if $y^{*}_{1} \\leq \\kappa$}\n         \\end{array}\n     \\right.\n\\]\nwhere \\(\\kappa\\) is an unobserved threshold.\nMake probabilities statements,\n\\[\nPr(y_i=1) = Pr(y^{*}_{1}&gt;\\kappa) \\nonumber \\\\ \\nonumber \\\\\n=Pr(\\mathbf{x_i \\beta}+\\epsilon_i&gt;\\kappa) \\nonumber \\\\ \\nonumber \\\\\n=Pr(\\epsilon_i&gt;\\kappa-\\mathbf{x_i \\beta}) \\nonumber\n\\]\nNormalizing \\(\\kappa=0\\),\n\\[\nPr(y_i=1)=Pr(\\epsilon_i&gt;-\\mathbf{x_i \\beta}) \\nonumber\n\\]\n\\[\nPr(y_i=1)=Pr(\\epsilon_i&gt;-\\mathbf{x_i \\beta}) \\nonumber \\\\ \\nonumber \\\\\n=1-F(-\\mathbf{x_i \\beta}) \\nonumber\n\\]\nassuming \\(F\\) is symmetric, with unit variance,\n\\[\n\\pi_i= Pr(y_i=1)=1-F(-\\mathbf{x_i \\beta}) = F(\\mathbf{x_i \\beta}) \\\\   \\nonumber \\\\\n1-\\pi_i=Pr(y_i=0)= 1-F(\\mathbf{x_i \\beta})\n\\]"
  },
  {
    "objectID": "likelihood24.html#binomial-likelihood-function",
    "href": "likelihood24.html#binomial-likelihood-function",
    "title": "Likelihood",
    "section": "Binomial Likelihood Function",
    "text": "Binomial Likelihood Function\nThe observed data are binary, assumed binomial (\\(\\pi\\)), \\(y_i=0,1\\). The likelihood function must have two parts, one for cases where \\(y_i=0\\), the other for cases where \\(y_i=1\\). Recalling that \\(\\pi=F(\\mathbf{x_i \\beta})\\), and \\(1-\\pi= 1-F(\\mathbf{x_i \\beta})\\),\n\\[\nPr(y_1,y_2,y_3 \\ldots y_n) =  \\prod_{y=1}F(\\mathbf{x_i \\beta}) \\prod_{y=0}[1-F(\\mathbf{x_i \\beta})]\\nonumber\n\\]\nThis is the joint probability we observe all the data, \\(Y\\), simultaneously. We can rewrite this as the likelihood of observing the data given \\(\\beta\\),\n\\[\n\\mathcal{L} (Y|\\beta) =  \\prod_{i=1}^{N} [F(\\mathbf{x_i \\beta})]^{y_i} [1-F(\\mathbf{x_i \\beta})]^{1-y_i}\\nonumber\n\\]\nAnd take the natural log\n\\[\n\\ln(\\mathcal{L} (Y|\\beta)) = \\ln( \\prod_{i=1}^{N} [F(\\mathbf{x_i \\beta})]^{y_i} [1-F(\\mathbf{x_i \\beta})]^{1-y_i})\\nonumber \\\\ \\nonumber \\\\\n= \\sum_{i=1}^{N} y_i \\ln F(\\mathbf{x_i \\beta})+ (1-y_i) \\ln[1-F(\\mathbf{x_i \\beta})] \\nonumber\n\\]\nNote the two parts of the LLF corresponding to the limited observations in the data, 0,1."
  },
  {
    "objectID": "likelihood24.html#choosing-a-link",
    "href": "likelihood24.html#choosing-a-link",
    "title": "Likelihood",
    "section": "Choosing a Link",
    "text": "Choosing a Link\nLet’s make this a probit model by assuming the link to the latent variable is standard normal, so \\(F(\\cdot)\\sim N_{i.i.d.}(0,1)\\):\n\\[\n\\ln \\mathcal{L} (Y|\\beta) = \\sum_{i=1}^{N} y_i \\ln \\Phi(\\mathbf{x_i \\beta})+ (1-y_i) \\ln[1-\\Phi(\\mathbf{x_i \\beta})] \\nonumber\n\\]\nLogit would look like this:\n\\[\n\\ln \\mathcal{L} (Y|\\beta) = \\sum_{i=1}^{N} \\left\\{ y_i \\ln \\left(\\frac{1}{1+e^{-\\mathbf{x_i \\beta}}}\\right)+ (1-y_i) \\ln \\left[1-\\left(\\frac{1}{1+e^{-\\mathbf{x_i \\beta}}}\\right)\\right] \\right\\}\\nonumber\n\\]"
  },
  {
    "objectID": "likelihood24.html#estimation",
    "href": "likelihood24.html#estimation",
    "title": "Likelihood",
    "section": "Estimation",
    "text": "Estimation\nIdeally, we’d like just to estimate by finding the values of \\(\\beta\\) that maximize the log-likelihood function, and do so analytically (i.e., using calculus). This is what we do in OLS, though with respect to minimizing the sum of the squared residuals. But because the solution is non-linear in \\(\\beta\\), there is no closed form or simple analytic solution.\nAs a result, ML models produce estimates of \\(\\beta\\) by using numerical optimization methods. These are generally iterative attempts to narrow down the range in which the maximum lies by plugging in different values of \\(\\beta\\) until the range is so small, we can safely say we’ve maximized the function using those values of \\(\\beta\\)."
  },
  {
    "objectID": "likelihood24.html#maximization",
    "href": "likelihood24.html#maximization",
    "title": "Likelihood",
    "section": "Maximization",
    "text": "Maximization\nSuppose we have binary data that look like this:\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\nOur question is what is the distribution parameter most likely responsible for having generated these observed data.\nWe need to plug a hypothetical value for the distribution parameter into the log-likelihood function, compute log-likelihoods for each observation, then do the same thing with other hypothetical values. Whichever value produces the biggest log-likelihoods is the value most likely responsible for producing the data we have.\nWhat do we mean by distribution parameter? Well, in the LLF below, we’ve referred to our unknown as \\(F(\\mathbf{x_i \\beta})\\), but we really mean we need an estimate of the parameter \\(\\Theta\\) which represents the effects of the \\(X\\)s via the functional form we’ve imposed by assuming a distribution of \\(\\epsilon\\). In this particular case (for simplicity) we don’t have any \\(X\\) variables.\n\\[\n\\ln(\\mathcal{L} (Y|\\Theta)) = \\sum_{i=1}^{N} y_i \\ln F(\\mathbf{\\Theta})+ (1-y_i) \\ln[1-F(\\mathbf{\\Theta})] \\nonumber\n\\]\nHere’s what we’ll do:\n\nchoose some hypothetical values of \\(\\Theta\\); since this is binary, and our latent variable a probability, let’s choose values from .2 to .8.\ncompute \\(N\\) log-likelihoods for each value of \\(\\Theta\\); N=20, and we have 7 values of \\(\\Theta\\).\nsum the \\(N\\) log-likelihoods for each value of \\(\\Theta\\); so we’ll end up with 7 summed log-likelihoods.\nevaluate the summed log-likelihoods, and see which is largest.\ndeclare the value of \\(\\Theta\\) that produced that largest summed log-likelihood as the parameter most likely to have generated the data.\n\nLet \\(\\Theta=.2\\):\n\\[\n\\ln(\\mathcal{L}_{i=1} (Y|\\Theta)) = 0 *\\ln (.2)+ (1-0) \\ln[1-.2] =-0.2231\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=2} (Y|\\Theta)) = 0 *\\ln (.2)+ (1-0) \\ln[1-.2]=-0.2231 \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=3} (Y|\\Theta)) = 0 *\\ln (.2)+ (1-0) \\ln[1-.2]=-0.2231 \\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=11} (Y|\\Theta)) = 1 *\\ln (.2)+ (1-1) \\ln[1-.2] =-1.609\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=12} (Y|\\Theta)) = 1 *\\ln (.2)+ (1-1) \\ln[1-.2]=-1.609 \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=13} (Y|\\Theta)) = 1 *\\ln (.2)+ (1-1) \\ln[1-.2] =-1.609\\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\sum_{i=1}^{20}=-18.32100 \\nonumber\n\\]\nLet \\(\\Theta=.3\\):\n\\[\n\\ln(\\mathcal{L}_{i=1} (Y|\\Theta)) = 0 *\\ln (.3)+ (1-0) \\ln[1-.3] =-0.3567\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=2} (Y|\\Theta)) = 0 *\\ln (.3)+ (1-0) \\ln[1-.3]=-0.3567 \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=3} (Y|\\Theta)) = 0 *\\ln (.3)+ (1-0) \\ln[1-.3]=-0.3567 \\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=11} (Y|\\Theta)) = 1 *\\ln (.3)+ (1-1) \\ln[1-.3] =-1.204\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=12} (Y|\\Theta)) = 1 *\\ln (.3)+ (1-1) \\ln[1-.3]=-1.204\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=13} (Y|\\Theta)) = 1 *\\ln (.3)+ (1-1) \\ln[1-.3] =-1.204\\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\sum_{i=1}^{20}=-15.60700 \\nonumber\n\\]\nLet \\(\\Theta=.4\\):\n\\[\n\\ln(\\mathcal{L}_{i=1} (Y|\\Theta)) = 0 *\\ln (.4)+ (1-0) \\ln[1-.5] =-0.5108\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=2} (Y|\\Theta)) = 0 *\\ln (.4)+ (1-0) \\ln[1-.5]=-0.5108 \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=3} (Y|\\Theta)) = 0 *\\ln (.4)+ (1-0) \\ln[1-.5]=-0.5108 \\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=11} (Y|\\Theta)) = 1 *\\ln (.4)+ (1-1) \\ln[1-.4] =-0.9163\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=12} (Y|\\Theta)) = 1 *\\ln (.4)+ (1-1) \\ln[1-.4]=-0.9163\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=13} (Y|\\Theta)) = 1 *\\ln (.4)+ (1-1) \\ln[1-.4] =-0.9163\\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\sum_{i=1}^{20}=-14.2711636 \\nonumber\n\\]\nLet \\(\\Theta=.5\\):\n\\[\n\\ln(\\mathcal{L}_{i=1} (Y|\\Theta)) = 0 *\\ln (.5)+ (1-0) \\ln[1-.5] =-0.6931\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=2} (Y|\\Theta)) = 0 *\\ln (.5)+ (1-0) \\ln[1-.5]=-0.6931 \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=3} (Y|\\Theta)) = 0 *\\ln (.5)+ (1-0) \\ln[1-.5]=-0.6931 \\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=11} (Y|\\Theta)) = 1 *\\ln (.5)+ (1-1) \\ln[1-.5] =-0.6931\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=12} (Y|\\Theta)) = 1 *\\ln (.5)+ (1-1) \\ln[1-.5]=-0.6931\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=13} (Y|\\Theta)) = 1 *\\ln (.5)+ (1-1) \\ln[1-.5] =-0.6931\\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\sum_{i=1}^{20}=-13.8629436 \\nonumber\n\\]\nLet’s compare what we have so far:\n\\[llf_{.2}&lt;llf_{.3}&lt;llf_{.4}&lt;llf_{.5}\\]\nYou can probably see some symmetry here due to the fact that half the data are ones, half zeros, so completing:\\~\\\n\\(llf_{.2}&lt;llf_{.3}&lt;llf_{.4}&lt;llf_{.5}&gt;llf_{.6}&gt;llf_{.7}&gt;llf_{.8}\\) \\~\\\nSo \\(\\Theta=.5\\) produces the largest log-likelihood (-13.86) and thus is the parameter most likely to have produced the observed data."
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "Binomial Models - video\nPrediction Methods - spring 2024 prediction video\nLikelihood Theory and Applications\nMaximization Methods - video\nBinary Model Extensions I\nDiscrete Hazards\nBinary Model Extensions II\nMechanism Paper Assignment\nInteractions in Nonlinear Models\nChoice Models I\nChoice Models II\nCount Models I\nCount Models II\nContinuous Time Hazards\n\n\n\n Back to top"
  },
  {
    "objectID": "binarymodels24.html",
    "href": "binarymodels24.html",
    "title": "Binary Response Models",
    "section": "",
    "text": "How can we model a binary \\(y\\) variable?\nDoes OLS (the linear probability model) work sufficiently well?\nHow can we build a maximum likelihood model?"
  },
  {
    "objectID": "binarymodels24.html#questions",
    "href": "binarymodels24.html#questions",
    "title": "Binary Response Models",
    "section": "",
    "text": "How can we model a binary \\(y\\) variable?\nDoes OLS (the linear probability model) work sufficiently well?\nHow can we build a maximum likelihood model?"
  },
  {
    "objectID": "binarymodels24.html#example---democratic-peace-data",
    "href": "binarymodels24.html#example---democratic-peace-data",
    "title": "Binary Response Models",
    "section": "Example - Democratic Peace data",
    "text": "Example - Democratic Peace data\nAs a running example, I’ll use the Democratic Peace data to estimate logit and probit models. These come from Oneal and Russett (1997)’s well-known study in ISQ. The units are dyad-years; the \\(y\\) variable is the presence or absence of a militarized dispute, and the \\(x\\) variables include a measure of democracy (the lowest of the two Polity scores in the dyad), and a set of controls. The principle expectation here is that as the lowest democracy score in the dyad increases, the probability of a militarized dispute decreases.\n\nPredictions out of bounds\nThis figured plots the predictions from a logit and OLS model. Unsurprisingly, the logit predictions are probabilities, so are in the \\([0,1]\\) interval. The OLS predictions are not, and are often out of bounds.\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\nlogitpreds &lt;- predict(m1, type=\"response\")\n\nm2 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"probit\"), data=dp )\nmols &lt;-lm(dispute ~ border+deml+caprat+ally, data=dp )\nolspreds &lt;- predict(mols)\n\ndf &lt;- data.frame(logitpreds, olspreds, dispute=as.factor(dp$dispute))\n\nggplot(df, aes(x=logitpreds, y=olspreds, color=dispute)) + \n  geom_point()+\n  labs(title=\"Predictions from Logit and OLS\", x=\"Logit Predictions\", y=\"OLS Predictions\")+\n  geom_hline(yintercept=0)+\n  theme_minimal() +\n  scale_color_manual(values=c(\"#005A43\", \"#6CC24A\")) +\n  annotate(\"text\", x=.05, y=-.05, label=\"2,147 Predictions out of bounds\", color=\"red\")\n\n\n\n\n\n\n\n\n\nHere’s the distribution of predictions from the OLS model - you’ll note the modal density is around .04 (which is the sample frequency of \\(y\\).), but that a substantial and long tail are negative, so out of probability bounds.\n\n\ncode\nggplot(df, aes(x=olspreds)) + \n  geom_density(alpha=.5)+\n  labs(title=\"Density of OLS Predictions\", x=\"Predictions\", y=\"Density\")+\n  theme_minimal()+\ngeom_vline(xintercept=0, linetype=\"dashed\")\n\n\n\n\n\n\n\n\n\n\n\nHeteroskedastic Residuals\nThe residuals from the OLS model appear heteroskedastic, and the distribution is not normal. In fact, the distribution appears more binomial, clustered around zero and one. This shouldn’t be surprising since the \\(y\\) variable only takes on values of zero and one, and since we compute the residuals by \\(u = y - \\hat{y}\\).\n\n\ncode\ndf &lt;- data.frame(df, mols$residuals)\n \nggplot(df, aes(x=mols.residuals, color=dispute)) + \n  geom_density()+\n  labs(title=\"Density of OLS Residuals\", x=\"Residuals\", y=\"Density\")+\n  theme_minimal()+\n    scale_color_manual(values=c(\"#005A43\", \"#6CC24A\")) +\n  geom_vline(xintercept=0, linetype=\"dashed\")"
  },
  {
    "objectID": "binarymodels24.html#when-is-the-lpm-appropriate",
    "href": "binarymodels24.html#when-is-the-lpm-appropriate",
    "title": "Binary Response Models",
    "section": "When is the LPM Appropriate?",
    "text": "When is the LPM Appropriate?\nThe best answer is never.\n\nThere seems to be a mild trend in the discipline to rehabilitate the LPM though it’s not clear why - that is, it’s hard to find statements about the advantages of doing so in any particular setting, or about the disadvantages of estimating a logit or probit model that would lead us to prefer the LPM.\n\nOLS is a rockin’ estimator, but it’s just not well suited to limited \\(y\\) variables. Efforts to rehabilitate the LPM are like putting lipstick on a pig."
  },
  {
    "objectID": "binarymodels24.html#examples-of-limited-dvs",
    "href": "binarymodels24.html#examples-of-limited-dvs",
    "title": "Binary Response Models",
    "section": "Examples of Limited DVs",
    "text": "Examples of Limited DVs\n\nbinary variables: 0=peace, 1=war; 0=vote, 1=don’t vote.\nunordered or nominal categorical variables: type of car you prefer: Honda, Toyota, Ford, Buick; policy choices; party or candidate choices.\nordered variables that take on few values: some survey responses.\ndiscrete count variables; number of episodes of scarring torture in a country-year, 0, 1, 2, 3, …, \\(\\infty\\); the number of flawed computer chips produced in a factory in a shift; the number of times a person has been arrested; the number of self-reported extramarital affairs; number of visits to your primary care doctor.\ntime to failure; how long a civil war lasts; how long a patient survives disease; how long a leader survives in office."
  },
  {
    "objectID": "binarymodels24.html#binary-y-variables-1",
    "href": "binarymodels24.html#binary-y-variables-1",
    "title": "Binary Response Models",
    "section": "Binary \\(y\\) variables",
    "text": "Binary \\(y\\) variables\nGenerally, we think of a binary variable as being the observable manifestation of some latent, unobserved continuous variable.\nIf we could adequately observe (and measure) the underlying continuous variable, we’d use some form of OLS regression to analyze that variable. But because we have limited observation, we turn to maximum likelihood methods to estimate a model that allows to use \\(y\\), but generate estimates of \\(y^*\\), the variable we wish we could measure."
  },
  {
    "objectID": "binarymodels24.html#a-nonlinear-model-for-binary-data",
    "href": "binarymodels24.html#a-nonlinear-model-for-binary-data",
    "title": "Binary Response Models",
    "section": "A nonlinear model for binary data",
    "text": "A nonlinear model for binary data\nSo \\(y\\) is binary, and we’ve established the linear model is not appropriate. The observed variable, \\(y\\), appears to be binomial (iid):\n\\[ y \\sim f_{binomial}(\\pi_i)\\]\n\\[ y = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{} \\pi_{i}\\\\\n         0, & \\mbox{} 1-\\pi_{i}\n         \\end{array}\n     \\right. \\]\n\\[ \\pi_i = F(x_i\\widehat{\\beta}) \\] \\[1- \\pi_i=1-F(x_i\\widehat{\\beta})\\]"
  },
  {
    "objectID": "binarymodels24.html#binomial-likelihood",
    "href": "binarymodels24.html#binomial-likelihood",
    "title": "Binary Response Models",
    "section": "Binomial Likelihood",
    "text": "Binomial Likelihood\nWrite the binomial density:\n\\[ Pr(y=1| \\pi) = \\pi_i^{y_i} (1-\\pi_i)^{1-y_i} \\]\nWrite the joint probability as a likelihood:\n\\[\\mathcal{L} (\\pi |\\ y) = \\prod \\limits_{i=1}^{n} \\left[ \\pi_i^{y_i} (1-\\pi_i)^{1-y_i}\\right]\\]\nTake the log of that likelihood:\n\\[\\ln \\mathcal{L} (\\pi| \\ y) = \\sum \\limits_{i=1}^{n}  \\left[ y_i \\ln ( \\pi_i) +  (1-y_i) \\ln(1-\\pi_i)\\right]\\]"
  },
  {
    "objectID": "binarymodels24.html#parameterize-the-model",
    "href": "binarymodels24.html#parameterize-the-model",
    "title": "Binary Response Models",
    "section": "Parameterize the model",
    "text": "Parameterize the model\nParameterize \\(\\pi_i\\) - make \\(\\pi_i\\) a function of some variables and their slope effects, \\(x\\beta\\) - this is the systematic component of the model:\n\\[\\pi_i= F(x \\beta)\\]\nThis is the binomial log-likelihood function.\n\\[\\ln \\mathcal{L} (\\pi| \\ y) = \\sum \\limits_{i=1}^{n}  \\left[ y_i \\ln (F(x_i\\widehat{\\beta})) +  (1-y_i) \\ln(1-F(x_i\\widehat{\\beta}))\\right]\\]\nBut we need to fill in \\(F\\), the link function."
  },
  {
    "objectID": "binarymodels24.html#link-function",
    "href": "binarymodels24.html#link-function",
    "title": "Binary Response Models",
    "section": "Link Function",
    "text": "Link Function\nWe parameterized \\(\\pi_i\\):\n\\[\\pi_i= F(x \\beta)\\]\nand now need to choose an appropriate link function for \\(F\\) such that:\n\nour prediction of \\(\\widehat{\\pi_i}\\) is bounded [0,1].\n\\(x_i \\widehat{\\beta}\\) can range over the interval \\([-\\infty, +\\infty]\\) and map onto the [0,1] interval.\n\nThere’s a large number of sigmoid shaped probability functions that will satisfy these needs.\n\n\n\n\n\n\nImportant Concept\n\n\n\nThe link function maps or transforms the linear prediction on the sigmoid probability space, and obeys the bounds of 0,1.\n\n\nThe most commonly used link functions are the standard normal (probit)}\n\\[Pr(y_i=1 | X) = \\Phi(x_i\\widehat{\\beta}) \\]\nand the logistic (logit) CDFs.\n\\[Pr(y_i=1 | X) = \\frac{1}{1+exp^{-(x_i\\widehat{\\beta})}} \\]\nHere are the logistic and Normal CDFs:\n\n\ncode\nz &lt;- seq(-5,5,.1)\nl &lt;- seq(0,1,.01)\ns1 &lt;- 1/(1+exp(-z))\ns2 &lt;- pnorm(z)\n\ndf &lt;- data.frame(z=z, l=l, s1=s1, s2=s2)\n\nggplot(df, aes(x=z, y=l)) + \n  geom_line(aes(x=z, y=l), color=\"black\", linetype=\"dashed\" )+\n  geom_line(aes(x=z, y=s1), color=\"#005A43\")+\n  geom_line(aes(x=z, y=s2), color=\"#6CC24A\")+\n  labs(title=\"Logistic and Normal CDFs\", x=expression(x*beta), y=\"Pr(y=1)\")+\n  theme_minimal() +\n  annotate(\"text\", x=1.3, y=.7, label=\"logistic\", color=\"black\")+\n  annotate(\"text\", x=-.2, y=.15, label=\"normal\", color=\"black\")\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nNote the sigmoid functions approach the limits at decreasing rates; the fastest rate of change is at \\(y=.5\\), a point around which the curves are symmetric. The point \\(y=.5\\) is the transition point below which we’d predict a zero, above which we’d predict a one if we were interested in classifying cases into zeros and ones. Classification is a common use for models like these, say distinguishing spam from non-spam emails, or predicting the presence or absence of a disease. More on this later."
  },
  {
    "objectID": "binarymodels24.html#probit-and-logit-llfs",
    "href": "binarymodels24.html#probit-and-logit-llfs",
    "title": "Binary Response Models",
    "section": "Probit and Logit LLFs",
    "text": "Probit and Logit LLFs\nProbit - link between \\(x\\hat{\\beta}\\) and \\(Pr(y=1)\\) is standard normal CDF: \\[\n\\ln \\mathcal{L} (Y|\\beta) = \\sum_{i=1}^{N} y_i \\ln \\Phi(\\mathbf{x_i \\beta})+ (1-y_i) \\ln[1-\\Phi(\\mathbf{x_i \\beta})] \\nonumber\n\\]\nLogit (logistic CDF):\n\\[\n\\ln \\mathcal{L} (Y|\\beta) = \\sum_{i=1}^{N} \\left\\{ y_i \\ln \\left(\\frac{1}{1+e^{-\\mathbf{x_i \\beta}}}\\right)+ (1-y_i) \\ln \\left[1-\\left(\\frac{1}{1+e^{-\\mathbf{x_i \\beta}}}\\right)\\right] \\right\\}\\nonumber\n\\]"
  },
  {
    "objectID": "binarymodels24.html#predicted-probabilities",
    "href": "binarymodels24.html#predicted-probabilities",
    "title": "Binary Response Models",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\nIn the nonlinear model, the most basic quantity is\n\\[F(x\\widehat{\\beta})\\]\nwhere \\(F\\) is the link function, mapping the linear prediction onto the probability space.\nFor the logit, the predicted probability is\n\\[Pr(y=1) = \\frac{1}{1+exp(-x\\widehat{\\beta})}\\]\nFor the probit, the predicted probability is\n\\[Pr(y=1) = \\Phi(x\\widehat{\\beta})\\]\nAgain, simply using the link function to map the linear prediction onto the probability space."
  },
  {
    "objectID": "binarymodels24.html#marginal-effects",
    "href": "binarymodels24.html#marginal-effects",
    "title": "Binary Response Models",
    "section": "Marginal Effects",
    "text": "Marginal Effects\nIn the linear model, the marginal effect of \\(x\\) is \\(\\widehat{\\beta}\\). That is, the effect of a one unit change in \\(x\\) on \\(y\\) is \\(\\widehat{\\beta}\\).\n\\[\n\\frac{\\partial \\widehat{y}}{\\partial x_k}= \\frac{\\partial x \\widehat{\\beta}}{\\partial x_k} \\nonumber \\\\  \\nonumber \\\\\n= \\widehat{\\beta} \\nonumber\n\\]\nThe marginal effect is constant with respect to \\(x_k\\). Take a look:\n\n\ncode\nx &lt;- seq(0,10,.1)\ny &lt;- 2*x\ndf &lt;- data.frame(x=x, y=y)\n\nggplot(df, aes(x=x, y=y)) + \n  geom_line()+\n  labs(title=\"Marginal Effect of x on y\", x=\"x\", y=\"y\")+\n  theme_minimal() +\n  annotate(\"text\", x=5, y=15, label=\"y = 2x\", color=\"black\")+\n  geom_segment(aes(x = 5, xend = 5, y = 0, yend = 10), color = \"red\")+\n  geom_segment(aes(x = 10, xend = 10, y = 0, yend = 20), color = \"red\")\n\n\n\n\n\n\n\n\n\nThe effect of \\(x\\) on \\(y\\) is 2 - it’s the same at \\(x=5\\) and at \\(x=10\\).\nIn the nonlinear model, the marginal effect of \\(x_k\\) depends on where \\(x\\widehat{\\beta}\\) lies with respect to the probability distribution \\(F(\\cdot)\\).\n\\[\n\\frac{\\partial Pr(y=1)}{\\partial x_k}= \\frac{\\partial F(x\\widehat{\\beta})}{\\partial x_k} \\nonumber \\\\  \\nonumber \\\\\n=  \\frac{\\partial F(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} \\cdot \\frac{\\partial (x\\widehat{\\beta})}{\\partial x_k}  \\nonumber\n\\]\nBoth of these terms simplify …\nRemember that\n\\[\n\\frac{\\partial (x\\widehat{\\beta})}{\\partial x} = \\widehat{\\beta} \\nonumber\n\\]\nand \\[\n\\frac{\\partial F(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} = f(x\\widehat{\\beta}) \\nonumber\n\\]\nwhere the derivative of the CDF is the PDF.\nPutting these together gives us:\n\\[\n\\frac{\\partial F(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} = f(x\\widehat{\\beta}) \\widehat{\\beta} \\nonumber\n\\]\nThis is \\(\\widehat{\\beta}\\) weighted by or measured at the ordinate on the PDF - the ordinate is the height of the PDF associated with a value of the \\(x\\) axis.\n\n\n\n\n\n\nImportant Concept\n\n\n\nThe effect of \\(x\\) on \\(Pr(y=1)\\) is not constant; it will be large for some values of \\(x\\) and small for others. This makes sense if we think about the sigmoid functions - the slope of the curve is steepest at \\(y=.5\\), and flattens as we move away from that point toward either limit. Take another look at Figure 1\n\n\n\nLogit Marginal Effects\n\n\nRecall \\(\\Lambda\\) is the logistic CDF = \\[1/(1+exp(-x_i\\widehat{\\beta}))\\].\n\\(\\lambda\\) is the logit PDF \\[1/(1+exp(-x_i\\widehat{\\beta}))^2\\]\nAlso, remember that\n\\[\\frac{e^{x_i\\widehat{\\beta}}}{1+e^{x_i\\widehat{\\beta}}} = \\frac{1}{1+e^{-x_i\\widehat{\\beta}}}\\]\n\\[\n\\begin{align}\n\\frac{\\partial \\Lambda(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} = \\lambda(x\\widehat{\\beta}) \\widehat{\\beta} \\\\\n= \\frac{e^{x_i\\widehat{\\beta}}}{(1+e^{x_i\\widehat{\\beta}})^2} \\widehat{\\beta}  \\\\\n=\\frac{e^{x_i\\widehat{\\beta}}}{1+e^{x_i\\widehat{\\beta}}} \\frac{1}{1+e^{x_i\\widehat{\\beta}}} \\widehat{\\beta}  \\\\\n=\\Lambda(x_i\\widehat{\\beta}) \\frac{1+e^{x_i\\widehat{\\beta}}-e^{x_i\\widehat{\\beta}}}{1+e^{x_i\\widehat{\\beta}}} \\widehat{\\beta}   \\\\\n=\\Lambda(x_i\\widehat{\\beta}) 1-\\frac{e^{x_i\\widehat{\\beta}}}{1+e^{x_i\\widehat{\\beta}}} \\widehat{\\beta}  \\\\\n=\\Lambda(x_i\\widehat{\\beta}) (1-\\Lambda(x_i\\widehat{\\beta})) \\widehat{\\beta}  \n\\end{align}\n\\]\nSo this last line indicates the marginal effect of \\(x\\) is the probability of a one times the probability of a zero times \\(\\widehat{\\beta}\\).\nThis is useful because the largest value this can take on is .25 \\((Pr(y_i=1)=0.5 \\cdot Pr(y_i=0)=0.5= 0.25)\\) - therefore, the maximum marginal effect any \\(x\\) can have is \\(0.25 \\widehat{\\beta}\\).\nLooking at the democratic peace model below, the coefficient on democracy is -.071, so the largest effect democracy can have on the probability of a militarized dispute is \\(0.25 \\cdot -.071 = -.01775\\).\n\ncode\nlibrary(stargazer)\n\nstargazer(m1,m2, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\nlogistic\n\n\nprobit\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n0.587*** (0.037)\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n-0.031*** (0.003)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n-0.001*** (0.0001)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n-0.350*** (0.038)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n-1.903*** (0.032)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n7,032.985\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\nIn the probit model, the marginal effect is:\n\\[\n\\frac{\\partial \\Phi(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} = \\phi(x\\widehat{\\beta}) \\widehat{\\beta} \\nonumber\n\\]\nThe ordinate at the maximum of the standard normal PDF is 0.3989 - rounding to 0.4, we can say that the maximum marginal effect of any \\(\\widehat{\\beta}\\) in the probit model is \\(0.4\\widehat{\\beta}\\).\nThe ordinate is at the maximum where \\(z=0\\); recall this is the standard normal, so \\(x_i\\widehat{\\beta}=z\\). When \\(z=0\\),\n\\[Pr(z)=\\frac{1}{\\sqrt{2 \\pi}} \\exp \\left[\\frac{-(z)^{2}}{2}\\right] \\nonumber \\\\ \\nonumber\\\\\n=\\frac{1}{\\sqrt{2 \\pi}} \\nonumber\\\\\n\\approx .4 \\nonumber \\]\nSo the maximum marginal effect of any \\(x\\) in the probit model is \\(0.4\\widehat{\\beta}\\)."
  },
  {
    "objectID": "binarymodels24.html#logit-odds-interpretation",
    "href": "binarymodels24.html#logit-odds-interpretation",
    "title": "Binary Response Models",
    "section": "Logit Odds Interpretation",
    "text": "Logit Odds Interpretation\nThe odds are given by the probability an event occurs divided by the probability it does not:\n\\[\n\\Omega(X) = \\frac{Pr(y=1)}{1-Pr(y=1)} \\nonumber\n= \\frac{\\Lambda(X\\widehat{\\beta})}{(1-\\Lambda(X\\widehat{\\beta}))} \\nonumber\n\\]"
  },
  {
    "objectID": "binarymodels24.html#logit-log-odds",
    "href": "binarymodels24.html#logit-log-odds",
    "title": "Binary Response Models",
    "section": "Logit Log-odds",
    "text": "Logit Log-odds\nLogging …\n\\[\\ln \\Omega(X) = \\ln \\left(\\frac{\\Lambda(X\\widehat{\\beta})}{(1-\\Lambda(X\\widehat{\\beta}))}\\right) =X\\widehat{\\beta} \\]\n\\[\n\\frac{\\partial \\ln \\Omega}{\\partial X} = \\widehat{\\beta} \\nonumber\n\\]\nWhich shows the change in the log-odds given a change in \\(X\\) is constant (and therefore linear). This quantity is sometimes called “the logit.”"
  },
  {
    "objectID": "binarymodels24.html#logit-odds-ratios",
    "href": "binarymodels24.html#logit-odds-ratios",
    "title": "Binary Response Models",
    "section": "Logit Odds Ratios",
    "text": "Logit Odds Ratios\nOdds ratios are very useful:\n\\[\n\\frac{ \\Omega x_k + 1}{\\Omega x_k} =exp(\\widehat{\\beta_k}) \\nonumber\n\\]\ncomparing the difference in odds between two values of \\(x_k\\); note the change in value does not have to be 1.\n\\[\n\\frac{ \\Omega x_k + \\iota}{\\Omega x_k} =exp(\\widehat{\\beta_k}* \\iota) \\nonumber\n\\]\nNot only is it simple to exponentiate \\(\\widehat{\\beta_k}\\), but the interpretation is that \\(x\\) increases/decreases \\(Pr(y=1)\\) by that factor, \\(exp(\\widehat{\\beta_k})\\), and more usefully, that:\n\\[\n100*(exp(\\widehat{\\beta_k})-1) \\nonumber\n\\]\nis the percentage change in the odds given a one unit change in \\(x_k\\).\nSo a logit coefficient of .226\n\\[\n100*(exp(.226)-1) =25.36 \\nonumber\n\\]\nProduces a 25.36% increase in the odds of \\(y\\) occurring."
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code",
    "section": "",
    "text": "exercise #1\nexercise #2\n\n\n\n Back to top"
  },
  {
    "objectID": "mlesyllabus24.html#seminar-description",
    "href": "mlesyllabus24.html#seminar-description",
    "title": "MLE Syllabus",
    "section": "Seminar Description",
    "text": "Seminar Description\nThis 4 credit hour seminar is a survey of maximum likelihood methods and their applications to empirical political questions. It presumes students have a detailed and intuitive knowledge of least squares, probability theory, basic skills in scalar and matrix algebra, and a basic understanding of calculus. The course will deal mainly in understanding the principles of maximum likelihood estimation, under what conditions we move away from least squares, and what particular models are appropriate given observed data. The seminar will focus on application and interpretation of ML models and linking theory to statistical models. The course emphasizes coding and data viz in R and Stata.\nThe class meets one time per week for three hours. My office hours are designed to be homework help hours where I’ll work in the grad lab with any of you who are working on the exercises. The most productive pathway for this class is for you to get in the habit of working together, and those office hours are a good time for this."
  },
  {
    "objectID": "mlesyllabus24.html#course-purpose",
    "href": "mlesyllabus24.html#course-purpose",
    "title": "MLE Syllabus",
    "section": "Course Purpose",
    "text": "Course Purpose\nThis seminar fulfills the advanced quantitative methods requirement in the Ph.D. curriculum. The method of maximum likelihood underlies a majority of quantitative models in Political Science; this class teaches students to be astute consumers of such models, and how to implement and interpret ML models. These are crucial skills for dissertations in Political Science, and for producing publishable quantitative research."
  },
  {
    "objectID": "mlesyllabus24.html#learning-objectives",
    "href": "mlesyllabus24.html#learning-objectives",
    "title": "MLE Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nStudents will encounter an array of maximum likelihood models in this course. By the end of the course, students will have mastered the theory of maximum likelihood sufficient to write and program likelihood functions in ; they will be able to choose, estimate, and interpret appropriate models, model specifications, and model evaluation tools given their data; and they will be able to produce sophisticated quantities of interest (e.g. predicted probabilities, expected values, confidence intervals) via a variety of techniques including simulation and end point transformation. Students will also be able to present model findings verbally and graphically."
  },
  {
    "objectID": "mlesyllabus24.html#class-meetings-office-hours-assignments",
    "href": "mlesyllabus24.html#class-meetings-office-hours-assignments",
    "title": "MLE Syllabus",
    "section": "Class Meetings, Office Hours, Assignments",
    "text": "Class Meetings, Office Hours, Assignments\nThe course will meet this fall entirely in-person in the Social Science Experiment Lab on Wednesdays 9:40am-12:40pm.\nOffice hours are Mondays 1:30pm-3:30pm. I’ll likely hold these in the grad work room to help with your assignments. For an appointment, email me and we’ll sort out a time.\nAll assignments should be turned in on Brightspace - please submit ::\n\nPDFs generated from LaTeX or R Markdown (Quarto).\nannotated R scripts.\nwhere necessary, data.\n\nAssignments should be instantly replicable - running the code file should produce all models, tables, plots, etc."
  },
  {
    "objectID": "mlesyllabus24.html#reading",
    "href": "mlesyllabus24.html#reading",
    "title": "MLE Syllabus",
    "section": "Reading",
    "text": "Reading\nThe reading material for the course is important because it often demonstrates application of various MLE models; seeing how folks apply these and how they motivate their applications is really informative, something you cannot miss. We often won’t directly discuss the readings, but don’t let that imply they’re not important. If I get the sense we’re not keeping up with reading, expect the syllabus to change to incorporate quizzes or other accountability measures.\nReading for the course will consist of several books and articles (listed by week below). The books listed below also have Amazon links - you’ll find most of these cheaper used online.\n\nRequired\n\nBox-Steffensmeier, Janet and Jones, Brad. 2004. Event History Modeling. Cambridge. ISBN 0521546737\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Sage Publications Inc. ISBN 0803973748\nWard, Michael D. and John S. Ahlquist. 2018 Maximum Likelihood for Social Science. Cambridge. ISBN 978-1316636824.\n\n\n\nRecommended\nUseful, but not required (though some required reading in the first one):\n\nGary King. 1998. Unifying Political Methodology. University of Michigan Press. ISBN 0472085549\nJ. Scott Long. 2014. Regression Models for Categorical Dependent Variables Using Stata. 3rd Ed. Stata Press. ISBN 1597181110 (this book is good for practical/applied examples even if R is your primary language)\n\nGary King’s book is regarded as seminal in developing ML applications in political science. Scott Long’s is a similarlyaccessible treatment of a host of ML models and applications (and the Stata book is a great applied companion). Together, these two books are probably the most important on the syllabus as they are both accessible, but comprehensive and technical enough to be useful. Ward and Ahlquist’s book is a new overview of applied ML in a political science setting. Box-Steffensmeier and Jones is a thorough and accessible treatment of hazard models in a variety of empirical settings.\n\n\nAdditional Resources\nOther useful books include:\n\nCameron, A. Colin and Trivedi, Pravin K. 1998. Regression Analysis of Count Data. Cambridge. ISBN 0521635675\nMaddala, Gregory. 1983. Limited Dependent and Qualitative Variables in Econometrics. Cambridge. ISBN 0521338255\nPaul D Allison - Event History Analysis : Regression for Longitudinal Event Data. Sage Publications Inc. ISBN 0803920555\nTim Futing Liao - Interpreting Probability Models : Logit, Probit, and Other Generalized Linear Models. Sage Publications Inc. ISBN 0803949995\nJohn H Aldrich and Forrest D Nelson - Linear Probability, Logit, and Probit Models. Sage Publications Inc. ISBN 0803921330\nFred C Pampel - Logistic Regression : A Primer. Sage Publications Inc. ISBN 0761920102\nVani Kant Borooah - Logit and Probit : Ordered and Multinomial Models. Sage Publications Inc. ISBN 0761922423\nScott R Eliason - Maximum Likelihood Estimation : Logic and Practice. Sage Publications Inc. ISBN 0803941072\nRichard Breen - Regression Models : Censored, Sample Selected, or Truncated Data. Sage Publications Inc. ISBN 0803957106\nKrishnan Namboodiri - Matrix Algebra : An Introduction. ISBN 0803920520"
  },
  {
    "objectID": "mlesyllabus24.html#course-requirements-and-grades",
    "href": "mlesyllabus24.html#course-requirements-and-grades",
    "title": "MLE Syllabus",
    "section": "Course Requirements and Grades",
    "text": "Course Requirements and Grades\nThe seminar requires the following:\n\nProblem sets - 60% total\nMechanism papers - 40%\n\nPlease note that all written assignments must be submitted as PDFs either compiled in LaTeX or in R markdown (Quarto).\nYou’ll complete a series of problem sets, mostly applied. How many will depend on how things move along during the term. Regarding the problem sets - the work you turn in for the problem sets should clearly be your own, but I urge you to work together - doing so is a great way to learn and to overcome problems.\nThe mechanism papers are a series of three short papers you’ll write during the semester aimed at learning to identify and describe causal mechanisms, then at producing a causal mechanism. More on these early in the term.\nA word about completeness - attempt everything. To receive a passing grade in the course, you must finish all elements of the course, so all problem sets, all exams, papers, etc. To complete an element, you must at least attempt all parts of the element - so if a problem set has 10 problems, you must attempt all 10 or the assignment is incomplete, you’ve not completed every element of the course, and you cannot pass. I realize there may be problems you have trouble with and even get wrong, but you must try - the bottom line is don’t turn in incomplete work.\nGrades will be assigned on the following scale:\n\n\n\n\n\n\n\n\n\n\n\nGrade\nRange\nGrade\nRange\n\n\n\n\nA\n94-100%\nC+\n77-79%\n\n\nA-\n90–93%\nC\n73-76%\n\n\nB+\n87–89%\nC-\n70-72%\n\n\nB\n83-86%\nD\n60-69%\n\n\nB-\n80-82%\nF\n&lt;60%"
  },
  {
    "objectID": "mlesyllabus24.html#course-policies",
    "href": "mlesyllabus24.html#course-policies",
    "title": "MLE Syllabus",
    "section": "Course Policies",
    "text": "Course Policies\n\nAttendance\nAttendance is expected, and is essential if you’re to succeed in this class.\n\n\nAcademic Integrity\nIdeas are the currency in academic exchange, so acknowledging where ideas come from is important. Acknowledging the sources of ideas also helps us identify an idea’s lineage which can be important for understanding how that line of thought has developed, and toward promoting future growth. As graduate students, you should have a good understanding of academic honesty and best practices. Here are details of Binghamton’s honesty policy."
  },
  {
    "objectID": "mlesyllabus24.html#course-schedule",
    "href": "mlesyllabus24.html#course-schedule",
    "title": "MLE Syllabus",
    "section": "Course Schedule",
    "text": "Course Schedule\nWeek 1, Aug 21 – Binary \\(y\\) Variables I - probit/logit, QI\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 1, 2, 4\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 3.    \n\nWeek 2, Aug 28 – Likelihood Theory and ML Estimation\n\nGary King. 1998. Unifying Political Methodology. Chapter 1-4\nJ. Scott Long. 1997. *Regression Models for Categorical and Limited Dependent Variables}. Chapters 1-2.\n\nWeek 3, Sept 4 – Binary \\(y\\) Variables II - symmetry, fit, diagnostics, prediction\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 3, 5, 6, 7\n\n\n\nNagler (1994)\nKing and Zeng (2001)\nFranklin and Kosaki (1989)\nC. Zorn (2005)\n\nWeek 4, Sept 11 – Binary \\(y\\) Variables III (discrete hazards)\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 11 \nBeck, Katz, and Tucker (1998)\nCarter and Signorino (2010)\n\nWeek 5, Sept 18 – Binary \\(y\\) Variables IV - variance, order\n\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 5.\nFranklin (1991)\nAlvarez and Brehm (1995)\nClark and Nordstrom (2005)\n\nWeek 6, Sept 25 – Assumptions and Specification - interactions, functional form, measurement of \\(y\\)\n\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 5.\nClark, Nordstrom, and Reed (2008)\nClarke and Stone (2008)\nBerry, Golder, and Milton (2012)\nBrambor, Clark, and Golder (2006)\n\nWeek 7, Oct 2 – No class, Yom Kippur\nWeek 8, Oct 9 – Choice Models I (Unordered \\(y\\) Variables) - MNL, MNP, CL (IIA)\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 9\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 6.\nAlvarez and Nagler (1998)\nLacy and Burden (1999)\nC. J. W. Zorn (1996)\n\nWeek 9, Oct 16– Choice Models II (Unordered Dependent Variables continued, and systems of eqs, ordered)\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 8\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 5.\nFranklin and Kosaki (1989)\n\nWeek 10, Oct 23 – Event Count Models I - poisson, dispersion\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 10\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 8.1, 8.2.\nGowa (1998)\nFordham (1998)\n\nWeek 11, Oct 30 – Event Count Models II - negative binomial, zero-altered\n\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 8.3-8.7.\nC. J. W. Zorn (1998)\nClark (2003)\n\nWeek 12, Nov 6 – no class, Peace Science\nWeek 13, Nov 13 – Continuous Time Hazard Models I - parametric, semi-parametric models\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 11\nJanet Box-Steffensmeier and Brad Jones. 2004. Event History Modeling. Chs. 1-4\nJ. M. Box-Steffensmeier, Arnold, and Zorn (1997)\n\nWeek 14, Nov 20 – Continuous Time Hazard Models II - parametric models, special topics\n\nJanet Box-Steffensmeier and Brad Jones. 2004. Event History Modeling. Chs. 5-11\nC. J. W. Zorn (2000)\nBennett and Stam (1996)\nJ. Box-Steffensmeier, Reiter, and Zorn (2003)\n\nWeek 15, Nov 27 – no class, Thanksgiving\nWeek 16, Dec 4 – Censored/Truncated Variables, Samples - selection models - J. Scott Long. 1997. *Regression Models for Categorical and Limited Dependent Variables}. Chapter 7\n\nReed (2000)\nSignorino (1999)\nTimpone (1998)"
  },
  {
    "objectID": "prediction24.html",
    "href": "prediction24.html",
    "title": "Prediction Methods for MLE Models",
    "section": "",
    "text": "Most MLE models are nonlinear, so their coefficients are not their marginal effects. As a result, most MLE models require a transformation of the linear prediction to generate quantities of interest. The methods outlined here apply to most MLE applications; the immediate interest and examples here use binary response models. These slides will form a foundation for prediction in other types of models we encounter.\n\n\nProbit and logit coefficients are directly interpretable in the senses that\n\nWe can interpret direction.\nWe can interpret statistical difference from zero.\nWe can say the largest marginal effect of \\(x \\approx 0.4\\cdot\\widehat{\\beta}\\) for the probit model.\nWe can say the largest marginal effect of \\(x \\approx 0.25\\cdot\\widehat{\\beta}\\) for the logit model.\nWe can say that \\(exp(\\widehat{\\beta_k})-1\\) is the percentage change in the odds that \\(y=1\\), for the logit model.\n\nIt’s still the case that we often want other quantities of interest like probabilities, and that requires the straightforward transformations of the linear prediction, \\(F(x_i\\widehat{\\beta})\\).\nLet’s look briefly at the intuition of the “maximum marginal effect” in the logit model.\n\n\ncode\nz &lt;- seq(-5,5,.1)\nncdf &lt;- pnorm(z)\nnpdf &lt;- dnorm(z)\nlcdf &lt;- plogis(z)\nlpdf &lt;- dlogis(z)\n\ndf &lt;- data.frame(ncdf=ncdf, npdf=npdf, lcdf=lcdf, lpdf=lpdf, z=z)\n\nggplot(df, aes(x=z, y=ncdf), color=\"black\") +\n  geom_line() +\n  geom_line(aes(x=z, y=lcdf), color=\"green\") +\n  geom_line(aes(x=z, y=npdf), color=\"black\") +\n  geom_line(aes(x=z, y=lpdf), color=\"green\") +\n  geom_hline(yintercept = .3989, linetype=\"dashed\") +\n  geom_hline(yintercept = .25, linetype=\"dashed\") +\n  labs(x=\"z\", y=\"Pr(y=1)\") +\n  theme_minimal() +\n  annotate(\"text\", x = 2.5, y = .36, label = \"Normal\", color = \"black\") +\n  annotate(\"text\", x = 2.5, y = .22, label = \"Logistic\", color = \"black\") \n\n\n\n\n\n\n\n\n\nThe highest points on the PDFs indicate the maximum marginal effect of \\(x\\) on \\(Pr(y=1)\\) in the logit and probit models."
  },
  {
    "objectID": "prediction24.html#binary-response-models",
    "href": "prediction24.html#binary-response-models",
    "title": "Prediction Methods for MLE Models",
    "section": "",
    "text": "Probit and logit coefficients are directly interpretable in the senses that\n\nWe can interpret direction.\nWe can interpret statistical difference from zero.\nWe can say the largest marginal effect of \\(x \\approx 0.4\\cdot\\widehat{\\beta}\\) for the probit model.\nWe can say the largest marginal effect of \\(x \\approx 0.25\\cdot\\widehat{\\beta}\\) for the logit model.\nWe can say that \\(exp(\\widehat{\\beta_k})-1\\) is the percentage change in the odds that \\(y=1\\), for the logit model.\n\nIt’s still the case that we often want other quantities of interest like probabilities, and that requires the straightforward transformations of the linear prediction, \\(F(x_i\\widehat{\\beta})\\).\nLet’s look briefly at the intuition of the “maximum marginal effect” in the logit model.\n\n\ncode\nz &lt;- seq(-5,5,.1)\nncdf &lt;- pnorm(z)\nnpdf &lt;- dnorm(z)\nlcdf &lt;- plogis(z)\nlpdf &lt;- dlogis(z)\n\ndf &lt;- data.frame(ncdf=ncdf, npdf=npdf, lcdf=lcdf, lpdf=lpdf, z=z)\n\nggplot(df, aes(x=z, y=ncdf), color=\"black\") +\n  geom_line() +\n  geom_line(aes(x=z, y=lcdf), color=\"green\") +\n  geom_line(aes(x=z, y=npdf), color=\"black\") +\n  geom_line(aes(x=z, y=lpdf), color=\"green\") +\n  geom_hline(yintercept = .3989, linetype=\"dashed\") +\n  geom_hline(yintercept = .25, linetype=\"dashed\") +\n  labs(x=\"z\", y=\"Pr(y=1)\") +\n  theme_minimal() +\n  annotate(\"text\", x = 2.5, y = .36, label = \"Normal\", color = \"black\") +\n  annotate(\"text\", x = 2.5, y = .22, label = \"Logistic\", color = \"black\") \n\n\n\n\n\n\n\n\n\nThe highest points on the PDFs indicate the maximum marginal effect of \\(x\\) on \\(Pr(y=1)\\) in the logit and probit models."
  },
  {
    "objectID": "prediction24.html#at-means-predictions",
    "href": "prediction24.html#at-means-predictions",
    "title": "Prediction Methods for MLE Models",
    "section": "At-Means Predictions",
    "text": "At-Means Predictions\nAt-means predictions are what they sound like - effects with independent variables set at central tendencies. These are sometimes called “adjusted predictions.”\n\nestimate model.\ncreate out of sample data.\nvary \\(x\\) of interest; set all other \\(x\\) variables to appropriate central tendencies - hence the “at Means.”\ngenerate QIs in out of sample data."
  },
  {
    "objectID": "prediction24.html#average-effects",
    "href": "prediction24.html#average-effects",
    "title": "Prediction Methods for MLE Models",
    "section": "Average Effects",
    "text": "Average Effects\nAverage Marginal Effects are in-sample but create a counterfactual for a variable of interest, assuming the entire sample looks like that case.\nFor instance, suppose a model of wages with covariates for education and gender. We might ask the question what would the predictions look like if the entire sample were male, but otherwise looked as it does? Alternatively, what would the predictions look like if the entire sample were female, but all other variables the same as they appear in the estimation data?\nTo answer these, we’d change the gender variable to male, generate \\(x{\\widehat{\\beta}}\\) for the entire sample, and take the average, then repeat with the gender variable set to female.\nTo generate Average Effects,\n\nestimate model.\nin estimation data, set variable of interest to a particular value for the entire estimation sample.\ngenerate QIs (expected values, standard errors).\ntake average of QIs, and save.\nrepeat for all values of variable of interest, and plot."
  },
  {
    "objectID": "prediction24.html#at-means-predictions-logit",
    "href": "prediction24.html#at-means-predictions-logit",
    "title": "Prediction Methods for MLE Models",
    "section": "At-means predictions (logit)",
    "text": "At-means predictions (logit)\nHere’s an example of at-means predictions for a logit model of the democratic peace. FIrst, let’s look at the model estimates:\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n\nstargazer(m1, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\nAs with any nonlinear model, we need to compute a linear prediction, \\(x\\widehat{\\beta}\\), and then transform that to a probability. For at-means predictions, we’ll vary democracy across its range, holding the remaining variables at appropriate central tendency (e.g, mode for dummy variables, median for categorical or skewed variables, etc.) Take a look at the code:\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\nlogitpreds &lt;- predict(m1, type=\"response\")\n\nm2 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"probit\"), data=dp )\n\n#new data frame for MEM prediction\nmem &lt;- data.frame(deml= c(seq(-10,10,1)), \n                  border=0, caprat=median(dp$caprat), ally=0)\n\n# type=\"link\" produces the linear predictions; transform by hand below w/EPT\nmem  &lt;-data.frame(mem, predict(m1, type=\"link\", newdata=mem, se=TRUE))\n\nmem &lt;- cbind(mem,lb=plogis(mem$fit-1.96*mem$se.fit),\n             ub=plogis(mem$fit+1.96*mem$se.fit), \n             p=plogis(mem$fit))\n\nggplot(mem, aes(x=deml, y=p)) +\n  geom_line() +\n  geom_ribbon(data=mem, aes(x=deml, ymin=lb, ymax=ub),fill = \"grey30\", alpha = .4, ) +\n  labs(x=\"Polity Score\", y=\"Pr(Dispute) (95% confidence interval)\")"
  },
  {
    "objectID": "prediction24.html#average-effects-logit",
    "href": "prediction24.html#average-effects-logit",
    "title": "Prediction Methods for MLE Models",
    "section": "Average effects (logit)",
    "text": "Average effects (logit)\nAverage effects are often a better choice because they represent the data more completely than central tendency can (in the at-mean effects). Here are average effects (using the logit estimates) across the range of polity, and for pairs of states that share borders and those that do not.\n\n\ncode\n#avg effects\n\n#identify the estimation sample\ndp$used &lt;- TRUE\ndp$used[na.action(m1)] &lt;- FALSE\ndpesample &lt;- dp %&gt;%  filter(used==\"TRUE\")\n\npolity &lt;- 0\nmedxbd0 &lt;- 0\nubxbd0 &lt;- 0\nlbxbd0 &lt;- 0\n# medse &lt;- 0\n# medxbd1 &lt;- 0\n# ubxbd1 &lt;- 0\n# lbxbd1 &lt;- 0\n\nfor(i in seq(1,21,1)){\n  dpesample$border&lt;- 0\n  dpesample$deml &lt;- i-11\n  polity[i] &lt;- i-11\n  allpreds &lt;- data.frame(predict(m1, type= \"response\", se.fit=TRUE, newdata = dpesample))  \n  medxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)\n  ubxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)+1.96*(median(allpreds$se.fit, na.rm=TRUE))\n  lbxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)-1.96*(median(allpreds$se.fit, na.rm=TRUE))\n}\n  \nnoborder &lt;- data.frame(polity, medxbd0, ubxbd0, lbxbd0)\n  \nfor(i in seq(1,21,1)){\n  dpesample$border&lt;- 1\n  dpesample$deml &lt;- i-11\n  polity[i] &lt;- i-11\n  allpreds &lt;- data.frame(predict(m1, type= \"response\", se.fit=TRUE, newdata = dpesample))  \n  medxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)\n  ubxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)+1.96*(median(allpreds$se.fit, na.rm=TRUE))\n  lbxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)-1.96*(median(allpreds$se.fit, na.rm=TRUE))\n}\n  \nborder &lt;- data.frame(polity, medxbd0, ubxbd0, lbxbd0)\n  \n\n\nggplot() +\n  geom_ribbon(data=noborder, aes(x=polity, ymin=lbxbd0, ymax=ubxbd0),fill = \"grey30\", alpha = .4, ) +\n  geom_line(data=noborder, aes(x=polity, y=medxbd0))+\n  geom_ribbon(data=border, aes(x=polity, ymin=lbxbd0, ymax=ubxbd0),fill = \"grey30\", alpha = .4, ) +\n  geom_line(data=border, aes(x=polity, y=medxbd0))+\n  labs ( colour = NULL, x = \"Polity Score\", y =  \"Pr(Dispute)\" )+\n  theme_minimal()+\n  ggtitle(\"Average Effects\")"
  },
  {
    "objectID": "prediction24.html#simulation",
    "href": "prediction24.html#simulation",
    "title": "Prediction Methods for MLE Models",
    "section": "Simulation",
    "text": "Simulation\nSimulation is an especially good approach for nonlinear models:\n\nestimate the model.\n\\(m\\) times (say, 1000 times), simulate the distribution of \\(\\widehat{\\beta}\\).\ngenerate the \\(m\\) linear predictions, \\(x\\widehat{\\beta}\\).\ntransform by the appropriate link function (logistic, standard normal CDF).\nidentify the 2.5, 50, and 97.5 percentiles.\nplot against \\(x\\).\n\n\n\ncode\n#draws from multivariate normal using logit model estimates\nsimL &lt;- data.frame(MASS::mvrnorm(1000, coef(m1), vcov(m1)))\n#draws from multivariate normal using probit model estimates\nsimP &lt;- data.frame(MASS::mvrnorm(1000, coef(m2), vcov(m2)))\n\n#Logit predictions\nlogitprobs &lt;- data.frame(dem= numeric(0) , lb=numeric(0), med= numeric(0), ub=numeric(0))\nfor (i in seq(1,21,1)) {\nsimpreds &lt;- quantile(simL$X.Intercept.+ simL$border*0+simL$deml*(i-11)+simL$caprat*median(dp$caprat)+simL$ally*0, probs=c(.025, .5, .975))\nlogitprobs[i,] &lt;- data.frame(dem=i, lb=plogis(simpreds[1]), med=plogis(simpreds[2]), ub=plogis(simpreds[3]))\n}\n\n#Probit predictions\nprobitprobs &lt;- data.frame(dem= numeric(0) , lb=numeric(0), med= numeric(0), ub=numeric(0))\nfor (i in seq(1,21,1)) {\nsimpreds &lt;- quantile(simP$X.Intercept.+ simP$border*0+simP$deml*(i-11)+simP$caprat*median(dp$caprat)+simP$ally*0, probs=c(.025, .5, .975))\nprobitprobs[i,] &lt;- data.frame(dem=i, lb=pnorm(simpreds[1]), med=pnorm(simpreds[2]), ub=pnorm(simpreds[3]))\n}\n\nlogit &lt;- ggplot() +\n  geom_ribbon(data=logitprobs, aes(x=dem, ymin=lb, ymax=ub),fill = \"grey30\", alpha = .4, ) +\n  geom_line(data=logitprobs, aes(x=dem, y=med))+\n  labs ( colour = NULL, x = \"Polity Score\", y =  \"Pr(Dispute)\" )+\n  theme_minimal()+\n  ggtitle(\"Logit Predictions\")\n\nprobit &lt;- ggplot() +\n  geom_ribbon(data=probitprobs, aes(x=dem, ymin=lb, ymax=ub),fill = \"grey30\", alpha = .4, ) +\n  geom_line(data=probitprobs, aes(x=dem, y=med))+\n  labs ( colour = NULL, x = \"Polity Score\", y =  \"Pr(Dispute)\" )+\n  theme_minimal()+\n  ggtitle(\"Probit Predictions\")\n\nlogit+probit"
  },
  {
    "objectID": "prediction24.html#simulating-combinations-of-binary-variables",
    "href": "prediction24.html#simulating-combinations-of-binary-variables",
    "title": "Prediction Methods for MLE Models",
    "section": "Simulating combinations of binary variables",
    "text": "Simulating combinations of binary variables\nLet’s look at the differences among the four combinations of the binary variables, border and ally.\n\n\ncode\n## simulating for binary combinations ----\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n\n#draws from multivariate normal using logit model estimates\nsimL &lt;- data.frame(MASS::mvrnorm(1000, coef(m1), vcov(m1)))\n\n\nlogitprobs &lt;- data.frame(b0a0= numeric(0) , b1a0=numeric(0), b0a1= numeric(0), b1a1=numeric(0))\n\n  b0a0 &lt;- plogis(simL$X.Intercept.+ simL$border*0+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*0)\nb1a0 &lt;- plogis(simL$X.Intercept.+ simL$border*1+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*0)\nb0a1 &lt;- plogis(simL$X.Intercept.+ simL$border*0+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*1)\nb1a1 &lt;- plogis(simL$X.Intercept.+ simL$border*1+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*1)\n\nlogitprobs &lt;- data.frame(b0a0, b1a0, b0a1, b1a1)\n\nggplot() +\n  geom_density(data=logitprobs, aes(x=b0a0), fill=\"grey30\", alpha = .4, ) +\n  geom_density(data=logitprobs, aes(x=b1a0), fill=\"grey30\", alpha = .4, ) +\n  geom_density(data=logitprobs, aes(x=b0a1), fill=\"grey30\", alpha = .4, ) +\n  geom_density(data=logitprobs, aes(x=b1a1), fill=\"grey30\", alpha = .4, ) +\n  labs ( colour = NULL, x = \"Pr(Dispute)\", y =  \"Density\" ) +\n  theme_minimal()+\n  annotate(\"text\", x = .07, y = 150, label = \"No border, not allies\", color = \"black\") +\n  annotate(\"text\", x = .13, y = 70, label = \"Border, not allies\", color = \"black\") +\n  annotate(\"text\", x = .04, y = 200, label = \"No border, allies\", color = \"black\") +\n  annotate(\"text\", x = .09, y = 50, label = \"Border, allies\", color = \"black\") +\n  ggtitle(\"Logit Predictions\")"
  },
  {
    "objectID": "prediction24.html#ml-standard-errors-of-linear-predictions",
    "href": "prediction24.html#ml-standard-errors-of-linear-predictions",
    "title": "Prediction Methods for MLE Models",
    "section": "ML Standard Errors of Linear Predictions",
    "text": "ML Standard Errors of Linear Predictions\nOne commonly used measure of uncertainty is the standard error of the linear prediction, \\(X\\widehat{\\beta}\\).\nConsider the linear prediction\n\\[X \\widehat{\\beta} \\]\nunder maximum likelihood theory:\n\\[var(X \\widehat{\\beta}) = \\mathbf{X V X'} \\]\nan \\(N x N\\) matrix, where \\(V\\) is the var-cov matrix of \\({\\widehat{\\beta}}\\). The main diagonal contains the variances of the \\(N\\) predictions. The standard errors are:\n\\[se(X \\widehat{\\beta}) = \\sqrt{diag(\\mathbf{X V X'})} \\]\nwhich is an \\(N x 1\\) vector. So now we have a column vector of standard errors for the linear prediction, \\(X\\widehat{\\beta}\\). Like the linear predictions, these are not transformed into probabilities, so when we compute confidence intervals, we need to map the upper and lower bounds onto the probability space.\n\\[F(X \\widehat{\\beta} - c*s.e.) \\leq F(X \\widehat{\\beta}) \\leq F(X \\widehat{\\beta} + c* s.e.)\\]"
  },
  {
    "objectID": "prediction24.html#delta-method-standard-errors",
    "href": "prediction24.html#delta-method-standard-errors",
    "title": "Prediction Methods for MLE Models",
    "section": "Delta Method standard errors",
    "text": "Delta Method standard errors\nThe maximum likelihood method is appropriate for monotonic functions of \\(X \\widehat{\\beta}\\), e.g. logit, probit. In other models (e.g., multinomial logit), the function is not monotonic in \\(X \\widehat{\\beta}\\) so we use the Delta Method - this creates a linear approximation of the function. Greene (2012) (693ff) gives this as a general derivation of the variance:\n\\[Var[F(X \\widehat{\\beta})] = f(\\mathbf{x'\\widehat{\\beta}})^2 \\mathbf{x' V x} \\]\nwhere this would generate variances for whatever \\(F(X \\widehat{\\beta})\\) is, perhaps a predicted probability.\n\nDelta method standard errors for Logit\nFor the logit, the delta standard errors are given by:\n\\[F(X \\widehat{\\beta}) * (1-F(X \\widehat{\\beta}) * \\mathbf(X V X')\\]\n\\[ = f(X \\widehat{\\beta})  *  \\mathbf{\\sqrt{X V X'}}\\]\nor\n\\[ p * (1-p) * stdp\\]\nwhere \\(stdp\\) is the standard error of the linear prediction."
  },
  {
    "objectID": "prediction24.html#ses-of-predictions-for-linear-combinations",
    "href": "prediction24.html#ses-of-predictions-for-linear-combinations",
    "title": "Prediction Methods for MLE Models",
    "section": "SEs of Predictions for linear combinations",
    "text": "SEs of Predictions for linear combinations\nA common circumstance that requires joint hypothesis tests is the case of polynomials (which are themselves interactions):\n\\[y = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_1 + \\widehat{\\beta}_2 x_{1}^2  + \\varepsilon \\]\nThe question is whether \\(\\widehat{\\beta}_1 = \\widehat{\\beta}_2  = 0\\) - the marginal effect is:\n\\[ \\widehat{\\beta}_1 + 2 \\widehat{\\beta}_2x_1\\]\nand requires the standard error for \\(\\widehat{\\beta}_1+\\widehat{\\beta}_2\\), which is:\n\\[ \\sqrt{var(\\widehat{\\beta}_1) + 4x_{1}^{2}var(\\widehat{\\beta}_2) +  4x_1 cov(\\widehat{\\beta}_1, \\widehat{\\beta}_2)  }\\]"
  },
  {
    "objectID": "prediction24.html#cis---end-point-transformation",
    "href": "prediction24.html#cis---end-point-transformation",
    "title": "Prediction Methods for MLE Models",
    "section": "CIs - End Point Transformation",
    "text": "CIs - End Point Transformation\nGenerate upper and lower bounds using either ML or Delta standard errors, such that\n\\[F(X \\widehat{\\beta} - c*s.e.) \\leq F(X \\widehat{\\beta}) \\leq F(X \\widehat{\\beta} + c* s.e.)\\]\n\nestimate the model, generate the linear prediction, and the standard error of the linear prediction using the either ML or Delta.\ngenerate linear boundary predictions, \\(x{\\widehat{\\beta}} \\pm c * \\text{st. err.}\\) where \\(c\\) is a critical value on the normal, eg. \\(z=1.96\\).\ntransform the linear prediction and the upper and lower boundary predictions by \\(F(\\cdot)\\).\nWith ML standard errors, EPT boundaries will obey distributional boundaries (ie, won’t fall outside 0-1 interval for probabilities); the linear end point predictions are symmetric, though they will not be symmetric in nonlinear models.\nWith delta standard errors, bounds may not obey distributional boundaries."
  },
  {
    "objectID": "prediction24.html#simulating-confidence-intervals-i",
    "href": "prediction24.html#simulating-confidence-intervals-i",
    "title": "Prediction Methods for MLE Models",
    "section": "Simulating confidence intervals, I",
    "text": "Simulating confidence intervals, I\n\ndraw a sample with replacement of size \\(\\tilde{N}\\) from the estimation sample.\nestimate the model parameters in that bootstrap sample.\nusing the bootstrap estimates, generate quantities of interest (e.g. \\(x\\widehat{\\beta}\\)) repeat \\(j\\) times.\ncollect all these bootstrap QIs and use either percentiles or standard deviations to measure uncertainty."
  },
  {
    "objectID": "prediction24.html#uncertainty-simulating-confidence-intervals-ii",
    "href": "prediction24.html#uncertainty-simulating-confidence-intervals-ii",
    "title": "Prediction Methods for MLE Models",
    "section": "Uncertainty: Simulating confidence intervals, II",
    "text": "Uncertainty: Simulating confidence intervals, II\n\nestimate the model.\ngenerate a large sample distribution of parameters (e.g. using drawnorm).\ngenerate quantities of interest for the distribution of parameters.\nuse either percentiles or standard deviations of the QI to measure uncertainty."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Maximum Likelihood, Fall 2024",
    "section": "",
    "text": "This is the course website for PLSC 606J, Maximum Likelihood Estimation, Fall 2024. The course is an advanced course in data science focusing on maximum likelihood methods and their applications in political science.\n\nSyllabus\nSlides\nCode\n\n\n\n\n Back to top"
  },
  {
    "objectID": "mlesyllabus24.html#resources",
    "href": "mlesyllabus24.html#resources",
    "title": "MLE Syllabus",
    "section": " Resources",
    "text": "Resources\nThere are lots of good, free  resources online. Here are a few:\n\nModern Statistics with R\nR for Data Science\nR Markdown: The Definitive Guide\nR Graphics Cookbook\nAdvanced R\nR Markdown Cookbook\nData Science:A First Introduction\nThe Big Book of R"
  },
  {
    "objectID": "llfmax.html",
    "href": "llfmax.html",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "The goal here is to walk through the process of writing a likelihood function based on the data we observe, then programming that likelihood function and optimizing it with respect to the data.\n\n\nLet’s recall the motivation for turning to ML. The data we have are not suitable for OLS because our observation of the data is limited. For instance, we observe a binary variable \\([0,1]\\) rather than the underlying continuous probability beneath it. If we could observe and measure that probability, we’d do so and perhaps use OLS to estimate a model. But since we only observe \\([0,1]\\), our observation is limited.\nOLS is poorly suited to model this variable especially if one of our goals is to generate predictions of that underlying probability. That is, one of our goals might be to estimate a measure of the \\(y\\) variable we wish we could measure directly.\nMLE is better suited to this task - it asks us what the data generating process is that produced the observed data, and to build a model appropriate to the limited observation of that \\(y\\) variable.\nTo do so, we need to:\n\ndescribe the observed distribution of \\(y\\)\nconsider what we wish we could measure - this points to the key quantities of interest we want to derive from the model.\ndescribe \\(y\\) according to a probability distribution; write that distribution\nwrite a log-likelihood function appropriate to that probability distribution.\nparameterize the log-likelihood function such that we have a link distribution to map the linear prediction, \\(x\\beta\\), onto the probability space of \\(y\\).\n\n\n\n\nLet’s start with a \\(y\\) variable as follows:\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                y\n                Freq\n              \n        \n        \n        \n                \n                  0\n                  707\n                \n                \n                  1\n                  293\n                \n        \n      \n    \n\n\n\nThe variable, \\(y\\), takes on values of zero and one - it appears binomial; let’s write this in terms of the binomial distribution parameter \\(\\pi\\), so it takes on the value of one with probability \\(\\pi\\) and zero with probability \\(1-\\pi\\).\n\\[ y_i = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{} \\pi_{i}\\\\\n         0, & \\mbox{} 1-\\pi_{i}\n         \\end{array}\n     \\right.\\]\nThe likelihood of a single observation is:\n\\[ \\mathcal{L}(\\pi_i|y) = \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\]\nThis is a statement of the likelihood that any particular value of \\(\\pi\\) generated an observation. We want the likelihood any value of \\(\\pi\\) generated the entire dataset - that is, we want the joint likelihood of all the observations.\n\\[ \\mathcal{L}(\\pi|y) = \\prod_{i=1}^{n} \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\] recalling that a joint probability is the product of individual probabilities.\nLet’s take the log of this likelihood function - the natural log makes computation easier. Even for computers, this reduces the computational intensity especially with respect to extremely small decimals.\n\\[ \\ln \\mathcal{L}(\\pi|y) = \\sum_{i=1}^{n} y_i \\ln(\\pi_i) + (1-y_i) \\ln(1-\\pi_i) \\]\nWe can take this a step further and parameterize \\(\\pi\\) as a function of \\(X\\beta\\) where \\(X\\) is the matrix of predictors and \\(\\beta\\) is the vector of coefficients such that\n\\[ \\pi_i = F({X_i\\beta}) \\]\nSo our estimate of the binomial probability is a function of the linear predictor \\(X\\beta\\). We map \\(X\\beta\\) onto the probability space \\(\\pi\\) using a link function, \\(F\\). The most common link functions are the logistic (logit model), and the standard normal (probit model). Let’s write the logit link:\n\\[ \\pi_i = \\frac{1}{1 + e^{-(X_i\\beta)}} \\]\nand now let’s write this all in the log-likelihood function:\n\\[ \\ln \\mathcal{L}(\\beta|y) = \\sum_{i=1}^{n} y_i \\ln(\\frac{1}{1 + e^{-(X_i\\beta)}}) + (1-y_i) \\ln(1-\\frac{1}{1 + e^{-(X_i\\beta)}}) \\]\nThis is the logit log-likelihood function for a binary \\(y\\) variable.\n\n\n\nHow do we solve this function for the data \\(y\\)? Put differently, what is the value of \\(\\pi\\) that most likely generated the data, \\(y\\)? There are a few ways to do this - one instructive method is to use a grid search, where we calculate the log-likelihood for a range of values of \\(\\pi\\) and find the value that maximizes the log-likelihood. We can repeat this to the desired level of specificity, progressively narrowing the grid. In this example, we’ll just do one search out to 3 decimal places. The steps are these:\n\ngenerate the range of candidate values of our parameter \\(\\pi\\) to plug into the log-likelihood function.\nplug each value into the log-likelihood function to compute the log-likelihood for that value.\nidentify which value of \\(\\pi\\) maximizes the log-likelihood.\n\nHere’s code to do this:\n\n\ncode\n# generate a vector of values of pi \n\npi_trials &lt;- seq(0, 1, by = 0.001)\n\n# write the log-likelihood function\n\nlog_likelihood &lt;- function(pi_trials, y) {\n  sum(y * log(pi_trials) + (1 - y) * log(1 - pi_trials))\n}\n\n# Calculate log-likelihood for each pi value\n\nll_values &lt;- sapply(pi_trials, log_likelihood, y = y)\n\n# Find the pi value that maximizes the log-likelihood\n\npi_hat &lt;- pi_trials[which.max(ll_values)]\nprint(pi_hat)\n\n\n[1] 0.293\n\n\nYou’ll note we recover the sample mean of \\(y\\). Let’s plot the log-likelihood against the values of \\(\\pi\\) to visualize the maximum.\n\n\ncode\n# Plot log-likelihood against pi\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(data.frame(pi = pi_trials, ll = ll_values), \"line\", hcaes(x = pi, y = ll)) %&gt;%\n  hc_title(text = \"Grid search - maximizing the log-likelihood\") %&gt;%\n  hc_xAxis(title = list(text = \"Pi\"), \n           plotLines = list(\n    list(color =\"red\", value = pi_hat ))) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_legend(enabled = FALSE) %&gt;%\n  hc_colors(bucolors)\n\n\n\n\n\n\n\n\n\nGrid searches are not sufficient for multivariate models, especially where we also need measures of uncertainty. Here, we turn to numerical optimization.\nNumerical optimization is a field aimed at finding “best” outcomes or answers depending on some set of criteria - often, the “best” is the maximum or minimum of some function - that’s the case in maximum likelihood.\n\n\nThere are lots of ways to optimize a function - in statistical software, most of them derive from Newton’s iterative method. A very common application of this is the Newton-Raphson method. Newton-Raphson is an iterative process that starts with an initial guess for the parameter \\(\\pi\\) (or the vector of unknowns, \\(\\beta\\)) and updates it in the direction of the maximum until it converges to the maximum. At each step, it computes the first and second derivatives of the likelihood function, then uses these to update the parameter estimate(s). The update rule is:\n\\[ \\pi_{new} = \\pi_{old} - H(\\pi)^{-1}*g(\\pi) \\]\nSo the new estimate of the parameter is the old estimate minus the first derivative (gradient) of the log-likelihood function pre-multiplied by the Hessian (second derivative) of the log-likelihood function. Here, \\(g(\\pi)\\) is the gradient of the log-likelihood function and \\(H(\\pi)\\) is the Hessian of the log-likelihood function. The algorithm iterates this process until the change in \\(\\pi\\) is sufficiently small, below a specified threshold. Since \\(g\\) and \\(H\\) are matrices, you should see how this straightforwardly applied to a multivariate regression.\nLet’s write the Newton-Raphson algorithm ourselves - take a look at the code chunk below. You’ll see it produces the same result as the grid search; the sample frequency of \\(y\\).\n\n\n\n\n\ncode\n# use same data generated above; same LLF as above, aiming to recover pi\n\n#write the gradient - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(pi, y) {\n  sum(y / pi - (1 - y) / (1 - pi))\n}\n\n#write the Hessian - matrix of partial second derivatives of the log-likelihood function.\n\nhessian &lt;- function(pi, y) {\n  sum(-y / pi^2 - (1 - y) / (1 - pi)^2)\n}\n\n#declare a starting value for pi\n\npi &lt;- 0.5\n\n#set convergence criteria - when new pi is within 1e-6 of old pi, stop iterating\n\ntol &lt;- 1e-6\n\n#initialize iteration counter, set maximum iterations; how many times will we do this before we stop if we don't converge prior? \n\niter &lt;- 0\nmax_iter &lt;- 100\n\n#iterate the Newton-Raphson algorithm\n\nfor (iter in 1:max_iter) {\n  # Compute gradient  \n  grad &lt;- gradient(pi, y)\n  \n  # Compute Hessian\n  hess &lt;- hessian(pi, y)\n  \n  # Update pi\n  pi_new &lt;- pi - (grad / hess)\n  \n  pi &lt;- pi_new\n  \n  # Check for convergence\n  if (abs(pi_new - pi) &lt; tol) {\n    break\n  }\n  \n  iter &lt;- iter + 1\n}\n\n#after convergence, print the result\n\nprint(pi)\n\n\n[1] 0.293\n\n\nSo what’s gone on here? We have 1000 observations of a binary variable; maximum likelihood is answering the question “what is the value of the parameter \\(\\pi\\) that makes the observed data most likely?” To get that answer, we have tried candidate values of \\(\\pi\\) to see which one maximizes the log-likelihood function.\nWe’ve maximized the function two ways - using a grid search process, and using the Newton-Raphson algorithm. The latter is more efficient and is the basis for most optimization algorithms in statistical software, and easy to adapt to estimate multiple parameters.\nHere’s one more approach also using the Newton-Raphson method. It calls the maxLik package to do the optimization instead of our having to write the algorithm ourselves.\n\n\ncode\nlibrary(maxLik)\n# Write the binomial log-likelihood function\nlog_likelihood &lt;- function(pi, y) {\n  sum(y * log(pi) + (1 - y) * log(1 - pi))\n}\n\n#maximize the function using maxLik\n\nm1m &lt;- maxLik(y=y, log_likelihood, start = c(0.5), method = \"NR\") #Newton-Raphson method, starting value of 0.5\n\nsummary(m1m)\n\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 3 iterations\nReturn code 2: successive function values within tolerance limit (tol)\nLog-Likelihood: -604.816 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(&gt; t)    \n[1,]  0.29300    0.01441   20.34  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\nAgain, the value of \\(\\pi\\) that maximizes the log-likelihood is the same as the sample frequency of \\(y\\), which is 0.293.\n\n\n\nLet’s make this more realistic in terms of resembling the models we want to run by adding an intercept term and multiple predictors to the model - the likelihood now has to account for those \\(X\\) variables and maximize with respect to them.\n\ngenerate some data on \\(y\\), and \\(X\\)\nwrite the likelihood\nwrite the Newton-Raphson algorithm\napply to the data\n\n\n\ncode\n# Generate binary y, and two X variables for regression; coefficients are -1, 0.5, -0.5\n\nset.seed(8675309)\nn &lt;- 10000\nX &lt;- matrix(rnorm(n * 2), ncol = 2)\ntrue_beta &lt;- c(-1, 0.5, -0.5)\nz &lt;- cbind(1, X) %*% true_beta\nprob &lt;- 1 / (1 + exp(-z))\ny &lt;- rbinom(n, 1, prob)\n\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson iterative algorithm\n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # Add intercept term to X if not already present\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#write the gradient function - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- plogis(X %*% beta)\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  k &lt;- ncol(X)\n  \n# set starting values for k columns of data (betas)\n  beta &lt;- rep(0, k) \n  \n# Store gradients and log-likelihoods for analysis\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n# iterate NR \n  for (iter in 1:max_iter) {\n    # compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # compute Hessian\n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # check for convergence\n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n    \n    # compute standard errors for this model by taking the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  }\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#html table comparing glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\", caption=\"Comparing GLM and Dave Estimates\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  column_spec(3, border_right = T) %&gt;%\n  column_spec(4, border_right = T) %&gt;%\n  collapse_rows(columns = 1:3, valign = \"top\") \n\n\n\n\nTable 1: Comparing GLM and Dave Estimates\n\n\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-0.9531826\n-0.9531826\n0.0236756\n0.0236748\n\n\nX1\n0.4938831\n0.4938831\n0.0238303\n0.0238295\n\n\nX2\n-0.4719703\n-0.4719703\n0.0240294\n0.0240286\n\n\n\n\n\n\n\n\n\n\n\nTable 1 compares the coefficients and standard errors from the Newton-Raphson algorithm to the coefficients and standard errors from the glm function. The estimates are the same, the standard errors very close.\nAlright, lots of fun here. Now, let’s use the program above to estimate a logit model using the democratic peace data.\n\n\n\nTable 2 compares the coefficients and standard errors from the program we wrote to those produced by the glm function. The regression here is a logit model predicting the onset of a militarized dispute as a function of the logged capabilities ratio, an indicator of whether the pair of states share a border, and the lowest democracy (polity) score in the pair.\n\n\ncode\ndp &lt;- read.csv(\"/users/dave/documents/teaching/606J-mle/2022/exercises/ex1/dp.csv\", header=TRUE)\n\n# for the log_likelihood function below, define y as the variable \"dispute\" in the dp data frame, and x1, x2, and x3 as the variables \"lncaprat\", \"border\", and \"deml\" in the dp data frame, respectively.\ndp$lncaprat &lt;- log(dp$caprat)\nX &lt;- as.matrix(dp[, c(\"lncaprat\", \"border\", \"deml\")])\ny &lt;- as.vector(dp$dispute)\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson \n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # check for constant; add one if necessary\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#write the gradient function -  partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- 1 / (1 + exp(-X %*% beta))\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  p &lt;- ncol(X)\n  \n  # set starting values for beta\n  beta &lt;- rep(0, p)\n  \n  # Store gradients and log-likelihoods\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n  for (iter in 1:max_iter) {\n    # Compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # Compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # Compute Hessian - 2nd partial derivatives \n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # Update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # Check for convergence - end if change in beta is less than tolerance value \n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n  }\n\n#compute standard errors for this model; take the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n# logit$coefficients\n# logit$st.errors\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#compare glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\", caption=\"Comparing GLM and Dave Estimates, Democratic Peace Model\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:3, valign = \"top\")\n\n\n\n\nTable 2: Comparing GLM and Dave Estimates, Democratic Peace Model\n\n\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-3.2407604\n-3.2407605\n0.1034544\n0.1034630\n\n\nXlncaprat\n-0.2028768\n-0.2028768\n0.0237847\n0.0237860\n\n\nXborder\n0.7827135\n0.7827135\n0.0852298\n0.0852337\n\n\nXdeml\n-0.0783467\n-0.0783467\n0.0068065\n0.0068078\n\n\n\n\n\n\n\n\n\n\n\nRecapping what we’ve done:\n\nWe wrote a log-likelihood function for a logistic regression model with multiple predictors.\nTo maximize the log-likelihood, we implemented a Newton-Raphson algorithm in the following steps:\n\nDefined the gradient and Hessian functions. The gradient is the vector of first partial derivatives of the log-likelihood, and the Hessian is the matrix of second partial derivatives.\nInitialized the beta coefficients and stored gradients and log-likelihoods.\nIterated the algorithm to update beta values.\nChecked for convergence based on the change in beta.\n\nOnce the model converged, we computed standard errors for the estimated coefficients by taking the square root of the main diagonal elements of the inverse negative Hessian.\n\n\n\n\nHere, you can see how the gradients change over each iteration.\n\n\ncode\n#plot gradient convergence\ngradients &lt;- do.call(cbind, logit$gradient_history)\ngradients &lt;- as.data.frame(gradients) %&gt;%\n  mutate(coef=c(\"Intercept\", \"lncaprat\", \"border\", \"deml\")) \n\ngradients &lt;- gradients %&gt;% \n  pivot_longer(-coef, names_to = \"iteration\", values_to = \"value\")\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(gradients, \"line\", hcaes(x=iteration, y=value, group=coef, color=coef)) %&gt;%\n  hc_title(text = \"Gradient Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Gradient\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\n\n\nAnd here, we can see how the log-likelihood changes over iterations:\n\n\ncode\n#plot ll convergence\n\nlls &lt;- as.data.frame(logit$ll_history) %&gt;%\n  mutate(iteration=1:nrow(.))\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(lls, \"line\", hcaes(x=iteration, y=`logit$ll_history`)) %&gt;%\n  hc_title(text = \"Log-Likelihood Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\n\n\nHere are two ways to recover the final log-likelihood from the glm model. The first uses the logLik function from the stats package. The second calculates the log-likelihood by hand - generating the predictions and plugging those into the log-likelihood function, then summing.\n\\[ LL = \\sum_{i=1}^{n} y_i \\ln(p) + (1-y_i) \\ln(1-p) \\]\n\n\ncode\n# use the logLik function from the stats package  \n\nllest &lt;- logLik(glm_fit)\n\n# or calculate it by hand - generate the predictions and plug those into the log-likelihood function, then sum.\n\npred &lt;- predict(glm_fit, type = \"response\")\nllestbyhand &lt;- sum(dp$dispute * log(pred) + (1 - dp$dispute) * log(1 - pred))\n\n\ndata.frame(llest, llestbyhand) %&gt;% \n  kable(\"html\", caption=\"Recovering the Log-Likelihood\" ) %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:2, valign = \"top\")\n\n\n\n\nRecovering the Log-Likelihood\n\n\nllest\nllestbyhand\n\n\n\n\n-3565.219\n-3565.219"
  },
  {
    "objectID": "llfmax.html#motivating-likelihood",
    "href": "llfmax.html#motivating-likelihood",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "Let’s recall the motivation for turning to ML. The data we have are not suitable for OLS because our observation of the data is limited. For instance, we observe a binary variable \\([0,1]\\) rather than the underlying continuous probability beneath it. If we could observe and measure that probability, we’d do so and perhaps use OLS to estimate a model. But since we only observe \\([0,1]\\), our observation is limited.\nOLS is poorly suited to model this variable especially if one of our goals is to generate predictions of that underlying probability. That is, one of our goals might be to estimate a measure of the \\(y\\) variable we wish we could measure directly.\nMLE is better suited to this task - it asks us what the data generating process is that produced the observed data, and to build a model appropriate to the limited observation of that \\(y\\) variable.\nTo do so, we need to:\n\ndescribe the observed distribution of \\(y\\)\nconsider what we wish we could measure - this points to the key quantities of interest we want to derive from the model.\ndescribe \\(y\\) according to a probability distribution; write that distribution\nwrite a log-likelihood function appropriate to that probability distribution.\nparameterize the log-likelihood function such that we have a link distribution to map the linear prediction, \\(x\\beta\\), onto the probability space of \\(y\\)."
  },
  {
    "objectID": "llfmax.html#binary-y-variable",
    "href": "llfmax.html#binary-y-variable",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "Let’s start with a \\(y\\) variable as follows:\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                y\n                Freq\n              \n        \n        \n        \n                \n                  0\n                  707\n                \n                \n                  1\n                  293\n                \n        \n      \n    \n\n\n\nThe variable, \\(y\\), takes on values of zero and one - it appears binomial; let’s write this in terms of the binomial distribution parameter \\(\\pi\\), so it takes on the value of one with probability \\(\\pi\\) and zero with probability \\(1-\\pi\\).\n\\[ y_i = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{} \\pi_{i}\\\\\n         0, & \\mbox{} 1-\\pi_{i}\n         \\end{array}\n     \\right.\\]\nThe likelihood of a single observation is:\n\\[ \\mathcal{L}(\\pi_i|y) = \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\]\nThis is a statement of the likelihood that any particular value of \\(\\pi\\) generated an observation. We want the likelihood any value of \\(\\pi\\) generated the entire dataset - that is, we want the joint likelihood of all the observations.\n\\[ \\mathcal{L}(\\pi|y) = \\prod_{i=1}^{n} \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\] recalling that a joint probability is the product of individual probabilities.\nLet’s take the log of this likelihood function - the natural log makes computation easier. Even for computers, this reduces the computational intensity especially with respect to extremely small decimals.\n\\[ \\ln \\mathcal{L}(\\pi|y) = \\sum_{i=1}^{n} y_i \\ln(\\pi_i) + (1-y_i) \\ln(1-\\pi_i) \\]\nWe can take this a step further and parameterize \\(\\pi\\) as a function of \\(X\\beta\\) where \\(X\\) is the matrix of predictors and \\(\\beta\\) is the vector of coefficients such that\n\\[ \\pi_i = F({X_i\\beta}) \\]\nSo our estimate of the binomial probability is a function of the linear predictor \\(X\\beta\\). We map \\(X\\beta\\) onto the probability space \\(\\pi\\) using a link function, \\(F\\). The most common link functions are the logistic (logit model), and the standard normal (probit model). Let’s write the logit link:\n\\[ \\pi_i = \\frac{1}{1 + e^{-(X_i\\beta)}} \\]\nand now let’s write this all in the log-likelihood function:\n\\[ \\ln \\mathcal{L}(\\beta|y) = \\sum_{i=1}^{n} y_i \\ln(\\frac{1}{1 + e^{-(X_i\\beta)}}) + (1-y_i) \\ln(1-\\frac{1}{1 + e^{-(X_i\\beta)}}) \\]\nThis is the logit log-likelihood function for a binary \\(y\\) variable."
  },
  {
    "objectID": "llfmax.html#maximizing-the-likelihood",
    "href": "llfmax.html#maximizing-the-likelihood",
    "title": "Writing the Likelihood",
    "section": "",
    "text": "How do we solve this function for the data \\(y\\)? Put differently, what is the value of \\(\\pi\\) that most likely generated the data, \\(y\\)? There are a few ways to do this - one instructive method is to use a grid search, where we calculate the log-likelihood for a range of values of \\(\\pi\\) and find the value that maximizes the log-likelihood. We can repeat this to the desired level of specificity, progressively narrowing the grid. In this example, we’ll just do one search out to 3 decimal places. The steps are these:\n\ngenerate the range of candidate values of our parameter \\(\\pi\\) to plug into the log-likelihood function.\nplug each value into the log-likelihood function to compute the log-likelihood for that value.\nidentify which value of \\(\\pi\\) maximizes the log-likelihood.\n\nHere’s code to do this:\n\n\ncode\n# generate a vector of values of pi \n\npi_trials &lt;- seq(0, 1, by = 0.001)\n\n# write the log-likelihood function\n\nlog_likelihood &lt;- function(pi_trials, y) {\n  sum(y * log(pi_trials) + (1 - y) * log(1 - pi_trials))\n}\n\n# Calculate log-likelihood for each pi value\n\nll_values &lt;- sapply(pi_trials, log_likelihood, y = y)\n\n# Find the pi value that maximizes the log-likelihood\n\npi_hat &lt;- pi_trials[which.max(ll_values)]\nprint(pi_hat)\n\n\n[1] 0.293\n\n\nYou’ll note we recover the sample mean of \\(y\\). Let’s plot the log-likelihood against the values of \\(\\pi\\) to visualize the maximum.\n\n\ncode\n# Plot log-likelihood against pi\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(data.frame(pi = pi_trials, ll = ll_values), \"line\", hcaes(x = pi, y = ll)) %&gt;%\n  hc_title(text = \"What value of Pi maximizes the log-likelihood?\") %&gt;%\n  hc_xAxis(title = list(text = \"Pi\"), \n           plotLines = list(\n    list(color =\"red\", value = pi_hat ))) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_legend(enabled = FALSE) %&gt;%\n  hc_colors(bucolors)"
  },
  {
    "objectID": "llfmax.html#optimization",
    "href": "llfmax.html#optimization",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "Grid searches are not sufficient for multivariate models, especially where we also need measures of uncertainty. Here, we turn to numerical optimization.\nNumerical optimization is a field aimed at finding “best” outcomes or answers depending on some set of criteria - often, the “best” is the maximum or minimum of some function - that’s the case in maximum likelihood.\n\n\nThere are lots of ways to optimize a function - in statistical software, most of them derive from Newton’s iterative method. A very common application of this is the Newton-Raphson method. Newton-Raphson is an iterative process that starts with an initial guess for the parameter \\(\\pi\\) (or the vector of unknowns, \\(\\beta\\)) and updates it in the direction of the maximum until it converges to the maximum. At each step, it computes the first and second derivatives of the likelihood function, then uses these to update the parameter estimate(s). The update rule is:\n\\[ \\pi_{new} = \\pi_{old} - H(\\pi)^{-1}*g(\\pi) \\]\nSo the new estimate of the parameter is the old estimate minus the first derivative (gradient) of the log-likelihood function pre-multiplied by the Hessian (second derivative) of the log-likelihood function. Here, \\(g(\\pi)\\) is the gradient of the log-likelihood function and \\(H(\\pi)\\) is the Hessian of the log-likelihood function. The algorithm iterates this process until the change in \\(\\pi\\) is sufficiently small, below a specified threshold. Since \\(g\\) and \\(H\\) are matrices, you should see how this straightforwardly applied to a multivariate regression.\nLet’s write the Newton-Raphson algorithm ourselves - take a look at the code chunk below. You’ll see it produces the same result as the grid search; the sample frequency of \\(y\\).\n\n\n\n\n\ncode\n# use same data generated above; same LLF as above, aiming to recover pi\n\n#write the gradient - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(pi, y) {\n  sum(y / pi - (1 - y) / (1 - pi))\n}\n\n#write the Hessian - matrix of partial second derivatives of the log-likelihood function.\n\nhessian &lt;- function(pi, y) {\n  sum(-y / pi^2 - (1 - y) / (1 - pi)^2)\n}\n\n#declare a starting value for pi\n\npi &lt;- 0.5\n\n#set convergence criteria - when new pi is within 1e-6 of old pi, stop iterating\n\ntol &lt;- 1e-6\n\n#initialize iteration counter, set maximum iterations; how many times will we do this before we stop if we don't converge prior? \n\niter &lt;- 0\nmax_iter &lt;- 100\n\n#iterate the Newton-Raphson algorithm\n\nfor (iter in 1:max_iter) {\n  # Compute gradient  \n  grad &lt;- gradient(pi, y)\n  \n  # Compute Hessian\n  hess &lt;- hessian(pi, y)\n  \n  # Update pi\n  pi_new &lt;- pi - (grad / hess)\n  \n  pi &lt;- pi_new\n  \n  # Check for convergence\n  if (abs(pi_new - pi) &lt; tol) {\n    break\n  }\n  \n  iter &lt;- iter + 1\n}\n\n#after convergence, print the result\n\nprint(pi)\n\n\n[1] 0.293\n\n\nSo what’s gone on here? We have 1000 observations of a binary variable; maximum likelihood is answering the question “what is the value of the parameter \\(\\pi\\) that makes the observed data most likely?” To get that answer, we have tried candidate values of \\(\\pi\\) to see which one maximizes the log-likelihood function.\nWe’ve maximized the function two ways - using a grid search process, and using the Newton-Raphson algorithm. The latter is more efficient and is the basis for most optimization algorithms in statistical software, and easy to adapt to estimate multiple parameters.\nHere’s one more approach also using the Newton-Raphson method. It calls the maxLik package to do the optimization instead of our having to write the algorithm ourselves.\n\n\ncode\nlibrary(maxLik)\n# Write the binomial log-likelihood function\nlog_likelihood &lt;- function(pi, y) {\n  sum(y * log(pi) + (1 - y) * log(1 - pi))\n}\n\n#maximize the function using maxLik\n\nm1m &lt;- maxLik(y=y, log_likelihood, start = c(0.5), method = \"NR\") #Newton-Raphson method, starting value of 0.5\n\nsummary(m1m)\n\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 3 iterations\nReturn code 2: successive function values within tolerance limit (tol)\nLog-Likelihood: -604.816 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(&gt; t)    \n[1,]  0.29300    0.01441   20.34  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\nAgain, the value of \\(\\pi\\) that maximizes the log-likelihood is the same as the sample frequency of \\(y\\), which is 0.293.\n\n\n\nLet’s make this more realistic in terms of resembling the models we want to run by adding an intercept term and multiple predictors to the model - the likelihood now has to account for those \\(X\\) variables and maximize with respect to them.\n\ngenerate some data on \\(y\\), and \\(X\\)\nwrite the likelihood\nwrite the Newton-Raphson algorithm\napply to the data\n\n\n\ncode\n# Generate binary y, and two X variables for regression; coefficients are -1, 0.5, -0.5\n\nset.seed(8675309)\nn &lt;- 10000\nX &lt;- matrix(rnorm(n * 2), ncol = 2)\ntrue_beta &lt;- c(-1, 0.5, -0.5)\nz &lt;- cbind(1, X) %*% true_beta\nprob &lt;- 1 / (1 + exp(-z))\ny &lt;- rbinom(n, 1, prob)\n\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson iterative algorithm\n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # Add intercept term to X if not already present\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#write the gradient function - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- plogis(X %*% beta)\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  k &lt;- ncol(X)\n  \n# set starting values for k columns of data (betas)\n  beta &lt;- rep(0, k) \n  \n# Store gradients and log-likelihoods for analysis\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n# iterate NR \n  for (iter in 1:max_iter) {\n    # compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # compute Hessian\n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # check for convergence\n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n    \n    # compute standard errors for this model by taking the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  }\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#html table comparing glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\", caption=\"Comparing GLM and Dave Estimates\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  column_spec(3, border_right = T) %&gt;%\n  column_spec(4, border_right = T) %&gt;%\n  collapse_rows(columns = 1:3, valign = \"top\") \n\n\n\n\nTable 1: Comparing GLM and Dave Estimates\n\n\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-0.9531826\n-0.9531826\n0.0236756\n0.0236748\n\n\nX1\n0.4938831\n0.4938831\n0.0238303\n0.0238295\n\n\nX2\n-0.4719703\n-0.4719703\n0.0240294\n0.0240286\n\n\n\n\n\n\n\n\n\n\n\nTable 1 compares the coefficients and standard errors from the Newton-Raphson algorithm to the coefficients and standard errors from the glm function. The estimates are the same, the standard errors very close.\nAlright, lots of fun here. Now, let’s use the program above to estimate a logit model using the democratic peace data.\n\n\n\nTable 2 compares the coefficients and standard errors from the program we wrote to those produced by the glm function. The regression here is a logit model predicting the onset of a militarized dispute as a function of the logged capabilities ratio, an indicator of whether the pair of states share a border, and the lowest democracy (polity) score in the pair.\n\n\ncode\ndp &lt;- read.csv(\"/users/dave/documents/teaching/606J-mle/2022/exercises/ex1/dp.csv\", header=TRUE)\n\n# for the log_likelihood function below, define y as the variable \"dispute\" in the dp data frame, and x1, x2, and x3 as the variables \"lncaprat\", \"border\", and \"deml\" in the dp data frame, respectively.\ndp$lncaprat &lt;- log(dp$caprat)\nX &lt;- as.matrix(dp[, c(\"lncaprat\", \"border\", \"deml\")])\ny &lt;- as.vector(dp$dispute)\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson \n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # check for constant; add one if necessary\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#write the gradient function -  partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- 1 / (1 + exp(-X %*% beta))\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  p &lt;- ncol(X)\n  \n  # set starting values for beta\n  beta &lt;- rep(0, p)\n  \n  # Store gradients and log-likelihoods\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n  for (iter in 1:max_iter) {\n    # Compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # Compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # Compute Hessian - 2nd partial derivatives \n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # Update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # Check for convergence - end if change in beta is less than tolerance value \n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n  }\n\n#compute standard errors for this model; take the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n# logit$coefficients\n# logit$st.errors\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#compare glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\", caption=\"Comparing GLM and Dave Estimates, Democratic Peace Model\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:3, valign = \"top\")\n\n\n\n\nTable 2: Comparing GLM and Dave Estimates, Democratic Peace Model\n\n\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-3.2407604\n-3.2407605\n0.1034544\n0.1034630\n\n\nXlncaprat\n-0.2028768\n-0.2028768\n0.0237847\n0.0237860\n\n\nXborder\n0.7827135\n0.7827135\n0.0852298\n0.0852337\n\n\nXdeml\n-0.0783467\n-0.0783467\n0.0068065\n0.0068078\n\n\n\n\n\n\n\n\n\n\n\nRecapping what we’ve done:\n\nWe wrote a log-likelihood function for a logistic regression model with multiple predictors.\nTo maximize the log-likelihood, we implemented a Newton-Raphson algorithm in the following steps:\n\nDefined the gradient and Hessian functions. The gradient is the vector of first partial derivatives of the log-likelihood, and the Hessian is the matrix of second partial derivatives.\nInitialized the beta coefficients and stored gradients and log-likelihoods.\nIterated the algorithm to update beta values.\nChecked for convergence based on the change in beta.\n\nOnce the model converged, we computed standard errors for the estimated coefficients by taking the square root of the main diagonal elements of the inverse negative Hessian.\n\n\n\n\nHere, you can see how the gradients change over each iteration.\n\n\ncode\n#plot gradient convergence\ngradients &lt;- do.call(cbind, logit$gradient_history)\ngradients &lt;- as.data.frame(gradients) %&gt;%\n  mutate(coef=c(\"Intercept\", \"lncaprat\", \"border\", \"deml\")) \n\ngradients &lt;- gradients %&gt;% \n  pivot_longer(-coef, names_to = \"iteration\", values_to = \"value\")\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(gradients, \"line\", hcaes(x=iteration, y=value, group=coef, color=coef)) %&gt;%\n  hc_title(text = \"Gradient Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Gradient\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\n\n\nAnd here, we can see how the log-likelihood changes over iterations:\n\n\ncode\n#plot ll convergence\n\nlls &lt;- as.data.frame(logit$ll_history) %&gt;%\n  mutate(iteration=1:nrow(.))\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(lls, \"line\", hcaes(x=iteration, y=`logit$ll_history`)) %&gt;%\n  hc_title(text = \"Log-Likelihood Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\n\n\nHere are two ways to recover the final log-likelihood from the glm model. The first uses the logLik function from the stats package. The second calculates the log-likelihood by hand - generating the predictions and plugging those into the log-likelihood function, then summing.\n\\[ LL = \\sum_{i=1}^{n} y_i \\ln(p) + (1-y_i) \\ln(1-p) \\]\n\n\ncode\n# use the logLik function from the stats package  \n\nllest &lt;- logLik(glm_fit)\n\n# or calculate it by hand - generate the predictions and plug those into the log-likelihood function, then sum.\n\npred &lt;- predict(glm_fit, type = \"response\")\nllestbyhand &lt;- sum(dp$dispute * log(pred) + (1 - dp$dispute) * log(1 - pred))\n\n\ndata.frame(llest, llestbyhand) %&gt;% \n  kable(\"html\", caption=\"Recovering the Log-Likelihood\" ) %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:2, valign = \"top\")\n\n\n\n\nRecovering the Log-Likelihood\n\n\nllest\nllestbyhand\n\n\n\n\n-3565.219\n-3565.219"
  },
  {
    "objectID": "llfmax.html#maximizing-the-likelihood---grid-search",
    "href": "llfmax.html#maximizing-the-likelihood---grid-search",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "How do we solve this function for the data \\(y\\)? Put differently, what is the value of \\(\\pi\\) that most likely generated the data, \\(y\\)? There are a few ways to do this - one instructive method is to use a grid search, where we calculate the log-likelihood for a range of values of \\(\\pi\\) and find the value that maximizes the log-likelihood. We can repeat this to the desired level of specificity, progressively narrowing the grid. In this example, we’ll just do one search out to 3 decimal places. The steps are these:\n\ngenerate the range of candidate values of our parameter \\(\\pi\\) to plug into the log-likelihood function.\nplug each value into the log-likelihood function to compute the log-likelihood for that value.\nidentify which value of \\(\\pi\\) maximizes the log-likelihood.\n\nHere’s code to do this:\n\n\ncode\n# generate a vector of values of pi \n\npi_trials &lt;- seq(0, 1, by = 0.001)\n\n# write the log-likelihood function\n\nlog_likelihood &lt;- function(pi_trials, y) {\n  sum(y * log(pi_trials) + (1 - y) * log(1 - pi_trials))\n}\n\n# Calculate log-likelihood for each pi value\n\nll_values &lt;- sapply(pi_trials, log_likelihood, y = y)\n\n# Find the pi value that maximizes the log-likelihood\n\npi_hat &lt;- pi_trials[which.max(ll_values)]\nprint(pi_hat)\n\n\n[1] 0.293\n\n\nYou’ll note we recover the sample mean of \\(y\\). Let’s plot the log-likelihood against the values of \\(\\pi\\) to visualize the maximum.\n\n\ncode\n# Plot log-likelihood against pi\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(data.frame(pi = pi_trials, ll = ll_values), \"line\", hcaes(x = pi, y = ll)) %&gt;%\n  hc_title(text = \"Grid search - maximizing the log-likelihood\") %&gt;%\n  hc_xAxis(title = list(text = \"Pi\"), \n           plotLines = list(\n    list(color =\"red\", value = pi_hat ))) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_legend(enabled = FALSE) %&gt;%\n  hc_colors(bucolors)"
  },
  {
    "objectID": "likelihood24.html",
    "href": "likelihood24.html",
    "title": "Likelihood",
    "section": "",
    "text": "Why do we turn to maximum likelihood instead of OLS, especially given the simplicity and robustness of OLS?\nOur data can’t meet the OLS assumptions; \\(y\\) is often limited* such that we observe \\(y\\) but really want to measure \\(y^*\\).\n\\(y^*\\) is often a continuous (unlimited) variable we wish we could measure - if we could, we’d use OLS to estimate its correlates.\nOLS asks us to make the data satisfy the model. MLE asks us to build a model based on the data. Since our data often don’t satisfy the OLS assumptions, MLE offers a flexible alternative.\n\n\n\n\n\nML asks us to think of the data as given and to imagine the model that might best represent the Data Generating Process.\n\nIn OLS, the model is fixed - the assumptions about \\(\\epsilon\\) are given.\nIn ML, the data are fixed - we construct a model that reflects the nature of the data.\nWhat we assume about the unobservables is informed by what we know about the observables, the \\(y\\) variable.\nA useful way to describe \\(y\\) is to characterize its observed or empirical distribution.\nOnce we know the distribution of \\(y\\), we can begin building a model based on that distribution.\n\n\n\n\n\nWe are limited in what we can observe and/or measure in \\(y\\). E.g., we observe an individual voting or not; we cannot observe the chances an individual votes.\n\nThe \\(y\\) variable has both observed and latent qualities; label the latent variable \\(\\widetilde{y}\\).\nOften, we are more interested in this latent variable. We are more interested in the latent chance of voting than the observed behavior.\n\\(\\widetilde{y}\\) is the variable we wish we could measure.\nThe latent and observed variables are often distributed differently. Observed voting \\(y=(0,1)\\); latent variable \\(Pr(y=1)\\).\nLinking \\(X\\) variables to the latent \\(\\widetilde{y}\\) requires rescaling; this is often because changes in \\(\\widetilde{y}\\) given \\(X\\) are nonlinear and in different units. In the example above, \\(y\\) is a binary realizing of voting, so is binomial. \\(\\widetilde{y}\\) is the probability of voting, so is continuous, bounded between 0 and 1.\nBesides, \\(\\widetilde{y}\\) is unobserved - so we have to generate it from the model.\nTo link \\(X\\) with \\(\\widetilde{y}\\) or \\(y\\), we assume their relationship follows some distribution; we call this the link distribution.\n\n\n\n\n\nOur data are often limited, and not suitable for OLS. OLS asks us to transform data to make it meet the model assumptions (e.g., Generalized Least Squares); MLE builds the model based on the data."
  },
  {
    "objectID": "likelihood24.html#ml-ols",
    "href": "likelihood24.html#ml-ols",
    "title": "Likelihood",
    "section": "",
    "text": "Why do we turn to maximum likelihood instead of OLS, especially given the simplicity and robustness of OLS?\nOur data can’t meet the OLS assumptions; \\(y\\) is often limited* such that we observe \\(y\\) but really want to measure \\(y^*\\).\n\\(y^*\\) is often a continuous (unlimited) variable we wish we could measure - if we could, we’d use OLS to estimate its correlates.\nOLS asks us to make the data satisfy the model. MLE asks us to build a model based on the data. Since our data often don’t satisfy the OLS assumptions, MLE offers a flexible alternative."
  },
  {
    "objectID": "likelihood24.html#ml",
    "href": "likelihood24.html#ml",
    "title": "Likelihood",
    "section": "",
    "text": "ML asks us to think of the data as given and to imagine the model that might best represent the Data Generating Process.\n\nIn OLS, the model is fixed - the assumptions about \\(\\epsilon\\) are given.\nIn ML, the data are fixed - we construct a model that reflects the nature of the data.\nWhat we assume about the unobservables is informed by what we know about the observables, the \\(y\\) variable.\nA useful way to describe \\(y\\) is to characterize its observed or empirical distribution.\nOnce we know the distribution of \\(y\\), we can begin building a model based on that distribution."
  },
  {
    "objectID": "likelihood24.html#limited-dependent-variables",
    "href": "likelihood24.html#limited-dependent-variables",
    "title": "Likelihood",
    "section": "",
    "text": "We are limited in what we can observe and/or measure in \\(y\\). E.g., we observe an individual voting or not; we cannot observe the chances an individual votes.\n\nThe \\(y\\) variable has both observed and latent qualities; label the latent variable \\(\\widetilde{y}\\).\nOften, we are more interested in this latent variable. We are more interested in the latent chance of voting than the observed behavior.\n\\(\\widetilde{y}\\) is the variable we wish we could measure.\nThe latent and observed variables are often distributed differently. Observed voting \\(y=(0,1)\\); latent variable \\(Pr(y=1)\\).\nLinking \\(X\\) variables to the latent \\(\\widetilde{y}\\) requires rescaling; this is often because changes in \\(\\widetilde{y}\\) given \\(X\\) are nonlinear and in different units. In the example above, \\(y\\) is a binary realizing of voting, so is binomial. \\(\\widetilde{y}\\) is the probability of voting, so is continuous, bounded between 0 and 1.\nBesides, \\(\\widetilde{y}\\) is unobserved - so we have to generate it from the model.\nTo link \\(X\\) with \\(\\widetilde{y}\\) or \\(y\\), we assume their relationship follows some distribution; we call this the link distribution."
  },
  {
    "objectID": "likelihood24.html#the-big-point",
    "href": "likelihood24.html#the-big-point",
    "title": "Likelihood",
    "section": "",
    "text": "Our data are often limited, and not suitable for OLS. OLS asks us to transform data to make it meet the model assumptions (e.g., Generalized Least Squares); MLE builds the model based on the data."
  },
  {
    "objectID": "likelihood24.html#for-example",
    "href": "likelihood24.html#for-example",
    "title": "Likelihood",
    "section": "For example …",
    "text": "For example …\n\n\\(Y = (0,1)\\), does an individual vote or not. This is binary, discrete, let’s say binomial.\n\\(\\tilde{y}\\) is the latent probability an individual votes. Wish we could measure this, use OLS.\nWrite a function based on \\(Y\\): \\(\\pi^y (1-\\pi)^{(1-y)}\\) , the binomial PDF (without the n-tuple).\n\\(\\pi\\) is given by some \\(X\\) variables we think affect voting - so \\(\\pi = x\\beta\\).\nSo substitute for \\(\\pi\\) …\\((x\\beta)^y (1-x\\beta)^{(1-y)}\\)\nNow, link the linear prediction to \\(\\tilde{Y}\\) - map \\(x\\beta\\) onto the Pr(vote). How about using the Standard Normal CDF \\(\\Phi\\)?\n\n\\(\\Phi(x\\beta)^y  \\Phi(1-x\\beta)^{(1-y)}\\) - this is now the core of the Probit."
  },
  {
    "objectID": "likelihood24.html#likelihood",
    "href": "likelihood24.html#likelihood",
    "title": "Likelihood",
    "section": "Likelihood",
    "text": "Likelihood\nKing (pp. 9, 14) puts it this way:\n\\[Y\\sim f(y|\\theta,\\alpha)\\]\nand\n\\[\\theta = G(X,\\beta)\\]\nso our data, \\(Y\\) has a probability distribution given by parameters \\(\\theta, \\alpha\\), and \\(\\theta\\) is a function of some variables, \\(X\\) and their parameters, \\(\\beta\\).\n\nAll of this comprises the model in King’s lingo, so a basic probability statement appears as:\n\\[Pr(y|\\mathcal{M}) \\equiv Pr(\\mathrm{data|model})\\]\nThis is a conditional probability resting on two conditions:\n\nThe data are random and unknown.\nThe model is known.\n\nUh oh. The model is known? The data aren’t? This works in many probability settings. What is the probability of rolling 3 threes in 6 rolls of a six-sided die? What is the probability you draw an ace from a standard deck of cards? In cases like these, the model is known even before we observe events (data).\n\nIn cases like these, the model’s parameters are known and fixed. In our applications, the model and its parameters are the unknowns, but the events or data are known, fixed, and given.\nSuppose I flip a coin to decide whether I roll a 20-sided die, or a 6-sided die. You do not observe any of this - I only tell you I rolled a 4 - which die did I roll? This is the problem we try to deal with in ML - what is the data generating process that most likely produced the observed data. Unlike the simple die roll, the model isn’t known. Instead, there are many possible models that could have produced the data."
  },
  {
    "objectID": "likelihood24.html#inverse-probability",
    "href": "likelihood24.html#inverse-probability",
    "title": "Likelihood",
    "section": "Inverse Probability",
    "text": "Inverse Probability\nGiven these conditions, the more sensible model would be:\n\\[Pr(\\mathcal{M}|y)\\]\nThis is the inverse probability model - but it requires knowledge (or strong assumptions) regarding an important element of the (unknown) model, \\(\\theta\\). Even Bayes can’t really do this."
  },
  {
    "objectID": "likelihood24.html#likelihood-1",
    "href": "likelihood24.html#likelihood-1",
    "title": "Likelihood",
    "section": "Likelihood",
    "text": "Likelihood\nLikelihood estimates the model, \\(\\mathcal{M}\\) given the data, but assumes that \\(\\theta\\) can take on different values, representing different (competing) hypothetical models.\nThe set of \\(\\theta\\)’s are the competing models or data generating processes that could have produced the observed data set.\n\nKeeping with King’s notation, the likelihood axiom is:\n\\[\\mathcal{L}(\\tilde{\\theta}|y,\\mathcal{M}*)\\equiv L(\\tilde{\\theta}|y) \\] \\[=k(y)Pr(y|\\tilde{\\theta})\\] \\[\\propto Pr(y|\\tilde{\\theta})\\]\nwhere \\(\\tilde{\\theta}\\) represents the hypothetical value of \\(\\theta\\) (rather than its true value).\nThe term \\(k(y)\\) (known as the “constant of proportionality”) is a constant across all the hypothetical values of \\(\\tilde{\\theta}\\) (and thus drops out) but is the key to the likelihood axiom; it represents the functional form through which the data (\\(y\\)) shape \\(\\tilde{\\theta}\\) and thus allow us to estimate the likelihood as a measure of relative (instead of absolute) uncertainty; our uncertainty in this case is relative to the other possible functions of \\(y\\) and the hypothetical values of \\(\\tilde{\\theta}\\).\nAs King (p. 61) puts it , \\(k(y)\\) “measures the relative likelihood of a specified hypothetical model \\(\\tilde{\\beta}\\) producing the data we observed.”\nIt turns out that the likelihood of observing the data is proportional to the probability of observing the data."
  },
  {
    "objectID": "likelihood24.html#take-away-points",
    "href": "likelihood24.html#take-away-points",
    "title": "Likelihood",
    "section": "Take Away Points",
    "text": "Take Away Points\n\nWe want to know the probability (the model) of observing some data; if we find the model parameters with the highest likelihood of generating the observed data, we also know the probability because the two are proportional.\nLikelihoods are always negative; they do not have a scale or specific meaning; they do not transform into probabilities or anything familiar.\nWe find the parameter values that produce the largest likelihood values; those parameters that maximize the likelihood then can be translated into sensible quantities - they can be mapped onto \\(\\tilde{y}\\), giving us a measure of the variable we wish we had.\nIn OLS, we compute parameter estimates that minimize the sum (squared) distance of all the observed points to the predicted points - we minimize the errors in this fashion.\nThe technology of MLE is trial and error - choose some values for \\(\\tilde{\\theta}\\), compute the likelihood, then repeat. Compare all the likelihoods - the values of \\(\\tilde{\\theta}\\) that produced the highest likelihood value are the ones that most likely generated the observed data."
  },
  {
    "objectID": "likelihood24.html#writing-the-likelihood",
    "href": "likelihood24.html#writing-the-likelihood",
    "title": "Likelihood",
    "section": "Writing the Likelihood",
    "text": "Writing the Likelihood\nSince we’ve determined \\(y\\) is normal, write the Normal PDF.\n\\[Pr(Y=y_{i})=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{\\left[\\frac{-(y_{i}-\\mu_{i})^{2}}{2\\sigma^{2}}\\right]}\\]\nWe’re interested in the joint probability of the observations, the probability the data result from a particular Data Generating Process. Assuming the observations in the data are independent of one another, the joint density is equal to the product of the marginal probabilities:\n\\[Pr(A~ \\mathrm{and}~ B)=Pr(A)\\cdot Pr(B)\\]\nso the joint probability of \\(y\\), written in terms of the likelihood, is given by\n\\[\\mathcal{L} (\\mu, \\sigma^{2}| y )= \\prod\\limits_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{\\left[\\frac{-(y_{i}-\\mu_{i})^{2}}{2\\sigma^{2}}\\right]}\\]\nAdding is easier than multiplying; since we can transform the likelihood function by any monotonic form, we can take its natural log to replace the products with summations:\n\\[\\ln \\mathcal{L} (\\mu, \\sigma^{2}|y) = \\ln \\prod\\limits_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{\\left[\\frac{-(y_{i}-\\mu_{i})^{2}}{2\\sigma^{2}}\\right]}\\]\n\\[= \\sum \\ln \\left[\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{\\left[\\frac{-(y_{i}-\\mu_{i})^{2}}{2\\sigma^{2}}\\right]} \\right]\\]\n\\[=\\sum\\left( -\\frac{1}{2}(\\ln(2\\pi))-\\frac{1}{2}(\\ln(\\sigma^{2}))-\\frac{1}{2\\sigma^{2}}\\left[\\sum\\limits_{i=1}^{n}(y_{i}-\\mu)^{2}\\right] \\right)\\]"
  },
  {
    "objectID": "likelihood24.html#the-linear-model",
    "href": "likelihood24.html#the-linear-model",
    "title": "Likelihood",
    "section": "The Linear model",
    "text": "The Linear model\nIt should be pretty evident this is the linear model.\n\nwe started with data that looked normal; continuous, unbounded, infinitely differentiable.\nassuming normality, we wrote a LLF in terms of the distribution parameters \\(\\mu, \\sigma^2\\).\nbut we want \\(\\mu\\) itself to vary with some \\(X\\) variables; \\(y\\) isn’t just characterized by a grand mean, but by a set of of conditional means given by \\(X\\).\nso we need to parameterize the model - write the distribution parameter as a function of some variables.\ngenerally, this is to declare \\(\\theta = F(x\\beta)\\).\nin the linear/normal case, to declare \\(\\mu=F(x\\beta)\\). Of course, \\(F\\) is linear, we just write \\(\\mu=x\\beta\\).\nin the LLF, substitute \\(x\\beta\\) for \\(\\mu\\), and now we’re taking the difference between \\(y\\) and \\(x\\beta\\), squaring those differences, and weighting them by the variance.\n\n\nTo put it all together \\(\\ldots\\)\n\\[\\ln \\mathcal{L}(\\mu, \\sigma^{2}|y) = \\ln \\prod\\limits_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} exp \\left[\\frac{-(y-x\\beta)^{2}}{2\\sigma^{2}}\\right] \\] \\[= \\sum \\ln \\left\\{\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} exp \\left[\\frac{-(y-x\\beta)^{2}}{2\\sigma^{2}}\\right]\\right\\}\\]\n\\[= \\sum\\left(-\\frac{1}{2}(\\ln(2\\pi)) -\\frac{1}{2}(\\ln(\\sigma^{2})) -\\frac{1}{2\\sigma^{2}}\\left[\\sum\\limits_{i=1}^{n}(y-x\\beta)^{2}\\right] \\right)\\]\nDoes this look familiar? A lot like deriving OLS, eh?"
  },
  {
    "objectID": "likelihood24.html#linear-regression-in-ml",
    "href": "likelihood24.html#linear-regression-in-ml",
    "title": "Likelihood",
    "section": "Linear regression in ML",
    "text": "Linear regression in ML\nThis is the Normal (linear) log-likelihood function. It presumes some data, \\(\\mathbf{y}\\) and some unknowns \\(\\mathbf{\\beta, \\sigma^2}\\). You should note the kernel of the function is the sum of the squared differences of \\(y\\) and \\(x\\beta\\).\n\\[\\ln\\mathcal{L} =\\sum\\left(-\\frac{1}{2}(\\ln(2\\pi)) -\\frac{1}{2}(\\ln(\\sigma^{2})) -\\frac{1}{2\\sigma^{2}}\\color{red}{\\left[\\sum\\limits_{i=1}^{n}(y-x\\beta)^{2}\\right]} \\right)\\]\nIt turns out if we take the derivative of this function with respect to \\(\\beta\\) and \\(\\sigma^2\\), the result is the ML estimator, and the OLS estimator. They’re the same."
  },
  {
    "objectID": "likelihood24.html#linearity",
    "href": "likelihood24.html#linearity",
    "title": "Likelihood",
    "section": "Linearity",
    "text": "Linearity\nThis model is linear - our estimates, \\(x\\beta\\) map directly onto \\(y\\) - there is no latent variable, \\(\\tilde{y}\\) to map onto - so \\(x\\beta = \\widehat{y}\\).\nNote that this is not because \\(y\\) is normal. Rather, the fact that \\(y\\) is continuous and contains a lot of information and is not limited makes it more likely \\(y\\) is normal.\nThough ML is most often used in cases where \\(y\\) is limited (so OLS is inappropriate), it’s important to see that we can estimate the linear model using either technology (OLS, ML) and get the same estimates."
  },
  {
    "objectID": "likelihood24.html#footnotes",
    "href": "likelihood24.html#footnotes",
    "title": "Likelihood",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee von Bortkiewicz, Ladislaus (1898). Das Gesetz der Kleinen Zahlen. Leipzig: Teubner.↩︎"
  },
  {
    "objectID": "likelihood24.html#numerical-method-intuition",
    "href": "likelihood24.html#numerical-method-intuition",
    "title": "Likelihood",
    "section": "Numerical method intuition",
    "text": "Numerical method intuition\n\nChoose starting values of \\(\\beta\\) (sometimes from OLS) to estimate the log-likelihood.\nTake the derivative of the log-likelihood with respect to the parameters to find the gradient}. The gradient (or the gradient matrix, a \\(kxk\\) matrix) tells us the direction of the slope of a line tangent to the curve at the point of the log-likelihood estimate.\nIf the gradient is positive (if the matrix is positive definite), then \\(ln \\mathcal{L}\\) is increasing in \\(\\beta\\) - the slope is increasing, so increase our estimate of \\(\\beta\\) and try again.\nIf the gradient is negative (if the matrix is negative definite), the \\(ln \\mathcal{L}\\) is decreasing in \\(\\beta\\) - the slope is decreasing, so we’ve passed the maximum; choose a smaller value for \\(\\beta\\) and try again.\nAs the log-likelihood approaches the maximum, the gradient approaches zero - the slope of the line tangent to the curve at the point of the log-likelihood estimate is approaching zero, indicating we’re reaching the maximum of the function. Stop the search and evaluate the estimates of \\(\\beta\\) that produced the zero gradient.\nThroughout this process, we need to evaluate the second derivatives in order to figure out the rate at which the slope is changing; this helps us tell how close or far we are from the maximum. The second derivative describes the curvature of the LLF, or the rate of change.\nThe matrix of second derivatives (the Hessian matrix) or its approximation also provide the source of our estimates of the variance, and thus the standard errors.\n\n\nThe first derivative tells us the direction in which the function is changing. This is obviously important since we’re trying to find the maximum.\nThink of this as trying to figure out when you’re exactly at the top of a hill. The slope (the grade, the gradient) is positive while you’re climbing to the top, it’s zero at the top, and it’s negative on the way down the other side.\nBut is the hill flat or steep? If it’s flat, then the change in the slope between point A and point B is likely to be very small - this, of course, can make it difficult to know exactly when we’re at the top (the maximum). On the other hand, if the hill is very steep, the change in the slope between two points is pretty substantial. Put another way, the rate of change in the slope is larger (faster) the steeper the slope; it’s smaller (slower) the flatter the slope.\nThis matters to maximization because the second derivatives tell us how big (or small) a step we should take up the hill as we try to find the top. Suppose that the function is very flat; as indicated above, the change in the slope between two points would be small, so we can take larger steps in order to try to find the maximum. The second derivatives would tell us that the rate of change is very small, so we should take larger steps.\nThe software performing the estimation will choose the next value of \\(\\beta\\) a bit further away from the last value it tried. On the other hand, if the second derivatives are large so the rate of change is fast, we want to take relatively small steps so we don’t step right over the maximum. In any case, that’s the intuition for why we need to know the matrix of second derivatives."
  },
  {
    "objectID": "llfmax24.html",
    "href": "llfmax24.html",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "The goal here is to walk through the process of writing a likelihood function based on the data we observe, then programming that likelihood function and optimizing it with respect to the data.\n\n\nLet’s recall the motivation for turning to ML. The data we have are not suitable for OLS because our observation of the data is limited. For instance, we observe a binary variable \\([0,1]\\) rather than the underlying continuous probability beneath it. If we could observe and measure that probability, we’d do so and perhaps use OLS to estimate a model. But since we only observe \\([0,1]\\), our observation is limited.\nOLS is poorly suited to model this variable especially if one of our goals is to generate predictions of that underlying probability. That is, one of our goals might be to estimate a measure of the \\(y\\) variable we wish we could measure directly.\nMLE is better suited to this task - it asks us what the data generating process is that produced the observed data, and to build a model appropriate to the limited observation of that \\(y\\) variable.\nTo do so, we need to:\n\ndescribe the observed distribution of \\(y\\)\nconsider what we wish we could measure - this points to the key quantities of interest we want to derive from the model.\ndescribe \\(y\\) according to a probability distribution; write that distribution\nwrite a log-likelihood function appropriate to that probability distribution.\nparameterize the log-likelihood function such that we have a link distribution to map the linear prediction, \\(x\\beta\\), onto the probability space of \\(y\\).\n\n\n\n\nLet’s start with a \\(y\\) variable as follows:\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                y\n                Freq\n              \n        \n        \n        \n                \n                  0\n                  707\n                \n                \n                  1\n                  293\n                \n        \n      \n    \n\n\n\nThe variable, \\(y\\), takes on values of zero and one - it appears binomial; let’s write this in terms of the binomial distribution parameter \\(\\pi\\), so it takes on the value of one with probability \\(\\pi\\) and zero with probability \\(1-\\pi\\).\n\\[ y_i = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{} \\pi_{i}\\\\\n         0, & \\mbox{} 1-\\pi_{i}\n         \\end{array}\n     \\right.\\]\nThe likelihood of a single observation is:\n\\[ \\mathcal{L}(\\pi_i|y) = \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\]\nThis is a statement of the likelihood that any particular value of \\(\\pi\\) generated an observation. We want the likelihood any value of \\(\\pi\\) generated the entire dataset - that is, we want the joint likelihood of all the observations.\n\\[ \\mathcal{L}(\\pi|y) = \\prod_{i=1}^{n} \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\] recalling that a joint probability is the product of individual probabilities.\nLet’s take the log of this likelihood function - the natural log makes computation easier. Even for computers, this reduces the computational intensity especially with respect to extremely small decimals.\n\\[ \\ln \\mathcal{L}(\\pi|y) = \\sum_{i=1}^{n} y_i \\ln(\\pi_i) + (1-y_i) \\ln(1-\\pi_i) \\]\nWe can take this a step further and parameterize \\(\\pi\\) as a function of \\(X\\beta\\) where \\(X\\) is the matrix of predictors and \\(\\beta\\) is the vector of coefficients such that\n\\[ \\pi_i = F({X_i\\beta}) \\]\nSo our estimate of the binomial probability is a function of the linear predictor \\(X\\beta\\). We map \\(X\\beta\\) onto the probability space \\(\\pi\\) using a link function, \\(F\\). The most common link functions are the logistic (logit model), and the standard normal (probit model). Let’s write the logit link:\n\\[ \\pi_i = \\frac{1}{1 + e^{-(X_i\\beta)}} \\]\nand now let’s write this all in the log-likelihood function:\n\\[ \\ln \\mathcal{L}(\\beta|y) = \\sum_{i=1}^{n} y_i \\ln(\\frac{1}{1 + e^{-(X_i\\beta)}}) + (1-y_i) \\ln(1-\\frac{1}{1 + e^{-(X_i\\beta)}}) \\]\nThis is the logit log-likelihood function for a binary \\(y\\) variable.\n\n\n\nHow do we solve this function for the data \\(y\\)? Put differently, what is the value of \\(\\pi\\) that most likely generated the data, \\(y\\)? There are a few ways to do this - one instructive method is to use a grid search, where we calculate the log-likelihood for a range of values of \\(\\pi\\) and find the value that maximizes the log-likelihood. We can repeat this to the desired level of specificity, progressively narrowing the grid. In this example, we’ll just do one search out to 3 decimal places. The steps are these:\n\ngenerate the range of candidate values of our parameter \\(\\pi\\) to plug into the log-likelihood function.\nplug each value into the log-likelihood function to compute the log-likelihood for that value.\nidentify which value of \\(\\pi\\) maximizes the log-likelihood.\n\nHere’s code to do this:\n\n\ncode\n# generate a vector of values of pi \n\npi_trials &lt;- seq(0, 1, by = 0.001)\n\n# write the log-likelihood function\n\nlog_likelihood &lt;- function(pi_trials, y) {\n  sum(y * log(pi_trials) + (1 - y) * log(1 - pi_trials))\n}\n\n# Calculate log-likelihood for each pi value\n\nll_values &lt;- sapply(pi_trials, log_likelihood, y = y)\n\n# Find the pi value that maximizes the log-likelihood\n\npi_hat &lt;- pi_trials[which.max(ll_values)]\nprint(pi_hat)\n\n\n[1] 0.293\n\n\nYou’ll note we recover the sample mean of \\(y\\). Let’s plot the log-likelihood against the values of \\(\\pi\\) to visualize the maximum.\n\n\ncode\n# Plot log-likelihood against pi\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(data.frame(pi = pi_trials, ll = ll_values), \"line\", hcaes(x = pi, y = ll)) %&gt;%\n  hc_title(text = \"Grid search - maximizing the log-likelihood\") %&gt;%\n  hc_xAxis(title = list(text = \"Pi\"), \n           plotLines = list(\n    list(color =\"red\", value = pi_hat ))) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_legend(enabled = FALSE) %&gt;%\n  hc_colors(bucolors)\n\n\n\n\n\n\n\n\n\nGrid searches are not sufficient for multivariate models, especially where we also need measures of uncertainty. Here, we turn to numerical optimization.\nNumerical optimization is a field aimed at finding “best” outcomes or answers depending on some set of criteria - often, the “best” is the maximum or minimum of some function - that’s the case in maximum likelihood.\n\n\nThere are lots of ways to optimize a function - in statistical software, most of them derive from Newton’s iterative method. A very common application of this is the Newton-Raphson method. Newton-Raphson is an iterative process that starts with an initial guess for the parameter \\(\\pi\\) (or the vector of unknowns, \\(\\beta\\)) and updates it in the direction of the maximum until it converges to the maximum. At each step, it computes the first and second derivatives of the likelihood function, then uses these to update the parameter estimate(s). The update rule is:\n\\[ \\pi_{new} = \\pi_{old} - H(\\pi)^{-1}*g(\\pi) \\]\nSo the new estimate of the parameter is the old estimate minus the first derivative (gradient) of the log-likelihood function pre-multiplied by the Hessian (second derivative) of the log-likelihood function. Here, \\(g(\\pi)\\) is the gradient of the log-likelihood function and \\(H(\\pi)\\) is the Hessian of the log-likelihood function. The algorithm iterates this process until the change in \\(\\pi\\) is sufficiently small, below a specified threshold. Since \\(g\\) and \\(H\\) are matrices, you should see how this straightforwardly applied to a multivariate regression.\nLet’s write the Newton-Raphson algorithm ourselves - take a look at the code chunk below. You’ll see it produces the same result as the grid search; the sample frequency of \\(y\\).\n\n\n\n\n\ncode\n# use same data generated above; same LLF as above, aiming to recover pi\n\n#write the gradient - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(pi, y) {\n  sum(y / pi - (1 - y) / (1 - pi))\n}\n\n#write the Hessian - matrix of partial second derivatives of the log-likelihood function.\n\nhessian &lt;- function(pi, y) {\n  sum(-y / pi^2 - (1 - y) / (1 - pi)^2)\n}\n\n#declare a starting value for pi\n\npi &lt;- 0.5\n\n#set convergence criteria - when new pi is within 1e-6 of old pi, stop iterating\n\ntol &lt;- 1e-6\n\n#initialize iteration counter, set maximum iterations; how many times will we do this before we stop if we don't converge prior? \n\niter &lt;- 0\nmax_iter &lt;- 100\n\n#iterate the Newton-Raphson algorithm\n\nfor (iter in 1:max_iter) {\n  # Compute gradient  \n  grad &lt;- gradient(pi, y)\n  \n  # Compute Hessian\n  hess &lt;- hessian(pi, y)\n  \n  # Update pi\n  pi_new &lt;- pi - (grad / hess)\n  \n  pi &lt;- pi_new\n  \n  # Check for convergence\n  if (abs(pi_new - pi) &lt; tol) {\n    break\n  }\n  \n  iter &lt;- iter + 1\n}\n\n#after convergence, print the result\n\nprint(pi)\n\n\n[1] 0.293\n\n\nSo what’s gone on here? We have 1000 observations of a binary variable; maximum likelihood is answering the question “what is the value of the parameter \\(\\pi\\) that makes the observed data most likely?” To get that answer, we have tried candidate values of \\(\\pi\\) to see which one maximizes the log-likelihood function.\nWe’ve maximized the function two ways - using a grid search process, and using the Newton-Raphson algorithm. The latter is more efficient and is the basis for most optimization algorithms in statistical software, and easy to adapt to estimate multiple parameters.\nHere’s one more approach also using the Newton-Raphson method. It calls the maxLik package to do the optimization instead of our having to write the algorithm ourselves.\n\n\ncode\nlibrary(maxLik)\n# Write the binomial log-likelihood function\nlog_likelihood &lt;- function(pi, y) {\n  sum(y * log(pi) + (1 - y) * log(1 - pi))\n}\n\n#maximize the function using maxLik\n\nm1m &lt;- maxLik(y=y, log_likelihood, start = c(0.5), method = \"NR\") #Newton-Raphson method, starting value of 0.5\n\nsummary(m1m)\n\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 3 iterations\nReturn code 2: successive function values within tolerance limit (tol)\nLog-Likelihood: -604.816 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(&gt; t)    \n[1,]  0.29300    0.01441   20.34  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\nAgain, the value of \\(\\pi\\) that maximizes the log-likelihood is the same as the sample frequency of \\(y\\), which is 0.293.\n\n\n\nLet’s make this more realistic in terms of resembling the models we want to run by adding an intercept term and multiple predictors to the model - the likelihood now has to account for those \\(X\\) variables and maximize with respect to them.\n\ngenerate some data on \\(y\\), and \\(X\\)\nwrite the likelihood\nwrite the Newton-Raphson algorithm\napply to the data\n\n\n\ncode\n# Generate binary y, and two X variables for regression; coefficients are -1, 0.5, -0.5\n\nset.seed(8675309)\nn &lt;- 10000\nX &lt;- matrix(rnorm(n * 2), ncol = 2)\ntrue_beta &lt;- c(-1, 0.5, -0.5)\nz &lt;- cbind(1, X) %*% true_beta\nprob &lt;- 1 / (1 + exp(-z))\ny &lt;- rbinom(n, 1, prob)\n\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson iterative algorithm\n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # Add intercept term to X if not already present\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#write the gradient function - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- plogis(X %*% beta)\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  k &lt;- ncol(X)\n  \n# set starting values for k columns of data (betas)\n  beta &lt;- rep(0, k) \n  \n# Store gradients and log-likelihoods for analysis\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n# iterate NR \n  for (iter in 1:max_iter) {\n    # compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # compute Hessian\n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # check for convergence\n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n    \n    # compute standard errors for this model by taking the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  }\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#html table comparing glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\", caption=\"Comparing GLM and Dave Estimates\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  column_spec(3, border_right = T) %&gt;%\n  column_spec(4, border_right = T) %&gt;%\n  collapse_rows(columns = 1:3, valign = \"top\") \n\n\n\n\nTable 1: Comparing GLM and Dave Estimates\n\n\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-0.9531826\n-0.9531826\n0.0236756\n0.0236748\n\n\nX1\n0.4938831\n0.4938831\n0.0238303\n0.0238295\n\n\nX2\n-0.4719703\n-0.4719703\n0.0240294\n0.0240286\n\n\n\n\n\n\n\n\n\n\n\nTable 1 compares the coefficients and standard errors from the Newton-Raphson algorithm to the coefficients and standard errors from the glm function. The estimates are the same, the standard errors very close.\nAlright, lots of fun here. Now, let’s use the program above to estimate a logit model using the democratic peace data.\n\n\n\nTable 2 compares the coefficients and standard errors from the program we wrote to those produced by the glm function. The regression here is a logit model predicting the onset of a militarized dispute as a function of the logged capabilities ratio, an indicator of whether the pair of states share a border, and the lowest democracy (polity) score in the pair.\n\n\ncode\ndp &lt;- read.csv(\"/users/dave/documents/teaching/606J-mle/2022/exercises/ex1/dp.csv\", header=TRUE)\n\n# for the log_likelihood function below, define y as the variable \"dispute\" in the dp data frame, and x1, x2, and x3 as the variables \"lncaprat\", \"border\", and \"deml\" in the dp data frame, respectively.\ndp$lncaprat &lt;- log(dp$caprat)\nX &lt;- as.matrix(dp[, c(\"lncaprat\", \"border\", \"deml\")])\ny &lt;- as.vector(dp$dispute)\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson \n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # check for constant; add one if necessary\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#write the gradient function -  partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- 1 / (1 + exp(-X %*% beta))\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  p &lt;- ncol(X)\n  \n  # set starting values for beta\n  beta &lt;- rep(0, p)\n  \n  # Store gradients and log-likelihoods\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n  for (iter in 1:max_iter) {\n    # Compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # Compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # Compute Hessian - 2nd partial derivatives \n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # Update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # Check for convergence - end if change in beta is less than tolerance value \n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n  }\n\n#compute standard errors for this model; take the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n# logit$coefficients\n# logit$st.errors\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#compare glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\", caption=\"Comparing GLM and Dave Estimates, Democratic Peace Model\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:3, valign = \"top\")\n\n\n\n\nTable 2: Comparing GLM and Dave Estimates, Democratic Peace Model\n\n\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-3.2407604\n-3.2407605\n0.1034544\n0.1034630\n\n\nXlncaprat\n-0.2028768\n-0.2028768\n0.0237847\n0.0237860\n\n\nXborder\n0.7827135\n0.7827135\n0.0852298\n0.0852337\n\n\nXdeml\n-0.0783467\n-0.0783467\n0.0068065\n0.0068078\n\n\n\n\n\n\n\n\n\n\n\nRecapping what we’ve done:\n\nWe wrote a log-likelihood function for a logistic regression model with multiple predictors.\nTo maximize the log-likelihood, we implemented a Newton-Raphson algorithm in the following steps:\n\nDefined the gradient and Hessian functions. The gradient is the vector of first partial derivatives of the log-likelihood, and the Hessian is the matrix of second partial derivatives.\nInitialized the beta coefficients and stored gradients and log-likelihoods.\nIterated the algorithm to update beta values.\nChecked for convergence based on the change in beta.\n\nOnce the model converged, we computed standard errors for the estimated coefficients by taking the square root of the main diagonal elements of the inverse negative Hessian.\n\n\n\n\nHere, you can see how the gradients change over each iteration.\n\n\ncode\n#plot gradient convergence\ngradients &lt;- do.call(cbind, logit$gradient_history)\ngradients &lt;- as.data.frame(gradients) %&gt;%\n  mutate(coef=c(\"Intercept\", \"lncaprat\", \"border\", \"deml\")) \n\ngradients &lt;- gradients %&gt;% \n  pivot_longer(-coef, names_to = \"iteration\", values_to = \"value\")\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(gradients, \"line\", hcaes(x=iteration, y=value, group=coef, color=coef)) %&gt;%\n  hc_title(text = \"Gradient Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Gradient\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\n\n\nAnd here, we can see how the log-likelihood changes over iterations:\n\n\ncode\n#plot ll convergence\n\nlls &lt;- as.data.frame(logit$ll_history) %&gt;%\n  mutate(iteration=1:nrow(.))\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(lls, \"line\", hcaes(x=iteration, y=`logit$ll_history`)) %&gt;%\n  hc_title(text = \"Log-Likelihood Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\n\n\nHere are two ways to recover the final log-likelihood from the glm model. The first uses the logLik function from the stats package. The second calculates the log-likelihood by hand - generating the predictions and plugging those into the log-likelihood function, then summing.\n\\[ LL = \\sum_{i=1}^{n} y_i \\ln(p) + (1-y_i) \\ln(1-p) \\]\n\n\ncode\n# use the logLik function from the stats package  \n\nllest &lt;- logLik(glm_fit)\n\n# or calculate it by hand - generate the predictions and plug those into the log-likelihood function, then sum.\n\npred &lt;- predict(glm_fit, type = \"response\")\nllestbyhand &lt;- sum(dp$dispute * log(pred) + (1 - dp$dispute) * log(1 - pred))\n\n\ndata.frame(llest, llestbyhand) %&gt;% \n  kable(\"html\", caption=\"Recovering the Log-Likelihood\" ) %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:2, valign = \"top\")\n\n\n\n\nRecovering the Log-Likelihood\n\n\nllest\nllestbyhand\n\n\n\n\n-3565.219\n-3565.219"
  },
  {
    "objectID": "llfmax24.html#motivating-likelihood",
    "href": "llfmax24.html#motivating-likelihood",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "Let’s recall the motivation for turning to ML. The data we have are not suitable for OLS because our observation of the data is limited. For instance, we observe a binary variable \\([0,1]\\) rather than the underlying continuous probability beneath it. If we could observe and measure that probability, we’d do so and perhaps use OLS to estimate a model. But since we only observe \\([0,1]\\), our observation is limited.\nOLS is poorly suited to model this variable especially if one of our goals is to generate predictions of that underlying probability. That is, one of our goals might be to estimate a measure of the \\(y\\) variable we wish we could measure directly.\nMLE is better suited to this task - it asks us what the data generating process is that produced the observed data, and to build a model appropriate to the limited observation of that \\(y\\) variable.\nTo do so, we need to:\n\ndescribe the observed distribution of \\(y\\)\nconsider what we wish we could measure - this points to the key quantities of interest we want to derive from the model.\ndescribe \\(y\\) according to a probability distribution; write that distribution\nwrite a log-likelihood function appropriate to that probability distribution.\nparameterize the log-likelihood function such that we have a link distribution to map the linear prediction, \\(x\\beta\\), onto the probability space of \\(y\\)."
  },
  {
    "objectID": "llfmax24.html#binary-y-variable",
    "href": "llfmax24.html#binary-y-variable",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "Let’s start with a \\(y\\) variable as follows:\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                y\n                Freq\n              \n        \n        \n        \n                \n                  0\n                  707\n                \n                \n                  1\n                  293\n                \n        \n      \n    \n\n\n\nThe variable, \\(y\\), takes on values of zero and one - it appears binomial; let’s write this in terms of the binomial distribution parameter \\(\\pi\\), so it takes on the value of one with probability \\(\\pi\\) and zero with probability \\(1-\\pi\\).\n\\[ y_i = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{} \\pi_{i}\\\\\n         0, & \\mbox{} 1-\\pi_{i}\n         \\end{array}\n     \\right.\\]\nThe likelihood of a single observation is:\n\\[ \\mathcal{L}(\\pi_i|y) = \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\]\nThis is a statement of the likelihood that any particular value of \\(\\pi\\) generated an observation. We want the likelihood any value of \\(\\pi\\) generated the entire dataset - that is, we want the joint likelihood of all the observations.\n\\[ \\mathcal{L}(\\pi|y) = \\prod_{i=1}^{n} \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\] recalling that a joint probability is the product of individual probabilities.\nLet’s take the log of this likelihood function - the natural log makes computation easier. Even for computers, this reduces the computational intensity especially with respect to extremely small decimals.\n\\[ \\ln \\mathcal{L}(\\pi|y) = \\sum_{i=1}^{n} y_i \\ln(\\pi_i) + (1-y_i) \\ln(1-\\pi_i) \\]\nWe can take this a step further and parameterize \\(\\pi\\) as a function of \\(X\\beta\\) where \\(X\\) is the matrix of predictors and \\(\\beta\\) is the vector of coefficients such that\n\\[ \\pi_i = F({X_i\\beta}) \\]\nSo our estimate of the binomial probability is a function of the linear predictor \\(X\\beta\\). We map \\(X\\beta\\) onto the probability space \\(\\pi\\) using a link function, \\(F\\). The most common link functions are the logistic (logit model), and the standard normal (probit model). Let’s write the logit link:\n\\[ \\pi_i = \\frac{1}{1 + e^{-(X_i\\beta)}} \\]\nand now let’s write this all in the log-likelihood function:\n\\[ \\ln \\mathcal{L}(\\beta|y) = \\sum_{i=1}^{n} y_i \\ln(\\frac{1}{1 + e^{-(X_i\\beta)}}) + (1-y_i) \\ln(1-\\frac{1}{1 + e^{-(X_i\\beta)}}) \\]\nThis is the logit log-likelihood function for a binary \\(y\\) variable."
  },
  {
    "objectID": "llfmax24.html#maximizing-the-likelihood---grid-search",
    "href": "llfmax24.html#maximizing-the-likelihood---grid-search",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "How do we solve this function for the data \\(y\\)? Put differently, what is the value of \\(\\pi\\) that most likely generated the data, \\(y\\)? There are a few ways to do this - one instructive method is to use a grid search, where we calculate the log-likelihood for a range of values of \\(\\pi\\) and find the value that maximizes the log-likelihood. We can repeat this to the desired level of specificity, progressively narrowing the grid. In this example, we’ll just do one search out to 3 decimal places. The steps are these:\n\ngenerate the range of candidate values of our parameter \\(\\pi\\) to plug into the log-likelihood function.\nplug each value into the log-likelihood function to compute the log-likelihood for that value.\nidentify which value of \\(\\pi\\) maximizes the log-likelihood.\n\nHere’s code to do this:\n\n\ncode\n# generate a vector of values of pi \n\npi_trials &lt;- seq(0, 1, by = 0.001)\n\n# write the log-likelihood function\n\nlog_likelihood &lt;- function(pi_trials, y) {\n  sum(y * log(pi_trials) + (1 - y) * log(1 - pi_trials))\n}\n\n# Calculate log-likelihood for each pi value\n\nll_values &lt;- sapply(pi_trials, log_likelihood, y = y)\n\n# Find the pi value that maximizes the log-likelihood\n\npi_hat &lt;- pi_trials[which.max(ll_values)]\nprint(pi_hat)\n\n\n[1] 0.293\n\n\nYou’ll note we recover the sample mean of \\(y\\). Let’s plot the log-likelihood against the values of \\(\\pi\\) to visualize the maximum.\n\n\ncode\n# Plot log-likelihood against pi\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(data.frame(pi = pi_trials, ll = ll_values), \"line\", hcaes(x = pi, y = ll)) %&gt;%\n  hc_title(text = \"Grid search - maximizing the log-likelihood\") %&gt;%\n  hc_xAxis(title = list(text = \"Pi\"), \n           plotLines = list(\n    list(color =\"red\", value = pi_hat ))) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_legend(enabled = FALSE) %&gt;%\n  hc_colors(bucolors)"
  },
  {
    "objectID": "llfmax24.html#optimization",
    "href": "llfmax24.html#optimization",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "Grid searches are not sufficient for multivariate models, especially where we also need measures of uncertainty. Here, we turn to numerical optimization.\nNumerical optimization is a field aimed at finding “best” outcomes or answers depending on some set of criteria - often, the “best” is the maximum or minimum of some function - that’s the case in maximum likelihood.\n\n\nThere are lots of ways to optimize a function - in statistical software, most of them derive from Newton’s iterative method. A very common application of this is the Newton-Raphson method. Newton-Raphson is an iterative process that starts with an initial guess for the parameter \\(\\pi\\) (or the vector of unknowns, \\(\\beta\\)) and updates it in the direction of the maximum until it converges to the maximum. At each step, it computes the first and second derivatives of the likelihood function, then uses these to update the parameter estimate(s). The update rule is:\n\\[ \\pi_{new} = \\pi_{old} - H(\\pi)^{-1}*g(\\pi) \\]\nSo the new estimate of the parameter is the old estimate minus the first derivative (gradient) of the log-likelihood function pre-multiplied by the Hessian (second derivative) of the log-likelihood function. Here, \\(g(\\pi)\\) is the gradient of the log-likelihood function and \\(H(\\pi)\\) is the Hessian of the log-likelihood function. The algorithm iterates this process until the change in \\(\\pi\\) is sufficiently small, below a specified threshold. Since \\(g\\) and \\(H\\) are matrices, you should see how this straightforwardly applied to a multivariate regression.\nLet’s write the Newton-Raphson algorithm ourselves - take a look at the code chunk below. You’ll see it produces the same result as the grid search; the sample frequency of \\(y\\).\n\n\n\n\n\ncode\n# use same data generated above; same LLF as above, aiming to recover pi\n\n#write the gradient - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(pi, y) {\n  sum(y / pi - (1 - y) / (1 - pi))\n}\n\n#write the Hessian - matrix of partial second derivatives of the log-likelihood function.\n\nhessian &lt;- function(pi, y) {\n  sum(-y / pi^2 - (1 - y) / (1 - pi)^2)\n}\n\n#declare a starting value for pi\n\npi &lt;- 0.5\n\n#set convergence criteria - when new pi is within 1e-6 of old pi, stop iterating\n\ntol &lt;- 1e-6\n\n#initialize iteration counter, set maximum iterations; how many times will we do this before we stop if we don't converge prior? \n\niter &lt;- 0\nmax_iter &lt;- 100\n\n#iterate the Newton-Raphson algorithm\n\nfor (iter in 1:max_iter) {\n  # Compute gradient  \n  grad &lt;- gradient(pi, y)\n  \n  # Compute Hessian\n  hess &lt;- hessian(pi, y)\n  \n  # Update pi\n  pi_new &lt;- pi - (grad / hess)\n  \n  pi &lt;- pi_new\n  \n  # Check for convergence\n  if (abs(pi_new - pi) &lt; tol) {\n    break\n  }\n  \n  iter &lt;- iter + 1\n}\n\n#after convergence, print the result\n\nprint(pi)\n\n\n[1] 0.293\n\n\nSo what’s gone on here? We have 1000 observations of a binary variable; maximum likelihood is answering the question “what is the value of the parameter \\(\\pi\\) that makes the observed data most likely?” To get that answer, we have tried candidate values of \\(\\pi\\) to see which one maximizes the log-likelihood function.\nWe’ve maximized the function two ways - using a grid search process, and using the Newton-Raphson algorithm. The latter is more efficient and is the basis for most optimization algorithms in statistical software, and easy to adapt to estimate multiple parameters.\nHere’s one more approach also using the Newton-Raphson method. It calls the maxLik package to do the optimization instead of our having to write the algorithm ourselves.\n\n\ncode\nlibrary(maxLik)\n# Write the binomial log-likelihood function\nlog_likelihood &lt;- function(pi, y) {\n  sum(y * log(pi) + (1 - y) * log(1 - pi))\n}\n\n#maximize the function using maxLik\n\nm1m &lt;- maxLik(y=y, log_likelihood, start = c(0.5), method = \"NR\") #Newton-Raphson method, starting value of 0.5\n\nsummary(m1m)\n\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 3 iterations\nReturn code 2: successive function values within tolerance limit (tol)\nLog-Likelihood: -604.816 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(&gt; t)    \n[1,]  0.29300    0.01441   20.34  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\nAgain, the value of \\(\\pi\\) that maximizes the log-likelihood is the same as the sample frequency of \\(y\\), which is 0.293.\n\n\n\nLet’s make this more realistic in terms of resembling the models we want to run by adding an intercept term and multiple predictors to the model - the likelihood now has to account for those \\(X\\) variables and maximize with respect to them.\n\ngenerate some data on \\(y\\), and \\(X\\)\nwrite the likelihood\nwrite the Newton-Raphson algorithm\napply to the data\n\n\n\ncode\n# Generate binary y, and two X variables for regression; coefficients are -1, 0.5, -0.5\n\nset.seed(8675309)\nn &lt;- 10000\nX &lt;- matrix(rnorm(n * 2), ncol = 2)\ntrue_beta &lt;- c(-1, 0.5, -0.5)\nz &lt;- cbind(1, X) %*% true_beta\nprob &lt;- 1 / (1 + exp(-z))\ny &lt;- rbinom(n, 1, prob)\n\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson iterative algorithm\n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # Add intercept term to X if not already present\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#write the gradient function - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- plogis(X %*% beta)\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  k &lt;- ncol(X)\n  \n# set starting values for k columns of data (betas)\n  beta &lt;- rep(0, k) \n  \n# Store gradients and log-likelihoods for analysis\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n# iterate NR \n  for (iter in 1:max_iter) {\n    # compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # compute Hessian\n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # check for convergence\n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n    \n    # compute standard errors for this model by taking the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  }\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#html table comparing glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\", caption=\"Comparing GLM and Dave Estimates\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  column_spec(3, border_right = T) %&gt;%\n  column_spec(4, border_right = T) %&gt;%\n  collapse_rows(columns = 1:3, valign = \"top\") \n\n\n\n\nTable 1: Comparing GLM and Dave Estimates\n\n\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-0.9531826\n-0.9531826\n0.0236756\n0.0236748\n\n\nX1\n0.4938831\n0.4938831\n0.0238303\n0.0238295\n\n\nX2\n-0.4719703\n-0.4719703\n0.0240294\n0.0240286\n\n\n\n\n\n\n\n\n\n\n\nTable 1 compares the coefficients and standard errors from the Newton-Raphson algorithm to the coefficients and standard errors from the glm function. The estimates are the same, the standard errors very close.\nAlright, lots of fun here. Now, let’s use the program above to estimate a logit model using the democratic peace data.\n\n\n\nTable 2 compares the coefficients and standard errors from the program we wrote to those produced by the glm function. The regression here is a logit model predicting the onset of a militarized dispute as a function of the logged capabilities ratio, an indicator of whether the pair of states share a border, and the lowest democracy (polity) score in the pair.\n\n\ncode\ndp &lt;- read.csv(\"/users/dave/documents/teaching/606J-mle/2022/exercises/ex1/dp.csv\", header=TRUE)\n\n# for the log_likelihood function below, define y as the variable \"dispute\" in the dp data frame, and x1, x2, and x3 as the variables \"lncaprat\", \"border\", and \"deml\" in the dp data frame, respectively.\ndp$lncaprat &lt;- log(dp$caprat)\nX &lt;- as.matrix(dp[, c(\"lncaprat\", \"border\", \"deml\")])\ny &lt;- as.vector(dp$dispute)\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson \n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # check for constant; add one if necessary\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#write the gradient function -  partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- 1 / (1 + exp(-X %*% beta))\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  p &lt;- ncol(X)\n  \n  # set starting values for beta\n  beta &lt;- rep(0, p)\n  \n  # Store gradients and log-likelihoods\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n  for (iter in 1:max_iter) {\n    # Compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # Compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # Compute Hessian - 2nd partial derivatives \n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # Update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # Check for convergence - end if change in beta is less than tolerance value \n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n  }\n\n#compute standard errors for this model; take the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n# logit$coefficients\n# logit$st.errors\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#compare glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\", caption=\"Comparing GLM and Dave Estimates, Democratic Peace Model\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:3, valign = \"top\")\n\n\n\n\nTable 2: Comparing GLM and Dave Estimates, Democratic Peace Model\n\n\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-3.2407604\n-3.2407605\n0.1034544\n0.1034630\n\n\nXlncaprat\n-0.2028768\n-0.2028768\n0.0237847\n0.0237860\n\n\nXborder\n0.7827135\n0.7827135\n0.0852298\n0.0852337\n\n\nXdeml\n-0.0783467\n-0.0783467\n0.0068065\n0.0068078\n\n\n\n\n\n\n\n\n\n\n\nRecapping what we’ve done:\n\nWe wrote a log-likelihood function for a logistic regression model with multiple predictors.\nTo maximize the log-likelihood, we implemented a Newton-Raphson algorithm in the following steps:\n\nDefined the gradient and Hessian functions. The gradient is the vector of first partial derivatives of the log-likelihood, and the Hessian is the matrix of second partial derivatives.\nInitialized the beta coefficients and stored gradients and log-likelihoods.\nIterated the algorithm to update beta values.\nChecked for convergence based on the change in beta.\n\nOnce the model converged, we computed standard errors for the estimated coefficients by taking the square root of the main diagonal elements of the inverse negative Hessian.\n\n\n\n\nHere, you can see how the gradients change over each iteration.\n\n\ncode\n#plot gradient convergence\ngradients &lt;- do.call(cbind, logit$gradient_history)\ngradients &lt;- as.data.frame(gradients) %&gt;%\n  mutate(coef=c(\"Intercept\", \"lncaprat\", \"border\", \"deml\")) \n\ngradients &lt;- gradients %&gt;% \n  pivot_longer(-coef, names_to = \"iteration\", values_to = \"value\")\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(gradients, \"line\", hcaes(x=iteration, y=value, group=coef, color=coef)) %&gt;%\n  hc_title(text = \"Gradient Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Gradient\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\n\n\nAnd here, we can see how the log-likelihood changes over iterations:\n\n\ncode\n#plot ll convergence\n\nlls &lt;- as.data.frame(logit$ll_history) %&gt;%\n  mutate(iteration=1:nrow(.))\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(lls, \"line\", hcaes(x=iteration, y=`logit$ll_history`)) %&gt;%\n  hc_title(text = \"Log-Likelihood Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\n\n\nHere are two ways to recover the final log-likelihood from the glm model. The first uses the logLik function from the stats package. The second calculates the log-likelihood by hand - generating the predictions and plugging those into the log-likelihood function, then summing.\n\\[ LL = \\sum_{i=1}^{n} y_i \\ln(p) + (1-y_i) \\ln(1-p) \\]\n\n\ncode\n# use the logLik function from the stats package  \n\nllest &lt;- logLik(glm_fit)\n\n# or calculate it by hand - generate the predictions and plug those into the log-likelihood function, then sum.\n\npred &lt;- predict(glm_fit, type = \"response\")\nllestbyhand &lt;- sum(dp$dispute * log(pred) + (1 - dp$dispute) * log(1 - pred))\n\n\ndata.frame(llest, llestbyhand) %&gt;% \n  kable(\"html\", caption=\"Recovering the Log-Likelihood\" ) %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:2, valign = \"top\")\n\n\n\n\nRecovering the Log-Likelihood\n\n\nllest\nllestbyhand\n\n\n\n\n-3565.219\n-3565.219"
  },
  {
    "objectID": "binaryextensions124.html",
    "href": "binaryextensions124.html",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "",
    "text": "symmetry - what are the implications of using symmetric links, especially given data on \\(y\\)?\nclassification\nmodel evaluation and fit\nrareness - what happens when there are few events?\n\nWe’re going to start with symmetry, then thinking about prediction and classification and fit."
  },
  {
    "objectID": "binaryextensions124.html#symmetry-and-asymmetry",
    "href": "binaryextensions124.html#symmetry-and-asymmetry",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Symmetry and Asymmetry",
    "text": "Symmetry and Asymmetry\nCompare three CDFs: the logistic, the clog-log, and a skewed logit function. The logistic is symmetric, the clog-log and skewed logit are not. You should notice the probability associated with x=0 for each function - the logistic is .5, the clog-log is about .64, and the skewed logit is about than .71.\nFor skewed binary \\(y\\) variables, it could be that one of these CDFs is a more appropriate link function than the symmetric logistic or normal. Implementing these merely requires substituting the appropriate CDF into the log-likelihood function.\n\n\ncode\n# Load required libraries\nlibrary(highcharter)\nlibrary(dplyr)\n\n# Binghamton University colors\nbinghamton_green &lt;- \"#005A43\"\nbinghamton_gray &lt;- \"#707070\"\nbinghamton_yellow &lt;- \"#FFC726\"\n\n# Generate data\nx &lt;- seq(-5, 5, length.out = 1000)\nlogistic_cdf &lt;- plogis(x)\ncloglog_cdf &lt;- 1 - exp(-exp(x))\n\n# Skewed logit function (shape parameter = 0.5)\n\nskewed_logit_cdf &lt;- 1 / (1 + exp(-x)) ^ 0.5 \n\n# Create data frame\ndf &lt;- data.frame(x = x, logistic = logistic_cdf, cloglog = cloglog_cdf, skewed_logit = skewed_logit_cdf)\n\n# Create the highchart\nhc &lt;- highchart() %&gt;%\n  hc_chart(type = \"line\") %&gt;%\n  hc_title(text = \"Comparison of CDFs: Logistic, Clog-log, and Skewed Logit\") %&gt;%\n  hc_xAxis(\n    title = list(text = \"x\"),\n    plotLines = list(\n      list(\n        color = \"#999\",\n        width = 2,\n        value = 0,\n        zIndex = 3,\n        label = list(text = \"x = 0\")\n      )\n    )\n  ) %&gt;%\n  hc_yAxis(\n    title = list(text = \"Cumulative Probability\"),\n    plotLines = list(\n      list(\n        color = \"#999\",\n        width = 2,\n        value = 0.5,\n        zIndex = 3,\n        label = list(text = \"y = 0.5\")\n      )\n    )\n  ) %&gt;%\n  hc_tooltip(\n    shared = TRUE,\n    formatter = JS(\"function() {\n      return 'x: ' + this.x.toFixed(4) + '&lt;br&gt;' +\n             'Logistic: ' + this.points[0].y.toFixed(4) + '&lt;br&gt;' +\n             'Clog-log: ' + this.points[1].y.toFixed(4) + '&lt;br&gt;' +\n             'Skewed Logit: ' + this.points[2].y.toFixed(4);\n    }\")\n  ) %&gt;%\n  hc_plotOptions(series = list(marker = list(enabled = FALSE))) %&gt;%\n  \n  # Add logistic CDF\n  hc_add_series(\n    data = df,\n    type = \"line\",\n    name = \"Logistic\",\n    color = binghamton_green,\n    hcaes(x = x, y = logistic)\n  ) %&gt;%\n  \n  # Add clog-log CDF\n  hc_add_series(\n    data = df,\n    type = \"line\",\n    name = \"Clog-log\",\n    color = binghamton_gray,\n    hcaes(x = x, y = cloglog)\n  ) %&gt;%\n  \n  # Add skewed logit CDF\n  hc_add_series(\n    data = df,\n    type = \"line\",\n    name = \"Skewed Logit\",\n    color = binghamton_yellow,\n    hcaes(x = x, y = skewed_logit)\n  )\n\n# Display the chart\nhc"
  },
  {
    "objectID": "binaryextensions124.html#skewed-logit",
    "href": "binaryextensions124.html#skewed-logit",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Skewed Logit",
    "text": "Skewed Logit\nNagler (1994) proposes the skewed logit (scobit) model - it’s a binary response model, the usual LLF, with a different link (the Burr-10):\n\\[ Pr(y=1) = \\frac{1}{(1+e^{-x\\beta})^\\alpha}\\]\nNote that if \\(\\alpha=1\\) this is the logistic CDF. If it is less than 1, the fastest rate of change is at \\(Pr(y =1 &lt; .5)\\); when greater than 1, the fastest rate of change, is at \\(Pr(y=1 &gt; .5)\\)\nNagler’s logic is that symmetric links require the assumption that individuals in the model are most sensitive to the effects of the \\(X\\) variables at or around \\(Pr(y=1) = .5\\). Looking at the (symmetric) logit curve above, you can see that’s where the derivative with respect to changes in \\(x\\) is greatest. If \\(y\\) is about half ones, half zeros, this may make sense - but often, we have \\(y\\) variables that are not symmetrically distributed like this. It makes sense in such cases not to assume the fastest rate of change, and the transition point from zero to one, is at \\(Pr(y=1) = .5\\).\nThe scobit model allows us to estimate the \\(\\alpha\\) parameter, which tells us where the fastest rate of change is in the CDF - that is, the transition point is an empirical question, not an assumption.\nThe model appears rarely in the political science literature; a Google Scholar search indicates most of its use is transportation analysis. A cursory survey also indicates the scobit estimates are often not that different from logit estimates. Estimation sometimes is funky insofar as we cannot always tell if changes in the likelihood are due to changes in \\(\\beta\\) or in \\(\\alpha\\)."
  },
  {
    "objectID": "binaryextensions124.html#skewed-logit-cdfs",
    "href": "binaryextensions124.html#skewed-logit-cdfs",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Skewed Logit CDFs",
    "text": "Skewed Logit CDFs\n\\includegraphics&lt;1&gt;[scale=.80]{/Users/dave/Documents/teaching/606J-mle/2022/slides/L3_binaryextensions/scobit22.pdf}"
  },
  {
    "objectID": "binaryextensions124.html#symmetry-1",
    "href": "binaryextensions124.html#symmetry-1",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Symmetry",
    "text": "Symmetry\nThe big point here is not that we should or should not use the scobit, but that we need to be very aware that the assumption in models with symmetric links is that the biggest effect of an \\(x\\) variable is at \\(Pr(y=1) = 0.5\\) which is where \\(x\\beta=0\\)."
  },
  {
    "objectID": "binaryextensions124.html#why-does-symmetry-matter",
    "href": "binaryextensions124.html#why-does-symmetry-matter",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Why does symmetry matter?",
    "text": "Why does symmetry matter?\n\nsymmetry determines where the greatest effect of \\(x\\) is.\nsymmetry ensures rates of change above and below \\(x=0\\) are the same as they approach the limits.\nsymmetry implies the theoretical threshold, \\(\\tau\\), separating observed zeros and ones is \\(\\tau=0.5\\).\nif we want to use the model to generate predicted values of \\(y\\) (rather than \\(y^*\\)), we need some threshold for classification.\n\nSome (very few) questions naturally link to a clear threshold like 0.5 …election outcomes? But …are we measuring the correct outcome variable?"
  },
  {
    "objectID": "binaryextensions124.html#confusion-matrix",
    "href": "binaryextensions124.html#confusion-matrix",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nThe “confusion matrix” (I didn’t make this up) illustrates that intersection and identifies where our classification is “confused.”\n\n\n\n\ncode\nlibrary(kableExtra)\n# opts &lt;- options(knitr.kable.NA = \"\")\ndf &lt;- data.frame(\"Predicted Positive\" = c(\"True Positive\", \"False Positive\"), \"Predicted Negative\" = c(\"False Positive\", \"True Negative\"), \"Rate\" = c(\"TPR=TP/P\", \"FPR=FP/N\"))\nrow.names(df) &lt;- c(\"Observed Positive\", \"Observed Negative\")\n\ntbl &lt;- kbl(df, align = rep('c', 4)) |&gt; \n  column_spec(2:4, border_left = T) |&gt; \n  column_spec(1,  bold=T) |&gt;\n  column_spec(1:4, width = \"2cm\", color = 'white', background = '#005A43', include_thead = TRUE) |&gt;\n  column_spec(2:4, background = \"inherit\", color=\"inherit\") |&gt;\n  row_spec(1, extra_css = \"border-bottom: 1px solid\") |&gt;\n  kable_paper(\"hover\", bootstrap_options = c(\"condensed\", \"responsive\"), full_width = F, font_size = 20)\ntbl\n\n\n\n\n\n\nPredicted.Positive\nPredicted.Negative\nRate\n\n\n\n\nObserved Positive\nTrue Positive\nFalse Positive\nTPR=TP/P\n\n\nObserved Negative\nFalse Positive\nTrue Negative\nFPR=FP/N\n\n\n\n\n\n\n\n\n\n\n\nTrue Positive Rate: correctly classify positive outcomes. This is often called “sensitivity.”\nFalse Positive Rate: we incorrectly classify negative outcomes (\\(y=0\\)) as positive (\\(y=1\\)). This is often called “1-specificity.” Specificity is the True Negative Rate, or the probability of correctly classifying a negative outcome (\\(y=0\\))."
  },
  {
    "objectID": "binaryextensions124.html#using-the-confusion-matrix-to-measure-model-fit",
    "href": "binaryextensions124.html#using-the-confusion-matrix-to-measure-model-fit",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Using the Confusion Matrix to Measure Model Fit",
    "text": "Using the Confusion Matrix to Measure Model Fit\nSo here’s the deal:\n\nestimate the model.\ngenerate the predicted probability \\(y=1\\) for each observation.\nassume a threshold separating zeros and ones; usually \\(\\tau=0.5\\).\nif \\(Pr(y=1 \\geq 0.5)\\), predict a positive outcome (predict \\(y=1\\)).\nif \\(Pr(y=0 &lt; 0.5)\\), predict a negative outcome (predict \\(y=0\\)).\nusing the observed and predicted outcomes, generate a confusion table, and compute measures of fit like “percent correctly predicted” (PCP) and “proportional reduction of error” (PRE).\n\n\nPercent Correctly Predicted (PCP)\nOne thing we can do is sum the main diagonal and divide by the estimation sample: \\((TP+TN)/N\\). This gives us the Percent Correctly Predicted (PCP).\n\n\nProportional Reduction of Error (PRE)}\nA second thing is to compute the Proportional Reduction of Error (PRE) - the difference between the correct predictions of the model versus a null model usually the unconditional frequency of \\(y\\) - in other words, we use the Percent in the Modal Category (PCM) of the \\(y\\) variable.\n\\[\\frac{correct_{\\text{informed}} - correct_{\\text{null}}}{N - correct_{\\text{null}}} \\]\nor\n\\[\\frac{PCP- PMC}{1-PMC} \\]\n\n\nExample: NAFTA vote, 1993\nSo here’s a model predicting members of the US House votes for or against the NAFTA treaty in 1993: 1 is a “yes” vote; the \\(x\\) variables are party (Democrat) and an ideology score.\n\n\ncode\nnafta &lt;- read.csv(\"NAFTA.csv\")\n\nnaftamodel &lt;- glm(vote ~ democrat + cope93, data=nafta, family=binomial(link=\"logit\"))\n\nstargazer(naftamodel, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: NAFTA vote\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Party (Democrat=1)\", \"Ideology (COPE score)\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: NAFTA vote\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParty (Democrat=1)\n\n\n1.990*** (0.504)\n\n\n\n\nIdeology (COPE score)\n\n\n-0.054*** (0.007)\n\n\n\n\nConstant\n\n\n2.380*** (0.272)\n\n\n\n\n\n\n\n\nObservations\n\n\n434\n\n\n\n\nAkaike Inf. Crit.\n\n\n465.904\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\nAnd here’s the confusion matrix from that model assuming a threshold of \\(\\tau=.5\\) - it compares the observed vote frequency against how we classify our predictions based on a predicted probability greater than or less than 0.5. This is generated using the caret package in R.\n\n\ncode\n# Load required libraries\nlibrary(caret)\n\n# Assuming you have a fitted GLM object called 'glm_model'\n# and test data 'test_data' with actual outcomes in 'test_data$actual'\n\ntest_data &lt;- nafta %&gt;% dplyr::select(vote, democrat, cope93) %&gt;% mutate(actual = vote)\n# Make predictions on the test data\npredictions &lt;- predict(naftamodel, newdata = test_data, type = \"response\")\n\n# Create confusion matrix\n# You may need to adjust the threshold (default is 0.5)\npredicted_classes &lt;- ifelse(predictions &gt; 0.5, 1, 0)\nconf_matrix &lt;- confusionMatrix(factor(predicted_classes), factor(test_data$actual))\n\n# Print confusion matrix\nprint(conf_matrix)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 160  77\n         1  40 157\n                                         \n               Accuracy : 0.7304         \n                 95% CI : (0.686, 0.7716)\n    No Information Rate : 0.5392         \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      \n                                         \n                  Kappa : 0.4647         \n                                         \n Mcnemar's Test P-Value : 0.0008741      \n                                         \n            Sensitivity : 0.8000         \n            Specificity : 0.6709         \n         Pos Pred Value : 0.6751         \n         Neg Pred Value : 0.7970         \n             Prevalence : 0.4608         \n         Detection Rate : 0.3687         \n   Detection Prevalence : 0.5461         \n      Balanced Accuracy : 0.7355         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\nYou should see the main diagonal presents the number of correct predictions - the off-diagonal elements are the incorrect predictions. If we sum the main diagonal and divide by \\(N\\), we get the Percent Correctly Predicted (PCP). In this case, the PCP is 0.735 - 73.5% of the votes are correctly predicted.\nThis all depends on the threshold (.5) - in the case of a Congressional vote, especially a relatively close vote like this one, the threshold might not be crazy. But it might be in other cases, and arbitrarily choosing a value for \\(\\tau\\) is problematic. So another approach is to compute the ROC curve.\nWhat makes this work relatively well in the NAFTA context? As you’ll see below, it works less well in the democratic peace models.\n\n\nExample: Democratic Peace\nHere’s a basic democratic peace model:\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\ndpm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n\nstargazer(dpm1, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\nLet’s generate the confusion matrix for the democratic peace model, threshold set at \\(\\tau =0.5\\):\n\n\ncode\nlibrary(kableExtra)\n# opts &lt;- options(knitr.kable.NA = \"\")\ndf &lt;- data.frame(\"Predicted Positive\" = c(\"0\", \"0\"), \"Predicted Negative\" = c(\"897\", \"19,245\"), \"Rate\" = c(\"TPR=0/897=0\", \"FPR=0/19,245=0\"))\nrow.names(df) &lt;- c(\"Observed Positive\", \"Observed Negative\")\n\ntbl &lt;- kbl(df, align = rep('c', 4)) |&gt; \n  column_spec(2:4, border_left = T) |&gt; \n  column_spec(1,  bold=T) |&gt;\n  column_spec(1:4, width = \"2cm\", color = 'white', background = '#005A43', include_thead = TRUE) |&gt;\n  column_spec(2:4, background = \"inherit\", color=\"inherit\") |&gt;\n  row_spec(1, extra_css = \"border-bottom: 1px solid\") |&gt;\n  kable_paper(\"hover\", bootstrap_options = c(\"condensed\", \"responsive\"), full_width = F, font_size = 20)\ntbl\n\n\n\n\n\n\nPredicted.Positive\nPredicted.Negative\nRate\n\n\n\n\nObserved Positive\n0\n897\nTPR=0/897=0\n\n\nObserved Negative\n0\n19,245\nFPR=0/19,245=0\n\n\n\n\n\n\n\nThe problem is \\(\\tau\\); at \\(\\tau=.5\\) we get none of the ones correct. Here’s the democratic peace at \\(\\tau = 0.1\\):\n\n\ncode\nlibrary(kableExtra)\n# opts &lt;- options(knitr.kable.NA = \"\")\ndf &lt;- data.frame(\"Predicted Positive\" = c(\"303\", \"1,764\"), \"Predicted Negative\" = c(\"594\", \"17,481\"), \"Rate\" = c(\"TPR=303/897=.34\", \"FPR=1,764/19,245=0.092\"))\nrow.names(df) &lt;- c(\"Observed Positive\", \"Observed Negative\")\n\ntbl &lt;- kbl(df, align = rep('c', 4)) |&gt; \n  column_spec(2:4, border_left = T) |&gt; \n  column_spec(1,  bold=T) |&gt;\n  column_spec(1:4, width = \"2cm\", color = 'white', background = '#005A43', include_thead = TRUE) |&gt;\n  column_spec(2:4, background = \"inherit\", color=\"inherit\") |&gt;\n  row_spec(1, extra_css = \"border-bottom: 1px solid\") |&gt;\n  kable_paper(\"hover\", bootstrap_options = c(\"condensed\", \"responsive\"), full_width = F, font_size = 20)\ntbl\n\n\n\n\n\n\nPredicted.Positive\nPredicted.Negative\nRate\n\n\n\n\nObserved Positive\n303\n594\nTPR=303/897=.34\n\n\nObserved Negative\n1,764\n17,481\nFPR=1,764/19,245=0.092\n\n\n\n\n\n\n\nAt \\(\\tau = 0.1\\), we get a TPR = 0.34 and FPR = 0.092. The PCP = (303+17481)/20142 = 0.88 …which is lower, but we get about 1/3 of the ones correct."
  },
  {
    "objectID": "binaryextensions124.html#percent-correctly-predicted-pcp",
    "href": "binaryextensions124.html#percent-correctly-predicted-pcp",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Percent Correctly Predicted (PCP)",
    "text": "Percent Correctly Predicted (PCP)\nOne thing we can do is sum the main diagonal and divide by the estimation sample: \\((TP+TN)/N\\). This gives us the Percent Correctly Predicted (PCP)."
  },
  {
    "objectID": "binaryextensions124.html#proportional-reduction-of-error-pre",
    "href": "binaryextensions124.html#proportional-reduction-of-error-pre",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Proportional Reduction of Error (PRE)}",
    "text": "Proportional Reduction of Error (PRE)}\nA second thing is to compute the Proportional Reduction of Error (PRE) - the difference between the correct predictions of the model versus a null model usually the unconditional frequency of \\(y\\) - in other words, we use the Percent in the Modal Category (PCM).\n\\[\\frac{correct_{\\text{informed}} - correct_{\\text{null}}}{N - correct_{\\text{null}}} \\]\nor\n\\[\\frac{PCP- PMC}{1-PMC} \\]"
  },
  {
    "objectID": "binaryextensions124.html#example-democratic-peace",
    "href": "binaryextensions124.html#example-democratic-peace",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Example: Democratic Peace",
    "text": "Example: Democratic Peace\nHere’s a basic democratic peace model:\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\ndpm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n\nstargazer(dpm1, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\nLet’s generate the confusion matrix for the democratic peace model, threshold set at \\(\\tau =0.5\\):\n\n\ncode\nlibrary(kableExtra)\n# opts &lt;- options(knitr.kable.NA = \"\")\ndf &lt;- data.frame(\"Predicted Positive\" = c(\"0\", \"0\"), \"Predicted Negative\" = c(\"897\", \"19,245\"), \"Rate\" = c(\"TPR=0/897=0\", \"FPR=0/19,245=0\"))\nrow.names(df) &lt;- c(\"Observed Positive\", \"Observed Negative\")\n\ntbl &lt;- kbl(df, align = rep('c', 4)) |&gt; \n  column_spec(2:4, border_left = T) |&gt; \n  column_spec(1,  bold=T) |&gt;\n  column_spec(1:4, width = \"2cm\", color = 'white', background = '#005A43', include_thead = TRUE) |&gt;\n  column_spec(2:4, background = \"inherit\", color=\"inherit\") |&gt;\n  row_spec(1, extra_css = \"border-bottom: 1px solid\") |&gt;\n  kable_paper(\"hover\", bootstrap_options = c(\"condensed\", \"responsive\"), full_width = F, font_size = 20)\ntbl\n\n\n\n\n\n\nPredicted.Positive\nPredicted.Negative\nRate\n\n\n\n\nObserved Positive\n0\n897\nTPR=0/897=0\n\n\nObserved Negative\n0\n19,245\nFPR=0/19,245=0\n\n\n\n\n\n\n\nThe problem is \\(\\tau\\); at \\(\\tau=.5\\) we get none of the ones correct. Here’s the democratic peace at \\(\\tau = 0.1\\):\n\n\ncode\nlibrary(kableExtra)\n# opts &lt;- options(knitr.kable.NA = \"\")\ndf &lt;- data.frame(\"Predicted Positive\" = c(\"303\", \"1,764\"), \"Predicted Negative\" = c(\"594\", \"17,481\"), \"Rate\" = c(\"TPR=303/897=.34\", \"FPR=1,764/19,245=0.092\"))\nrow.names(df) &lt;- c(\"Observed Positive\", \"Observed Negative\")\n\ntbl &lt;- kbl(df, align = rep('c', 4)) |&gt; \n  column_spec(2:4, border_left = T) |&gt; \n  column_spec(1,  bold=T) |&gt;\n  column_spec(1:4, width = \"2cm\", color = 'white', background = '#005A43', include_thead = TRUE) |&gt;\n  column_spec(2:4, background = \"inherit\", color=\"inherit\") |&gt;\n  row_spec(1, extra_css = \"border-bottom: 1px solid\") |&gt;\n  kable_paper(\"hover\", bootstrap_options = c(\"condensed\", \"responsive\"), full_width = F, font_size = 20)\ntbl\n\n\n\n\n\n\nPredicted.Positive\nPredicted.Negative\nRate\n\n\n\n\nObserved Positive\n303\n594\nTPR=303/897=.34\n\n\nObserved Negative\n1,764\n17,481\nFPR=1,764/19,245=0.092\n\n\n\n\n\n\n\nAt \\(\\tau = 0.1\\), we get a TPR = 0.34 and FPR = 0.092. The PCP = (303+17481)/20142 = 0.88 …which is lower, but we get about 1/3 of the ones correct."
  },
  {
    "objectID": "binaryextensions124.html#receiver-operator-characteristic-roc-curves",
    "href": "binaryextensions124.html#receiver-operator-characteristic-roc-curves",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Receiver-Operator Characteristic (ROC) Curves",
    "text": "Receiver-Operator Characteristic (ROC) Curves\nThe problem is choosing the threshold - imagine that we compute a confusion matrix for all possible thresholds, \\(\\tau=.01, .02, .03 \\ldots 1\\), then compute TPR and FPR, and plot them against one another. This is an ROC curve.\nROCs originate in efforts to distinguish signal from noise in radar returns - the British built a radar system before WWII to detect German air attacks; they had the problem of distinguishing planes (signal) from flocks of geese (noise). As the turned up the sensitivity of the radar, they more often correctly detected planes, but they also lacked specificity and detected more geese too. So there was a tradeoff between sensitivity (correctly identifying positive signals as positive) and specificity (incorrectly identify negative signals as positive).\nROCs measure these two dimensions and graph them against one another:\n\nsensitivity - true positive rate at every possible latent threshold between zero and one.\n1- specificity - false positive rate at every possible latent threshold between zero and one. This is 1 minus the True Negative Rate\n\nHere’s the ROC for the NAFTA model - we’ll use the pROC package in R to compute the ROC and plot it:\n\n\ncode\nlibrary(pROC)\n\n# NAFTA\n\nnaftaroc &lt;- roc(test_data$actual, predictions, plot=TRUE, grid=TRUE, partial.auc.correct=TRUE,\n         print.auc=TRUE)"
  },
  {
    "objectID": "binaryextensions124.html#roc-intepretation",
    "href": "binaryextensions124.html#roc-intepretation",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "ROC Intepretation",
    "text": "ROC Intepretation\n\nthe diagonal is a model guessing randomly zero or one - no better than a coin toss.\nabove that line, the model is improving our classification over random guesses.\nthe top left corner would indicate a model that classifies perfectly - 100% sensitivity (TPR), and 0% FPR.\nbelow the diagonal line, the model is classifying worse than a coin toss would.\nwe can compute the Area Under the Curve (AUC) as a percentage - AUC is often reported to indicate model fit. In the NAFTA model, the AUC is .843. AUC closer to one indicates better fit; closer to .5 indicates worse fit, similar to random guessing.\nwe can plot ROCs from different models on the same space and compare their fits.\nThe x-axis is 1-specificity, or the False Positive Rate."
  },
  {
    "objectID": "binaryextensions124.html#correctly-predicted",
    "href": "binaryextensions124.html#correctly-predicted",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Correctly Predicted",
    "text": "Correctly Predicted"
  },
  {
    "objectID": "binaryextensions124.html#roc-democratic-peace",
    "href": "binaryextensions124.html#roc-democratic-peace",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "ROC Democratic Peace",
    "text": "ROC Democratic Peace\nHere’s we compare fit for two models, one including “borders,” the other excluding it. Here are the two models :\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\ndpm1 &lt;-glm(dispute ~ border+deml+caprat+ally+border, family=binomial(link=\"logit\"), data=dp )\ndpm2 &lt;-glm(dispute ~ deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n  \nstargazer(list(dpm1,dpm2), type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n-0.078*** (0.007)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n-0.005*** (0.0005)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n-0.374*** (0.076)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n-2.979*** (0.064)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n7,262.635\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\nAnd compute the ROC for each model - here Claude.ai and I have written a function to compute the ROC and AUC for each model, and then plot them on the same space.\n\n\ncode\n# part written by Claude.ai\n# compute ROC curve\ncompute_roc &lt;- function(actual, predictions) {\n  # Sort actual and predictions in descending order of predicted probabilities\n  sorted_data &lt;- data.frame(actual, predictions)\n  sorted_data &lt;- sorted_data[order(sorted_data$predictions, decreasing = TRUE), ]\n\n  # Initialize variables\n  n_positive &lt;- sum(actual)\n  n_negative &lt;- length(actual) - n_positive\n  tp &lt;- 0\n  fp &lt;- 0\n  tpr &lt;- c()\n  fpr &lt;- c()\n\n  # Iterate over sorted data\n  for (i in 1:nrow(sorted_data)) {\n    if (sorted_data$actual[i] == 1) {\n      tp &lt;- tp + 1\n    } else {\n      fp &lt;- fp + 1\n    }\n\n    # Calculate true positive rate (TPR) and false positive rate (FPR)\n    tpr &lt;- c(tpr, tp / n_positive)\n    fpr &lt;- c(fpr, fp / n_negative)\n  }\n\n  # Create ROC curve\n  roc_curve &lt;- data.frame(fpr, tpr)\n  return(roc_curve)\n}\n\n# Function to compute AUC\ncompute_auc &lt;- function(fpr, tpr) {\n  # Sort FPR and TPR\n  ord &lt;- order(fpr)\n  fpr &lt;- fpr[ord]\n  tpr &lt;- tpr[ord]\n  \n  # Compute AUC using trapezoidal rule\n  auc &lt;- sum((fpr[-1] - fpr[-length(fpr)]) * (tpr[-1] + tpr[-length(tpr)]) / 2)\n  return(auc)\n}\n\n# Democratic Peace models\n\ntest_dataB &lt;- dp %&gt;% dplyr::select(border,deml,caprat,ally) %&gt;% \n  mutate(actual = dp$dispute)\n\ntest_dataNB &lt;- dp %&gt;% dplyr::select(deml,caprat,ally) %&gt;% \n  mutate(actual = dp$dispute)\n\n# Make predictions on the test data\npredictionsB &lt;- predict(dpm1, newdata = test_dataB, type = \"response\")\npredictionsNB &lt;- predict(dpm2, newdata = test_dataNB, type = \"response\")\n\n\nroc_curveB &lt;- compute_roc(test_dataB$actual, predictionsB)\n#AUC\nauc_border &lt;- compute_auc(roc_curveB$fpr, roc_curveB$tpr)\nroc_curveNB &lt;- compute_roc(test_dataNB$actual, predictionsNB)\nauc_noborder &lt;- compute_auc(roc_curveNB$fpr, roc_curveNB$tpr)\n\n# ROC Plot, pasting auc_border and auc_noborder on plot\n\nggplot() +\n  geom_line(data = roc_curveB, aes(x = fpr, y = tpr, color = \"Borders\")) +\n  geom_line(data = roc_curveNB, aes(x = fpr, y = tpr, color = \"No Borders\")) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"solid\", color = \"red\") +\n  labs(title = \"ROC Curve: Democratic Peace\", x = \"False Positive Rate\", y = \"True Positive Rate\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"Borders\" = \"#005A43\", \"No Borders\" = \"#6CC24A\"))+\n  annotate(\"text\", x = 0.75, y = 0.5, label = paste(\"AUC Model 1: \", round(auc_border, 2)), color = \"#005A43\") +\n  annotate(\"text\", x = 0.75, y = 0.4, label = paste(\"AUC Model 2: \", round(auc_noborder, 2)), color = \"#6CC24A\")\n\n\n\n\n\n\n\n\n\ncode\n#bucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nAlso, note the measure Area Under the Curve (AUC) for each model - the AUC is often reported to indicate model fit. The AUC for the model including borders is 0.75, while the AUC for the model excluding borders is 0.72. A model with an AUC of 0.5 is no better than a coin toss, while a model with an AUC of 1 is perfect."
  },
  {
    "objectID": "binaryextensions124.html#single-coefficient-estimates",
    "href": "binaryextensions124.html#single-coefficient-estimates",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Single Coefficient Estimates",
    "text": "Single Coefficient Estimates\nOne property of MLEs is they are asymptotically multivariate normal; inference is straightforward because the variances are also normal so the ratio of \\(\\beta /se\\) is a z-score."
  },
  {
    "objectID": "binaryextensions124.html#model-evaluation",
    "href": "binaryextensions124.html#model-evaluation",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nMost commonly, we evaluate model fit using one of the “trinity” of tests:\n\nlog-likelihood ratio tests (LLR)\nWald tests\nLagrange Multiplier tests (LM)\n\nThe first two are the most common, and it’s not clear one is better than the other.\n\nLog-Likelihood Ratio Test\nThe LLR test requires estimating two models - a null or constrained model, (\\(M_0\\)), and informed (unconstrained) model (\\(M_1\\)) - it compares the heights of the log-likelihood functions of the two models:\n\\[ \\chi^2 = -2 (ln\\mathcal{L}(M_0) - ln\\mathcal{L}(M_1))  \\]\nThe log-likelihoods here are literally the values of the \\(ln\\mathcal{L}\\) at the estimated maxima of the functions. Their difference is distributed \\(\\chi^2\\) with \\(k_1-k_0\\) degrees of freedom.\nThe LLR is simple to compute (you can do it in your head), but requires estimating two nested models. Recall, two models are nested iff the regressors in the constrained model are a strict subset of those in the unconstrained model, and the samples are identical.\n\n\nWald \\(\\chi^2\\)\nThe Wald test is similar, but only requires the unconstrained or informed model. During maximization, it examines the distance between \\(M_0\\) and \\(M_1\\), and weights that distance by the rate of change between the two (second derivative). If the distance is large and the rate of change is fast, the informed model improves a good bit on the uninformed one. You can imagine other combinations of distance and curvature. Long (p. 88) has a great illustration of this.\n\n\nIn Practice\nThese two tests are asymptotically equivalent. In practice, it makes little difference in most cases which you choose, provided the models are nested.\nStata reports LLR for most models, but reports Wald if you request robust standard errors.\n\n\nLimits\nThe limits of these tests is they apply only to nested models - models where the regressors in the constrained model are a strict subset of those in the unconstrained model and where the samples are identical.\n\n\nInformation Criterion Tests\nAlternatively, we can use information criterion tests - the two most common are the Akaike and Bayesian Information Criteria tests (AIC, BIC). These are both formulated to penalize likelihoods for the number of parameters estimated; this in effect rewards better specification (good variables, but few) and penalizes “garbage can” approaches (including lots of poor predictors).\nIC tests are useful for either nested or nonnested models. This is a significant though under-appreciated virtue of such tests.\n\n\nAkaike and Bayesian Information Criterion tests\n\\[AIC =  -2 ln(\\mathcal{L}) + 2k \\]\n\\[BIC =  -2 ln(\\mathcal{L}) + ln(N) k \\]\nwhere \\(k\\) is the number of parameters.\n\nProcess\nAIC: Estimate model 1; generate the AIC. Estimate model 2; generate the AIC. The model with the smaller AIC is the preferred model (see Long 1997: 110).\nBIC: Estimate model 1; generate the BIC. Estimate model 2; generate the BIC. Compute \\(BIC_1 - BIC_2\\) - the smaller BIC value is the preferable model. The strength of the test statistic is given by Rafferty (1996): absolute value of this difference 0-2 = weak; 2-6= Positive; 6-10= Strong; greater than 10 = Very Strong (see Long 1997: 112)."
  },
  {
    "objectID": "binaryextensions124.html#rare-events-logit",
    "href": "binaryextensions124.html#rare-events-logit",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Rare Events Logit",
    "text": "Rare Events Logit\nWhat they propose is:\n\nSelect all the cases with events (failures).\nRandomly choose a sample of the non-event (censored) cases (they say 2-5 times the size of the failure group).\nThis smaller sample makes data collection possible (compared to the gigantic number of zeros in some event data).\nEstimate a logit on the new, smaller sample, and adjust the estimates for the sample.\n\nThe Rare Events Logit doesn’t appear in the literature much, though it’s not uncommon for reviewers to ask for it. In my experience inferences from this model don’t vary much from the usual logit. The model does present a major opportunity for data collection efforts."
  },
  {
    "objectID": "binaryextensions124.html#example-nafta-vote-1993",
    "href": "binaryextensions124.html#example-nafta-vote-1993",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Example: NAFTA vote, 1993",
    "text": "Example: NAFTA vote, 1993\nSo here’s a model predicting members of the US House votes for or against the NAFTA treaty in 1993: 1 is a “yes” vote; the \\(x\\) variables are party (Democrat) and an ideology score.\n\n\ncode\nnafta &lt;- read.csv(\"NAFTA.csv\")\n\nnaftamodel &lt;- glm(vote ~ democrat + cope93, data=nafta, family=binomial(link=\"logit\"))\n\nstargazer(naftamodel, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: NAFTA vote\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Party (Democrat=1)\", \"Ideology (COPE score)\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: NAFTA vote\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParty (Democrat=1)\n\n\n1.990*** (0.504)\n\n\n\n\nIdeology (COPE score)\n\n\n-0.054*** (0.007)\n\n\n\n\nConstant\n\n\n2.380*** (0.272)\n\n\n\n\n\n\n\n\nObservations\n\n\n434\n\n\n\n\nAkaike Inf. Crit.\n\n\n465.904\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\nAnd here’s the confusion matrix from that model - it compares the observed vote frequency against how we classify our predictions based on a predicted probability greater than or less than 0.5.\n\n\ncode\n# Load required libraries\nlibrary(pROC)\nlibrary(caret)\n\n# Assuming you have a fitted GLM object called 'glm_model'\n# and test data 'test_data' with actual outcomes in 'test_data$actual'\n\ntest_data &lt;- nafta %&gt;% dplyr::select(vote, democrat, cope93) %&gt;% mutate(actual = vote)\n# Make predictions on the test data\npredictions &lt;- predict(naftamodel, newdata = test_data, type = \"response\")\n\n# Create confusion matrix\n# You may need to adjust the threshold (default is 0.5)\npredicted_classes &lt;- ifelse(predictions &gt; 0.5, 1, 0)\nconf_matrix &lt;- confusionMatrix(factor(predicted_classes), factor(test_data$actual))\n\n# Print confusion matrix\nprint(conf_matrix)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 160  77\n         1  40 157\n                                         \n               Accuracy : 0.7304         \n                 95% CI : (0.686, 0.7716)\n    No Information Rate : 0.5392         \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      \n                                         \n                  Kappa : 0.4647         \n                                         \n Mcnemar's Test P-Value : 0.0008741      \n                                         \n            Sensitivity : 0.8000         \n            Specificity : 0.6709         \n         Pos Pred Value : 0.6751         \n         Neg Pred Value : 0.7970         \n             Prevalence : 0.4608         \n         Detection Rate : 0.3687         \n   Detection Prevalence : 0.5461         \n      Balanced Accuracy : 0.7355         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\ncode\n# # Create ROC curve\n# roc_obj &lt;- roc(test_data$actual, predictions)\n# \n# # Plot ROC curve\n# plot(roc_obj, main = \"ROC Curve\", col = \"blue\", lwd = 2)\n# abline(a = 0, b = 1, lty = 2, col = \"red\")\n# \n# # Compute AUC\n# auc_value &lt;- auc(roc_obj)\n# print(paste(\"AUC:\", round(auc_value, 4)))\n\n\n\n\ncode\n# Function to compute ROC curve\ncompute_roc &lt;- function(actual, predictions) {\n  # Sort actual and predictions in descending order of predicted probabilities\n  sorted_data &lt;- data.frame(actual, predictions)\n  sorted_data &lt;- sorted_data[order(sorted_data$predictions, decreasing = TRUE), ]\n\n  # Initialize variables\n  n_positive &lt;- sum(actual)\n  n_negative &lt;- length(actual) - n_positive\n  tp &lt;- 0\n  fp &lt;- 0\n  tpr &lt;- c()\n  fpr &lt;- c()\n\n  # Iterate over sorted data\n  for (i in 1:nrow(sorted_data)) {\n    if (sorted_data$actual[i] == 1) {\n      tp &lt;- tp + 1\n    } else {\n      fp &lt;- fp + 1\n    }\n\n    # Calculate true positive rate (TPR) and false positive rate (FPR)\n    tpr &lt;- c(tpr, tp / n_positive)\n    fpr &lt;- c(fpr, fp / n_negative)\n  }\n\n  # Create ROC curve\n  roc_curve &lt;- data.frame(fpr, tpr)\n  return(roc_curve)\n}\n\n# Example usage\n\ntest_data &lt;- nafta %&gt;% dplyr::select(vote, democrat, cope93) %&gt;% mutate(actual = vote)\n# Make predictions on the test data\npredictions &lt;- predict(naftamodel, newdata = test_data, type = \"response\")\n\n\nroc_curve &lt;- compute_roc(test_data$actual, predictions)\n\n# Plot ROC curve\nplot(roc_curve$fpr, roc_curve$tpr, type = \"l\", xlab = \"False Positive Rate\", ylab = \"True Positive Rate\")\n\n# Add diagonal line\nabline(a = 0, b = 1, lty = 2, col = \"red\")\n\n# Add AUC to plot\n\nauc_value &lt;- auc(roc_curve$fpr, roc_curve$tpr)\ntext(0.5, 0.5, paste(\"AUC =\", round(auc_value, 2)), pos = 4)\n\n\n\n\n\n\n\n\n\ncode\n#use ggplot to plot the ROC curve\n\nroc_curve &lt;- data.frame(fpr = roc_curve$fpr, tpr = roc_curve$tpr)\n\nggplot(roc_curve, aes(x = fpr, y = tpr)) +\n  geom_line() +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"ROC Curve\", x = \"False Positive Rate\", y = \"True Positive Rate\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nYou’ll note these examples using data on the US House vote on the NAFTA treaty make some sense - these measures of goodness of fit tell us how much our covariates improve classification.\nWhat makes this work in the NAFTA context? Because the next example using the democratic peace data, does not work so well."
  },
  {
    "objectID": "binaryextensions124.html#footnotes",
    "href": "binaryextensions124.html#footnotes",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nsee what I did there?↩︎"
  },
  {
    "objectID": "discretehazards24.html",
    "href": "discretehazards24.html",
    "title": "Discrete Time Hazard Models",
    "section": "",
    "text": "consider what “memory” might look like in a binary time series setting.\nintroduce concepts underlying hazard models.\nunderstand the discrete time hazard model."
  },
  {
    "objectID": "discretehazards24.html#memoryless",
    "href": "discretehazards24.html#memoryless",
    "title": "Discrete Time Hazard Models",
    "section": "Memoryless",
    "text": "Memoryless\nBy construction, this model lacks any memory. \\(Pr(Y_{i,t}=1)\\) is a function of \\(X_{i,t}\\), but is independent of anything that happened prior to \\(t\\).\nThis means dyads that have been at peace for 2 years and for 22 years are treated as the same, varying only on the \\(X\\) variables.\nNote this is by choice - we’re assuming \\(y_t\\) has no bearing on \\(y_{t+1}\\), so there is no persistence or memory from period to period. We’ll encounter a range of models that are explicitly aimed at understanding this question: How does having survived up until now affect the chances of failing now?"
  },
  {
    "objectID": "discretehazards24.html#time",
    "href": "discretehazards24.html#time",
    "title": "Discrete Time Hazard Models",
    "section": "Time",
    "text": "Time\nWhat would data for mortality (or any sort of spells) look like? There are two basic types:\nSurvival:\n\\[Age = 1, 2, 3, 4, 5, \\ldots 87, 88, 89\\]\nor\nFailure:\n\\[Death = 0, 0, 0, 0, 0, \\ldots 0, 0, 1\\]\nNote that these convey the two parts of the hazard - how many periods the individual survives, and at what period the individual fails.\nThese also represent two ways to think about time: continuously:\n\\[Age = 1, 2, 3, 4, 5, \\ldots 87, 88, 89\\]\nor discretely:\n\\[Death = 0, 0, 0, 0, 0, \\ldots 0, 0, 1\\]\nIn truth, we almost always measure time discretely in the sense that failure can only happen in certain intervals (days, weeks, years, etc.), even though time is continuous and failure can happen in much smaller increments than these (e.g. minutes, seconds). Still, most treatments consider two types of hazard models:\n\nContinuous time models using data like \\(Age = 1, 2, 3, 4, 5, \\ldots 87, 88, 89\\) as the \\(y\\) variable.\nDiscrete time models using data like \\(Death = 0, 0, 0, 0, 0, \\ldots 0, 0, 1\\) as the \\(y\\) variable. These are usually estimated using a binomial LLF (e.g. the logit model)."
  },
  {
    "objectID": "discretehazards24.html#hazard-models",
    "href": "discretehazards24.html#hazard-models",
    "title": "Discrete Time Hazard Models",
    "section": "Hazard models",
    "text": "Hazard models\nA key feature of any hazard model is that the model accounts for both time until failure, and the realization of failure. In continuous or discrete time models, both of these are part of the estimation.\n\n\n\n\n\n\nNote\n\n\n\nAn aside on naming - models like these are interchangeably called “hazard models,” “survival models,” “duration models,” or “event history models.” They all refer to the same basic idea - modeling the time until an event occurs. They can sometimes indicate whether the quantity of interest is the hazard or survival - note that these are opposites in the sense that the hazard is the probability of failure at a particular point in time, while survival is the probability of surviving up to that point in time. “Event history” often refers to discrete time models."
  },
  {
    "objectID": "discretehazards24.html#binary-time-series-cross-section-data",
    "href": "discretehazards24.html#binary-time-series-cross-section-data",
    "title": "Discrete Time Hazard Models",
    "section": "Binary Time Series Cross Section data",
    "text": "Binary Time Series Cross Section data\nIt’s common to have binary \\(y\\) variables observed for cross sections over time - these are Binary Time Series Cross Section (BTSCS) data. This is the form the democratic peace data takes, and is a common form of data in the social sciences. BTSCS data are grouped duration data, and failure is measured in discrete time.\nHere’s an example of BTSCS data thinking of disputes in dyads over time.\n\n\n\n\nBTSCS data\n\n\n\n\n\n\n\n\n\nState\nYear\nDispute\nCensored\n\n\n\nUS-Cuba\n1960\n0\n0\n\n\n\nUS-Cuba\n1961\n1\n0\n\n\n\nUS-Cuba\n1962\n0\n0\n\n\n\nUS-Cuba\n1963\n0\n0\n\n\n\nUS-Cuba\n1964\n0\n0\n\n\n\nUS-Cuba\n1965\n0\n0\n\n\n\nUS-Cuba\n1966\n0\n0\n\n\n\nUS-Cuba\n1967\n1\n0\n\n\n\nUS-Cuba\n1968\n0\n0\n\n\n\nUS-Cuba\n1969\n0\n0\n\n\n\nUS-Cuba\n1970\n0\n1\n\n\n\nUS-UK\n1960\n0\n0\n\n\n\nUS-UK\n1961\n0\n0\n\n\n\nUS-UK\n1962\n0\n0\n\n\n\nUS-UK\n1963\n0\n0\n\n\n\nUS-UK\n1964\n0\n0\n\n\n\nUS-UK\n1965\n0\n0\n\n\n\nUS-UK\n1966\n0\n0\n\n\n\nUS-UK\n1967\n0\n0"
  },
  {
    "objectID": "discretehazards24.html#terminology",
    "href": "discretehazards24.html#terminology",
    "title": "Discrete Time Hazard Models",
    "section": "Terminology",
    "text": "Terminology\nLet’s begin thinking about terminology:\n\nwe observe at each point \\(t\\) whether a unit fails or not. Failure means experiencing the event of interest. In mortality studies, this is is death; in the democratic peace data, the event is a militarized dispute.\neach unit is at risk until it exits the data either because the period of observation ends, or because it fails and can only fail once. In mortality studies, an individual can only fail once; in the democratic peace data, a dyad can fail multiple times.\na unit survives some spell up to the point at which it fails. We can count these time periods to measure survival time.\nthe period a unit survives is called a spell; spells end at failure.\nwe have no idea what happened to these units prior to 1960; the units are left-censored.\nwe have no idea what happens to these units after 1970; the units are right censored. Any unit that does not experience the failure event during the period of study is right-censored."
  },
  {
    "objectID": "discretehazards24.html#spells",
    "href": "discretehazards24.html#spells",
    "title": "Discrete Time Hazard Models",
    "section": "Spells",
    "text": "Spells\nHere are different spells:\n\n\ncode\nlibrary(tidyverse)\nlibrary(highcharter)\n\n# Binghamton University colors\nbinghamton_colors &lt;- c(\"#005A43\", \"#8C2132\", \"#FFD100\", \"#000000\", \"#636466\")\n\n# Create dataframes for each case with updated labels\ncases &lt;- list(\n  list(x = c(3, 6), y = c(1, 1), name = \"uncensored\"),\n  list(x = c(-0.5, 2.5), y = c(2, 2), name = \"left censored\"),\n  list(x = c(2.8, 8), y = c(3, 3), name = \"fails at last period\"),\n  list(x = c(3.5, 10), y = c(4, 4), name = \"right censored\"),\n  list(x = c(5.5, 7), y = c(5, 5), name = \"uncensored\")\n)\n\n# Create the plot\nhc &lt;- highchart() %&gt;%\n  hc_chart(type = \"line\") %&gt;%\n  hc_xAxis(\n    title = list(text = \"time\"),\n    plotLines = list(\n      list(value = 2, width = 2, color = \"black\"),\n      list(value = 8, width = 2, color = \"black\")\n    ),\n    min = 0,\n    max = 10\n  ) %&gt;%\n  hc_yAxis(\n    title = list(text = \"case\"),\n    min = 0,\n    max = 5,\n    tickInterval = 1\n  ) %&gt;%\n  hc_plotOptions(\n    series = list(\n      lineWidth = 3,\n      marker = list(enabled = FALSE)\n    )\n  ) %&gt;%\n  hc_legend(enabled = FALSE)\n\n# Add each case as a separate series with Binghamton colors\nfor (i in seq_along(cases)) {\n  hc &lt;- hc %&gt;% hc_add_series(\n    data = list_parse2(data.frame(x = cases[[i]]$x, y = cases[[i]]$y)),\n    name = cases[[i]]$name,\n    color = binghamton_colors[i]\n  )\n}\n\n# Display the plot\nhc\n\n\n\n\n\n\n\nsome units survive through the end of the study; these units are right-censored. That is, they do not fail during the period of observation.\nfailure is only observed per year; so failure is grouped by year; these are grouped duration data. We could, for instance, graph the density of failures at each point in time, effectively grouping them by failure time.\nthe chances of failing at \\(t\\), given survival til \\(t\\) is the hazard of failure; at any point in time, this is called the hazard rate, denoted \\(h(t)\\).\nin the model above, \\(h(t)\\) does not depend on what happened at \\(t-1\\), so \\(h(t)\\) is constant over time or is time invariant, or is duration independent."
  },
  {
    "objectID": "discretehazards24.html#survival-spells",
    "href": "discretehazards24.html#survival-spells",
    "title": "Discrete Time Hazard Models",
    "section": "Survival Spells",
    "text": "Survival Spells\nWe can measure survival spells; time elapsed until failure or censoring. These are the same data as above, just re-formed so the units are different. Note the summed survival time is equal to the total time at risk. So for the US-Cuba dyad, the total time at risk is 11 years. Also, notice that the US-Cuba dyad is censored in 1970. It survives 3 years since its last dispute, but the end of that spell is our observation period, not another dispute.\n\n\n\n\nSpell data\n\n\n\n\n\n\n\n\n\n\nState\nYear\nDispute\nfail\ncensored\nsurvival\n\n\nUS-Cuba\n1961\n1\n1\n0\n2\n\n\nUS-Cuba\n1967\n1\n1\n0\n6\n\n\nUS-Cuba\n1970\n0\n0\n1\n3\n\n\nUS-UK\n1970\n0\n0\n1\n11"
  },
  {
    "objectID": "discretehazards24.html#why-not-use-ols",
    "href": "discretehazards24.html#why-not-use-ols",
    "title": "Discrete Time Hazard Models",
    "section": "Why not use OLS?",
    "text": "Why not use OLS?\nIt looks almost as if we could estimate a linear regression of the survival variable. Time is continuous, after all. Problems with doing so include:\n\nwe can’t have negative survival time.\nfailing at \\(t = 8\\) is conditional on having survived until \\(t=8\\); can’t include this in a linear model.\nsome observations never fail during the period of observation (are censored).\nthe outcome variable of interest is latent - it’s the hazard rate."
  },
  {
    "objectID": "discretehazards24.html#discrete-time-hazards",
    "href": "discretehazards24.html#discrete-time-hazards",
    "title": "Discrete Time Hazard Models",
    "section": "Discrete Time Hazards",
    "text": "Discrete Time Hazards\nUntil the late 1990s, studies using BTSCS data ignored memory. Put differently, the conventional way to model these data was using a binomial model like the logit. Beck, Katz, and Tucker’s (1998) paper pointed out the problems with doing this, and suggested an easy fix.\n\nThe problem\nThe standard logit/probit model in these data assumes the errors are i.i.d. - that the disturbances are uncorrelated. A somewhat more interesting observation is that the model assumes no relationship between the outcome at \\(t\\) and the outcome at \\(t-1, t-2 \\ldots t-k\\). So the observations on \\(y\\) arise independently of one another …almost as if each observation is an independent Bernoulli trial. The model is misspecified, and likely the parameter estimates are biased.\n\n\nThe solution\nInclude “survival time” as a right hand side variable. Doing so explicitly models the effect of surviving up til \\(t\\) on the probability of failing at \\(t\\).\nBKT suggest including a function of survival time so the effect of time isn’t constrained to be monotonic. They suggested using cubic splines of survival time; Carter and Signorino (2010) later show polynomials for survival time are just as good and easier to compute/understand.\n\n\nThe result\nThis fundamentally changed models on BTSCS data - the state of the art is to include survival time, thereby measuring “memory” in the \\(y\\) series. While most BTSCS models since Beck, Katz, and Tucker (1998) include survival time, relatively few interpret it; that’s okay insofar as the effect of survival might not be of theoretical interest. Most incorrectly interpret the predictions as probabilities - they are hazards."
  },
  {
    "objectID": "discretehazards24.html#survival-time",
    "href": "discretehazards24.html#survival-time",
    "title": "Discrete Time Hazard Models",
    "section": "Survival time",
    "text": "Survival time\nSurvival time: the time up to failure, in the interval \\(t_0, t_{\\infty}\\) such that \\(t \\in \\{1,2,3 \\ldots t_{\\infty} \\}\\)"
  },
  {
    "objectID": "discretehazards24.html#failure",
    "href": "discretehazards24.html#failure",
    "title": "Discrete Time Hazard Models",
    "section": "Failure",
    "text": "Failure\nThe probability of the failure event:\n\\[\\begin{aligned}\nf(t) = Pr(t_i=t) \\nonumber\n\\end{aligned}\\]\nThis is the density."
  },
  {
    "objectID": "discretehazards24.html#cumulative-function",
    "href": "discretehazards24.html#cumulative-function",
    "title": "Discrete Time Hazard Models",
    "section": "Cumulative Function",
    "text": "Cumulative Function\nWrite the cumulative probability of failure up to \\(t_i\\).\n\\[\\begin{aligned}\nF(t) = \\sum_{i=1}^{\\infty} f(t_i) \\nonumber\n\\end{aligned}\\]\nNow, consider the probability of surviving up until \\(t\\) - this is equal to 1 minus the CDF, so\n\\[S(t) = 1-F(t) = P(t_{i} \\geq t)\\]\nMost importantly, the conditional probability given by the probability of failing at \\(t_i\\) given survival up until \\(t_i\\):\n\\[\\begin{aligned}\nh(t)=Pr(t=t_i | t \\geq t_i) \\nonumber \\\\\n= \\frac{f(t)}{S(t)} \\nonumber\n\\end{aligned}\\]\nThis is the hazard rate."
  },
  {
    "objectID": "discretehazards24.html#hazard-rate",
    "href": "discretehazards24.html#hazard-rate",
    "title": "Discrete Time Hazard Models",
    "section": "Hazard Rate",
    "text": "Hazard Rate\n\\[\\begin{aligned}\nh(t)=Pr(t=t_i | t \\geq t_i) \\nonumber \\\\\n= \\frac{f(t)}{S(t)} \\nonumber\n\\end{aligned}\\]\nThe hazard rate is conceptually important because it explicitly relates the past to the present, thereby incorporating memory into the statistical model. The hazard is different from \\(Pr(y_t=1)\\) because it conditions on what has happened prior to \\(t\\)."
  },
  {
    "objectID": "discretehazards24.html#discrete-time-ht",
    "href": "discretehazards24.html#discrete-time-ht",
    "title": "Discrete Time Hazard Models",
    "section": "Discrete time h(t)",
    "text": "Discrete time h(t)\nWhat we have derived is the discrete time hazard function - time is measured in discrete units (e.g. years, not parts of years like months or days); some processes only make sense in discrete terms - e.g. a member of the US House can only be turned out by voters every two years, not before."
  },
  {
    "objectID": "discretehazards24.html#density",
    "href": "discretehazards24.html#density",
    "title": "Discrete Time Hazard Models",
    "section": "Density",
    "text": "Density\nSince the probability of survival at some value of \\(t\\) is the probability of survival at \\(t\\) given survival up to \\(t\\), the conditional probability of survival is 1 minus the hazard rate:\n\\[Pr(t_j&gt;t | t_j\\geq t) = 1 - h(t)\\]\nWe can rewrite the survivor function as a product of the probabilities of surviving up to \\(t\\):\n\\[S(t) = \\prod_{j=0}^{t} \\{1-h(t-j)\\}\\]\nWe can rewrite the density \\(f(t)\\):\n\\[f(t) = h(t)S(t)\\]"
  },
  {
    "objectID": "discretehazards24.html#estimation",
    "href": "discretehazards24.html#estimation",
    "title": "Discrete Time Hazard Models",
    "section": "Estimation",
    "text": "Estimation\nLet’s build a likelihood - as you might have guessed, it needs to involve \\(f(t)\\) and \\(S(t)\\) (failure and survival times) so we can estimate \\(h(t)\\).\n\\[ \\mathcal{L} = \\prod_{t_i\\leq t} f(t_i) \\prod_{t_i\\geq t} S(t_i) \\]\nthen, think of censoring where \\(y_{i,t}\\) indicates when, and whether a subject ever fails; if zero, censored, if one, uncensored (fails during our period of observation).\n\\[ \\mathcal{L} = \\prod_{t_i\\leq t} f(t_i)^{y_{i,t}} \\prod_{t_i\\geq t} S(t_i)^{1- y_{i,t}} \\]\nThis should be looking familiar.\nNow, substituting:\n\\[ \\mathcal{L} = \\prod_{i=1}^{N} \\Bigg\\{ h(t) \\prod_{j=1}^{t-1}   [1-h(t-i)] \\Bigg\\} ^{y_{i,t}} \\Bigg\\{ \\prod_{j=1}^{t}   [1-h(t-i)] \\Bigg\\}^{1- y_{i,t}}\\]\nAnd substitute an appropriate link density for \\(f(t)\\) and \\(S(t)\\), e.g., exponential,\n\\[f(t) = \\lambda(t) exp^{\\lambda(t)}\\] \\[S(t) = exp^{-\\lambda(t)}\\] \\[h(t) = \\lambda\\]\nWeibull: \\[f(t) = \\lambda p (\\lambda(t))^{p-1} exp^{-(\\lambda t)^p}\\] \\[S(t) = exp^{-(\\lambda t)^p}\\] \\[h(t) = \\lambda p (\\lambda t)^{p-1}\\]\netc …"
  },
  {
    "objectID": "discretehazards24.html#constant-ht---no-memory",
    "href": "discretehazards24.html#constant-ht---no-memory",
    "title": "Discrete Time Hazard Models",
    "section": "Constant \\(h(t)\\) - no memory",
    "text": "Constant \\(h(t)\\) - no memory\nRevisiting …\nThis is the case where\n\\[h_0(t) = \\frac{1}{1+ exp(\\beta_0)}\\]\nthe baseline hazard is the constant. Even with \\(x\\) variables, there is still no accounting for time - the \\(x\\) effects are only shifts around this baseline hazard.\n\\[h_0(t) = \\frac{1}{1+ exp(\\beta_0+ x'\\beta)}\\]\nthis is still a constant baseline hazard with the effects of \\(x\\) deviating around it."
  },
  {
    "objectID": "discretehazards24.html#non-constant-ht",
    "href": "discretehazards24.html#non-constant-ht",
    "title": "Discrete Time Hazard Models",
    "section": "Non-constant \\(h(t)\\)",
    "text": "Non-constant \\(h(t)\\)\nSo how to deal with this, incorporating memory: thinking in terms of hazards rather than probabilities (i.e., conditional rather than unconditional probabilities), what if we measure survival time?\n\nThe binary \\(y\\) variable is an indicator of failure at \\(t\\); the model estimates \\(f(t)\\), which we’ve said is not especially informative since subjects might fail before \\(t\\).\nThink of the number of periods up to failure as the cumulative survival time, \\(S(t)\\).\n\nSee how we’re starting to construct the hazard rate by its parts."
  },
  {
    "objectID": "discretehazards24.html#measuring-survival-time",
    "href": "discretehazards24.html#measuring-survival-time",
    "title": "Discrete Time Hazard Models",
    "section": "Measuring survival time",
    "text": "Measuring survival time\n\ncount periods of survival up to failure. This is a counter of survival time. generate a binary variable for each survival period.\nEither include those survival dummies in the logit, or include the survival counter itself with polynomials, e.g. \\(t^2, t^3, \\ldots\\).\ninterpret those coefficients as baseline hazards for groups that survive to \\(t_i\\).\nwith all \\(x\\) variables set to zero, the probability of failure is now given by the constant and the appropriate dummy or counter coefficient.\nNote the quantity of interest is not constant across time: it’s a conditional probability; the probability of failing at \\(t\\) given the estimated probability of survival through \\(t-1\\) - \\(h(t)|S(t)\\)."
  },
  {
    "objectID": "discretehazards24.html#understanding-substantive-variables-in-the-hazard-context",
    "href": "discretehazards24.html#understanding-substantive-variables-in-the-hazard-context",
    "title": "Discrete Time Hazard Models",
    "section": "Understanding substantive variables in the hazard context",
    "text": "Understanding substantive variables in the hazard context\nThe survival variables now permit the baseline hazard to vary. The effects of \\(x\\) variables can be thought of as deviations from those baseline hazards. For example, think about the models presented above, but with democracy. The estimates on democracy will shift the baseline hazard up or down.\n\n\ncode\nsurvivaldp &lt;- survivaldp %&gt;% mutate(stime2=stime^2, stime3=stime^3)\n\ndpm4 &lt;-glm(dispute ~ stime+stime2+stime3+deml, family=binomial(link=\"logit\"), data=survivaldp )\n\nstargazer(dpm4, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Survival Time\", \"Survival Time2\", \"Survival Time3\", \"Democracy\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Time\n\n\n-0.823*** (0.035)\n\n\n\n\nSurvival Time2\n\n\n0.051*** (0.004)\n\n\n\n\nSurvival Time3\n\n\n-0.001*** (0.0001)\n\n\n\n\nDemocracy\n\n\n-0.065*** (0.007)\n\n\n\n\nConstant\n\n\n-1.298*** (0.065)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n5,650.709\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\n\ncode\ndpm4 &lt;-glm(dispute ~ stime+stime2+stime3+deml+caprat, family=binomial(link=\"logit\"), data=survivaldp )\n\n# copy estimation data for avg effects\ndppred &lt;- survivaldp\n\n# df for output\ndf &lt;- data.frame(time=seq(1,34,1))\nfor (d in seq(-10,10,10)) {\n    df[paste0(\"p\", d+10)] &lt;- NA\n  df[paste0(\"Se\", d+10)] &lt;- NA\n}\n\n# predictions\nfor (d in seq(-10,10,10)) {\n  dppred$deml &lt;- d\n for (t in seq(1,34,1)) {  \n  dppred$stime &lt;- t\n  dppred$stime2 &lt;- t^2 \n  dppred$stime3 &lt;- t^3\n  pred &lt;- predict(dpm4, newdata=dppred, type=\"response\", se=TRUE)\n  df[t, paste0(\"p\", d+10)] &lt;- mean(pred$fit, na.rm=TRUE)\n  df[t, paste0(\"Se\", d+10)] &lt;- mean(pred$se.fit, na.rm=TRUE)\n  df$time[t] &lt;- t\n}\n}\n\n# plot\nggplot(df, aes(x=time, y=p0)) +\n  geom_line(color=\"#005A43\") +\n  geom_ribbon(aes(ymin=p0-1.96*Se0, ymax=p0+1.96*Se0), fill=\"#6CC24A\", alpha=0.4) +\n  geom_line(aes(y=p10), color=\"#005A43\") +\n  geom_ribbon(aes(ymin=p10-1.96*Se10, ymax=p10+1.96*Se10), fill=\"#A7DA92\", alpha=0.4) +\n  geom_line(aes(y=p20), color=\"#005A43\") +\n  geom_ribbon(aes(ymin=p20-1.96*Se20, ymax=p20+1.96*Se20), fill=\"#005A43\", alpha=0.4) +\n  labs(x=\"Time since last dispute\", y=\"Pr(Dispute)\") +\n  theme_minimal()"
  },
  {
    "objectID": "discretehazards24.html#summary",
    "href": "discretehazards24.html#summary",
    "title": "Discrete Time Hazard Models",
    "section": "Summary",
    "text": "Summary\n\nWe relaxed the assumption of temporal independence.\nDid so by conceiving of the QI as a hazard rather than a probability.\nBuilt a model that estimates \\(h(t)\\) such that we don’t have to assume \\(h(t)\\) is constant.\nHave permitted memory in the model such that the past can shape the present."
  },
  {
    "objectID": "discretehazards24.html#temporal-independence",
    "href": "discretehazards24.html#temporal-independence",
    "title": "Discrete Time Hazard Models",
    "section": "Temporal Independence",
    "text": "Temporal Independence\nIn this model, \\(Pr(y_i=1) = F(x_i\\beta + \\beta_0)\\), \\(x_i \\beta\\) induces deviation from the constant or baseline level; but there is no temporal variation, temporal persistence, or memory. What happened last year has no bearing on what we observe this year. Repeating, it is as if these are Bernoulli trials.\n\nAn alternative - transitions\nWhat happens if we lag \\(y\\) as we might with a continuous variable (say, in OLS), such that the estimation model is\n\\[y_t = \\beta_0 + \\beta_1(x_1) + \\ldots + \\gamma(y_{t-1}) + \\varepsilon\\]\nWith binary time series data, lagging \\(y\\) would measure changes of state - these are a class known as transition models (there are a variety of these).\nIn general, these are interactive models where\n\\[Pr(y_i=1) = F(x_{i,t}\\beta + y_{i,t-1}*x_{i,t} \\gamma)\\]\nand \\(\\gamma\\) measures the difference in effect when \\(y_{i,t-1} = 0\\) (this is just \\(\\beta\\)), and when \\(y_{i,t-1}  = 1\\); denote this \\(\\alpha\\). So \\(\\gamma = \\beta - \\alpha\\). That difference indicates the conditional probability of state transitions from the state where \\(y\\) takes on one value, to the state where it takes on the other value.\nTransition models are useful, but measure something fundamentally different from the latent hazard rate, or the chances of failure given a history of survival. Put differently, lagging \\(y\\) in a binary variable model does not measure memory or persistence; it does not measure the extent to which the observed value today depends on the value yesterday; it does not measure how the latent probability of failure today depends on surviving through yesterday."
  },
  {
    "objectID": "discretehazards24.html#data",
    "href": "discretehazards24.html#data",
    "title": "Discrete Time Hazard Models",
    "section": "Data",
    "text": "Data\nThe democratic peace data is panel data - composed of cross sections observed over time. The \\(y\\) variable is binary, and measures a rare event - conflict. So the \\(y\\) variable for any particular panel is usually a string of zeros, occasionally punctuated by a one.\nWhat we’d really like to know is if/how strings of zeros affect the chances of a one at any given point in time. This is a question of hazards."
  },
  {
    "objectID": "discretehazards24.html#discrete-time---binary-time-series-cross-section-data",
    "href": "discretehazards24.html#discrete-time---binary-time-series-cross-section-data",
    "title": "Discrete Time Hazard Models",
    "section": "Discrete Time - Binary Time Series Cross Section data",
    "text": "Discrete Time - Binary Time Series Cross Section data\nTime is continuous insofar as time units are infinitely divisible, but in practice, we measure time in discrete units like days, months, years, etc. In the democratic peace data above, we will have two dyads “fail” (have disputes) at year 3; but it’s nearly certain one of those dyads started its dispute before the other one. We group the data by these time intervals (years, in this case). So we are measuring time discretely (i.e, in years), but the underlying process is continuous. Moreover, the fact these can be grouped by failure time makes them grouped duration data.\nIt’s common to have binary \\(y\\) variables observed for cross sections over time - these are Binary Time Series Cross Section (BTSCS) data. This is the form the democratic peace data takes, and is a common form of data in the social sciences. BTSCS data are grouped duration data, and failure is measured in discrete time.\nHere’s an example of BTSCS data thinking of disputes in dyads over time.\n\n\n\n\nBTSCS data\n\n\n\n\n\n\n\n\n\nState\nYear\nDispute\nCensored\n\n\n\nUS-Cuba\n1960\n0\n0\n\n\n\nUS-Cuba\n1961\n1\n0\n\n\n\nUS-Cuba\n1962\n0\n0\n\n\n\nUS-Cuba\n1963\n0\n0\n\n\n\nUS-Cuba\n1964\n0\n0\n\n\n\nUS-Cuba\n1965\n0\n0\n\n\n\nUS-Cuba\n1966\n0\n0\n\n\n\nUS-Cuba\n1967\n1\n0\n\n\n\nUS-Cuba\n1968\n0\n0\n\n\n\nUS-Cuba\n1969\n0\n0\n\n\n\nUS-Cuba\n1970\n0\n1\n\n\n\nUS-UK\n1960\n0\n0\n\n\n\nUS-UK\n1961\n0\n0\n\n\n\nUS-UK\n1962\n0\n0\n\n\n\nUS-UK\n1963\n0\n0\n\n\n\nUS-UK\n1964\n0\n0\n\n\n\nUS-UK\n1965\n0\n0\n\n\n\nUS-UK\n1966\n0\n0\n\n\n\nUS-UK\n1967\n0\n0"
  },
  {
    "objectID": "discretehazards24.html#illustration",
    "href": "discretehazards24.html#illustration",
    "title": "Discrete Time Hazard Models",
    "section": "Illustration",
    "text": "Illustration\nHere are different spells:\n\n\ncode\nlibrary(tidyverse)\nlibrary(highcharter)\n\n# Binghamton University colors\nbinghamton_colors &lt;- c(\"#005A43\", \"#8C2132\", \"#FFD100\", \"#000000\", \"#636466\")\n\n# Create dataframes for each case with updated labels\ncases &lt;- list(\n  list(x = c(3, 6), y = c(1, 1), name = \"uncensored\"),\n  list(x = c(-0.5, 2.5), y = c(2, 2), name = \"left censored\"),\n  list(x = c(2.8, 8), y = c(3, 3), name = \"fails at last period\"),\n  list(x = c(3.5, 10), y = c(4, 4), name = \"right censored\"),\n  list(x = c(5.5, 7), y = c(5, 5), name = \"uncensored\")\n)\n\n# Create the plot\nhc &lt;- highchart() %&gt;%\n  hc_chart(type = \"line\") %&gt;%\n  hc_xAxis(\n    title = list(text = \"time\"),\n    plotLines = list(\n      list(value = 2, width = 2, color = \"black\"),\n      list(value = 8, width = 2, color = \"black\")\n    ),\n    min = 0,\n    max = 10\n  ) %&gt;%\n  hc_yAxis(\n    title = list(text = \"case\"),\n    min = 0,\n    max = 5,\n    tickInterval = 1\n  ) %&gt;%\n  hc_plotOptions(\n    series = list(\n      lineWidth = 3,\n      marker = list(enabled = FALSE)\n    )\n  ) %&gt;%\n  hc_legend(enabled = FALSE)\n\n# Add each case as a separate series with Binghamton colors\nfor (i in seq_along(cases)) {\n  hc &lt;- hc %&gt;% hc_add_series(\n    data = list_parse2(data.frame(x = cases[[i]]$x, y = cases[[i]]$y)),\n    name = cases[[i]]$name,\n    color = binghamton_colors[i]\n  )\n}\n\n# Display the plot\nhc\n\n\n\n\n\n\n\nsome units survive through the end of the study; these units are right censored. That is, they do not fail during the period of observation.\nfailure is only observed per year; so failure is grouped by year; these are grouped duration data. We could, for instance, graph the density of failures at each point in time, effectively grouping them by failure time.\nthe probability of failing at \\(t\\), given survival til \\(t\\) is the hazard of failure; at any point in time, this is called the hazard rate, denoted \\(h(t)\\).\nin the democratic peace model above, \\(h(t)\\) does not depend on what happened at \\(t-1\\), so \\(h(t)\\) is constant over time or is time invariant, or is duration independent."
  },
  {
    "objectID": "discretehazards24.html#the-problem",
    "href": "discretehazards24.html#the-problem",
    "title": "Discrete Time Hazard Models",
    "section": "The problem",
    "text": "The problem\nThe standard logit/probit model in these data assumes the errors are i.i.d. - that the disturbances are uncorrelated. A somewhat more interesting observation is that the model assumes no relationship between the outcome at \\(t\\) and the outcome at \\(t-1, t-2 \\ldots t-k\\). So the observations on \\(y\\) arise independently of one another …almost as if each observation is an independent Bernoulli trial. If this isn’t true, the model is misspecified, and likely the parameter estimates are biased.\nIn the context of the democratic peace data, this means the probability of a dispute at any point in time is unrelated to how long that particular dyad has been at peace. Whether it’s been at peace for 1 year or 10 years has no bearing on the chances of conflict now. On its face, this is a heroic assumption."
  },
  {
    "objectID": "discretehazards24.html#the-solution",
    "href": "discretehazards24.html#the-solution",
    "title": "Discrete Time Hazard Models",
    "section": "The solution",
    "text": "The solution\nAt its root, this is a model specification issue - we think time since last dispute is probably related to the chances of a dispute today, but no such measure is in the model. BKT suggest including “survival time” as a right hand side variable. Doing so explicitly models the effect of surviving up til \\(t\\) on the probability of failing at \\(t\\).\nBKT suggest including nonlinear functions of survival time so the effect of time isn’t constrained to be monotonic. They suggested using cubic splines of survival time; Carter and Signorino (2010) later show polynomials for survival time are just as good and easier to compute/understand."
  },
  {
    "objectID": "discretehazards24.html#the-result",
    "href": "discretehazards24.html#the-result",
    "title": "Discrete Time Hazard Models",
    "section": "The result",
    "text": "The result\nThis fundamentally changed models on BTSCS data - the state of the art since then is to include survival time, thereby measuring “memory” in the \\(y\\) series. While most BTSCS models since Beck, Katz, and Tucker (1998) include survival time, relatively few interpret it; that’s okay insofar as the effect of survival might not be of theoretical interest. Most incorrectly interpret the predictions as probabilities - they are now conditional probabilities, $pr(fail | survival), so hazards.\n\nConstant \\(h(t)\\) - no memory\nRevisiting …in this model, \\(Pr(y_i=1) = F(x_i\\beta + \\beta_0)\\), \\(x_i \\beta\\) induces deviation from the constant or baseline level; but there is no temporal variation, temporal persistence, or memory. What happened last year has no bearing on what we observe this year. Repeating, it is as if these are Bernoulli trials.\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\ndpm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n\nstargazer(dpm1, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\n\ncode\n# at mean data over 30 observations\n\natmean &lt;- data.frame(stime=seq(1,34,1), deml=median(dp$deml), border=0, caprat=median(dp$caprat), ally=0)\n\npredictions &lt;- data.frame(atmean, predict(dpm1, newdata=atmean, type=\"response\", se=TRUE)) %&gt;% mutate(fit=round(fit, 2) )\n\n\nggplot(predictions, aes(x=stime, y=fit)) +\n  geom_line(color=\"#005A43\", size=1) +\n  labs(x=\"Time since last dispute\", y=\"Pr(Dispute)\") +\n  geom_ribbon(aes(ymin=fit-1.96*se.fit, ymax=fit+1.96*se.fit), fill=\"grey90\", alpha=0.4) +\n  theme_minimal() +\n  annotate(\"text\", x = 15, y = 0.041, label = \"Effect of Democracy\", color = \"red\", size = 3) +\n  theme(panel.grid.major = element_line(color = \"black\",\n                                        size = 0.25,\n                                        linetype = 1)) +\n  theme(panel.grid.minor = element_line(color = \"black\",\n                                        size = 0.15,\n                                        linetype = 1))\n\n\n\n\n\n\n\n\n\nThis is the case where\n\\[h_0(t) = \\frac{1}{1+ exp(\\beta_0)}\\]\nthe baseline hazard is the constant. Even with \\(x\\) variables, there is still no accounting for time - the \\(x\\) effects are only shifts around this baseline hazard.\n\\[h_0(t) = \\frac{1}{1+ exp(\\beta_0+ x'\\beta)}\\]\nthis is still a constant baseline hazard with the effects of \\(x\\) deviating around it.\n\n\nMeasuring survival time\n\ncount periods of survival up to failure. This is a counter of survival time. generate a binary variable for each survival period.\nEither include those survival dummies in the logit, or include the survival counter itself with polynomials, e.g. \\(t^2, t^3, \\ldots\\).\ninterpret those coefficients as baseline hazards for groups that survive to \\(t_i\\).\nwith all \\(x\\) variables set to zero, the probability of failure is now given by the constant and the appropriate dummy or counter coefficient.\nNote the quantity of interest is not constant across time: it’s a conditional probability; the probability of failing at \\(t\\) given the estimated probability of survival through \\(t-1\\) - \\(h(t)|S(t)\\).\n\n\nHere’s what the survival time counter looks like in the democratic peace data:\n\n\ncode\nsdpshort &lt;- survivaldp %&gt;% head(160) %&gt;% dplyr::select(dyad, year, dispute, stime)\n\nlibrary(kableExtra)\ntibble(sdpshort)%&gt;% \n    kable(\"html\", caption=\"Survival Time, Democratic Peace Data\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) \n\n\n\nSurvival Time, Democratic Peace Data\n\n\ndyad\nyear\ndispute\nstime\n\n\n\n\n2020\n1951\n0\n0\n\n\n2020\n1952\n0\n1\n\n\n2020\n1953\n0\n2\n\n\n2020\n1954\n0\n3\n\n\n2020\n1955\n0\n4\n\n\n2020\n1956\n0\n5\n\n\n2020\n1957\n0\n6\n\n\n2020\n1958\n0\n7\n\n\n2020\n1959\n0\n8\n\n\n2020\n1960\n0\n9\n\n\n2020\n1961\n0\n10\n\n\n2020\n1962\n0\n11\n\n\n2020\n1963\n0\n12\n\n\n2020\n1964\n0\n13\n\n\n2020\n1965\n0\n14\n\n\n2020\n1966\n0\n15\n\n\n2020\n1967\n0\n16\n\n\n2020\n1968\n0\n17\n\n\n2020\n1969\n0\n18\n\n\n2020\n1970\n0\n19\n\n\n2020\n1971\n0\n20\n\n\n2020\n1972\n0\n21\n\n\n2020\n1973\n0\n22\n\n\n2020\n1974\n1\n23\n\n\n2020\n1975\n1\n0\n\n\n2020\n1976\n0\n0\n\n\n2020\n1977\n0\n1\n\n\n2020\n1978\n0\n2\n\n\n2020\n1979\n1\n3\n\n\n2020\n1980\n0\n0\n\n\n2020\n1981\n0\n1\n\n\n2020\n1982\n0\n2\n\n\n2020\n1983\n0\n3\n\n\n2020\n1984\n0\n4\n\n\n2020\n1985\n0\n5\n\n\n2041\n1961\n0\n0\n\n\n2041\n1962\n0\n1\n\n\n2041\n1963\n1\n2\n\n\n2041\n1964\n0\n0\n\n\n2041\n1965\n0\n1\n\n\n2041\n1966\n0\n2\n\n\n2041\n1967\n0\n3\n\n\n2041\n1968\n0\n4\n\n\n2041\n1969\n0\n5\n\n\n2041\n1970\n0\n6\n\n\n2041\n1971\n0\n7\n\n\n2041\n1972\n0\n8\n\n\n2041\n1973\n0\n9\n\n\n2041\n1974\n0\n10\n\n\n2041\n1975\n0\n11\n\n\n2041\n1976\n0\n12\n\n\n2041\n1977\n0\n13\n\n\n2041\n1978\n0\n14\n\n\n2041\n1979\n0\n15\n\n\n2041\n1980\n0\n16\n\n\n2041\n1981\n0\n17\n\n\n2041\n1982\n0\n18\n\n\n2041\n1983\n0\n19\n\n\n2041\n1984\n0\n20\n\n\n2041\n1985\n0\n21\n\n\n2042\n1951\n0\n0\n\n\n2042\n1952\n0\n1\n\n\n2042\n1953\n0\n2\n\n\n2042\n1954\n0\n3\n\n\n2042\n1955\n0\n4\n\n\n2042\n1956\n0\n5\n\n\n2042\n1957\n0\n6\n\n\n2042\n1958\n0\n7\n\n\n2042\n1959\n1\n8\n\n\n2042\n1960\n0\n0\n\n\n2042\n1962\n0\n1\n\n\n2042\n1964\n0\n2\n\n\n2042\n1965\n0\n3\n\n\n2042\n1966\n0\n4\n\n\n2042\n1967\n0\n5\n\n\n2042\n1968\n0\n6\n\n\n2042\n1969\n0\n7\n\n\n2042\n1970\n0\n8\n\n\n2042\n1971\n0\n9\n\n\n2042\n1972\n0\n10\n\n\n2042\n1973\n0\n11\n\n\n2042\n1974\n0\n12\n\n\n2042\n1975\n0\n13\n\n\n2042\n1976\n0\n14\n\n\n2042\n1977\n0\n15\n\n\n2042\n1978\n0\n16\n\n\n2042\n1979\n0\n17\n\n\n2042\n1980\n0\n18\n\n\n2042\n1981\n0\n19\n\n\n2042\n1982\n0\n20\n\n\n2042\n1983\n0\n21\n\n\n2042\n1984\n0\n22\n\n\n2042\n1985\n0\n23\n\n\n2051\n1962\n0\n0\n\n\n2051\n1963\n0\n1\n\n\n2051\n1964\n0\n2\n\n\n2051\n1965\n0\n3\n\n\n2051\n1966\n0\n4\n\n\n2051\n1967\n0\n5\n\n\n2051\n1968\n0\n6\n\n\n2051\n1969\n0\n7\n\n\n2051\n1970\n0\n8\n\n\n2051\n1971\n0\n9\n\n\n2051\n1972\n0\n10\n\n\n2051\n1973\n0\n11\n\n\n2051\n1974\n0\n12\n\n\n2051\n1975\n0\n13\n\n\n2051\n1976\n0\n14\n\n\n2051\n1977\n0\n15\n\n\n2051\n1978\n0\n16\n\n\n2051\n1979\n0\n17\n\n\n2051\n1980\n0\n18\n\n\n2051\n1981\n0\n19\n\n\n2051\n1982\n0\n20\n\n\n2051\n1983\n0\n21\n\n\n2051\n1984\n0\n22\n\n\n2051\n1985\n0\n23\n\n\n2052\n1962\n0\n0\n\n\n2052\n1963\n0\n1\n\n\n2052\n1964\n0\n2\n\n\n2052\n1965\n0\n3\n\n\n2052\n1966\n0\n4\n\n\n2052\n1967\n0\n5\n\n\n2052\n1968\n0\n6\n\n\n2052\n1969\n0\n7\n\n\n2052\n1970\n0\n8\n\n\n2052\n1971\n0\n9\n\n\n2052\n1972\n0\n10\n\n\n2052\n1973\n0\n11\n\n\n2052\n1974\n0\n12\n\n\n2052\n1975\n0\n13\n\n\n2052\n1976\n0\n14\n\n\n2052\n1977\n0\n15\n\n\n2052\n1978\n0\n16\n\n\n2052\n1979\n0\n17\n\n\n2052\n1980\n0\n18\n\n\n2052\n1981\n0\n19\n\n\n2052\n1982\n0\n20\n\n\n2052\n1983\n0\n21\n\n\n2052\n1984\n0\n22\n\n\n2052\n1985\n0\n23\n\n\n2070\n1951\n0\n0\n\n\n2070\n1952\n0\n1\n\n\n2070\n1953\n0\n2\n\n\n2070\n1954\n0\n3\n\n\n2070\n1955\n0\n4\n\n\n2070\n1956\n1\n5\n\n\n2070\n1957\n0\n0\n\n\n2070\n1958\n0\n1\n\n\n2070\n1959\n0\n2\n\n\n2070\n1960\n0\n3\n\n\n2070\n1961\n0\n4\n\n\n2070\n1962\n0\n5\n\n\n2070\n1963\n0\n6\n\n\n2070\n1964\n0\n7\n\n\n2070\n1965\n0\n8\n\n\n2070\n1966\n0\n9\n\n\n2070\n1967\n0\n10\n\n\n2070\n1968\n0\n11\n\n\n2070\n1969\n0\n12\n\n\n\n\n\n\n\n\n\nMonotonic hazard\nSo how to deal with this, incorporating memory: thinking in terms of hazards rather than probabilities (i.e., conditional rather than unconditional probabilities), what if we measure survival time?\n\nThe binary \\(y\\) variable is an indicator of failure at \\(t\\); the model estimates \\(f(t)\\), which we’ve said is not especially informative since subjects might fail before \\(t\\).\nThink of the number of periods up to failure as the cumulative survival time, \\(S(t)\\).\n\nSee how we’re starting to construct the hazard rate by its parts.\nHere’s an example in the democratic peace data:\n\\(dispute = \\beta_0+ \\beta_1(survival)\\)\n\n\ncode\ndpm2 &lt;-glm(dispute ~ stime, family=binomial(link=\"logit\"), data=survivaldp )\n\nstargazer(dpm2, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Survival Time\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Time\n\n\n-0.265*** (0.010)\n\n\n\n\nConstant\n\n\n-1.513*** (0.045)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n6,208.082\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\nAnd here are predicted probabilities from the model.\n\n\ncode\natmean &lt;- data.frame(stime=seq(1,34,1))\n\npredictions &lt;- data.frame(atmean, predict(dpm2, newdata=atmean, type=\"response\", se=TRUE)) \n\nggplot(predictions, aes(x=stime, y=fit)) +\n  geom_line() +\n  labs(x=\"Time since last dispute\", y=\"Pr(Dispute)\") +\n  geom_ribbon(aes(ymin=fit-1.96*se.fit, ymax=fit+1.96*se.fit), fill=\"#005A43\", alpha=0.4) +\n  theme_minimal()+\n  theme(panel.grid.major = element_line(color = \"black\",\n                                        size = 0.25,\n                                        linetype = 1)) +\n  theme(panel.grid.minor = element_line(color = \"black\",\n                                        size = 0.15,\n                                        linetype = 1))\n\n\n\n\n\n\n\n\n\nWhere the baseline hazard is no longer constant - it can increase or decrease monotonically:\n\\[h_0(t) = \\frac{1}{1+ exp(\\beta_0 + \\gamma_0(t))}\\]\nthe baseline hazard is the constant plus the effect of survival time - the \\(x\\) effects are shifts around this baseline hazard.\n\\[h_0(t) = \\frac{1}{1+ exp(\\beta_0 + \\gamma_0(t)) + x'\\beta)}\\]\nThe baseline hazard is no longer constrained to be constant, though it can be if \\(\\gamma_0=0\\).\n\nThis model accounts for “memory” - the QI is now the hazard.\nThe hazard is not constrained to be constant, but is constrained to be monotonic.\nTo relax this, we can\n\ninclude dummy variables - these are discrete time indicators based on the counter.\ninclude cubic splines or lowess estimates - these are smoothed time functions based on the counter.\ninclude polynomials of the time counter.\n\n\n\n\nNon-monotonic hazard\n\n\ncode\nsurvivaldp &lt;- survivaldp %&gt;% mutate(stime2=stime^2, stime3=stime^3)\n\ndpm3 &lt;-glm(dispute ~ stime+stime2+stime3, family=binomial(link=\"logit\"), data=survivaldp )\n\nstargazer(dpm3, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Survival Time\", \"Survival Time2\", \"Survival Time3\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Time\n\n\n-0.830*** (0.035)\n\n\n\n\nSurvival Time2\n\n\n0.052*** (0.004)\n\n\n\n\nSurvival Time3\n\n\n-0.001*** (0.0001)\n\n\n\n\nConstant\n\n\n-0.950*** (0.048)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n5,742.615\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\n\ncode\natmean &lt;- data.frame(stime=seq(1,34,1), stime2=seq(1,34,1)^2, stime3=seq(1,34,1)^3)\n\npredictions &lt;- data.frame(atmean, predict(dpm3, newdata=atmean, type=\"response\", se=TRUE)) \n\nggplot(predictions, aes(x=stime, y=fit)) +\n  geom_line() +\n  labs(x=\"Time since last dispute\", y=\"Pr(Dispute)\") +\n  geom_ribbon(aes(ymin=fit-1.96*se.fit, ymax=fit+1.96*se.fit), fill=\"#005A43\", alpha=0.4) +\n  theme_minimal()+\n  theme(panel.grid.major = element_line(color = \"black\",\n                                        size = 0.25,\n                                        linetype = 1)) +\n  theme(panel.grid.minor = element_line(color = \"black\",\n                                        size = 0.15,\n                                        linetype = 1))\n\n\n\n\n\n\n\n\n\nThis last is a close approximation of a Cox proportional hazards model. The hazard is non monotonic; it nests the exponential (constant hazard), and the monotonic (Weibull) hazard, and is very general. Besides, it’s very easy to estimate and interpret.\n\n\nUnderstanding substantive variables in the hazard context\nThe survival variables now permit the baseline hazard to vary. The effects of \\(x\\) variables can be thought of as deviations from those baseline hazards. For example, think about the models presented above, but with democracy. The estimates on democracy will shift the baseline hazard up or down.\n\n\ncode\nsurvivaldp &lt;- survivaldp %&gt;% mutate(stime2=stime^2, stime3=stime^3)\n\ndpm4 &lt;-glm(dispute ~ stime+stime2+stime3+deml, family=binomial(link=\"logit\"), data=survivaldp )\n\nstargazer(dpm4, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Survival Time\", \"Survival Time2\", \"Survival Time3\", \"Democracy\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Time\n\n\n-0.823*** (0.035)\n\n\n\n\nSurvival Time2\n\n\n0.051*** (0.004)\n\n\n\n\nSurvival Time3\n\n\n-0.001*** (0.0001)\n\n\n\n\nDemocracy\n\n\n-0.065*** (0.007)\n\n\n\n\nConstant\n\n\n-1.298*** (0.065)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n5,650.709\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\n\ncode\ndpm4 &lt;-glm(dispute ~ stime+stime2+stime3+deml+caprat, family=binomial(link=\"logit\"), data=survivaldp )\n\n# copy estimation data for avg effects\ndppred &lt;- survivaldp\n\n# df for output\ndf &lt;- data.frame(time=seq(1,34,1))\nfor (d in seq(-10,10,10)) {\n    df[paste0(\"p\", d+10)] &lt;- NA\n  df[paste0(\"Se\", d+10)] &lt;- NA\n}\n\n# predictions\nfor (d in seq(-10,10,10)) {\n  dppred$deml &lt;- d\n for (t in seq(1,34,1)) {  \n  dppred$stime &lt;- t\n  dppred$stime2 &lt;- t^2 \n  dppred$stime3 &lt;- t^3\n  pred &lt;- predict(dpm4, newdata=dppred, type=\"response\", se=TRUE)\n  df[t, paste0(\"p\", d+10)] &lt;- mean(pred$fit, na.rm=TRUE)\n  df[t, paste0(\"Se\", d+10)] &lt;- mean(pred$se.fit, na.rm=TRUE)\n  df$time[t] &lt;- t\n}\n}\n\n# plot\nggplot(df, aes(x=time, y=p0)) +\n  geom_line(color=\"#005A43\") +\n  geom_ribbon(aes(ymin=p0-1.96*Se0, ymax=p0+1.96*Se0), fill=\"#6CC24A\", alpha=0.4) +\n  geom_line(aes(y=p10), color=\"#005A43\") +\n  geom_ribbon(aes(ymin=p10-1.96*Se10, ymax=p10+1.96*Se10), fill=\"#A7DA92\", alpha=0.4) +\n  geom_line(aes(y=p20), color=\"#005A43\") +\n  geom_ribbon(aes(ymin=p20-1.96*Se20, ymax=p20+1.96*Se20), fill=\"#005A43\", alpha=0.4) +\n  labs(x=\"Time since last dispute\", y=\"Pr(Dispute)\") +\n  theme_minimal()+\n  theme(panel.grid.major = element_line(color = \"black\",\n                                        size = 0.25,\n                                        linetype = 1)) +\n  theme(panel.grid.minor = element_line(color = \"black\",\n                                        size = 0.15,\n                                        linetype = 1))"
  },
  {
    "objectID": "discretehazards24.html#an-alternative---transitions",
    "href": "discretehazards24.html#an-alternative---transitions",
    "title": "Discrete Time Hazard Models",
    "section": "An alternative - transitions",
    "text": "An alternative - transitions\nWhat happens if we lag \\(y\\) as we might with a continuous variable (say, in OLS), such that the estimation model is\n\\[y_t = \\beta_0 + \\beta_1(x_1) + \\ldots + \\gamma(y_{t-1}) + \\varepsilon\\]\nWith binary time series data, lagging \\(y\\) would measure changes of state - these are a class known as transition models (there are a variety of these).\nIn general, these are interactive models where\n\\[Pr(y_i=1) = F(x_{i,t}\\beta + y_{i,t-1}*x_{i,t} \\gamma)\\]\nand \\(\\gamma\\) measures the difference in effect when \\(y_{i,t-1} = 0\\) (this is just \\(\\beta\\)), and when \\(y_{i,t-1}  = 1\\); denote this \\(\\alpha\\). So \\(\\gamma = \\beta - \\alpha\\). That difference indicates the conditional probability of state transitions from the state where \\(y\\) takes on one value, to the state where it takes on the other value.\nTransition models are useful, but measure something fundamentally different from the latent hazard rate, or the chances of failure given a history of survival. Put differently, lagging \\(y\\) in a binary variable model does not measure memory or persistence; it does not measure the extent to which the observed value today depends on the value yesterday; it does not measure how the latent probability of failure today depends on surviving through yesterday."
  },
  {
    "objectID": "binaryextensions224.html",
    "href": "binaryextensions224.html",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "",
    "text": "The binary model serves as the root, or really a special case, of many other models. More generally, the binomial distribution informs many models beyond the binary variable ones.\n\ninstead of a binary choice, we can consider multiple choices simply by extending the utility model logic, and expanding the on/off switches in the LLF (weeks 8, 9).\nwe can consider multiple ordered choices (we’ll look at this today).\nwe can relax the assumption observations are independent over time, that the DGP has no memory (discrete hazards - last time).\nwe can relax the constant variance assumption (today).\nwe can relax the symmetry assumption in the link function (2 weeks ago).\nwe can take binary events, binomially distributed, count them up over time, and treat them as discrete (Poisson) event counts (week 10).\n\nToday, we’ll discuss models for ordered \\(y\\) variables, and how to address non-constant variance in the probit model."
  },
  {
    "objectID": "binaryextensions224.html#symmetry-and-asymmetry",
    "href": "binaryextensions224.html#symmetry-and-asymmetry",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Symmetry and Asymmetry",
    "text": "Symmetry and Asymmetry\nCompare three CDFs: the logistic, the clog-log, and a skewed logit function. The logistic is symmetric, the clog-log and skewed logit are not. You should notice the probability associated with x=0 for each function - the logistic is .5, the clog-log is about .64, and the skewed logit is about than .71.\nFor skewed binary \\(y\\) variables, it could be that one of these CDFs is a more appropriate link function than the symmetric logistic or normal. Implementing these merely requires substituting the appropriate CDF into the log-likelihood function.\n\n\ncode\n# Load required libraries\nlibrary(highcharter)\nlibrary(dplyr)\n\n# Binghamton University colors\nbinghamton_green &lt;- \"#005A43\"\nbinghamton_gray &lt;- \"#707070\"\nbinghamton_yellow &lt;- \"#FFC726\"\n\n# Generate data\nx &lt;- seq(-5, 5, length.out = 1000)\nlogistic_cdf &lt;- plogis(x)\ncloglog_cdf &lt;- 1 - exp(-exp(x))\n\n# Skewed logit function (shape parameter = 0.5)\n\nskewed_logit_cdf &lt;- 1 / (1 + exp(-x)) ^ 0.5 \n\n# Create data frame\ndf &lt;- data.frame(x = x, logistic = logistic_cdf, cloglog = cloglog_cdf, skewed_logit = skewed_logit_cdf)\n\n# Create the highchart\nhc &lt;- highchart() %&gt;%\n  hc_chart(type = \"line\") %&gt;%\n  hc_title(text = \"Comparison of CDFs: Logistic, Clog-log, and Skewed Logit\") %&gt;%\n  hc_xAxis(\n    title = list(text = \"x\"),\n    plotLines = list(\n      list(\n        color = \"#999\",\n        width = 2,\n        value = 0,\n        zIndex = 3,\n        label = list(text = \"x = 0\")\n      )\n    )\n  ) %&gt;%\n  hc_yAxis(\n    title = list(text = \"Cumulative Probability\"),\n    plotLines = list(\n      list(\n        color = \"#999\",\n        width = 2,\n        value = 0.5,\n        zIndex = 3,\n        label = list(text = \"y = 0.5\")\n      )\n    )\n  ) %&gt;%\n  hc_tooltip(\n    shared = TRUE,\n    formatter = JS(\"function() {\n      return 'x: ' + this.x.toFixed(4) + '&lt;br&gt;' +\n             'Logistic: ' + this.points[0].y.toFixed(4) + '&lt;br&gt;' +\n             'Clog-log: ' + this.points[1].y.toFixed(4) + '&lt;br&gt;' +\n             'Skewed Logit: ' + this.points[2].y.toFixed(4);\n    }\")\n  ) %&gt;%\n  hc_plotOptions(series = list(marker = list(enabled = FALSE))) %&gt;%\n  \n  # Add logistic CDF\n  hc_add_series(\n    data = df,\n    type = \"line\",\n    name = \"Logistic\",\n    color = binghamton_green,\n    hcaes(x = x, y = logistic)\n  ) %&gt;%\n  \n  # Add clog-log CDF\n  hc_add_series(\n    data = df,\n    type = \"line\",\n    name = \"Clog-log\",\n    color = binghamton_gray,\n    hcaes(x = x, y = cloglog)\n  ) %&gt;%\n  \n  # Add skewed logit CDF\n  hc_add_series(\n    data = df,\n    type = \"line\",\n    name = \"Skewed Logit\",\n    color = binghamton_yellow,\n    hcaes(x = x, y = skewed_logit)\n  )\n\n# Display the chart\nhc"
  },
  {
    "objectID": "binaryextensions224.html#skewed-logit",
    "href": "binaryextensions224.html#skewed-logit",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Skewed Logit",
    "text": "Skewed Logit\nNagler (1994) proposes the skewed logit (scobit) model - it’s a binary response model, the usual LLF, with a different link (the Burr-10):\n\\[ Pr(y=1) = \\frac{1}{(1+e^{-x\\beta})^\\alpha}\\]\nNote that if \\(\\alpha=1\\) this is the logistic CDF. If it is less than 1, the fastest rate of change is at \\(Pr(y =1 &lt; .5)\\); when greater than 1, the fastest rate of change, is at \\(Pr(y=1 &gt; .5)\\)\nNagler’s logic is that symmetric links require the assumption that individuals in the model are most sensitive to the effects of the \\(X\\) variables at or around \\(Pr(y=1) = .5\\). Looking at the (symmetric) logit curve above, you can see that’s where the derivative with respect to changes in \\(x\\) is greatest. If \\(y\\) is about half ones, half zeros, this may make sense - but often, we have \\(y\\) variables that are not symmetrically distributed like this. It makes sense in such cases not to assume the fastest rate of change, and the transition point from zero to one, is at \\(Pr(y=1) = .5\\).\nThe scobit model allows us to estimate the \\(\\alpha\\) parameter, which tells us where the fastest rate of change is in the CDF - that is, the transition point is an empirical question, not an assumption.\nThe model appears rarely in the political science literature; a Google Scholar search indicates most of its use is transportation analysis. A cursory survey also indicates the scobit estimates are often not that different from logit estimates. Estimation sometimes is funky insofar as we cannot always tell if changes in the likelihood are due to changes in \\(\\beta\\) or in \\(\\alpha\\)."
  },
  {
    "objectID": "binaryextensions224.html#symmetry-1",
    "href": "binaryextensions224.html#symmetry-1",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Symmetry",
    "text": "Symmetry\nThe big point here is not that we should or should not use the scobit, but that we need to be very aware that the assumption in models with symmetric links is that the biggest effect of an \\(x\\) variable is at \\(Pr(y=1) = 0.5\\) which is where \\(x\\beta=0\\)."
  },
  {
    "objectID": "binaryextensions224.html#why-does-symmetry-matter",
    "href": "binaryextensions224.html#why-does-symmetry-matter",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Why does symmetry matter?",
    "text": "Why does symmetry matter?\n\nsymmetry determines where the greatest effect of \\(x\\) is.\nsymmetry ensures rates of change above and below \\(x=0\\) are the same as they approach the limits.\nsymmetry implies the theoretical threshold, \\(\\tau\\), separating observed zeros and ones is \\(\\tau=0.5\\).\nif we want to use the model to generate predicted values of \\(y\\) (rather than \\(y^*\\)), we need some threshold for classification.\n\nSome (very few) questions naturally link to a clear threshold like 0.5 …election outcomes? But …are we measuring the correct outcome variable?"
  },
  {
    "objectID": "binaryextensions224.html#confusion-matrix",
    "href": "binaryextensions224.html#confusion-matrix",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nThe “confusion matrix” (I didn’t make this up) illustrates that intersection and identifies where our classification is “confused.”\n\n\n\n\ncode\nlibrary(kableExtra)\n# opts &lt;- options(knitr.kable.NA = \"\")\ndf &lt;- data.frame(\"Predicted Positive\" = c(\"True Positive\", \"False Positive\"), \"Predicted Negative\" = c(\"False Positive\", \"True Negative\"), \"Rate\" = c(\"TPR=TP/P\", \"FPR=FP/N\"))\nrow.names(df) &lt;- c(\"Observed Positive\", \"Observed Negative\")\n\ntbl &lt;- kbl(df, align = rep('c', 4)) |&gt; \n  column_spec(2:4, border_left = T) |&gt; \n  column_spec(1,  bold=T) |&gt;\n  column_spec(1:4, width = \"2cm\", color = 'white', background = '#005A43', include_thead = TRUE) |&gt;\n  column_spec(2:4, background = \"inherit\", color=\"inherit\") |&gt;\n  row_spec(1, extra_css = \"border-bottom: 1px solid\") |&gt;\n  kable_paper(\"hover\", bootstrap_options = c(\"condensed\", \"responsive\"), full_width = F, font_size = 20)\ntbl\n\n\n\n\n\n\nPredicted.Positive\nPredicted.Negative\nRate\n\n\n\n\nObserved Positive\nTrue Positive\nFalse Positive\nTPR=TP/P\n\n\nObserved Negative\nFalse Positive\nTrue Negative\nFPR=FP/N\n\n\n\n\n\n\n\n\n\n\n\nTrue Positive Rate: correctly classify positive outcomes. This is often called “sensitivity.”\nFalse Positive Rate: we incorrectly classify negative outcomes (\\(y=0\\)) as positive (\\(y=1\\)). This is often called “1-specificity.” Specificity is the True Negative Rate, or the probability of correctly classifying a negative outcome (\\(y=0\\))."
  },
  {
    "objectID": "binaryextensions224.html#using-the-confusion-matrix-to-measure-model-fit",
    "href": "binaryextensions224.html#using-the-confusion-matrix-to-measure-model-fit",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Using the Confusion Matrix to Measure Model Fit",
    "text": "Using the Confusion Matrix to Measure Model Fit\nSo here’s the deal:\n\nestimate the model.\ngenerate the predicted probability \\(y=1\\) for each observation.\nassume a threshold separating zeros and ones; usually \\(\\tau=0.5\\).\nif \\(Pr(y=1 \\geq 0.5)\\), predict a positive outcome (predict \\(y=1\\)).\nif \\(Pr(y=0 &lt; 0.5)\\), predict a negative outcome (predict \\(y=0\\)).\nusing the observed and predicted outcomes, generate a confusion table, and compute measures of fit like “percent correctly predicted” (PCP) and “proportional reduction of error” (PRE).\n\n\nPercent Correctly Predicted (PCP)\nOne thing we can do is sum the main diagonal and divide by the estimation sample: \\((TP+TN)/N\\). This gives us the Percent Correctly Predicted (PCP).\n\n\nProportional Reduction of Error (PRE)}\nA second thing is to compute the Proportional Reduction of Error (PRE) - the difference between the correct predictions of the model versus a null model usually the unconditional frequency of \\(y\\) - in other words, we use the Percent in the Modal Category (PCM) of the \\(y\\) variable.\n\\[\\frac{correct_{\\text{informed}} - correct_{\\text{null}}}{N - correct_{\\text{null}}} \\]\nor\n\\[\\frac{PCP- PMC}{1-PMC} \\]\n\n\nExample: NAFTA vote, 1993\nSo here’s a model predicting members of the US House votes for or against the NAFTA treaty in 1993: 1 is a “yes” vote; the \\(x\\) variables are party (Democrat) and an ideology score.\n\n\ncode\nnafta &lt;- read.csv(\"NAFTA.csv\")\n\nnaftamodel &lt;- glm(vote ~ democrat + cope93, data=nafta, family=binomial(link=\"logit\"))\n\nstargazer(naftamodel, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: NAFTA vote\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Party (Democrat=1)\", \"Ideology (COPE score)\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: NAFTA vote\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParty (Democrat=1)\n\n\n1.990*** (0.504)\n\n\n\n\nIdeology (COPE score)\n\n\n-0.054*** (0.007)\n\n\n\n\nConstant\n\n\n2.380*** (0.272)\n\n\n\n\n\n\n\n\nObservations\n\n\n434\n\n\n\n\nAkaike Inf. Crit.\n\n\n465.904\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\nAnd here’s the confusion matrix from that model assuming a threshold of \\(\\tau=.5\\) - it compares the observed vote frequency against how we classify our predictions based on a predicted probability greater than or less than 0.5. This is generated using the caret package in R.\n\n\ncode\n# Load required libraries\nlibrary(caret)\n\n# Assuming you have a fitted GLM object called 'glm_model'\n# and test data 'test_data' with actual outcomes in 'test_data$actual'\n\ntest_data &lt;- nafta %&gt;% dplyr::select(vote, democrat, cope93) %&gt;% mutate(actual = vote)\n# Make predictions on the test data\npredictions &lt;- predict(naftamodel, newdata = test_data, type = \"response\")\n\n# Create confusion matrix\n# You may need to adjust the threshold (default is 0.5)\npredicted_classes &lt;- ifelse(predictions &gt; 0.5, 1, 0)\nconf_matrix &lt;- confusionMatrix(factor(predicted_classes), factor(test_data$actual))\n\n# Print confusion matrix\nprint(conf_matrix)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 160  77\n         1  40 157\n                                         \n               Accuracy : 0.7304         \n                 95% CI : (0.686, 0.7716)\n    No Information Rate : 0.5392         \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      \n                                         \n                  Kappa : 0.4647         \n                                         \n Mcnemar's Test P-Value : 0.0008741      \n                                         \n            Sensitivity : 0.8000         \n            Specificity : 0.6709         \n         Pos Pred Value : 0.6751         \n         Neg Pred Value : 0.7970         \n             Prevalence : 0.4608         \n         Detection Rate : 0.3687         \n   Detection Prevalence : 0.5461         \n      Balanced Accuracy : 0.7355         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\nYou should see the main diagonal presents the number of correct predictions - the off-diagonal elements are the incorrect predictions. If we sum the main diagonal and divide by \\(N\\), we get the Percent Correctly Predicted (PCP). In this case, the PCP is 0.735 - 73.5% of the votes are correctly predicted.\nThis all depends on the threshold (.5) - in the case of a Congressional vote, especially a relatively close vote like this one, the threshold might not be crazy. But it might be in other cases, and arbitrarily choosing a value for \\(\\tau\\) is problematic. So another approach is to compute the ROC curve.\nWhat makes this work relatively well in the NAFTA context? As you’ll see below, it works less well in the democratic peace models.\n\n\nExample: Democratic Peace\nHere’s a basic democratic peace model:\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\ndpm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n\nstargazer(dpm1, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\nLet’s generate the confusion matrix for the democratic peace model, threshold set at \\(\\tau =0.5\\):\n\n\ncode\nlibrary(kableExtra)\n# opts &lt;- options(knitr.kable.NA = \"\")\ndf &lt;- data.frame(\"Predicted Positive\" = c(\"0\", \"0\"), \"Predicted Negative\" = c(\"897\", \"19,245\"), \"Rate\" = c(\"TPR=0/897=0\", \"FPR=0/19,245=0\"))\nrow.names(df) &lt;- c(\"Observed Positive\", \"Observed Negative\")\n\ntbl &lt;- kbl(df, align = rep('c', 4)) |&gt; \n  column_spec(2:4, border_left = T) |&gt; \n  column_spec(1,  bold=T) |&gt;\n  column_spec(1:4, width = \"2cm\", color = 'white', background = '#005A43', include_thead = TRUE) |&gt;\n  column_spec(2:4, background = \"inherit\", color=\"inherit\") |&gt;\n  row_spec(1, extra_css = \"border-bottom: 1px solid\") |&gt;\n  kable_paper(\"hover\", bootstrap_options = c(\"condensed\", \"responsive\"), full_width = F, font_size = 20)\ntbl\n\n\n\n\n\n\nPredicted.Positive\nPredicted.Negative\nRate\n\n\n\n\nObserved Positive\n0\n897\nTPR=0/897=0\n\n\nObserved Negative\n0\n19,245\nFPR=0/19,245=0\n\n\n\n\n\n\n\nThe problem is \\(\\tau\\); at \\(\\tau=.5\\) we get none of the ones correct. Here’s the democratic peace at \\(\\tau = 0.1\\):\n\n\ncode\nlibrary(kableExtra)\n# opts &lt;- options(knitr.kable.NA = \"\")\ndf &lt;- data.frame(\"Predicted Positive\" = c(\"303\", \"1,764\"), \"Predicted Negative\" = c(\"594\", \"17,481\"), \"Rate\" = c(\"TPR=303/897=.34\", \"FPR=1,764/19,245=0.092\"))\nrow.names(df) &lt;- c(\"Observed Positive\", \"Observed Negative\")\n\ntbl &lt;- kbl(df, align = rep('c', 4)) |&gt; \n  column_spec(2:4, border_left = T) |&gt; \n  column_spec(1,  bold=T) |&gt;\n  column_spec(1:4, width = \"2cm\", color = 'white', background = '#005A43', include_thead = TRUE) |&gt;\n  column_spec(2:4, background = \"inherit\", color=\"inherit\") |&gt;\n  row_spec(1, extra_css = \"border-bottom: 1px solid\") |&gt;\n  kable_paper(\"hover\", bootstrap_options = c(\"condensed\", \"responsive\"), full_width = F, font_size = 20)\ntbl\n\n\n\n\n\n\nPredicted.Positive\nPredicted.Negative\nRate\n\n\n\n\nObserved Positive\n303\n594\nTPR=303/897=.34\n\n\nObserved Negative\n1,764\n17,481\nFPR=1,764/19,245=0.092\n\n\n\n\n\n\n\nAt \\(\\tau = 0.1\\), we get a TPR = 0.34 and FPR = 0.092. The PCP = (303+17481)/20142 = 0.88 …which is lower, but we get about 1/3 of the ones correct."
  },
  {
    "objectID": "binaryextensions224.html#receiver-operator-characteristic-roc-curves",
    "href": "binaryextensions224.html#receiver-operator-characteristic-roc-curves",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Receiver-Operator Characteristic (ROC) Curves",
    "text": "Receiver-Operator Characteristic (ROC) Curves\nThe problem is choosing the threshold - imagine that we compute a confusion matrix for all possible thresholds, \\(\\tau=.01, .02, .03 \\ldots 1\\), then compute TPR and FPR, and plot them against one another. This is an ROC curve.\nROCs originate in efforts to distinguish signal from noise in radar returns - the British built a radar system before WWII to detect German air attacks; they had the problem of distinguishing planes (signal) from flocks of geese (noise). As the turned up the sensitivity of the radar, they more often correctly detected planes, but they also lacked specificity and detected more geese too. So there was a tradeoff between sensitivity (correctly identifying positive signals as positive) and specificity (incorrectly identify negative signals as positive).\nROCs measure these two dimensions and graph them against one another:\n\nsensitivity - true positive rate at every possible latent threshold between zero and one.\n1- specificity - false positive rate at every possible latent threshold between zero and one. This is 1 minus the True Negative Rate\n\nHere’s the ROC for the NAFTA model - we’ll use the pROC package in R to compute the ROC and plot it:\n\n\ncode\nlibrary(pROC)\n\n# NAFTA\n\nnaftaroc &lt;- roc(test_data$actual, predictions, plot=TRUE, grid=TRUE, partial.auc.correct=TRUE,\n         print.auc=TRUE)"
  },
  {
    "objectID": "binaryextensions224.html#roc-intepretation",
    "href": "binaryextensions224.html#roc-intepretation",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "ROC Intepretation",
    "text": "ROC Intepretation\n\nthe diagonal is a model guessing randomly zero or one - no better than a coin toss.\nabove that line, the model is improving our classification over random guesses.\nthe top left corner would indicate a model that classifies perfectly - 100% sensitivity (TPR), and 0% FPR.\nbelow the diagonal line, the model is classifying worse than a coin toss would.\nwe can compute the Area Under the Curve (AUC) as a percentage - AUC is often reported to indicate model fit. In the NAFTA model, the AUC is .843. AUC closer to one indicates better fit; closer to .5 indicates worse fit, similar to random guessing.\nwe can plot ROCs from different models on the same space and compare their fits.\nThe x-axis is 1-specificity, or the False Positive Rate."
  },
  {
    "objectID": "binaryextensions224.html#roc-democratic-peace",
    "href": "binaryextensions224.html#roc-democratic-peace",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "ROC Democratic Peace",
    "text": "ROC Democratic Peace\nHere’s we compare fit for two models, one including “borders,” the other excluding it. Here are the two models :\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\ndpm1 &lt;-glm(dispute ~ border+deml+caprat+ally+border, family=binomial(link=\"logit\"), data=dp )\ndpm2 &lt;-glm(dispute ~ deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n  \nstargazer(list(dpm1,dpm2), type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n-0.078*** (0.007)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n-0.005*** (0.0005)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n-0.374*** (0.076)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n-2.979*** (0.064)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n7,262.635\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\nAnd compute the ROC for each model - here Claude.ai and I have written a function to compute the ROC and AUC for each model, and then plot them on the same space.\n\n\ncode\n# part written by Claude.ai\n# compute ROC curve\ncompute_roc &lt;- function(actual, predictions) {\n  # Sort actual and predictions in descending order of predicted probabilities\n  sorted_data &lt;- data.frame(actual, predictions)\n  sorted_data &lt;- sorted_data[order(sorted_data$predictions, decreasing = TRUE), ]\n\n  # Initialize variables\n  n_positive &lt;- sum(actual)\n  n_negative &lt;- length(actual) - n_positive\n  tp &lt;- 0\n  fp &lt;- 0\n  tpr &lt;- c()\n  fpr &lt;- c()\n\n  # Iterate over sorted data\n  for (i in 1:nrow(sorted_data)) {\n    if (sorted_data$actual[i] == 1) {\n      tp &lt;- tp + 1\n    } else {\n      fp &lt;- fp + 1\n    }\n\n    # Calculate true positive rate (TPR) and false positive rate (FPR)\n    tpr &lt;- c(tpr, tp / n_positive)\n    fpr &lt;- c(fpr, fp / n_negative)\n  }\n\n  # Create ROC curve\n  roc_curve &lt;- data.frame(fpr, tpr)\n  return(roc_curve)\n}\n\n# Function to compute AUC\ncompute_auc &lt;- function(fpr, tpr) {\n  # Sort FPR and TPR\n  ord &lt;- order(fpr)\n  fpr &lt;- fpr[ord]\n  tpr &lt;- tpr[ord]\n  \n  # Compute AUC using trapezoidal rule\n  auc &lt;- sum((fpr[-1] - fpr[-length(fpr)]) * (tpr[-1] + tpr[-length(tpr)]) / 2)\n  return(auc)\n}\n\n# Democratic Peace models\n\ntest_dataB &lt;- dp %&gt;% dplyr::select(border,deml,caprat,ally) %&gt;% \n  mutate(actual = dp$dispute)\n\ntest_dataNB &lt;- dp %&gt;% dplyr::select(deml,caprat,ally) %&gt;% \n  mutate(actual = dp$dispute)\n\n# Make predictions on the test data\npredictionsB &lt;- predict(dpm1, newdata = test_dataB, type = \"response\")\npredictionsNB &lt;- predict(dpm2, newdata = test_dataNB, type = \"response\")\n\n\nroc_curveB &lt;- compute_roc(test_dataB$actual, predictionsB)\n#AUC\nauc_border &lt;- compute_auc(roc_curveB$fpr, roc_curveB$tpr)\nroc_curveNB &lt;- compute_roc(test_dataNB$actual, predictionsNB)\nauc_noborder &lt;- compute_auc(roc_curveNB$fpr, roc_curveNB$tpr)\n\n# ROC Plot, pasting auc_border and auc_noborder on plot\n\nggplot() +\n  geom_line(data = roc_curveB, aes(x = fpr, y = tpr, color = \"Borders\")) +\n  geom_line(data = roc_curveNB, aes(x = fpr, y = tpr, color = \"No Borders\")) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"solid\", color = \"red\") +\n  labs(title = \"ROC Curve: Democratic Peace\", x = \"False Positive Rate\", y = \"True Positive Rate\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"Borders\" = \"#005A43\", \"No Borders\" = \"#6CC24A\"))+\n  annotate(\"text\", x = 0.75, y = 0.5, label = paste(\"AUC Model 1: \", round(auc_border, 2)), color = \"#005A43\") +\n  annotate(\"text\", x = 0.75, y = 0.4, label = paste(\"AUC Model 2: \", round(auc_noborder, 2)), color = \"#6CC24A\")\n\n\n\n\n\n\n\n\n\ncode\n#bucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nAlso, note the measure Area Under the Curve (AUC) for each model - the AUC is often reported to indicate model fit. The AUC for the model including borders is 0.75, while the AUC for the model excluding borders is 0.72. A model with an AUC of 0.5 is no better than a coin toss, while a model with an AUC of 1 is perfect."
  },
  {
    "objectID": "binaryextensions224.html#single-coefficient-estimates",
    "href": "binaryextensions224.html#single-coefficient-estimates",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Single Coefficient Estimates",
    "text": "Single Coefficient Estimates\nOne property of MLEs is they are asymptotically multivariate normal; inference is straightforward because the variances are also normal so the ratio of \\(\\beta /se\\) is a z-score."
  },
  {
    "objectID": "binaryextensions224.html#model-evaluation",
    "href": "binaryextensions224.html#model-evaluation",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nMost commonly, we evaluate model fit using one of the “trinity” of tests:\n\nlog-likelihood ratio tests (LLR)\nWald tests\nLagrange Multiplier tests (LM)\n\nThe first two are the most common, and it’s not clear one is better than the other.\n\nLog-Likelihood Ratio Test\nThe LLR test requires estimating two models - a null or constrained model, (\\(M_0\\)), and informed (unconstrained) model (\\(M_1\\)) - it compares the heights of the log-likelihood functions of the two models:\n\\[ \\chi^2 = -2 (ln\\mathcal{L}(M_0) - ln\\mathcal{L}(M_1))  \\]\nThe log-likelihoods here are literally the values of the \\(ln\\mathcal{L}\\) at the estimated maxima of the functions. Their difference is distributed \\(\\chi^2\\) with \\(k_1-k_0\\) degrees of freedom.\nThe LLR is simple to compute (you can do it in your head), but requires estimating two nested models. Recall, two models are nested iff the regressors in the constrained model are a strict subset of those in the unconstrained model, and the samples are identical.\n\n\nWald \\(\\chi^2\\)\nThe Wald test is similar, but only requires the unconstrained or informed model. During maximization, it examines the distance between \\(M_0\\) and \\(M_1\\), and weights that distance by the rate of change between the two (second derivative). If the distance is large and the rate of change is fast, the informed model improves a good bit on the uninformed one. You can imagine other combinations of distance and curvature. Long (p. 88) has a great illustration of this.\n\n\nIn Practice\nThese two tests are asymptotically equivalent. In practice, it makes little difference in most cases which you choose, provided the models are nested.\nStata reports LLR for most models, but reports Wald if you request robust standard errors.\n\n\nLimits\nThe limits of these tests is they apply only to nested models - models where the regressors in the constrained model are a strict subset of those in the unconstrained model and where the samples are identical.\n\n\nInformation Criterion Tests\nAlternatively, we can use information criterion tests - the two most common are the Akaike and Bayesian Information Criteria tests (AIC, BIC). These are both formulated to penalize likelihoods for the number of parameters estimated; this in effect rewards better specification (good variables, but few) and penalizes “garbage can” approaches (including lots of poor predictors).\nIC tests are useful for either nested or nonnested models. This is a significant though under-appreciated virtue of such tests.\n\n\nAkaike and Bayesian Information Criterion tests\n\\[AIC =  -2 ln(\\mathcal{L}) + 2k \\]\n\\[BIC =  -2 ln(\\mathcal{L}) + ln(N) k \\]\nwhere \\(k\\) is the number of parameters.\n\nProcess\nAIC: Estimate model 1; generate the AIC. Estimate model 2; generate the AIC. The model with the smaller AIC is the preferred model (see Long 1997: 110).\nBIC: Estimate model 1; generate the BIC. Estimate model 2; generate the BIC. Compute \\(BIC_1 - BIC_2\\) - the smaller BIC value is the preferable model. The strength of the test statistic is given by Rafferty (1996): absolute value of this difference 0-2 = weak; 2-6= Positive; 6-10= Strong; greater than 10 = Very Strong (see Long 1997: 112)."
  },
  {
    "objectID": "binaryextensions224.html#rare-events-logit",
    "href": "binaryextensions224.html#rare-events-logit",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Rare Events Logit",
    "text": "Rare Events Logit\nWhat they propose is:\n\nSelect all the cases with events (failures).\nRandomly choose a sample of the non-event (censored) cases (they say 2-5 times the size of the failure group).\nThis smaller sample makes data collection possible (compared to the gigantic number of zeros in some event data).\nEstimate a logit on the new, smaller sample, and adjust the estimates for the sample.\n\nThe Rare Events Logit doesn’t appear in the literature much, though it’s not uncommon for reviewers to ask for it. In my experience inferences from this model don’t vary much from the usual logit. The model does present a major opportunity for data collection efforts."
  },
  {
    "objectID": "binaryextensions224.html#footnotes",
    "href": "binaryextensions224.html#footnotes",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nsee what I did there?↩︎"
  },
  {
    "objectID": "ex22024answers.html",
    "href": "ex22024answers.html",
    "title": "Exercise #2 Answers",
    "section": "",
    "text": "Please answer the following questions. All answers should be coded in R and submitted in PDF format, created either in LaTeX or in R Markdown (Quarto). Please turn in both R scripts and PDFs on Brightspace.\nFor this assignment, the only R libraries you need (should use) are\nThis is the hardest assignment of the semester. I want you to know that, and to know that I know it’s hard. Work hard to get this stuff - it’ll pay off.\nThe assignment asks you to write a program to estimate binomial regression models with different link functions. The aim here is to emphasize the parts of the log-likelihood function, and how straightforwardly we can change the link function. The assignment also is designed to push on the programming skill of writing functions.\nShould you pull parts of my code from the maximization slides? Yes, of course. Should you consult AI bots? As a secondary source, sure, but please don’t start there. Please do not rely on bots to write this because I’ll ask you to explain your work, and things will devolve accordingly."
  },
  {
    "objectID": "ex22024answers.html#logit",
    "href": "ex22024answers.html#logit",
    "title": "Exercise #2 Answers",
    "section": "Logit",
    "text": "Logit\n\n\ncode\nces &lt;- read.csv(\"/users/dave/documents/teaching/606J-mle/2024/exercises/ex1/ces.csv\", header=TRUE)\n\ncesanalysis &lt;- ces %&gt;% mutate(pro = ifelse(prochoice == \"Support\", 1, 0), concealw = ifelse(conceal == \"Support\", 1, 0), buildwall = ifelse(wall == \"Support\", 1, 0), repealaca=ifelse(aca == \"Support\", 1, 0), white=ifelse(race == \"White\", 1, 0), vote=ifelse(votechoice == \"Joe Biden (Democrat)\", 1, ifelse(votechoice == \"Donald J. Trump (Republican)\", 0, NA))) \n\n# remove all na values\n\ncesanalysis &lt;- na.omit(cesanalysis)\n\n# llf\nllf &lt;- function(beta, y, X) {\n  probs &lt;- plogis(X %*% beta)\n  loglik &lt;- sum(y * log(probs) + (1 - y) * log(1 - probs))\n  return(loglik)\n}\n\n# logit set up - enter data\nlogit&lt;- function(data, y_var, x_vars) {\n  \n  # Prepare the data, add constant\n  y &lt;- data[[y_var]]\n  X &lt;- as.matrix(cbind(1, data[, x_vars]))\n\n  # starting values\n  start_b &lt;- rep(0, ncol(X))\n\n#Maximize the log-likelihood\n  result &lt;- maxLik(logLik = llf,\n                   start = start_b,\n                   method = \"BFGS\",\n                   y = y,\n                   X = X)\n\n  # Return the results\n  return(result)\n}\n\n\nm1 &lt;- logit(cesanalysis, y_var = \"vote\", x_vars = c(\"age\", \"white\", \"pro\"))\nsummary(m1)\n\n\n--------------------------------------------\nMaximum Likelihood estimation\nBFGS maximization, 47 iterations\nReturn code 0: successful convergence \nLog-Likelihood: -18186.29 \n4  free parameters\nEstimates:\n      Estimate Std. error t value Pr(&gt; t)    \n[1,]  0.716538   0.044679   16.04  &lt;2e-16 ***\n[2,] -0.015078   0.000482  -31.29  &lt;2e-16 ***\n[3,] -1.372134   0.035957  -38.16  &lt;2e-16 ***\n[4,]  2.899563   0.027328  106.10  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\ncode\nmglm &lt;- glm(vote ~ age + white + pro, data = cesanalysis, family = binomial(link = \"logit\"))\nsummary(mglm)\n\n\n\nCall:\nglm(formula = vote ~ age + white + pro, family = binomial(link = \"logit\"), \n    data = cesanalysis)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.7165192  0.0578847   12.38   &lt;2e-16 ***\nage         -0.0150772  0.0008272  -18.23   &lt;2e-16 ***\nwhite       -1.3721731  0.0408534  -33.59   &lt;2e-16 ***\npro          2.8995397  0.0269491  107.59   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 53824  on 39751  degrees of freedom\nResidual deviance: 36373  on 39748  degrees of freedom\nAIC: 36381\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nUsing the same program structure, alter the program to estimate a probit model.\nEstimate the CES model using your probit program, and report the results in a professional table. Compare them to the GLM model from last week - are they the same? [no need to report the GLM, just comment on the comparison.]"
  },
  {
    "objectID": "ex22024answers.html#probit",
    "href": "ex22024answers.html#probit",
    "title": "Exercise #2 Answers",
    "section": "Probit",
    "text": "Probit\n\n\ncode\nces &lt;- read.csv(\"/users/dave/documents/teaching/606J-mle/2024/exercises/ex1/ces.csv\", header=TRUE)\n\ncesanalysis &lt;- ces %&gt;% mutate(pro = ifelse(prochoice == \"Support\", 1, 0), concealw = ifelse(conceal == \"Support\", 1, 0), buildwall = ifelse(wall == \"Support\", 1, 0), repealaca=ifelse(aca == \"Support\", 1, 0), white=ifelse(race == \"White\", 1, 0), vote=ifelse(votechoice == \"Joe Biden (Democrat)\", 1, ifelse(votechoice == \"Donald J. Trump (Republican)\", 0, NA))) \n\n# remove all na values\n\ncesanalysis &lt;- na.omit(cesanalysis)\n\n# llf\nllf &lt;- function(beta, y, X) {\n  probs &lt;- pnorm(X %*% beta)\n  loglik &lt;- sum(y * log(probs) + (1 - y) * log(1 - probs))\n  return(loglik)\n}\n\n# logit set up - enter data\nprobit&lt;- function(data, y_var, x_vars) {\n  \n  # Prepare the data, add constant\n  y &lt;- data[[y_var]]\n  X &lt;- as.matrix(cbind(1, data[, x_vars]))\n\n  # starting values\n  start_b &lt;- rep(0, ncol(X))\n\n#Maximize the log-likelihood\n  result &lt;- maxLik(logLik = llf,\n                   start = start_b,\n                   method = \"BFGS\",\n                   y = y,\n                   X = X)\n\n  # Return the results\n  return(result)\n}\n\n\nm1 &lt;- probit(cesanalysis, y_var = \"vote\", x_vars = c(\"age\", \"white\", \"pro\"))\nsummary(m1)\n\n\n--------------------------------------------\nMaximum Likelihood estimation\nBFGS maximization, 71 iterations\nReturn code 0: successful convergence \nLog-Likelihood: -18208.16 \n4  free parameters\nEstimates:\n       Estimate Std. error t value Pr(&gt; t)    \n[1,]  0.3303225  0.0264991   12.46  &lt;2e-16 ***\n[2,] -0.0084557  0.0004428  -19.09  &lt;2e-16 ***\n[3,] -0.7513117  0.0216933  -34.63  &lt;2e-16 ***\n[4,]  1.7341769  0.0153122  113.25  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\ncode\nmglm &lt;- glm(vote ~ age + white + pro, data = cesanalysis, family = binomial(link = \"probit\"))\nsummary(mglm)\n\n\n\nCall:\nglm(formula = vote ~ age + white + pro, family = binomial(link = \"probit\"), \n    data = cesanalysis)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.3301610  0.0331127   9.971   &lt;2e-16 ***\nage         -0.0084563  0.0004656 -18.162   &lt;2e-16 ***\nwhite       -0.7510290  0.0227045 -33.078   &lt;2e-16 ***\npro          1.7340770  0.0151102 114.762   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 53824  on 39751  degrees of freedom\nResidual deviance: 36416  on 39748  degrees of freedom\nAIC: 36424\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nUsing the same program structure, please write a program to estimate a cloglog regression (so the link function is the cloglog).\nEstimate the CES model using your cloglog program, and report the results in a professional table."
  },
  {
    "objectID": "ex22024answers.html#cloglog",
    "href": "ex22024answers.html#cloglog",
    "title": "Exercise #2 Answers",
    "section": "Cloglog",
    "text": "Cloglog\n\n\ncode\nces &lt;- read.csv(\"/users/dave/documents/teaching/606J-mle/2024/exercises/ex1/ces.csv\", header=TRUE)\n\ncesanalysis &lt;- ces %&gt;% mutate(pro = ifelse(prochoice == \"Support\", 1, 0), concealw = ifelse(conceal == \"Support\", 1, 0), buildwall = ifelse(wall == \"Support\", 1, 0), repealaca=ifelse(aca == \"Support\", 1, 0), white=ifelse(race == \"White\", 1, 0), vote=ifelse(votechoice == \"Joe Biden (Democrat)\", 1, ifelse(votechoice == \"Donald J. Trump (Republican)\", 0, NA))) \n\n# remove all na values\n\ncesanalysis &lt;- na.omit(cesanalysis)\n\n# llf\nllf &lt;- function(beta, y, X) {\n  probs &lt;- 1-exp(-exp(X %*% beta))\n  loglik &lt;- sum(y * log(probs) + (1 - y) * log(1 - probs))\n  return(loglik)\n}\n\n# logit set up - enter data\ncloglog&lt;- function(data, y_var, x_vars) {\n  \n  # Prepare the data, add constant\n  y &lt;- data[[y_var]]\n  X &lt;- as.matrix(cbind(1, data[, x_vars]))\n\n  # starting values\n  start_b &lt;- rep(0, ncol(X))\n\n#Maximize the log-likelihood\n  result &lt;- maxLik(logLik = llf,\n                   start = start_b,\n                   method = \"BFGS\",\n                   y = y,\n                   X = X)\n\n  # Return the results\n  return(result)\n}\n\n\nm1 &lt;- cloglog(cesanalysis, y_var = \"vote\", x_vars = c(\"age\", \"white\", \"pro\"))\nsummary(m1)\n\n\n--------------------------------------------\nMaximum Likelihood estimation\nBFGS maximization, 48 iterations\nReturn code 0: successful convergence \nLog-Likelihood: -18368.19 \n4  free parameters\nEstimates:\n       Estimate Std. error t value Pr(&gt; t)    \n[1,] -0.4177767  0.0307841  -13.57  &lt;2e-16 ***\n[2,] -0.0077632  0.0003905  -19.88  &lt;2e-16 ***\n[3,] -0.6305224  0.0197149  -31.98  &lt;2e-16 ***\n[4,]  1.9395862  0.0187080  103.68  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\ncode\nmglm &lt;- glm(vote ~ age + white + pro, data = cesanalysis, family = binomial(link = \"cloglog\"))\nsummary(mglm)\n\n\n\nCall:\nglm(formula = vote ~ age + white + pro, family = binomial(link = \"cloglog\"), \n    data = cesanalysis)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.4178023  0.0342270  -12.21   &lt;2e-16 ***\nage         -0.0077630  0.0004625  -16.78   &lt;2e-16 ***\nwhite       -0.6307838  0.0207458  -30.41   &lt;2e-16 ***\npro          1.9398906  0.0189437  102.40   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 53824  on 39751  degrees of freedom\nResidual deviance: 36736  on 39748  degrees of freedom\nAIC: 36744\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nUsing the cloglog estimates, please produce average effects and present them in a professional plot.\n\n\n\ncode\nmcloglog &lt;- glm(vote ~ age + white + pro, data = cesanalysis, family = binomial(link = \"cloglog\"))\n\n# Average effects\n\n\n# average effects\n#pro=0\ncesplotdata &lt;- cesanalysis\nmedianpred &lt;-numeric(length(95))\nage &lt;- 0\nmedianse &lt;- numeric(length(95))\nfor (i in seq(18, 95, 1)) {\n  cesplotdata$pro &lt;- 0\n  cesplotdata$age &lt;- i-17\n  predictions &lt;- data.frame(predict(mcloglog, type = \"response\", se=TRUE, newdata=cesplotdata))\n  medianpred[i-17]&lt;- median(predictions$fit, na.rm=TRUE)\n  medianse[i-17] &lt;- median(predictions$se.fit, na.rm=TRUE)\n  age[i-17] &lt;- i\n  pro &lt;-0\n}\n  p1&lt;- data.frame(age= age, pro=pro, xb = medianpred, se=medianse)\n# pro = 1\nfor (i in seq(18, 95, 1)) {\n  cesplotdata$pro &lt;- 1\n  cesplotdata$age &lt;- i-17\n  predictions &lt;- data.frame(predict(mcloglog, type = \"response\", se=TRUE, newdata=cesplotdata))\n  medianpred[i-17]&lt;- median(predictions$fit, na.rm=TRUE)\n  medianse[i-17] &lt;- median(predictions$se.fit, na.rm=TRUE)\n  age[i-17] &lt;- i\n  pro &lt;- 1\n}\n  p2&lt;- data.frame(age= age, pro= pro, xb = medianpred ,se=medianse)\nallpredictions &lt;- rbind(p1, p2)  \n  \n## predictions by prochoice using color=\"#005A43\" and \"#6CC24A\" for ribbon fills\n\nggplot(data=allpredictions, aes(x=age, y=xb, color=factor(pro))) +\n  geom_line() +\n  geom_ribbon(aes(ymin=xb-se, ymax=xb+se, fill=factor(pro)), alpha=.2) +\n  labs(y=\"Predicted Probability\", x=\"Age\") +\n  ggtitle(\"Predicted Probabilities of Voting for Biden by Age and Pro-Choice Stance\") +\n  scale_color_manual(values=c(\"#005A43\", \"#6CC24A\")) +\n  scale_fill_manual(values=c(\"#005A43\", \"#6CC24A\")) +\n  theme_minimal()"
  },
  {
    "objectID": "ex12024answers.html",
    "href": "ex12024answers.html",
    "title": "Exercise #1 Answers",
    "section": "",
    "text": "Please answer the following questions. All answers should be coded in R and submitted in PDF format, created either in LaTeX or in R Markdown (Quarto). Please turn in both R scripts and PDFs on Brightspace.\nFor this assignment, the only R libraries you need (should use) are\nThis assignment is aimed at developing coding skills/habits, an understanding of probability distributions, their relationship to data, and some basic data management.\nFor this section, please write out all PDF/CDF equations - that is, do not use built-in R functions (e.g. plogis, etc.).\ncode\nxb &lt;- runif(1000, min=-4, max=4)\nlogitcdf &lt;- 1/(1+exp(-xb))\ncauchycdf &lt;- 1/2 + atan(xb)/pi\nlogitpdf &lt;- exp(xb)/(1+exp(xb))^2\ncauchypdf &lt;- 1/(pi*(1+xb^2))\n\ndf &lt;- data.frame(xb, logitcdf, cauchycdf, logitpdf, cauchypdf)\n\n#cdf  \ncdf &lt;- ggplot(data=df, aes(x=xb, y=logitcdf)) +\n  geom_line() +\n  geom_line(aes(y=cauchycdf),  linetype=\"dotted\"  ) +\n  annotate(\"text\", x = 0, y = .9, label = \"Logistic\") +\n  annotate(\"text\", x = -3, y = .15, label = \"Cauchy\") +\n  labs(y=\"Pr(Y=1)\",  x=expression(x*beta)) +\n  ggtitle(\"Cauchy and Logistic CDFs\")\n            \npdf &lt;- ggplot(data=df, aes(x=xb, y=logitpdf)) +\n  geom_line() +\n  geom_line(aes(y=cauchypdf), linetype=\"dotted\" ) +\n  annotate(\"text\", x = 0, y = .35, label = \"Cauchy\") +\n  annotate(\"text\", x = -2.8, y = .2, label = \"Logistic\") +\n  labs(y=\"Pr(Y=1)\", x=expression(x*beta)) +\n  ggtitle(\"Cauchy and Logistic PDFs\") +\n  theme_minimal()\n\npdf-cdf\nComment on the differences among the three.\ncode\n# plot CDFs for logistic, cloglog, and loglog distributions\n\nxb &lt;- seq(-4, 4, length.out=1000)\nlogistic &lt;- 1/(1+exp(-xb))\ncloglog &lt;- 1-exp(-exp(xb))\nloglog &lt;- exp(-exp(-xb)) \n\ndf &lt;- data.frame(xb, logistic, cloglog, loglog)\n\nggplot(data=df, aes(x=xb, y=logistic)) +\n  geom_line() +\n  geom_line(aes(y=cloglog), linetype=\"dotted\") +\n  geom_line(aes(y=loglog), linetype=\"twodash\") +\n  annotate(\"text\", x = 1.25, y = .85, label = \"Logistic\") +\n  annotate(\"text\", x = -.5, y = .8, label = \"Cloglog\") +\n  annotate(\"text\", x = 0, y = .2, label = \"Loglog\") +\n  labs(y=\"Pr(Y=1)\", x=expression(x*beta)) +\n  ggtitle(\"Logistic, Cloglog, and Loglog CDFs\")\nPlease describe what you see happening at the intercept changes. Compare these intercept shifts to intercept shifts in the linear model.\ncode\n#logit cdf at -2, -1, 0, 1, 2\n\nxb &lt;- seq(-4, 4, length.out=1000)\n\nlogitcdfm1 &lt;- 1/(1+exp(-xb-1))\nlogitcdf0 &lt;- 1/(1+exp(-xb))\nlogitcdf1 &lt;- 1/(1+exp(-xb+1))\n\ndf &lt;- data.frame(xb, logitcdfm1, logitcdf0, logitcdf1)\n\nggplot(data=df, aes(x=xb, y=logitcdf0)) +\n  geom_line() +\n  geom_line(aes(y=logitcdfm1), linetype=\"longdash\") +\n  geom_line(aes(y=logitcdf1), linetype=\"dashed\") +\n  annotate(\"text\", x = 1.5, y = .4, label = \"Intercept=-1\") +\n  annotate(\"text\", x = 0, y = .5, label = \"Intercept=0\") +\n  annotate(\"text\", x = -1.7, y = .6, label = \"Intercept=+1\") +\n  labs(y=\"Pr(Y=1)\", x=expression(x*beta)) +\n  ggtitle(\"Logistic CDFs with Intercept Shifts\")\nPlease describe what you see happening at the shape parameter changes. How are these similar/different from the logistic CDF?\ncode\n#overlay skewed logit cdfs\n\nxb &lt;- seq(-4, 4, length.out=1000)\n\nscobit1 &lt;- 1/((1+exp(-xb))^1)\nscobit0 &lt;- 1/((1+exp(-xb))^2)\nscobitp5 &lt;- 1/((1+exp(-xb))^.5)\n\ndf &lt;- data.frame(xb, scobit1, scobit0, scobitp5)\n\nggplot(data=df, aes(x=xb, y=scobit1)) +\n  geom_line() +\n  geom_line(aes(y=scobit0), linetype=\"dotted\") +\n  geom_line(aes(y=scobitp5), linetype=\"longdash\") +\n  annotate(\"text\", x = 1.5, y = .4, label = \"shape=1\") +\n  annotate(\"text\", x = 0, y = .5, label = \"shape=0\") +\n  annotate(\"text\", x = -1.7, y = .6, label = \"shape=-1\") +\n  labs(y=\"Pr(Y=1)\", x=expression(x*beta)) +\n  ggtitle(\"Skewed Logit CDFs\")\ncode\nset.seed(8675309)\nn &lt;- 10000\nX &lt;- matrix(rnorm(n * 2), ncol = 2)\ntrue_beta &lt;- c(-1, 0.5, -0.5)\nz &lt;- cbind(1, X) %*% true_beta\nprob &lt;- 1 / (1 + exp(-z))\ny &lt;- rbinom(n, 1, prob)\ncloglog link function:\n\\[ g(p) = 1 - \\exp(-\\exp(x \\beta)) \\] cloglog likelihood function:\n\\[ L(\\beta | y) = \\prod_{i=1}^n \\left[ \\left(1 - \\exp(-\\exp(x \\beta)) \\right)^{y_i} \\left( 1 - 1 - \\exp(-\\exp(x \\beta)) \\right)^{1-y_i} \\right] \\]\ncloglog log-likelihood function:\n\\[ \\ln L(\\beta | y) = \\sum_{i=1}^n \\left[ y_i \\log \\left(1 - \\exp(-\\exp(x \\beta)) \\right) + (1 - y_i) \\log \\left(1 - 1 - \\exp(-\\exp(x \\beta)) \\right) \\right] \\]\ncode\nclogloglik &lt;- function(beta, y, X) {\n  xb &lt;- cbind(1, X) %*% beta\n  prob &lt;- 1 - exp(- exp(xb))\n  -sum(y * log(prob) + (1 - y) * log(1 - prob))\n}\nYou’ll find a dataset alongside this assignment called ces.csv. It contains data from the Cooperative Election Study for the 2020 election via the “American Voter Bot” on Twitter. Variable descriptions are in the data. More information is in the notes section below.\nYou’ll need to look carefully at the variables, recode/clean as necessary prior to estimation - please include all code for this so it can be replicated.\ncode\n# average effects\n#pro=0\ncesplotdata &lt;- cesanalysis\nmedianpred &lt;-numeric(length(95))\nage &lt;- 0\nmedianse &lt;- numeric(length(95))\nfor (i in seq(18, 95, 1)) {\n  cesplotdata$pro &lt;- 0\n  cesplotdata$age &lt;- i-17\n  predictions &lt;- data.frame(predict(m3, type = \"response\", se=TRUE, newdata=cesplotdata))\n  medianpred[i-17]&lt;- median(predictions$fit, na.rm=TRUE)\n  medianse[i-17] &lt;- median(predictions$se.fit, na.rm=TRUE)\n  age[i-17] &lt;- i\n  pro &lt;-0\n}\n  p1&lt;- data.frame(age= age, pro=pro, xb = medianpred, se=medianse)\n# pro = 1\nfor (i in seq(18, 95, 1)) {\n  cesplotdata$pro &lt;- 1\n  cesplotdata$age &lt;- i-17\n  predictions &lt;- data.frame(predict(m3, type = \"response\", se=TRUE, newdata=cesplotdata))\n  medianpred[i-17]&lt;- median(predictions$fit, na.rm=TRUE)\n  medianse[i-17] &lt;- median(predictions$se.fit, na.rm=TRUE)\n  age[i-17] &lt;- i\n  pro &lt;- 1\n}\n  p2&lt;- data.frame(age= age, pro= pro, xb = medianpred ,se=medianse)\nallpredictions &lt;- rbind(p1, p2)  \n  \n## predictions by prochoice using color=\"#005A43\" and \"#6CC24A\" for ribbon fills\n\nggplot(data=allpredictions, aes(x=age, y=xb, color=factor(pro))) +\n  geom_line() +\n  geom_ribbon(aes(ymin=xb-se, ymax=xb+se, fill=factor(pro)), alpha=.2) +\n  labs(y=\"Predicted Probability\", x=\"Age\") +\n  ggtitle(\"Predicted Probabilities of Voting for Biden by Age and Pro-Choice Stance\") +\n  scale_color_manual(values=c(\"#005A43\", \"#6CC24A\")) +\n  scale_fill_manual(values=c(\"#005A43\", \"#6CC24A\")) +\n  theme_minimal()"
  },
  {
    "objectID": "ex12024answers.html#notes",
    "href": "ex12024answers.html#notes",
    "title": "Exercise #1 Answers",
    "section": "Notes",
    "text": "Notes\nThe CES project site is https://cces.gov.harvard.edu. Here’s an article about the “American Voter Bot” project: https://www.nytimes.com/2020/01/20/opinion/twitter-democratic-debate.html?smtyp=cur&smid=tw-nytopinion"
  },
  {
    "objectID": "binaryextensions224.html#revisiting-continuous-v.-discrete-measures",
    "href": "binaryextensions224.html#revisiting-continuous-v.-discrete-measures",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Revisiting continuous v. discrete measures",
    "text": "Revisiting continuous v. discrete measures\n\nContinuous variables (interval, ratio level) have meaningful distances between discrete observations and can be infinitely divided.\n\nWe could, for instance, infinitely divide a measure of income into finer and finer units. The distances between these units would be equal and meaningful in some fashion.\n\nDiscrete variables, on the other hand, only take on certain values.\nThose values may or may not indicate order, value or magnitude, and the intervals between values generally are not meaningful nor are they equal.\n\nDiscrete indicators cannot reasonably be subdivided in meaningful ways as a general rule.\n\nIn spite of this distinction, it is not uncommon for scholars to use a continuous data model (e.g. OLS) to examine data that are by their nature discrete. While the results of doing so are not always awful, they are suboptimal. If we treat a discrete ordered variable as if it is continuous, we are explicitly assuming that the distances between categories are equal. If this assumption is met, our estimates of \\(\\beta\\) might be unbiased (though the errors will be heteroskedastic and nonnormal); if this assumption is not met, then even the \\(\\beta\\)s are biased."
  },
  {
    "objectID": "binaryextensions224.html#discrete-ordered-variables",
    "href": "binaryextensions224.html#discrete-ordered-variables",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Discrete Ordered Variables",
    "text": "Discrete Ordered Variables\nWhen we are interested in predicting an ordered (discrete) variable, we can turn to the ordered logit and probit models - these are natural extensions of the binary models we’ve already discussed and are not that much more complicated. I will focus on the ordered probit because it is more common in the literature, and because it can be easily extended to accommodate non constant errors.\nWhat sorts of measures are ordinal? Likert scales, ranks, survey responses, eg. how much do you trust government to protect you from terrorists?\n\nnot at all.\nsomewhat.\nsubstantially.\ncompletely.\n\nWe can receive 4 meaningful responses (the fifth less meaningful response would be “no opinion, don’t know”) - we’d need this category for exhaustiveness.\nThese observed responses represent some latent variable, trust in government. We cannot directly observe this variable but can measure and observe these discrete manifestations of the unobserved variable.\nWhen we estimate and interpret the discrete variable model, we are interested in the effect of \\(x_i\\) on the underlying latent variable \\(y^{*}\\), though we only measure \\(y_{i}\\). We are really interested in the probability of any given level of trust in government, not in whether the mean of \\(y\\) is 2.3 or 2.6; in fact, is this mean even meaningful? Given the latent variable motivation, no. This is part of the problem with linear regression on such variables."
  },
  {
    "objectID": "binaryextensions224.html#latent-variable-motivation",
    "href": "binaryextensions224.html#latent-variable-motivation",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Latent Variable Motivation",
    "text": "Latent Variable Motivation\nBegin with the binary case - assume a latent quantity we’re interested, denoted \\(y^*\\), but our observations of \\(y\\) are limited to successes (\\(y_i=1\\)) and failures (\\(y_i=0\\)).\n\\[\\begin{aligned}\ny^{*}=\\mathbf{x_i \\beta}+\\epsilon_i \\nonumber\n\\end{aligned}\\]\nfor \\(y^{*}\\), the latent variable,\n\\[ y_{i} = \\left\\{ \\begin{array}{ll}\n         0, & \\mbox{if $-\\infty &lt; y^{*}_{i} \\leq \\tau_1 $} \\\\\n         1, & \\mbox{if $\\tau_1 &lt; y^{*}_{i} \\leq \\infty$}\n         \\end{array}\n     \\right. \\]\nwhere \\(\\tau_1\\) is an unobserved threshold.\nMake probabilities statements, and let \\(\\tau_1=0\\),\n\\[\\begin{aligned}\nPr(y_i=1) = Pr(y^{*}_{1}&gt;0) \\nonumber \\\\ \\nonumber \\\\\n=Pr(\\mathbf{x_i \\beta}+\\epsilon_i&gt;0) \\nonumber \\\\ \\nonumber \\\\\n=Pr(\\epsilon_i&gt;-\\mathbf{x_i \\beta}) \\nonumber\n\\end{aligned}\\]\nIf \\(F(\\cdot)\\) is symmetric, then\n\\[\\begin{aligned}\nPr(y_i=1)= Pr(\\epsilon_i&lt;\\mathbf{x_i \\beta}) \\nonumber\n\\end{aligned}\\]\n\\[\\begin{aligned}\nPr(y_i=1)=Pr(\\epsilon_i&lt;\\mathbf{x_i \\beta}) \\nonumber \\\\ \\nonumber \\\\\n=F(\\mathbf{x_i \\beta}) \\nonumber\n\\end{aligned}\\]\nRelate to the binomial:\n\\[\\begin{aligned}\n\\pi_i= Pr(y_i=1)=  F(\\mathbf{x_i \\beta})    \\nonumber \\\\\n1-\\pi_i=Pr(y_i=0)= 1-F(\\mathbf{x_i \\beta}) \\nonumber\n\\end{aligned}\\]\nLet \\(\\tau_1=0\\) (as above); let \\(-\\infty = \\tau_0\\); let \\(\\infty=\\tau_2\\). We now have \\(j\\) categories of \\(y\\) (\\(j=2\\) in the binary case above), and we have \\(j+1=3\\) unobserved thresholds, \\(\\tau_0, \\tau_1, \\tau_2\\):\n\\[ y_{i} = \\left\\{ \\begin{array}{ll}\n         0, & \\mbox{if $\\tau_0 &lt; y^{*}_{i} \\leq \\tau_1 $} \\\\\n         1, & \\mbox{if $\\tau_1 &lt; y^{*}_{i} \\leq \\tau_2$}\n         \\end{array}\n     \\right. \\]\nand extending,\n\\[ y_{i} = \\left\\{ \\begin{array}{ll}\n         0, & \\mbox{if $\\tau_0 &lt; y^{*}_{i} \\leq \\tau_1 $} \\\\\n         1, & \\mbox{if $\\tau_1 &lt; y^{*}_{i} \\leq \\tau_2$} \\\\\n         2, & \\mbox{if $\\tau_2 &lt; y^{*}_{i} \\leq \\tau_3$} \\\\\n         3, & \\mbox{if $\\tau_3 &lt; y^{*}_{i} \\leq \\tau_4 $} \\\\\n          &$\\ldots$ \\\\\n          j, & \\mbox{if $\\tau_{j-1} &lt; y^{*}_{i} \\leq \\tau_j $} \\\\\n         \\end{array}\n     \\right. \\]\nAs before, let \\(\\tau_0= -\\infty\\), \\(\\tau_j=\\infty\\), and \\(\\tau_1=0\\); so we need to estimate \\(j-3\\) thresholds or values of \\(\\tau\\).\nLet’s relate the formal statement of what we observe (\\(y_i=  j, \\mbox{if }  \\tau_{j-1} \\leq y^{*}\\) etc.) to the survey example.\nWhat are the \\(\\tau\\)s? These are the cutpoints or dividing thresholds between categories of our observed variable, \\(y\\). Thus, \\(\\tau_{1}\\) is the threshold between respondents that fall in the zero category and those that fall in the one category; \\(\\tau_{2}\\) is the threshold between one and two, etc.\nThese thresholds represent an important link between our underlying latent variable and the observed variable insofar as the thresholds measure the probability on the normal curve where we make the transition from one category to the other.\nRecall where we started with the latent variable as a linear function of systematic and random components:\n\\[\\begin{aligned}\ny^{*}=\\mathbf{x_i \\beta}+\\epsilon_i \\nonumber\n\\end{aligned}\\]\nSubstitute:\n\\[y_{i} = \\left\\{ \\begin{array}{ll}\n         0, & \\mbox{if $\\tau_0 &lt; x_i \\beta+\\epsilon_i \\leq \\tau_1 $} \\\\\n         1, & \\mbox{if $\\tau_1 &lt; x_i \\beta+\\epsilon_i \\leq \\tau_2$} \\\\\n         2, & \\mbox{if $\\tau_2 &lt; x_i \\beta+\\epsilon_i \\leq \\tau_3$} \\\\\n         3, & \\mbox{if $\\tau_3 &lt; x_i \\beta+\\epsilon_i \\leq \\tau_4 $} \\\\\n          &$\\ldots$ \\\\\n          j, & \\mbox{if $\\tau_{j-1} &lt; x_i \\beta+\\epsilon_i\\leq \\tau_j $} \\\\\n         \\end{array}\n     \\right. \\]\nIf we had a survey question asking “How secure do you feel personally given the Department of Homeland Security’s handling of terrorist threats?” with the following results,\n\nnot secure at all, I hide under the bed most days - 32%\nsomewhat secure, i’ve got lots of duct tape - 12%\nvery secure, i like the color charts a lot - 22%\nmore secure than I’ve ever felt, bring it on big daddy - 30%\n\nthen these percentages represent the probability at which we shift from one category to the next. So these probabilities divide the normal (in the probit model) into 4 regions each containing the correct mass.\n\\[\\begin{aligned}\n\\tau_1 = \\Phi^{-1} (.32) = z(.32) = -.47  \\nonumber\n\\end{aligned}\\]\nso the cutpoint or threshold is at -.47. The second threshold (\\(\\tau_2\\)) would be at\n\\[\\begin{aligned}\n\\tau_2 = \\Phi^{-1} (.32 + .12) =  -.15 \\nonumber \\\\\n\\tau_3 = \\Phi^{-1} (.32 + .12 + .22) =  .52  \\nonumber\n\\end{aligned}\\]"
  },
  {
    "objectID": "binaryextensions224.html#cut-points",
    "href": "binaryextensions224.html#cut-points",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Cut points",
    "text": "Cut points\n\n\ncode\n# normal pdf with vertical lines at -0.47, -.15, .52\n\nz &lt;- runif(100, -3, 3 )\npz &lt;- dnorm(z)\n\nggplot() + \n  geom_line(aes(x=z, y=pz), color=\"blue\") +\n  geom_vline(xintercept = -0.47, color=\"red\") +\n  geom_vline(xintercept = -0.15, color=\"red\") +\n  geom_vline(xintercept = 0.52, color=\"red\") +\n  labs(title=\"Normal Distribution with Cutpoints\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nAs a general rule, the probability \\(y_i=j\\) is given by:\n\\[\\begin{aligned}\nPr(y_i=j)= F(\\tau_{j} -x_i\\beta) - F(\\tau_{j-1} - x_i\\beta) \\nonumber\n\\end{aligned}\\]\nso in the survey, the probability \\(y_i=1\\) is the area between the two thresholds that delineate \\(y_i=1\\); so the area beneath \\(\\tau_1\\) and \\(\\tau_0=-\\infty=0\\)\n\\[\\begin{aligned}\nPr(y_i=1)= F(\\tau_{1} - x_i\\beta) - 0 \\nonumber \\\\\n\\mbox{or in the probit model} \\nonumber \\\\\nPr(y_i=1)= \\Phi(\\tau_{1} - x_i\\beta)  \\nonumber\n\\end{aligned}\\]\nThis formulation gives us the probability any observation falls in the area between thresholds, and thus provides us estimates of the probability of observing any particular value of the dependent variable.\n\\[\\begin{aligned}\nPr(y_i=1)= \\Phi(\\tau_{1} - x_i\\beta)  \\nonumber \\\\\nPr(y_i=2)= \\Phi(\\tau_{2} - x_i\\beta) - \\Phi(\\tau_{1} - x_i\\beta)  \\nonumber \\\\\nPr(y_i=3)= \\Phi(\\tau_{3} - x_i\\beta) -\\Phi(\\tau_{2} - x_i\\beta)  \\nonumber \\\\\nPr(y_i=4)= 1- \\Phi(\\tau_{4} - x_i\\beta) \\nonumber\n\\end{aligned}\\]\nbecause \\(\\tau_4 = \\infty = 1\\).\n\nAside on Link Functions\nI’ve only written this with a Normal (probit) link but the logit is just as easy:\n\\[\\begin{aligned}\nPr(y_i=1)= F(\\tau_{1} - x_i\\beta) - 0 \\nonumber \\\\\nPr(y_i=1)= \\Lambda(\\tau_{1} - x_i\\beta) \\\\ \\nonumber\nPr(y_i=1)= 1/(1+exp(-(\\tau_{1} - x_i\\beta)))  \\nonumber\n\\end{aligned}\\]"
  },
  {
    "objectID": "binaryextensions224.html#example",
    "href": "binaryextensions224.html#example",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Example",
    "text": "Example\nRecall our survey questions:\n\nnot secure at all.\nsomewhat secure.\nvery secure.\nmore secure than I’ve ever felt, bring it on big daddy.\n\nAnd imagine we also have data on respondents’\n\nyears of education (0-16)\ngender (0 = female; 1 = male)\nparty id (0 = democrat; 1 = republican)\n\nSuppose we use an ordered probit to regress survey response (increasing in security), on education, gender, and party:\n\n\n\nFake Estimates: Perceptions of Security\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX\n\\(\\widehat{\\beta}\\)\ns.e.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngender\n-1.0\n(.22)\n\n\n\n\n\n\n\n\n\nparty id\n+2.0\n(.034)\n\n\n\n\n\n\n\n\n\neducation\n+0.6\n(.11)\n\n\n\n\n\n\n\n\n\nconstant\n-4.0\n(1.2)\n\n\n\n\n\n\n\n\n\n\\(\\tau_2\\)\n+1.5\n(.51)\n\n\n\n\n\n\n\n\n\n\\(\\tau_3\\)\n+3\n(.42)\n\n\n\n\n\n\n\n\n\n\n\n\nRecall that \\(\\tau_0=-\\infty\\); \\(\\tau_1=0\\); and \\(\\tau_4=+\\infty =1\\); the constant shifts \\(\\tau_1\\).\nShifting \\(x\\beta\\) shifts the curve left and right …\n\n\ncode\n# Load required libraries\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Create data\nn &lt;- 1000\nz &lt;- seq(-5, 10, length.out = n)\n\n# Calculate normal densities\na &lt;- dnorm(z, mean = 4.2, sd = sqrt(12))\nb &lt;- dnorm(z, mean = 5.2, sd = sqrt(12))\n\n\n# Define thresholds\nt1 &lt;- 0\nt2 &lt;- 1.5\nt3 &lt;- 4\n\n# Create data frame\ndf &lt;- data.frame(z = z, a = a, b = b)\n\n# Function to create plots\ncreate_plot &lt;- function(data, y_var, title, x_beta) {\n  ggplot(data, aes(x = z)) +\n    geom_line(aes_string(y = y_var)) +\n    geom_vline(xintercept = c(t1, t2, t3), linetype = \"dotted\") +\n    geom_text(aes(x = 0, y = 0.014, label = \"τ[1]\"), parse = TRUE, hjust = 0) +\n    geom_text(aes(x = 1.5, y = 0.014, label = \"τ[2]\"), parse = TRUE, hjust = 0) +\n    geom_text(aes(x = 4, y = 0.014, label = \"τ[3]\"), parse = TRUE, hjust = 0) +\n    geom_text(aes(x = -3, y = 0.057, label = paste0(\"xβ=\", x_beta)), hjust = 0) +\n    geom_text(aes(x = -4, y = 0.072, label = title), hjust = 0) +\n    #theme_minimal() +\n    theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(),\n          axis.title = element_blank(), legend.position = \"none\")  \n}\n\n# Create plots\np1 &lt;- create_plot(df, \"a\", \"Male, Republican, HS grad\", 4.2)\np2 &lt;- create_plot(df, \"b\", \"Female, Republican, HS grad\", 5.2)\n\n# Combined plot\np3 &lt;- ggplot(df, aes(x = z)) +\n  geom_line(aes(y = a), color = \"blue\") +\n  geom_line(aes(y = b), color = \"red\") +\n  geom_vline(xintercept = c(t1, t2, t3), linetype = \"dotted\") +\n  geom_text(aes(x = 0, y = 0.014, label = \"τ[1]\"), parse = TRUE, hjust = 0) +\n  geom_text(aes(x = 1.5, y = 0.014, label = \"τ[2]\"), parse = TRUE, hjust = 0) +\n  geom_text(aes(x = 4, y = 0.014, label = \"τ[3]\"), parse = TRUE, hjust = 0) +\n  geom_text(aes(x = -3, y = 0.05, label = \"xβ=4.2\"), hjust = 0) +\n  geom_text(aes(x = 6.0, y = 0.099, label = \"Female, Republican, HS grad\"), hjust = 0) +\n  geom_text(aes(x = 6.5, y = 0.08, label = \"xβ=5.2\"), hjust = 0) +\n  geom_text(aes(x = -4, y = 0.067, label = \"Male, Republican, HS grad\"), hjust = 0) +\n  #geom_text(aes(x = -4, y = 0.072, label = \"Shift in latent variable given change in xβ. Difference is estimate on Gender (β=-1)\"), hjust = 0) +\n  theme_minimal() +\n  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(),\n        axis.title = element_blank(), legend.position = \"none\") +\n   labs(caption=\"Shift in latent variable given change in xβ. Difference is estimate on Gender (β=-1)\")\n \n\n\np1/p2/p3\n\n\n\n\n\n\n\n\n\n\n\ncode\n# Load required libraries\nlibrary(plotly)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Create data\nn &lt;- 1000\nz &lt;- seq(-5, 10, length.out = n)\n\n# Calculate normal densities\na &lt;- dnorm(z, mean = 4.2, sd = sqrt(12))\nb &lt;- dnorm(z, mean = 5.2, sd = sqrt(12))\n\n# Define thresholds\nt1 &lt;- 0\nt2 &lt;- 1.5\nt3 &lt;- 4\n\n# Create data frame\ndf &lt;- data.frame(z = z, a = a, b = b)\n\n# Create the interactive plot\nplot &lt;- plot_ly() %&gt;%\n  add_lines(x = ~z, y = ~a, data = df, name = \"Male\", line = list(color = \"#005A43\"), visible = TRUE) %&gt;%\n  add_lines(x = ~z, y = ~b, data = df, name = \"Female\", line = list(color = \"#6CC24A\"), visible = FALSE) %&gt;%\n  add_segments(x = t1, xend = t1, y = 0, yend = 0.045, line = list(dash = \"solid\",color = \"#6CC24A\"), showlegend = FALSE) %&gt;%\n  add_segments(x = t2, xend = t2, y = 0, yend = 0.045, line = list(dash = \"solid\",color = \"#6CC24A\"), showlegend = FALSE) %&gt;%\n  add_segments(x = t3, xend = t3, y = 0, yend = 0.045, line = list(dash = \"solid\", color = \"#6CC24A\"), showlegend = FALSE) %&gt;%\n  add_annotations(x = c(0, 1.5, 4), y = 0.014, text = c(\"τ[1]\", \"τ[2]\", \"τ[3]\"), showarrow = FALSE) %&gt;%\n  layout(\n    showlegend = FALSE,\n    xaxis = list(title = \"\", range = c(-5, 10)),\n    yaxis = list(title = \"\", showticklabels = FALSE),\n    annotations = list(\n      list(x = -3, y = 0.05, text = \"xβ=4.2\", showarrow = FALSE, visible = TRUE),\n      list(x = -4, y = 0.059, text = \"Male, Republican, HS grad\", showarrow = FALSE, visible = TRUE),\n      list(x = -3, y = 0.01, text = \"xβ=5.2\", showarrow = FALSE, visible = FALSE),\n      list(x = -4, y = 0.059, text = \"Female, Republican, HS grad\", showarrow = FALSE, visible = FALSE),\n      list(x = -4, y = 0.028, text = \"Shift in latent variable given change in xβ.&lt;br&gt;Difference is estimate on Gender (β=-1)\", showarrow = FALSE, visible = FALSE)\n    ),\n    updatemenus = list(\n      list(\n        type = \"buttons\",\n        direction = \"right\",\n        x = 0.1,\n        y = 1.2,\n        buttons = list(\n          list(method = \"update\",\n               args = list(\n                 list(visible = c(TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE)),\n                 list(title = \"Male, Republican, HS grad\",\n                      annotations = list(\n                        list(x = -3, y = 0.05, text = \"xβ=4.2\", showarrow = FALSE, visible = TRUE),\n                        list(x = -4, y = 0.059, text = \"Male, Republican, HS grad\", showarrow = FALSE, visible = TRUE)\n                      ))\n               ),\n               label = \"Figure 1\"),\n          list(method = \"update\",\n               args = list(\n                 list(visible = c(FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE)),\n                 list(title = \"Female, Republican, HS grad\",\n                      annotations = list(\n                        list(x = -3, y = 0.05, text = \"xβ=5.2\", showarrow = FALSE, visible = TRUE),\n                        list(x = -4, y = 0.059, text = \"Female, Republican, HS grad\", showarrow = FALSE, visible = TRUE)\n                      ))\n               ),\n               label = \"Figure 2\"),\n          list(method = \"update\",\n               args = list(\n                 list(visible = c(TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE)),\n                 list(title = \"Combined Plot\",\n                      annotations = list(\n                        list(x = -3, y = 0.05, text = \"xβ=4.2\", showarrow = FALSE, visible = TRUE),\n                        list(x = -4, y = 0.059, text = \"Male, Republican, HS grad\", showarrow = FALSE, visible = TRUE),\n                        list(x = 6.5, y = 0.059, text = \"Female, Republican, HS grad\", showarrow = FALSE, visible = TRUE),\n                        list(x = 6.5, y = 0.05, text = \"xβ=5.2\", showarrow = FALSE, visible = TRUE),\n                        list(x = -4, y = 0.028, text = \"Shift in latent variable given change in xβ.&lt;br&gt;Difference is estimate on Gender (β=-1)\", showarrow = FALSE, visible = TRUE)\n                      ))\n               ),\n               label = \"Figure 3\")\n        )\n      )\n    )\n  )\n\n# Save the plot as an HTML file\nhtmlwidgets::saveWidget(plot, \"interactive_ordered_probit_plot.html\")\n\n\nplot"
  },
  {
    "objectID": "binaryextensions224.html#estimation",
    "href": "binaryextensions224.html#estimation",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Estimation",
    "text": "Estimation\nBecause the ordered probit and logit models are straightforward extensions of the binary models, estimation is relatively simple. Let’s recall the probit likelihood function:\n\\[\\begin{aligned}\nL(\\beta|Y,X) = \\prod\\limits_{i=1}^{n} [\\Phi(X \\beta)]^{y_{i}} [1-\\Phi(X \\beta)]^{1-y_{i}}  \\nonumber\n\\end{aligned}\\] and the log likelihood as \\[\\begin{aligned}\n\\ln L(\\beta|Y,X) = \\sum\\limits_{i=1}^{n} y_{i} \\ln(\\Phi(X \\beta)) + (1-y_{i}) \\ln(1-\\Phi(X \\beta))  \\nonumber\n\\end{aligned}\\]\n\nProbit …\n\\[\\begin{aligned}\nL(\\beta, \\tau |Y,X) = \\prod\\limits_{i=1}^{n} \\prod\\limits_{j=1}^{m} \\left[ \\Phi(\\tau_{j} - \\beta'X)- \\Phi(\\tau_{j-1}- \\beta'X) \\right]^{y_{ij}}  \\nonumber\n\\end{aligned}\\] and the log likelihood is \\[\\begin{aligned}\n\\ln L(\\beta, \\tau |Y,X) = \\sum\\limits_{i=1}^{n} \\sum\\limits_{j=1}^{m} y_{ij} \\ln \\left[ \\Phi(\\tau_{j}- \\beta'X) - \\Phi(\\tau_{j-1} - \\beta'X) \\right]  \\nonumber\n\\end{aligned}\\]\nNote the LLF will turn on for \\(y=j\\) and off for \\(y\\neq j\\) just as in the binary case.\nand as is the case in the binary probit model, we assume \\(\\sigma^{2}\\) to be one (and in the ordered logit model to be \\(\\frac{\\pi^{2}}{3}\\)); put another way, we assume the model is homoskedastic. Relaxing this assumption is just as easy in the ordered probit model as it is in the binary model since:\n\\[\\begin{aligned}\n\\ln L(\\beta, \\tau |Y,X) = \\sum\\limits_{i=1}^{n} \\sum\\limits_{j=1}^{m} y_{ij} \\ln \\left[ \\Phi \\left(\\frac{\\tau_{j}- \\beta'X}{\\sigma^{2}} \\right) - \\Phi \\left( \\frac{\\tau_{j-1} - \\beta'X)}{\\sigma^{2}} \\right) \\right]  \\nonumber\n\\end{aligned}\\]\nand we can parameterize \\(\\sigma^{2}\\) as \\(\\exp(z \\gamma)\\) just as in the binary model."
  },
  {
    "objectID": "binaryextensions224.html#parallel-regression-assumption",
    "href": "binaryextensions224.html#parallel-regression-assumption",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Parallel Regression Assumption",
    "text": "Parallel Regression Assumption\nOrdered models rest on the parallel regression assumption (in the ordered logit, this is sometimes called the proportional odds assumption). The parallel regression assumption requires that the effect of \\(X_{i}\\) is \\(\\beta\\) for all categories of \\(Y\\). That is, the regression of \\(y_i\\) on \\(x\\) is parallel to the regression of \\(y_j\\) on \\(x\\) and so forth.\nSuppose we have some reason to expect that \\(X_{i}\\) increases the probability of \\(Y=1\\), but decreases the probability of \\(Y=2\\). First, it seems likely we should revisit whether or not \\(Y\\) is ordinal. Second, since we’re only estimating one value of \\(\\beta\\), it cannot simultaneously represent our expectations that \\(X_{i}\\) increases one probability while decreasing another. So, if we run the ordered model, we will only get one value of \\(\\beta\\) and its effect will be the same on all categories of \\(Y\\).\nHere’s what non-parallel regressions might look like:\n\n\ncode\n# Load required libraries\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Define Binghamton University colors\nbinghamton_green &lt;- \"#005A43\"\nbinghamton_gray &lt;- \"#8C8C8C\"\nbinghamton_black &lt;- \"#000000\"\nbinghamton_light_green &lt;- \"#4C7C6F\"\n\n# Define education data\neducation &lt;- seq(0, 20, by = 0.1)\n\n# Function to calculate cumulative probabilities\ncalc_cum_probs &lt;- function(education, beta, thresholds) {\n  xb &lt;- beta * education\n  probs &lt;- lapply(thresholds, function(t) pnorm(t - xb))\n  do.call(cbind, probs)\n}\n\n# Parameters for parallel case\nbeta_parallel &lt;- 0.2\nthresholds_parallel &lt;- c(-1, 0, 1)\n\n# Parameters for non-parallel case\nbeta_non_parallel &lt;- c(0.1, 0.2, 0.3)\nthresholds_non_parallel &lt;- c(-1, 0, 1)\n\n# Calculate probabilities\nprobs_parallel &lt;- calc_cum_probs(education, beta_parallel, thresholds_parallel)\nprobs_non_parallel &lt;- cbind(\n  pnorm(thresholds_non_parallel[1] - beta_non_parallel[1] * education),\n  pnorm(thresholds_non_parallel[2] - beta_non_parallel[2] * education),\n  pnorm(thresholds_non_parallel[3] - beta_non_parallel[3] * education)\n)\n\n# Create data frames\ndf_parallel &lt;- data.frame(education = education, probs_parallel)\ndf_non_parallel &lt;- data.frame(education = education, probs_non_parallel)\n\n# Function to create plot\ncreate_plot &lt;- function(df, title) {\n  ggplot(df, aes(x = education)) +\n    geom_line(aes(y = X1, color = \"Y ≤ 1\"), size = 1) +\n    geom_line(aes(y = X2, color = \"Y ≤ 2\"), size = 1) +\n    geom_line(aes(y = X3, color = \"Y ≤ 3\"), size = 1) +\n    scale_color_manual(values = c(\"Y ≤ 1\" = binghamton_green,\n                                  \"Y ≤ 2\" = binghamton_gray,\n                                  \"Y ≤ 3\" = binghamton_black)) +\n    labs(x = \"Years of Education\", y = \"Cumulative Probability\", title = title, color = \"Category\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\",\n          axis.title = element_text(color = binghamton_black),\n          axis.text = element_text(color = binghamton_black),\n          panel.grid = element_line(color = \"#E0E0E0\"),\n          plot.title = element_text(hjust = 0.5, face = \"bold\"))\n}\n\n# Create plots\np1 &lt;- create_plot(df_parallel, \"Parallel Regression (Assumption Satisfied)\")\np2 &lt;- create_plot(df_non_parallel, \"Non-Parallel Regression (Assumption Violated)\")\n\n# Combine plots\ncombined_plot &lt;- grid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\nEvaluating the parallel regression assumption:\n\nEstimate the ordered logit model and a multinomial model and compare how well they fit the data (compare the log-likelihood \\(\\chi^{2}\\) values). Also examine the MNL estimates of \\(\\beta\\) for \\(x_i\\) and see if they are the same across categories. MNL relaxes the parallel regression assumption. - A second informal way is to estimate \\(j-1\\) individual probit or logit models, one for each additional value of \\(Y\\). Compare the estimates of \\(\\beta_{j}\\) with the ordered probit/logit estimate of \\(\\beta\\). If the estimates are roughly the same, the parallel regression assumption is likely met. This is equivalent to the ordered logit/MNL comparison above.\nformal tests exist as well comparing models where parallel regressions is relaxed and where it’s not.\n\nThe parallel lines assumption is, in my experience, difficult to satisfy. Long & Freese (p. 168) seem to have the same experience. The assumption is extremely restrictive, and perhaps difficult to meet for two reasons:\n\n\\(y\\) may not actually be ordered.\n\\(y | \\mathbf{X}\\) is very likely not ordered."
  },
  {
    "objectID": "binaryextensions224.html#cumulative-probabilities",
    "href": "binaryextensions224.html#cumulative-probabilities",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Cumulative Probabilities",
    "text": "Cumulative Probabilities\n\n\ncode\nt1 &lt;- 0\nt2 &lt;- 1.5\nt3 &lt;- 4\n\neducation &lt;- 1:16\nxb_RM &lt;- (2 - 1 - 4) + 0.4 * education\n\n# probabilities\nc0 &lt;- pnorm(t1 - xb_RM)\nc1 &lt;- pnorm((t2 - xb_RM) + (t1 - xb_RM))\nc2 &lt;- pnorm((t3 - xb_RM) + (t2 - xb_RM))\nc3 &lt;- 1 - pnorm(t3 - xb_RM)\n\n# Create data frame\ndf &lt;- data.frame(education = education, c0 = c0, c1 = c1, c2 = c2, c3 = c3)\n\nbinghamton_green &lt;- \"#005A43\"\nbinghamton_gray &lt;- \"#6CC24A\"\nbinghamton_black &lt;- \"#000000\"\n\nggplot(df, aes(x = education)) +\n  geom_line(aes(y = c0, color = \"Not Secure\"), size = .5) +\n  geom_line(aes(y = c1, color = \"Somewhat Secure\"), size = .5) +\n  geom_line(aes(y = c2, color = \"Very Secure\"), size = .5) +\n  #geom_line(aes(y = c3, color = \"Extremely Secure\"), size = 1) +\n  scale_color_manual(values = c(\"Not Secure\" = binghamton_green,\n                                \"Somewhat Secure\" = binghamton_gray,\n                                \"Very Secure\" = binghamton_black)) +\n  annotate(\"text\", x = 1, y = 0.7, label = \"Not Secure\", hjust = 0, color = binghamton_green) +\n  annotate(\"text\", x = 5, y = 0.9, label = \"Somewhat Secure\", hjust = 0, color = binghamton_gray) +\n  annotate(\"text\", x = 10, y = 0.7, label = \"Very Secure\", hjust = 0, color = binghamton_black) +\n  labs(x = \"Years of Education\", y = \"Pr(y=j)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "binaryextensions224.html#example-1",
    "href": "binaryextensions224.html#example-1",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Example",
    "text": "Example\nThis example uses a dataset on beer quality and price. The dataset contains the following variables:\n\nquality: quality of the beer (1 = fair, 2 = good, 3 = very good, 4 = excellent)\nprice: price of the beer\ncalories: calories per serving\ncraftbeer: indicator for craft beer\nbitter: bitterness score\nmalty: maltiness score\n\nThe \\(y\\) variable here is the four category quality score.\n\n\ncode\n# Load required libraries\nlibrary(haven)\nlibrary(dplyr)\nlibrary(MASS)\nlibrary(ggplot2)\n\n# Read the beer dataset\nbeer &lt;- read_dta(\"/Users/dave/Documents/teaching/606J-mle/2024/topics/ordered-variance/beer.dta\")\n\n# Create quality4 variable\nbeer &lt;- beer %&gt;%\n  mutate(quality4 = ntile(quality, 4))\n\n# Fit ordered logistic regression model\nmodel &lt;- polr(factor(quality4) ~ price + calories + craftbeer + bitter + malty, data = beer)\n\nstargazer::stargazer(model, type = \"html\")\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nquality4\n\n\n\n\n\n\n\n\nprice\n\n\n-0.521*\n\n\n\n\n\n\n(0.297)\n\n\n\n\n\n\n\n\n\n\ncalories\n\n\n0.045***\n\n\n\n\n\n\n(0.012)\n\n\n\n\n\n\n\n\n\n\ncraftbeer\n\n\n-1.738*\n\n\n\n\n\n\n(0.942)\n\n\n\n\n\n\n\n\n\n\nbitter\n\n\n-0.027\n\n\n\n\n\n\n(0.042)\n\n\n\n\n\n\n\n\n\n\nmalty\n\n\n0.054**\n\n\n\n\n\n\n(0.025)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n69\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\nLet’s make predictions of quality over the values of price (at-means):\n\n\ncode\n# at means data\nbeersim &lt;- data.frame(\n  calories = 60:200,\n  price = 4.96,\n  craftbeer = 0,\n  bitter = 35.44,\n  malty = 33.13\n)\n\n# Predict probabilities\nprobs &lt;- predict(model, newdata = beersim, type = \"probs\")\nbeersim &lt;- cbind(beersim, probs)\nnames(beersim)[6:9] &lt;- c(\"ProbFair\", \"ProbGood\", \"ProbVG\", \"ProbExc\")\n\n#  plot\nggplot(beersim, aes(x = calories)) +\n  geom_line(aes(y = ProbFair, color = \"Fair\"), size = 1) +\n  geom_line(aes(y = ProbGood, color = \"Good\"), size = 1, linetype = \"dashed\") +\n  geom_line(aes(y = ProbVG, color = \"Very Good\"), size = 1, linetype = \"longdash\") +\n  geom_line(aes(y = ProbExc, color = \"Excellent\"), size = 1, linetype = \"dotdash\") +\n  scale_color_manual(values = c(\"Fair\" = \"black\", \"Good\" = \"red\", \"Very Good\" = \"navy\", \"Excellent\" = \"darkgreen\")) +\n  labs(y = \"Predicted Probabilities\", x = \"Calories per Serving\", color = \"Quality\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nAnd here, let’s plot the cumulative probabilities:\n\n\ncode\n# Calculate cumulative probabilities\nbeersim &lt;- beersim %&gt;%\n  mutate(\n    CDzero = 0,\n    CDFair = ProbFair,\n    CDGood = ProbFair + ProbGood,\n    CDVG = ProbFair + ProbGood + ProbVG,\n    CDExcellent = ProbFair + ProbGood + ProbVG + ProbExc\n  )\n\n# plot\nggplot(beersim, aes(x = calories)) +\n  geom_area(aes(y = CDExcellent), fill = \"gray20\") +\n  geom_area(aes(y = CDVG), fill = \"gray50\") +\n  geom_area(aes(y = CDGood), fill = \"gray80\") +\n  geom_area(aes(y = CDFair), fill = \"white\") +\n  geom_line(aes(y = CDExcellent), color = \"black\") +\n  geom_line(aes(y = CDVG), color = \"black\") +\n  geom_line(aes(y = CDGood), color = \"black\") +\n  geom_line(aes(y = CDFair), color = \"black\") +\n  labs(y = \"Cumulative Probabilities\", x = \"Calories per Serving\") +\n  scale_x_continuous(limits = c(60, 200), breaks = seq(60, 200, by = 20)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "binaryextensions224.html#parameterizing-sigma2",
    "href": "binaryextensions224.html#parameterizing-sigma2",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Parameterizing \\(\\sigma^2\\)",
    "text": "Parameterizing \\(\\sigma^2\\)\nThe notion here is that the variance is neither constant nor random - it potentially arises as a function of variables, so the goal is to write the variance as a systematic function of variables and coefficients. For the probit model, let\n\\[\\begin{aligned}\nVar[\\epsilon] = \\sigma^{2}=[e^{z\\gamma}]^{2} \\nonumber \\\\\n\\mbox{so} ~~~\\sigma = e^{z\\gamma} \\nonumber\n\\end{aligned}\\]\nand\n\\[\\begin{aligned}\n\\mathcal{L} = \\prod\\limits_{i=1}^{n} \\left[\\Phi \\frac{(X \\beta)}{e^{z\\gamma}} \\right ]^{y_{i}} \\left[1-\\Phi \\frac{(X \\beta)}{e^{z\\gamma}} \\right ]^{1-y_{i}}  \\nonumber\n\\end{aligned}\\]"
  },
  {
    "objectID": "binaryextensions224.html#heteroskedastic-probit-llf",
    "href": "binaryextensions224.html#heteroskedastic-probit-llf",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Heteroskedastic Probit LLF",
    "text": "Heteroskedastic Probit LLF\n\\[\\begin{aligned}\n\\ln \\mathcal{L} = \\sum\\limits_{i=1}^{n} y_{i} \\ln \\Phi \\left( \\frac{X \\beta}{e^{z\\gamma}} \\right) + (1-y_{i}) \\ln \\left[1- \\Phi \\left (\\frac{X \\beta}{e^{z\\gamma}} \\right) \\right] \\nonumber\n\\end{aligned}\\]\nThe LLF now has two unknowns, \\(\\widehat{\\beta}\\) and \\(\\widehat{\\gamma}\\), where \\(\\widehat{\\beta}\\) represents the effects of \\(X\\) on the mean probability of \\(y\\), and \\(\\widehat{\\gamma}\\) represents the effects of \\(Z\\) on the variance of \\(y\\). \\(X\\) and \\(Z\\) can be the same - we can anticipate that the same variables (or some of the same variables) influence the mean of \\(y\\) and the variance of \\(y\\)."
  },
  {
    "objectID": "binaryextensions224.html#link-distribution-for-sigma2",
    "href": "binaryextensions224.html#link-distribution-for-sigma2",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Link Distribution for \\(\\sigma^2\\)",
    "text": "Link Distribution for \\(\\sigma^2\\)\nWhy is\n\\[\\begin{aligned}\n\\sigma^{2}=[e^{z\\gamma}]^{2} \\nonumber\n\\end{aligned}\\]\nThe variance must:\n\nmust be non-negative.\nif the effect of \\(Z\\) on the variance is zero, then \\(\\sigma^{2}\\) must revert to one.\n\nExponentiating $z $ accomplishes both of these goals: it will always be positive and if \\(z\\gamma\\) equals 0, then \\(e^{z\\gamma}\\) will equal one and the model is homoskedastic."
  },
  {
    "objectID": "binaryextensions224.html#what-if-variance-is-not-constant",
    "href": "binaryextensions224.html#what-if-variance-is-not-constant",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "What if variance is not constant?",
    "text": "What if variance is not constant?\nYou notice that dividing the estimate by the variance presents a significant problem if the variance is larger for some groups in the data, smaller for others, but we restrict it to 1:\n\nfor a group with larger variance, but restricted to 1, we over estimate \\(\\beta\\). - for a group with smaller variance, but restricted to 1, we under estimate \\(\\beta\\).\n\nSo the estimates are inconsistent and the standard errors are incorrect. The bottom line is the heteroskedasticity is a bigger deal in binary response models than in the linear model."
  },
  {
    "objectID": "binaryextensions224.html#thinking-about-the-variance",
    "href": "binaryextensions224.html#thinking-about-the-variance",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Thinking about the variance",
    "text": "Thinking about the variance\nWhat does it mean for the variance to be different for different groups in the data? We are accustomed to thinking of groups in the data having different means - this is not so different.\n\none group in the data, given by some \\(x\\) variable, is more diffuse or variant in its behavior on \\(y\\) than another group.\nthose groups may or may not share the same mean behavior."
  },
  {
    "objectID": "binaryextensions224.html#variances",
    "href": "binaryextensions224.html#variances",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Variances",
    "text": "Variances\n\n\ncode\n# plot normal PDFs with mean 0, var 1; mean -1, var 2; mean 1, var 0.5\n\nz &lt;- seq(-5, 5, length.out = 1000)\na &lt;- dnorm(z, mean = 0, sd = 1)\nb &lt;- dnorm(z, mean = -1, sd = sqrt(2))\nc &lt;- dnorm(z, mean = 1, sd = sqrt(0.5))\n\ndf &lt;- data.frame(z = z, a = a, b = b, c = c)\n\n#plot using binghamton colors and annotate in the plot; exclude legend\nggplot(df, aes(x = z)) +\n  geom_line(aes(y = a, color = \"Mean 0, Var 1\"), size = 1) +\n  geom_line(aes(y = b, color = \"Mean -1, Var 2\"), size = 1) +\n  geom_line(aes(y = c, color = \"Mean 1, Var 0.5\"), size = 1) +\n  scale_color_manual(values = c(\"Mean 0, Var 1\" = binghamton_green,\n                                \"Mean -1, Var 2\" = binghamton_gray,\n                                \"Mean 1, Var 0.5\" = binghamton_black)) +\n  labs(x = \"z\", y = \"Density\", color = \"Group\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n##Variance of \\(\\epsilon\\)\nThe variance of \\(\\epsilon\\) in any model can be thought of like this:\n\\[\\begin{aligned}\nvar(\\epsilon_i)=var(\\epsilon_j) \\forall i,j \\ldots n \\nonumber\n\\end{aligned}\\]\nThis is explicitly why we write the variance of the errors without a subscript - var(\\(\\epsilon\\)) - it is constant across all \\(i\\).\nPut slightly differently, the distribution of \\(\\epsilon\\) is the same for all \\(i\\). If this does not hold, then the errors are not independent and identically distributed (i.i.d.) - their distributions are different. This is just another way to state the problem of nonconstant variance."
  },
  {
    "objectID": "binaryextensions224.html#varepsilon_ivarepsilon_j",
    "href": "binaryextensions224.html#varepsilon_ivarepsilon_j",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "\\(var(\\epsilon_i)=var(\\epsilon_j)\\)",
    "text": "\\(var(\\epsilon_i)=var(\\epsilon_j)\\)\n\n\ncode\nz &lt;- seq(-5, 5, length.out = 1000)\na &lt;- dnorm(z, mean = -1, sd = .5)\nb &lt;- dnorm(z, mean = 1, sd = .5)\nc &lt;- dnorm(z, mean = -1, sd = .5)\nd &lt;- dnorm(z, mean = 1, sd = sqrt(2))\n\ndf1 &lt;- data.frame(z = z, a = a, b = b)\n\ndf2 &lt;- data.frame(z = z, c = c, d = d)\n\n#plot using binghamton colors and annotate in the plot; exclude legend\n\np1 &lt;- ggplot(df1, aes(x = z)) +\n  geom_line(aes(y = a, color = \"Mean -1, Var 1\"), size = 1) +\n  geom_line(aes(y = b, color = \"Mean 1, Var 1\"), size = 1) +\n  scale_color_manual(values = c(\"Mean -1, Var 1\" = binghamton_green,\n                                \"Mean 1, Var 1\" = binghamton_gray)) +\n  labs(x = \"z\", y = \"Density\", color = \"Group\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")+\n  ggtitle(\"Constant Variance\")\n\np2 &lt;- ggplot(df2, aes(x = z)) +\n  geom_line(aes(y = c, color = \"Mean -1, Var 1\"), size = 1) +\n  geom_line(aes(y = d, color = \"Mean 1, Var 2\"), size = 1) +\n  scale_color_manual(values = c(\"Mean -1, Var 1\" = binghamton_green,\n                                \"Mean 1, Var 2\" = binghamton_gray)) +\n  labs(x = \"z\", y = \"Density\", color = \"Group\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")+\n  ggtitle(\"Non-Constant Variance\")\n\np1/p2"
  },
  {
    "objectID": "binaryextensions224.html#a-framework-for-theory-about-variance",
    "href": "binaryextensions224.html#a-framework-for-theory-about-variance",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "A Framework for Theory about Variance",
    "text": "A Framework for Theory about Variance\nWe’ve spent a lot of time fretting about the information in our data w.r.t. maximizing the LLF. A good bit of that information is related to the variability in the data. It makes sense to think about the sources of that variability. In building arguments, quantitative social scientists tend to obsess over central tendency, but to neglect thinking about what the dispersion in the data means.\nHere’s an attempt at a basic framework for thinking about variance:"
  },
  {
    "objectID": "binaryextensions224.html#variance-framework",
    "href": "binaryextensions224.html#variance-framework",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Variance Framework",
    "text": "Variance Framework\nSubstantively, what can variance represent?\n\namount of information (certainty, uncertainty)\nprecision, accuracy\nuniformity, diversity, heterogeneity\nchoice, constraint\nability, inability\nambivalence\n\nImagine a data set of \\(y\\) and \\(x\\) – suppose \\(x\\) is binary - it might relate to the mean of \\(y\\) and to the variance of \\(y\\), such that:\n\nan increase in \\(x\\) is related to an increase (decrease) in \\(y\\).\nan increase in \\(x\\) is related to an increase (decrease) in the variance of \\(y\\).\n\nThis might be because:\n\nthere are two groups of observations in the data w.r.t. \\(x\\)\none group has a higher/lower mean of \\(y\\) than the other.\none group is more/less heterogeneous in \\(y\\) than the other.\n\nPerhaps this is because:\n\nas individuals become more informed, they prefer more \\(y\\). This is an expectation about the mean of \\(y\\) - as \\(x\\) increases, the mean of \\(y\\) increases.\nas individuals become more informed, they behave more uniformly in preferring \\(y\\). This is an expectation about the variance of \\(y\\) - as \\(x\\) increases, the variance surrounding \\(y\\) decreases.\nless informed individuals prefer less \\(y\\), but choose more diffusely.\n\n\\(x\\) has two effects - increasing the mean and decreasing the variance of \\(y\\)."
  },
  {
    "objectID": "binaryextensions224.html#variance-framework-1",
    "href": "binaryextensions224.html#variance-framework-1",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Variance Framework",
    "text": "Variance Framework\nImagine a data set of \\(y\\) and \\(x\\) – suppose \\(x\\) is binary - it might relate to the mean of \\(y\\) and to the variance of \\(y\\), such that:\n\nan increase in \\(x\\) is related to an increase (decrease) in \\(y\\).\nan increase in \\(x\\) is related to an increase (decrease) in the variance of \\(y\\)."
  },
  {
    "objectID": "binaryextensions224.html#relaxing-parallel-regression",
    "href": "binaryextensions224.html#relaxing-parallel-regression",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Relaxing Parallel Regression",
    "text": "Relaxing Parallel Regression\nThe Generalized ordered logit/probit is one pathway to relaxing the parallel regression assumption. The generalized ordered logit/probit allows for the effects of \\(X\\) to vary across categories of \\(y\\), such that the model estimates \\(k-1\\) values of \\(\\beta\\). The model can be unstable and produce predictions out of bounds. Here’s the same model predicting beer quality, but we’re relaxing the parallel regression assumption in the variables “price” and “calories” - you’ll note each of these has \\(k-1\\) coefficients.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nlibrary(readr)\nlibrary(dplyr)\nlibrary(VGAM)\nlibrary(ggplot2)\n\nbeer &lt;- read_dta(\"/Users/dave/Documents/teaching/606J-mle/2024/topics/ordered-variance/beer.dta\")\n\n\nbeer &lt;- beer %&gt;%\n  mutate(quality4 = ntile(quality, 4))\n\n# Fit the generalized ordered logit model\ngologit_model &lt;- vglm(quality4 ~ price + calories + craftbeer + bitter + malty,\n      family = cumulative(parallel = FALSE~price+calories, reverse = TRUE), data = beer)\n\nmodelsummary::modelsummary(gologit_model)\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept) × 1\n                  -1.586 \n                \n                \n                                 \n                  (1.703)\n                \n                \n                  (Intercept) × 2\n                  -5.784 \n                \n                \n                                 \n                  (2.474)\n                \n                \n                  (Intercept) × 3\n                  -12.013\n                \n                \n                                 \n                  (2.919)\n                \n                \n                  price × 1      \n                  -0.599 \n                \n                \n                                 \n                  (0.357)\n                \n                \n                  price × 2      \n                  -0.447 \n                \n                \n                                 \n                  (0.324)\n                \n                \n                  price × 3      \n                  -0.399 \n                \n                \n                                 \n                  (0.346)\n                \n                \n                  calories × 1   \n                  0.043  \n                \n                \n                                 \n                  (0.013)\n                \n                \n                  calories × 2   \n                  0.057  \n                \n                \n                                 \n                  (0.018)\n                \n                \n                  calories × 3   \n                  0.087  \n                \n                \n                                 \n                  (0.020)\n                \n                \n                  craftbeer      \n                  -1.668 \n                \n                \n                                 \n                  (0.956)\n                \n                \n                  bitter         \n                  -0.029 \n                \n                \n                                 \n                  (0.042)\n                \n                \n                  malty          \n                  0.046  \n                \n                \n                                 \n                  (0.024)\n                \n                \n                  Num.Obs.       \n                  69     \n                \n                \n                  AIC            \n                  182.3  \n                \n                \n                  BIC            \n                  209.1  \n                \n                \n                  RMSE           \n                  2.40   \n                \n        \n      \n    \n\n\nPrice and Calories can exert effects in different directions across categories of \\(y\\), hence relaxing the parallel regression assumption."
  },
  {
    "objectID": "binaryextensions224.html#use-ordered-models-with-caution",
    "href": "binaryextensions224.html#use-ordered-models-with-caution",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Use Ordered Models with Caution …",
    "text": "Use Ordered Models with Caution …\nMy own view is ordered models rarely fit the data - our outcome variables are rarely conditionally ordered, i.e, the expected value of \\(y\\) ordered given the variables in the model. Other models (choice models in particular) are better suited for much of our data - and we are better able to satisfy their assumptions. As the wise man says, just because something can be ordered doesn’t mean it should be. Or, as the econometrician Amemyia (1985) says,\n“A model is unordered if it is not ordered.” (Amemyia 1985, 292)."
  },
  {
    "objectID": "binaryextensions224.html#variance-framework-2",
    "href": "binaryextensions224.html#variance-framework-2",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Variance Framework",
    "text": "Variance Framework\nThis might be because:\n\nthere are two groups of observations in the data w.r.t. \\(x\\)\none group has a higher/lower mean of \\(y\\) than the other.\none group is more/less heterogeneous in \\(y\\) than the other.\n\nPerhaps this is because:\n\nas individuals become more informed, they prefer more \\(y\\). This is an expectation about the mean of \\(y\\) - as \\(x\\) increases, the mean of \\(y\\) increases.\nas individuals become more informed, they behave more uniformly in preferring \\(y\\). This is an expectation about the variance of \\(y\\) - as \\(x\\) increases, the variance surrounding \\(y\\) decreases.\nless informed individuals prefer less \\(y\\), but choose more diffusely.\n\n\\(x\\) has two effects - increasing the mean and decreasing the variance of \\(y\\).\n\n\n\nExample\nAs an illustratio, let’s estimate a model predicting whether respondents believe Barack Obama is a secret Muslim. This analysis uses data from the ANES 2016 Pilot. We’ll specify two models. The probit model regresses responses to a question about whether Obama is a secret Muslim on a set of demographic and political variables; the heteroskedastic probit model posits age affects both the mean (as in the first model) and the variance. The expectation is that older voters are more likely to believe Obama is a secret Muslim, and that older voters believe this less uniformly or more diffusely than do younger voters. So we expect the effect of age on the mean to be positive, and the effect of age on the variance to be negative. Results are below - the first model is the standard probit model, the second is the heteroskedastic probit model.\n\n\ncode\nlibrary(Rchoice)\n\n# ANES 2016 data\nanes &lt;- read_csv(\"/Users/dave/Documents/teaching/606J-mle/2020/slides/L3_binaryextensions/code/anes_pilot_2016.csv\")\n\n# Select variables\nvars_to_keep &lt;- c(\"bo_muslim\", \"pid7\", \"disc_wo\", \"lazyb\", \"disc_b\", \"faminc\", \"birthyr\", \"race\", \"autism\", \"disc_selfsex\", \"gender\", \"vaccine\")\nanes &lt;- anes[, vars_to_keep]\n\n# Recode variables\nanes &lt;- anes %&gt;%\n  mutate(\n    pid7 = as.numeric(pid7),\n    bo_muslim = case_when(\n      bo_muslim == 2 ~ 0,\n      bo_muslim == 1 ~ 1,\n      bo_muslim == 8 ~ NA_real_,\n      TRUE ~ bo_muslim\n    ),\n    pid7 = ifelse(pid7 &gt; 7, NA, pid7),\n    disc_wo = ifelse(disc_wo &gt; 7, NA, disc_wo),\n    lazyb = ifelse(lazyb &gt; 7, NA, lazyb),\n    disc_b = ifelse(disc_b &gt; 5, NA, disc_b),\n    faminc = ifelse(faminc &gt; 16, NA, faminc),\n    age = 2016 - birthyr,\n    white = ifelse(race == 1, 1, 0),\n    # Reverse coding\n    disc_wo = -1 * disc_wo + 6,\n    disc_selfsex = -1 * disc_selfsex + 6,\n    autism = -1 * autism + 7,\n    disc_b = -1 * disc_b + 6\n  )\n# Ensure disc_wo is binary for probit model\nanes &lt;- anes %&gt;%\n  mutate(disc_wo_binary = ifelse(disc_wo &gt; median(disc_wo, na.rm = TRUE), 1, 0))\n\n# model \nhetprob_model &lt;- hetprob(bo_muslim ~ white+disc_b+pid7+age+ faminc+autism | age, data = anes, link=\"probit\")\n\n# Compare with standard probit model\nprobit_model &lt;- glm(bo_muslim ~ white + disc_b + pid7 + age + faminc + autism, data = anes, family = binomial(link = \"probit\"))\n\n\n# Load required libraries\nlibrary(Rchoice)\nlibrary(kableExtra)\nlibrary(dplyr)\n\n# Function to extract and format Rchoice model results\nextract_rchoice_results &lt;- function(model) {\n  coef &lt;- tryCatch(coef(model), error = function(e) NULL)\n  vcov_matrix &lt;- tryCatch(vcov(model), error = function(e) NULL)\n  \n  if (is.null(coef) || is.null(vcov_matrix)) {\n    stop(\"Unable to extract coefficients or variance-covariance matrix from the model.\")\n  }\n  \n  se &lt;- sqrt(diag(vcov_matrix))\n  p_value &lt;- 2 * (1 - pnorm(abs(coef / se)))\n  \n  results_df &lt;- data.frame(\n    Estimate = coef,\n    `Std. Error` = se,\n    `Pr(&gt;|z|)` = p_value\n  )\n  \n  return(results_df)\n}\n\n# Function to create a formatted HTML table for two models\ncreate_two_model_table &lt;- function(model1, model2, model1_name = \"Model 1\", model2_name = \"Model 2\") {\n  tryCatch({\n    # Extract results for both models\n    results1 &lt;- extract_rchoice_results(model1)\n    results2 &lt;- extract_rchoice_results(model2)\n    \n    # Combine results\n    combined_results &lt;- full_join(\n      results1 %&gt;% mutate(Variable = rownames(results1)),\n      results2 %&gt;% mutate(Variable = rownames(results2)),\n      by = \"Variable\",\n      suffix = c(\".1\", \".2\")\n    ) %&gt;%\n      select(Variable, everything()) %&gt;%\n      arrange(Variable)\n    \n    # Rename columns\n    names(combined_results) &lt;- c(\"Variable\",\n                                 \"Estimate.1\", \"Std. Error.1\", \"Pr(&gt;|z|).1\",\n                                 \"Estimate.2\", \"Std. Error.2\", \"Pr(&gt;|z|).2\")\n    \n    # Create HTML table\n    html_table &lt;- kable(combined_results, \n                        format = \"html\",\n                        digits = 3,\n                        caption = \"Comparison of  Probit Models\") %&gt;%\n      kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) %&gt;%\n      add_header_above(c(\" \" = 1, \n                         model1_name = 3, \n                         model2_name = 3)) \n    \n    # Save the HTML table to a file\n    #writeLines(as.character(html_table), \"two_model_results.html\")\n    \n    # Print the HTML code\n    print(html_table)\n  }, error = function(e) {\n    cat(\"An error occurred:\", conditionMessage(e), \"\\n\")\n  })\n}\n\n\n# Usage example (replace with your actual models):\ncreate_two_model_table(probit_model, hetprob_model, model1_name =\"Base Model\", model2_name=\"Extended Model\")\n\n\n\nComparison of Probit Models\n\n\n\n\n\n\n\nmodel1_name\n\n\n\n\nmodel2_name\n\n\n\n\n\nVariable\n\n\nEstimate.1\n\n\nStd. Error.1\n\n\nPr(&gt;|z|).1\n\n\nEstimate.2\n\n\nStd. Error.2\n\n\nPr(&gt;|z|).2\n\n\n\n\n\n\n(Intercept)\n\n\n-1.808\n\n\n0.278\n\n\n0.000\n\n\n-1.253\n\n\n0.284\n\n\n0.000\n\n\n\n\nage\n\n\n0.013\n\n\n0.003\n\n\n0.000\n\n\n0.010\n\n\n0.002\n\n\n0.000\n\n\n\n\nautism\n\n\n0.199\n\n\n0.030\n\n\n0.000\n\n\n0.128\n\n\n0.033\n\n\n0.000\n\n\n\n\ndisc_b\n\n\n-0.145\n\n\n0.043\n\n\n0.001\n\n\n-0.101\n\n\n0.033\n\n\n0.002\n\n\n\n\nfaminc\n\n\n-0.055\n\n\n0.015\n\n\n0.000\n\n\n-0.035\n\n\n0.012\n\n\n0.004\n\n\n\n\nhet.age\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n-0.009\n\n\n0.004\n\n\n0.028\n\n\n\n\npid7\n\n\n0.231\n\n\n0.024\n\n\n0.000\n\n\n0.148\n\n\n0.035\n\n\n0.000\n\n\n\n\nwhite\n\n\n0.128\n\n\n0.116\n\n\n0.271\n\n\n0.074\n\n\n0.078\n\n\n0.341\n\n\n\n\n\nHere are predictions from the two models. The main takeaway is that if you change things, things change - so there’s not much substantively to take away from the differences between the predictions. Strong theory expectations would potentially drive expectations about these differences.\n\n\ncode\npred_data &lt;- expand.grid(\n  disc_b = median(anes$disc_selfsex, na.rm = TRUE),\n  age = seq(min(anes$age, na.rm = TRUE), max(anes$age, na.rm = TRUE), length.out = 100),\n  faminc = median(anes$faminc, na.rm = TRUE),\n  pid7 = median(anes$pid7, na.rm = TRUE),\n  autism = median(anes$autism, na.rm = TRUE),\n  white = 1\n)\n\n# Predict probabilities\npred_probs_het &lt;- predict(hetprob_model, newdata = pred_data, type = \"pr\")\npred_probs_std &lt;- predict(probit_model, newdata = pred_data, type = \"response\")\n\n# Combine predictions\nplot_data &lt;- cbind(pred_data, \n                   Het_Prob = pred_probs_het,\n                   Std_Prob = pred_probs_std)\n\n# Create the plot\nggplot(plot_data, aes(x = age)) +\n  geom_line(aes(y = Het_Prob, color = \"Heteroskedastic Probit\")) +\n  geom_line(aes(y = Std_Prob, color = \"Standard Probit\")) +\n  labs(x = \"Age\", y = \"Pr(Secret Muslim)\", color = \"Model\") +\n  theme_minimal() +\n  ggtitle(\"Comparison of Heteroskedastic and Standard Probit Models\")"
  },
  {
    "objectID": "binaryextensions224.html#example-2",
    "href": "binaryextensions224.html#example-2",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Example",
    "text": "Example\nAs an illustratio, let’s estimate a model predicting whether respondents believe Barack Obama is a secret Muslim. This analysis uses data from the ANES 2016 Pilot. We’ll specify two models. The probit model regresses responses to a question about whether Obama is a secret Muslim on a set of demographic and political variables; the heteroskedastic probit model posits age affects both the mean (as in the first model) and the variance. The expectation is that older voters are more likely to believe Obama is a secret Muslim, and that older voters believe this less uniformly or more diffusely than do younger voters. So we expect the effect of age on the mean to be positive, and the effect of age on the variance to be negative. Results are below - the first model is the standard probit model, the second is the heteroskedastic probit model.\n\n\ncode\nlibrary(Rchoice)\n\n# ANES 2016 data\nanes &lt;- read_csv(\"/Users/dave/Documents/teaching/606J-mle/2020/slides/L3_binaryextensions/code/anes_pilot_2016.csv\")\n\n# Select variables\nvars_to_keep &lt;- c(\"bo_muslim\", \"pid7\", \"disc_wo\", \"lazyb\", \"disc_b\", \"faminc\", \"birthyr\", \"race\", \"autism\", \"disc_selfsex\", \"gender\", \"vaccine\")\nanes &lt;- anes[, vars_to_keep]\n\n# Recode variables\nanes &lt;- anes %&gt;%\n  mutate(\n    pid7 = as.numeric(pid7),\n    bo_muslim = case_when(\n      bo_muslim == 2 ~ 0,\n      bo_muslim == 1 ~ 1,\n      bo_muslim == 8 ~ NA_real_,\n      TRUE ~ bo_muslim\n    ),\n    pid7 = ifelse(pid7 &gt; 7, NA, pid7),\n    disc_wo = ifelse(disc_wo &gt; 7, NA, disc_wo),\n    lazyb = ifelse(lazyb &gt; 7, NA, lazyb),\n    disc_b = ifelse(disc_b &gt; 5, NA, disc_b),\n    faminc = ifelse(faminc &gt; 16, NA, faminc),\n    age = 2016 - birthyr,\n    white = ifelse(race == 1, 1, 0),\n    # Reverse coding\n    disc_wo = -1 * disc_wo + 6,\n    disc_selfsex = -1 * disc_selfsex + 6,\n    autism = -1 * autism + 7,\n    disc_b = -1 * disc_b + 6\n  )\n# Ensure disc_wo is binary for probit model\nanes &lt;- anes %&gt;%\n  mutate(disc_wo_binary = ifelse(disc_wo &gt; median(disc_wo, na.rm = TRUE), 1, 0))\n\n# model \nhetprob_model &lt;- hetprob(bo_muslim ~ white+disc_b+pid7+age+ faminc+autism | age, data = anes, link=\"probit\")\n\n# Compare with standard probit model\nprobit_model &lt;- glm(bo_muslim ~ white + disc_b + pid7 + age + faminc + autism, data = anes, family = binomial(link = \"probit\"))\n\n\n# Load required libraries\nlibrary(Rchoice)\nlibrary(kableExtra)\nlibrary(dplyr)\n\n# Function to extract and format Rchoice model results\nextract_rchoice_results &lt;- function(model) {\n  coef &lt;- tryCatch(coef(model), error = function(e) NULL)\n  vcov_matrix &lt;- tryCatch(vcov(model), error = function(e) NULL)\n  \n  if (is.null(coef) || is.null(vcov_matrix)) {\n    stop(\"Unable to extract coefficients or variance-covariance matrix from the model.\")\n  }\n  \n  se &lt;- sqrt(diag(vcov_matrix))\n  p_value &lt;- 2 * (1 - pnorm(abs(coef / se)))\n  \n  results_df &lt;- data.frame(\n    Estimate = coef,\n    `Std. Error` = se,\n    `Pr(&gt;|z|)` = p_value\n  )\n  \n  return(results_df)\n}\n\n# Function to create a formatted HTML table for two models\ncreate_two_model_table &lt;- function(model1, model2, model1_name = \"Model 1\", model2_name = \"Model 2\") {\n  tryCatch({\n    # Extract results for both models\n    results1 &lt;- extract_rchoice_results(model1)\n    results2 &lt;- extract_rchoice_results(model2)\n    \n    # Combine results\n    combined_results &lt;- full_join(\n      results1 %&gt;% mutate(Variable = rownames(results1)),\n      results2 %&gt;% mutate(Variable = rownames(results2)),\n      by = \"Variable\",\n      suffix = c(\".1\", \".2\")\n    ) %&gt;%\n      select(Variable, everything()) %&gt;%\n      arrange(Variable)\n    \n    # Rename columns\n    names(combined_results) &lt;- c(\"Variable\",\n                                 \"Estimate.1\", \"Std. Error.1\", \"Pr(&gt;|z|).1\",\n                                 \"Estimate.2\", \"Std. Error.2\", \"Pr(&gt;|z|).2\")\n    \n    # Create HTML table\n    html_table &lt;- kable(combined_results, \n                        format = \"html\",\n                        digits = 3,\n                        caption = \"Comparison of  Probit Models\") %&gt;%\n      kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) %&gt;%\n      add_header_above(c(\" \" = 1, \n                         model1_name = 3, \n                         model2_name = 3)) \n    \n    # Save the HTML table to a file\n    #writeLines(as.character(html_table), \"two_model_results.html\")\n    \n    # Print the HTML code\n    print(html_table)\n  }, error = function(e) {\n    cat(\"An error occurred:\", conditionMessage(e), \"\\n\")\n  })\n}\n\n\n# Usage example (replace with your actual models):\ncreate_two_model_table(probit_model, hetprob_model, model1_name =\"Base Model\", model2_name=\"Extended Model\")\n\n\n\nComparison of Probit Models\n\n\n\n\n\n\n\nmodel1_name\n\n\n\n\nmodel2_name\n\n\n\n\n\nVariable\n\n\nEstimate.1\n\n\nStd. Error.1\n\n\nPr(&gt;|z|).1\n\n\nEstimate.2\n\n\nStd. Error.2\n\n\nPr(&gt;|z|).2\n\n\n\n\n\n\n(Intercept)\n\n\n-1.808\n\n\n0.278\n\n\n0.000\n\n\n-1.253\n\n\n0.284\n\n\n0.000\n\n\n\n\nage\n\n\n0.013\n\n\n0.003\n\n\n0.000\n\n\n0.010\n\n\n0.002\n\n\n0.000\n\n\n\n\nautism\n\n\n0.199\n\n\n0.030\n\n\n0.000\n\n\n0.128\n\n\n0.033\n\n\n0.000\n\n\n\n\ndisc_b\n\n\n-0.145\n\n\n0.043\n\n\n0.001\n\n\n-0.101\n\n\n0.033\n\n\n0.002\n\n\n\n\nfaminc\n\n\n-0.055\n\n\n0.015\n\n\n0.000\n\n\n-0.035\n\n\n0.012\n\n\n0.004\n\n\n\n\nhet.age\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n-0.009\n\n\n0.004\n\n\n0.028\n\n\n\n\npid7\n\n\n0.231\n\n\n0.024\n\n\n0.000\n\n\n0.148\n\n\n0.035\n\n\n0.000\n\n\n\n\nwhite\n\n\n0.128\n\n\n0.116\n\n\n0.271\n\n\n0.074\n\n\n0.078\n\n\n0.341\n\n\n\n\n\nHere are predictions from the two models. The main takeaway is that if you change things, things change - so there’s not much substantively to take away from the differences between the predictions. Strong theory expectations would potentially drive expectations about these differences.\n\n\ncode\npred_data &lt;- expand.grid(\n  disc_b = median(anes$disc_selfsex, na.rm = TRUE),\n  age = seq(min(anes$age, na.rm = TRUE), max(anes$age, na.rm = TRUE), length.out = 100),\n  faminc = median(anes$faminc, na.rm = TRUE),\n  pid7 = median(anes$pid7, na.rm = TRUE),\n  autism = median(anes$autism, na.rm = TRUE),\n  white = 1\n)\n\n# Predict probabilities\npred_probs_het &lt;- predict(hetprob_model, newdata = pred_data, type = \"pr\")\npred_probs_std &lt;- predict(probit_model, newdata = pred_data, type = \"response\")\n\n# Combine predictions\nplot_data &lt;- cbind(pred_data, \n                   Het_Prob = pred_probs_het,\n                   Std_Prob = pred_probs_std)\n\n# Create the plot\nggplot(plot_data, aes(x = age)) +\n  geom_line(aes(y = Het_Prob, color = \"Heteroskedastic Probit\")) +\n  geom_line(aes(y = Std_Prob, color = \"Standard Probit\")) +\n  labs(x = \"Age\", y = \"Pr(Secret Muslim)\", color = \"Model\") +\n  theme_minimal() +\n  ggtitle(\"Comparison of Heteroskedastic and Standard Probit Models\")"
  },
  {
    "objectID": "binaryextensions224.html#variance-of-epsilon",
    "href": "binaryextensions224.html#variance-of-epsilon",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Variance of \\(\\epsilon\\)",
    "text": "Variance of \\(\\epsilon\\)\nThe variance of \\(\\epsilon\\) in any model can be thought of like this:\n\\[\\begin{aligned}\nvar(\\epsilon_i)=var(\\epsilon_j) \\forall i,j \\ldots n \\nonumber\n\\end{aligned}\\]\nThis is explicitly why we write the variance of the errors without a subscript - var(\\(\\epsilon\\)) - it is constant across all \\(i\\).\nPut slightly differently, the distribution of \\(\\epsilon\\) is the same for all \\(i\\). If this does not hold, then the errors are not independent and identically distributed (i.i.d.) - their distributions are different. This is just another way to state the problem of nonconstant variance.\nThis figure illustrates what it means for the variance to be different for different groups in the data. In the top panel, the means differ, but variances are the same. This approximates the homoskedastic case. In the bottom panel, the means differ, as do the variances. This is the heteroskedastic case.\n\n\ncode\nz &lt;- seq(-5, 5, length.out = 1000)\na &lt;- dnorm(z, mean = -1, sd = .5)\nb &lt;- dnorm(z, mean = 1, sd = .5)\nc &lt;- dnorm(z, mean = -1, sd = .5)\nd &lt;- dnorm(z, mean = 1, sd = sqrt(2))\n\ndf1 &lt;- data.frame(z = z, a = a, b = b)\n\ndf2 &lt;- data.frame(z = z, c = c, d = d)\n\n#plot using binghamton colors and annotate in the plot; exclude legend\n\np1 &lt;- ggplot(df1, aes(x = z)) +\n  geom_line(aes(y = a, color = \"Mean -1, Var 1\"), size = 1) +\n  geom_line(aes(y = b, color = \"Mean 1, Var 1\"), size = 1) +\n  scale_color_manual(values = c(\"Mean -1, Var 1\" = binghamton_green,\n                                \"Mean 1, Var 1\" = binghamton_gray)) +\n  labs(x = \"z\", y = \"Density\", color = \"Group\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")+\n  ggtitle(\"Constant Variance\")\n\np2 &lt;- ggplot(df2, aes(x = z)) +\n  geom_line(aes(y = c, color = \"Mean -1, Var 1\"), size = 1) +\n  geom_line(aes(y = d, color = \"Mean 1, Var 2\"), size = 1) +\n  scale_color_manual(values = c(\"Mean -1, Var 1\" = binghamton_green,\n                                \"Mean 1, Var 2\" = binghamton_gray)) +\n  labs(x = \"z\", y = \"Density\", color = \"Group\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")+\n  ggtitle(\"Non-Constant Variance\")\n\np1/p2"
  },
  {
    "objectID": "mechanismpaper24.html",
    "href": "mechanismpaper24.html",
    "title": "Mechanism Paper Assignment",
    "section": "",
    "text": "The assignment: During the semester, write three short papers.The goal is to identify a causal mechanism you find interesting, discuss its mechanics, its use in the literature, and then to propose a new place to apply it, or a modification of the mechanism applied in an existing location.\nWhat is a mechanism? For our purposes, a mechanism is a set of rules and incentives that point actors toward some behavior they might not otherwise choose. A syllabus in a class can be thought of as a set of mechanisms working together to get students to do three things they otherwise would not do: show up; pay attention; read. A cumulative exam incentivizes keeping up during the semester. Requiring a topic page for a paper assignment incentivizes starting the paper earlier than would otherwise happen. Pop quizzes incentivize attendance and reading.\nHere’s another perhaps less trivial example. 17th century pirate ships needed crew members to be willing to fight hard, not to hold back in fear of injury. If pirates were cautious, they would be less effective at taking ships, and would not develop the reputations pirates needed - those fearsome reputations actually made violence less necessary and pirating somewhat safer. To get the crew to fight, captains often made a practice of providing social benefits including health care to those injured. Instead of throwing incapacitated pirates over the side, they provided social insurance, thereby encouraging others to fight hard knowing that injury did not guarantee death. The result was pirate crews willing to take risks and fight aggressively, success in taking target ships, and reputations as fearsome fighters. Captains who guaranteed social benefits created moral hazard, induced risk-taking behavior that benefited the ship.1\nHere’s another, even less trivial example, but also a puzzle. Governments of all types extract revenue from citizens via taxation - the more productive citizens are, the more they collect in taxes (holding the rate constant). So governments need to encourage productivity. Productivity requires investment (rather than consumption) - investment pays off in the future while consumption pays off now. For investment to make sense, citizens have to believe the future is valuable and safe. One way governments might try to encourage investment and persuade them the future is safe is to reassure citizens government will not steal from them - if citizens believe government might “change the deal” and seize assets at any time, citizens will neither save, nor invest - they will consume. How can governments maintain the monopoly on violence, but persuade citizens government won’t use that monopoly on violence to seize everything, thereby encouraging investment, and thereby increasing tax revenue? More succinctly, how can a government powerful enough to protect private property guarantee it won’t confiscate private property?2"
  },
  {
    "objectID": "mechanismpaper24.html#footnotes",
    "href": "mechanismpaper24.html#footnotes",
    "title": "Mechanism Paper Assignment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFrom Peter Leeson’s The Invisible Hook, available online through BU’s library.↩︎\nThis puzzle appears in a number of places, due in part to Douglas North and Barry Weingast; a form of this is what Weingast calls the fundamental political dilemma of an economic system.↩︎"
  },
  {
    "objectID": "interactions24.html",
    "href": "interactions24.html",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "Everything you know about multiplicative interactions in linear models applies in the nonlinear context. It would very much be worth reviewing the slides on interactions in the linear model before proceeding.\n\n\n\ncoefficients on dummy variables represent intercept or level shifts; differential intercepts between groups on the dummy.\nstructural stability (same slope across groups) - relaxed with multiplicative interactions.\ninclude constituents variables in the model.\nconstituent interpretation is always conditional.\ninteraction coefficient interpretation is always conditional.\ninference is always conditional. To evaluate whether \\(\\beta_1 + \\beta_3 \\neq 0\\), we need to compute a standard error on \\(\\beta_1 + \\beta_3\\). Matt Golder’s web site is an excellent reference for anything interaction-related, and particularly for guidance on computing standard errors.\n\n\n\n\nIn nonlinear models, the effects of \\(x\\) variables on \\(Pr(y=1)\\) are increasingly compressed as \\(x\\beta\\) moves away from zero, and as \\(Pr(y=1)\\) moves towards its limits. Berry, DeMeritt, and Esarey (2010) seem to originate the term “compression” referring to the changing effects of \\(x\\) on \\(y\\) as \\(x\\beta\\) changes in monotonic link functions (e.g. logit, probit, etc.). Compression is describing what Nagler (1994) calls “inherent” interaction in nonlinear models; it describes why the effects of \\(x\\) depend on the values of the other \\(x\\) variables in the model.\nIn the linear model, the effects of \\(x\\) are unconditional unless we include an interaction; in the nonlinear model, the effects are always conditional due to compression, even without a multiplicative interaction. So we need to answer two questions:\n\nhow can we take advantage of “inherent interaction” due to compression?\nif models are inherently interactive, when or why do we need multiplicative interactions?\n\nFor the second question, this is a debate in the literature. Berry, DeMeritt, and Esarey (2010) suggest interaction terms are not always necessary; Rainey (2016) suggests otherwise.\nLet’s think about the probability space and quantities of interest as a way to understand compression.\n\n\n\nPredicted probability: \\[Pr(y=1) = F(\\beta_0 + \\beta_1x_1+ \\beta_2x_2)\\]\n\n\n\n\n\\[\\frac{\\partial y}{\\partial x_1} =  \\beta_1\\]\n\n\n\n\\[\\frac{\\partial y}{\\partial x_k} = \\frac{\\partial F(x\\beta)}{\\partial x_k} =   \\frac{\\partial F(x\\beta)}{\\partial x\\beta} \\cdot  \\frac{\\partial x\\beta}{\\partial x_k}  = f(x\\beta)\\beta_k\\]\n\n\n\n\n\n\n\\[\\frac{\\partial y}{\\partial x_k} = \\frac{\\partial \\Lambda(x\\beta)}{\\partial x_k} =\\lambda(x\\beta)\\beta_k\\] since the derivative of the CDF is the PDF, and where the logit pdf is\n\\[\\lambda =\\frac{e^{x\\beta}}{(1+e^{x\\beta})^2}\\]\n\n\n\n\nThe logit ME can be rewritten:\n\\[\\begin{eqnarray}\n\\frac{\\partial \\Lambda(x\\beta)}{\\partial x_k} =\\lambda(x\\beta) \\cdot \\beta_k \\nonumber \\\\\n=\\left[\\frac{e^{x\\beta}}{(1+e^{x\\beta})^2}\\right] \\cdot \\beta_k \\nonumber \\\\\n= \\left[\\frac{e^{x\\beta}}{(1+e^{x\\beta})} \\frac{1}{(1+e^{x\\beta})}\\right] \\cdot \\beta_k \\nonumber \\\\\n= \\Lambda(x\\beta) \\left(1-\\frac{e^{x\\beta}}{1+e^{x\\beta}}\\right) \\cdot \\beta_k \\nonumber \\\\\n= \\Lambda(x\\beta)\\cdot (1-\\Lambda(x\\beta)) \\cdot\\beta_k \\nonumber \\\\\n= Pr(y=0) \\cdot Pr(y=1) \\cdot  \\beta_k \\nonumber\n\\end{eqnarray}\\]\n\n\n\n\\[\\frac{\\partial y}{\\partial x_k} = \\frac{\\partial \\Phi(x\\beta)}{\\partial x_k} =\\phi(x\\beta)\\beta_k\\]\nIn both cases, you can see the effect of \\(x_k\\) depends explicitly on the value of \\(x\\beta\\). Put differently, the marginal effect of \\(x_k\\) is {} on \\(x\\beta\\), or where we are on the \\(x\\) axis.\n\n\n\n\nCompression refers to the different rates of change in the \\(Pr(Y=1)\\) across the probability range.\n\nthe effects of changes in \\(x\\) are greatest at \\(Pr(Y=1)=.5\\).\nthe effects of changes in \\(x\\) decline as \\(Pr(Y=1)\\) approaches the limits.\nthe same change in \\(x\\) has an increasingly “compressed” smaller effect as \\(Pr(Y=1)\\Rightarrow(0,1)\\).\n\n\n\n\nIn the simulation below notice how the effects of \\(x\\) on \\(y\\) are different due to compression effects - that is, the effect of \\(x\\) is greatest at \\(x=0, y=.5\\), and declines as \\(|x|\\) increases. Also, notice how additive changes to the intercept shift the curve left-right, but do not change the slope at \\(Pr(Y=1)\\). Multiplicative (interactive) changes change the derivative of the curve. Notice that compression exists in both the interactive and non-interactive settings.\n\n\n\n\n\n\n\nThis helps answer when/why we’d use an interaction. The derivatives at \\(Pr(Y=1)\\) shift in the interactive model, remain constant in the non-interactive model. Note several things:\n\nCompression effects are present regardless of the presence or absence of interactions. So the interactive model has compression and multiplicative interaction effects at work.\nThe CDF shifts left-right based on intercept changes in \\(x_i \\beta\\), but derivatives remain the same for values of \\(y\\).\nWith interactions, the derivatives changes for values of \\(y\\); the derivative at \\(Pr(Y=1)\\) changes.\nChoosing the multiplicative model implies our theory tells us the slope at \\(Pr(Y=1)\\) is different due to \\(x \\dot z\\).\nIn the non-interactive model, \\(\\frac{\\partial Pr(Y=1)}{\\partial x_i\\beta}=\\frac{\\partial Pr(Y=1)}{\\partial x_j\\beta}\\); in the interactive model, this is not true.\n\n\n\n\n\nwhen we believe that the changes in \\(y\\) are a function of \\(x|z=i\\ldots j\\).\nwhen we specifically think the slope (derivative) at a particular value of \\(y\\) given \\(x\\) is not structurally stable; i.e. it varies by groups on some variable \\(z\\).\nRainey (2016) suggests we err on the side of including interaction terms.\n\n\n\n\n\nalways explore compression effects. Postestimation, examine how changes in variable values shift the curve in interesting ways. Explore interesting combinations of variables.\nnever rely on compression to handle a conditional expectation - if you have a conditional expectation, use a multiplicative interaction.\nthere’s continuing interest in how to think about compression and interaction in the literature, so read what folks are saying.\n\n\n\n\nComputing QIs for interactions is not difficult, but has a number of moving parts. The go-to technique is laid out by Matt Golder as part of the materials accompanying Brambor, Clark, and Golder (2006). That technique unfolds as follows:\n\nestimate the model\nsimulate the parameter distribution\ngenerate \\(pr(y=j)\\) for different values of \\(x_1\\), holding \\(x_2\\) constant, moving the interaction term.\nfrom the distribution, plot percentiles.\n\nAnother technique, illustrated below, is to compute “average effects” just as we have in other settings. The example below compares average effects from interaction models and non-interactive models just (so just compression effects).\n\n\n\nUsing the democratic peace data, let’s look at two models. In the first, lets think about how democracy and shared borders intersect. We’ll examine a model including both variables but no interaction, and so examine compression effects. Then, let’s interact democracy and border and look at the effects. In the second case, we’ll do the same thing with democracy and the balance of power under the idea that differences in power may affect conflict differently for democrats and non democrats.\n\n\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n \ndp$deml_border &lt;- dp$deml*dp$border\n\ndemborderint &lt;- glm(dispute~deml+border+deml_border+caprat+jio, data=dp, family=binomial(link=\"logit\"))\n\ndembordercomp &lt;- glm(dispute~deml+border+caprat+jio, data=dp, family=binomial(link=\"logit\"))\n\nstargazer(dembordercomp, demborderint, type=\"html\", title=\"Democracy and Shared Borders\")\n\n\n\nDemocracy and Shared Borders\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\ndispute\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\ndeml\n\n\n-0.048***\n\n\n-0.067***\n\n\n\n\n\n\n(0.008)\n\n\n(0.012)\n\n\n\n\n\n\n\n\n\n\n\n\nborder\n\n\n1.143***\n\n\n1.338***\n\n\n\n\n\n\n(0.078)\n\n\n(0.118)\n\n\n\n\n\n\n\n\n\n\n\n\ndeml_border\n\n\n\n\n0.033**\n\n\n\n\n\n\n\n\n(0.015)\n\n\n\n\n\n\n\n\n\n\n\n\ncaprat\n\n\n-0.003***\n\n\n-0.003***\n\n\n\n\n\n\n(0.0004)\n\n\n(0.0004)\n\n\n\n\n\n\n\n\n\n\n\n\njio\n\n\n-0.024***\n\n\n-0.024***\n\n\n\n\n\n\n(0.003)\n\n\n(0.003)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-2.860***\n\n\n-2.966***\n\n\n\n\n\n\n(0.115)\n\n\n(0.128)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n20,142\n\n\n20,142\n\n\n\n\nLog Likelihood\n\n\n-3,341.355\n\n\n-3,338.786\n\n\n\n\nAkaike Inf. Crit.\n\n\n6,692.710\n\n\n6,689.571\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\nAnd let’s plot average effects for these two models.\n\n\ncode\nlibrary(averagemarginaleffects)\n\n# Compression\n# compute effects using the averagemarginaleffects package\n\ncompression &lt;- compute_average_effects(\n  model = dembordercomp,\n  data = dp,\n  x_variable = \"deml\",\n  pred_type = \"response\",\n  mediator = \"border\",\n  interaction = NULL,\n  quiet=TRUE\n)\n\n\n\n# plot the compression effect\n\ncomp_plot &lt;- ggplot(compression, aes(x = x, y = median_prediction, color = factor(z))) +\n  geom_line() +\n  geom_ribbon(aes(ymin = median_prediction - 1.96 * median_se, ymax = median_prediction + 1.96 * median_se, fill = factor(z)), alpha = 0.2) +\n  labs(title = \"Compression Effects\",\n       x = \"Polity\",\n       y = \"Pr(Dispute)\",\n       color = \"Shared Border\",\n       fill = \"Shared Border\") +\n  scale_color_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  scale_fill_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n# Interaction \n# compute effects using the averagemarginaleffects package\n\ninteraction &lt;- compute_average_effects(\n  model = demborderint,\n  data = dp,\n  x_variable = \"deml\",\n  interaction = list(vars = c(\"deml\", \"border\"), int_var = \"deml_border\"),\n  pred_type = \"response\",\n  mediator = NULL,\n  quiet=TRUE\n)\n\n# plot the interaction effect\n\nint_plot &lt;- ggplot(interaction, aes(x = x, y = median_prediction, color = factor(z))) +\n  geom_line() +\n  geom_ribbon(aes(ymin = median_prediction - 1.96 * median_se, ymax = median_prediction + 1.96 * median_se, fill = factor(z)), alpha = 0.2) +\n  labs(title = \"Interaction Effects\",\n       x = \"Polity\",\n       y = \"Pr(Dispute)\",\n       color = \"Shared Border\",\n       fill = \"Shared Border\") +\n  scale_color_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  scale_fill_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\ncomp_plot+int_plot\n\n\n\n\n\n\n\n\n\nThe predictions of the compression and interactive models are very similar in this case.\n\n\n\n\n\ncode\n#ln of caprat\ndp$lncaprat &lt;- log(dp$caprat)\n#interaction term, deml*lncaprat\ndp$deml_caprat &lt;- dp$deml*dp$lncaprat\n\n#model\n\nmcompression &lt;- glm(dispute~deml+lncaprat+border+jio, data=dp, family=binomial(link=\"logit\"))\n\nminteraction &lt;- glm(dispute~deml+lncaprat+deml_caprat+border+jio, data=dp, family=binomial(link=\"logit\"))\n\nstargazer(mcompression, minteraction, type=\"html\", title=\"Democracy and Capabilities\")\n\n\n\nDemocracy and Capabilities\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\ndispute\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\ndeml\n\n\n-0.045***\n\n\n-0.077***\n\n\n\n\n\n\n(0.008)\n\n\n(0.013)\n\n\n\n\n\n\n\n\n\n\n\n\nlncaprat\n\n\n-0.254***\n\n\n-0.178***\n\n\n\n\n\n\n(0.025)\n\n\n(0.033)\n\n\n\n\n\n\n\n\n\n\n\n\ndeml_caprat\n\n\n\n\n0.013***\n\n\n\n\n\n\n\n\n(0.004)\n\n\n\n\n\n\n\n\n\n\n\n\nborder\n\n\n0.919***\n\n\n0.893***\n\n\n\n\n\n\n(0.089)\n\n\n(0.090)\n\n\n\n\n\n\n\n\n\n\n\n\njio\n\n\n-0.026***\n\n\n-0.025***\n\n\n\n\n\n\n(0.003)\n\n\n(0.003)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-2.219***\n\n\n-2.425***\n\n\n\n\n\n\n(0.149)\n\n\n(0.165)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n20,142\n\n\n20,142\n\n\n\n\nLog Likelihood\n\n\n-3,344.418\n\n\n-3,339.020\n\n\n\n\nAkaike Inf. Crit.\n\n\n6,698.835\n\n\n6,690.039\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\n\n\ncode\nlibrary(averagemarginaleffects)\n\n\n# average effects\n\ncap_int &lt;- compute_average_effects(\n  model = minteraction,\n  data = dp,\n  x_variable = \"lncaprat\",\n  interaction = list(vars = c(\"lncaprat\", \"deml\"), int_var = \"deml_caprat\"),\n  pred_type = \"response\",\n  mediator = NULL,\n  z_values = c(-10,  10),\n  quiet=TRUE\n)\n\n#plot\n\nint &lt;- ggplot(cap_int, aes(x = x, y = median_prediction, color = factor(z))) +\n  geom_line() +\n  geom_ribbon(aes(ymin = median_prediction - 1.96 * median_se, ymax = median_prediction + 1.96 * median_se, fill = factor(z)), alpha = 0.2) +\n  labs(title = \"Interaction Effects\",\n       x = \"ln(Capabilities Ratio)\",\n       y = \"Pr(Dispute)\",\n       color = \"Polity\",\n       fill = \"Polity\") +\n  scale_color_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  scale_fill_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n\n\ncomp &lt;- compute_average_effects(\n  model = mcompression,\n  data = dp,\n  x_variable = \"lncaprat\",\n  pred_type = \"response\",\n  mediator = \"deml\",\n  z_values = c(-10,  10),\n  quiet=TRUE\n)\n\n\ncomp &lt;- ggplot(comp, aes(x = x, y = median_prediction, color = factor(z))) +\n  geom_line() +\n  geom_ribbon(aes(ymin = median_prediction - 1.96 * median_se, ymax = median_prediction + 1.96 * median_se, fill = factor(z)), alpha = 0.2) +\n  labs(title = \"Compression Effects\",\n       x = \"ln(Capabilities Ratio)\",\n       y = \"Pr(Dispute)\",\n       color = \"Polity\",\n       fill = \"Polity\") +\n  scale_color_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  scale_fill_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\ncomp+int\n\n\n\n\n\n\n\n\n\nIn this set of models, the predictions are substantially different. The slopes in the noninteractive model are very similar (unsurprisingly - see the simulation above), but those slopes over capabilities vary dramatically in the interactive model. In the interactive model, the chance of conflict in autocratic dyads declines precipitously as the capabilities ratio grows (as the pair becomes less balanced). In democratic pairs, the chances of conflict are lower and practically constant across the range of capabilities ratio, indicating democratic dyads fight less often than do autocratic pairs, and that the balance of capabilities matters little to their chances of conflict."
  },
  {
    "objectID": "interactions24.html#linear-models",
    "href": "interactions24.html#linear-models",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "coefficients on dummy variables represent intercept or level shifts; differential intercepts between groups on the dummy.\nstructural stability (same slope across groups) - relaxed with multiplicative interactions.\ninclude constituents.\nconstituent interpretation is always conditional.\ninteraction coefficient interpretation is always conditional.\ninference is always conditional."
  },
  {
    "objectID": "interactions24.html#whats-the-difference-compression",
    "href": "interactions24.html#whats-the-difference-compression",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "In nonlinear models, the effects of \\(x\\) variables on \\(Pr(y=1)\\) are increasingly compressed as \\(x\\beta\\) moves away from zero, and as \\(Pr(y=1)\\) moves towards its limits.\nIn the linear model, the effects of \\(x\\) are unconditional unless we include an interaction; in the nonlinear model, the effects are always conditional due to compression, even without a multiplicative interaction.\nCompression at larger absolute values of \\(x\\beta\\) creates what Nagler (1994) calls “inherent” interaction in nonlinear models. So we need to answer two questions:\n\nhow can we take advantage of “inherent interaction” due to compression?\nif models are inherently interactive, when or why do we need multiplicative interactions?\n\nLet’s think about the probability space and quantities of interest as a way to understand compression."
  },
  {
    "objectID": "interactions24.html#quantities-of-interest",
    "href": "interactions24.html#quantities-of-interest",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "Predicted probability: \\[Pr(y=1) = F(\\beta_0 + \\beta_1x_1+ \\beta_2x_2)\\]\n\n\n\n\n\\[\\frac{\\partial y}{\\partial x_1} =  \\beta_1\\]\n\n\n\n\\[\\frac{\\partial y}{\\partial x_k} = \\frac{\\partial F(x\\beta)}{\\partial x_k} =   \\frac{\\partial F(x\\beta)}{\\partial x\\beta} \\cdot  \\frac{\\partial x\\beta}{\\partial x_k}  = f(x\\beta)\\beta_k\\]\n\n\n\n\n\n\n\\[\\frac{\\partial y}{\\partial x_k} = \\frac{\\partial \\Lambda(x\\beta)}{\\partial x_k} =\\lambda(x\\beta)\\beta_k\\] since the derivative of the CDF is the PDF, and where the logit pdf is\n\\[\\lambda =\\frac{e^{x\\beta}}{(1+e^{x\\beta})^2}\\]\n\n\n\n\nThe logit ME can be rewritten:\n\\[\\begin{eqnarray}\n\\frac{\\partial \\Lambda(x\\beta)}{\\partial x_k} =\\lambda(x\\beta) \\cdot \\beta_k \\nonumber \\\\\n=\\left[\\frac{e^{x\\beta}}{(1+e^{x\\beta})^2}\\right] \\cdot \\beta_k \\nonumber \\\\\n= \\left[\\frac{e^{x\\beta}}{(1+e^{x\\beta})} \\frac{1}{(1+e^{x\\beta})}\\right] \\cdot \\beta_k \\nonumber \\\\\n= \\Lambda(x\\beta) \\left(1-\\frac{e^{x\\beta}}{1+e^{x\\beta}}\\right) \\cdot \\beta_k \\nonumber \\\\\n= \\Lambda(x\\beta)\\cdot (1-\\Lambda(x\\beta)) \\cdot\\beta_k \\nonumber \\\\\n= Pr(y=0) \\cdot Pr(y=1) \\cdot  \\beta_k \\nonumber\n\\end{eqnarray}\\]\n\n\n\n\\[\\frac{\\partial y}{\\partial x_k} = \\frac{\\partial \\Phi(x\\beta)}{\\partial x_k} =\\phi(x\\beta)\\beta_k\\]\nIn both cases, you can see the effect of \\(x_k\\) depends explicitly on the value of \\(x\\beta\\). Put differently, the marginal effect of \\(x_k\\) is {} on \\(x\\beta\\), or where we are on the \\(x\\) axis."
  },
  {
    "objectID": "interactions24.html#compression-effects",
    "href": "interactions24.html#compression-effects",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "Compression refers to the different rates of change in the \\(Pr(Y=1)\\) across the probability range.\n\nthe effects of changes in \\(x\\) are greatest at \\(Pr(Y=1)=.5\\).\nthe effects of changes in \\(x\\) decline as \\(Pr(Y=1)\\) approaches the limits.\nthe same change in \\(x\\) has an increasingly “compressed” smaller effect as \\(Pr(Y=1)\\Rightarrow(0,1)\\)."
  },
  {
    "objectID": "interactions24.html#compression-v.-interaction",
    "href": "interactions24.html#compression-v.-interaction",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "On the preceding slide, notice how the effects of \\(x\\) on \\(y\\) are different due to compression effects in all three curves. But also notice that the derivatives at \\(Pr(Y=1)\\) are the same for the non-interactive curves, different for the interactive one. Look at their slopes (derivatives) in the following inset:\n\n\ncode\nknitr::include_app(url = \"https://clavedark.shinyapps.io/interactions/\", height = \"1000px\")"
  },
  {
    "objectID": "interactions24.html#why-use-an-interaction",
    "href": "interactions24.html#why-use-an-interaction",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "This helps answer when/why we’d use an interaction. The derivatives at \\(Pr(Y=1)\\) shift in the interactive model, remain constant in the non-interactive model. Note several things:\n\nCompression effects are present regardless of the presence or absence of interactions. So the interactive model has compression and multiplicative interaction effects at work.\nThe CDF shifts left-right based on intercept changes in \\(x_i \\beta\\), but derivatives remain the same for values of \\(y\\).\nWith interactions, the derivatives changes for values of \\(y\\); the derivative at \\(Pr(Y=1)\\) changes.\nChoosing the multiplicative model implies our theory tells us the slope at \\(Pr(Y=1)\\) is different due to \\(x \\dot z\\).\nIn the non-interactive model, \\(\\frac{\\partial Pr(Y=1)}{\\partial x_i\\beta}=\\frac{\\partial Pr(Y=1)}{\\partial x_j\\beta}\\); in the interactive model, this is not true."
  },
  {
    "objectID": "interactions24.html#when-do-we-specify-a-multiplicative-interaction",
    "href": "interactions24.html#when-do-we-specify-a-multiplicative-interaction",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "when we believe that the changes in \\(y\\) are a function of \\(x|z=i\\ldots j\\).\nwhen we specifically think the slope (derivative) at a particular value of \\(y\\) given \\(x\\) is not structurally stable; i.e. it varies by groups on some variable \\(z\\).\nRainey (2016) suggests we err on the side of including interaction terms."
  },
  {
    "objectID": "interactions24.html#compression-interaction",
    "href": "interactions24.html#compression-interaction",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "always explore compression effects. Postestimation, examine how changes in variable values shift the curve in interesting ways. Explore interesting combinations of variables.\nnever rely on compression to handle a conditional expectation - if you have a conditional expectation, use a multiplicative interaction.\nthere’s continuing interest in how to think about compression and interaction in the literature, so read what folks are saying."
  },
  {
    "objectID": "interactions24.html#method",
    "href": "interactions24.html#method",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "Computing QIs for interactions is not difficult, but has a number of moving parts. The go-to technique is laid out by Matt Golder as part of the materials accompanying Brambor, Clark, and Golder (2006). That technique unfolds as follows:\n\nestimate the model\nsimulate the parameter distribution\ngenerate \\(pr(y=j)\\) for different values of \\(x_1\\), holding \\(x_2\\) constant, moving the interaction term.\nfrom the distribution, plot percentiles.\n\nAnother technique, illustrated below, is to compute “average effects” just as we have in other settings. The example below compares average effects from interaction models and non-interactive models just (so just compression effects)."
  },
  {
    "objectID": "interactions24.html#example",
    "href": "interactions24.html#example",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "Using the democratic peace data, let’s compare the usual regression to one where we interact democracy and the balance of power under the idea that differences in power may affect conflict differently for democrats and non democrats.\n\n\ncode\nlibrary(averagemarginaleffects)\n\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n \ndp$deml_border &lt;- dp$deml*dp$border\n\ndemborderint &lt;- glm(dispute~deml+border+deml_border+caprat+jio, data=dp, family=binomial(link=\"logit\"))\n\ndembordercomp &lt;- glm(dispute~deml+border+caprat+jio, data=dp, family=binomial(link=\"logit\"))\n\n# Compression\n# compute effects using the averagemarginaleffects package\n\ncompression &lt;- compute_average_effects(\n  model = dembordercomp,\n  data = dp,\n  x_variable = \"deml\",\n  pred_type = \"response\",\n  mediator = \"border\",\n  interaction = NULL,\n  quiet=TRUE\n)\n\n\n\n# plot the compression effect\n\ncomp_plot &lt;- ggplot(compression, aes(x = x, y = median_prediction, color = factor(z))) +\n  geom_line() +\n  geom_ribbon(aes(ymin = median_prediction - 1.96 * median_se, ymax = median_prediction + 1.96 * median_se, fill = factor(z)), alpha = 0.2) +\n  labs(title = \"Compression Effects\",\n       x = \"Polity\",\n       y = \"Pr(Dispute)\",\n       color = \"Shared Border\",\n       fill = \"Shared Border\") +\n  scale_color_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  scale_fill_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n# Interaction \n# compute effects using the averagemarginaleffects package\n\ninteraction &lt;- compute_average_effects(\n  model = demborderint,\n  data = dp,\n  x_variable = \"deml\",\n  interaction = list(vars = c(\"deml\", \"border\"), int_var = \"deml_border\"),\n  pred_type = \"response\",\n  mediator = NULL,\n  quiet=TRUE\n)\n\n# plot the interaction effect\n\nint_plot &lt;- ggplot(interaction, aes(x = x, y = median_prediction, color = factor(z))) +\n  geom_line() +\n  geom_ribbon(aes(ymin = median_prediction - 1.96 * median_se, ymax = median_prediction + 1.96 * median_se, fill = factor(z)), alpha = 0.2) +\n  labs(title = \"Interaction Effects\",\n       x = \"Polity\",\n       y = \"Pr(Dispute)\",\n       color = \"Shared Border\",\n       fill = \"Shared Border\") +\n  scale_color_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  scale_fill_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\ncomp_plot+int_plot\n\n\n\n\n\n\n\n\n\n\n\ncode\nlibrary(averagemarginaleffects)\n\n#ln of caprat\ndp$lncaprat &lt;- log(dp$caprat)\n#interaction term, deml*lncaprat\ndp$deml_caprat &lt;- dp$deml*dp$lncaprat\n\n#model\n\nmcompression &lt;- glm(dispute~deml+lncaprat+border+jio, data=dp, family=binomial(link=\"logit\"))\n\nminteraction &lt;- glm(dispute~deml+lncaprat+deml_caprat+border+jio, data=dp, family=binomial(link=\"logit\"))\n\n\n# average effects\n\ncap_int &lt;- compute_average_effects(\n  model = minteraction,\n  data = dp,\n  x_variable = \"lncaprat\",\n  interaction = list(vars = c(\"lncaprat\", \"deml\"), int_var = \"deml_caprat\"),\n  pred_type = \"response\",\n  mediator = NULL,\n  z_values = c(-10,  10),\n  quiet=TRUE\n)\n\n#plot\n\nint &lt;- ggplot(cap_int, aes(x = x, y = median_prediction, color = factor(z))) +\n  geom_line() +\n  geom_ribbon(aes(ymin = median_prediction - 1.96 * median_se, ymax = median_prediction + 1.96 * median_se, fill = factor(z)), alpha = 0.2) +\n  labs(title = \"Capabilities and Shared Border - Interaction\",\n       x = \"ln(Capabilities Ratio)\",\n       y = \"Pr(Dispute)\",\n       color = \"Polity\",\n       fill = \"Polity\") +\n  scale_color_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  scale_fill_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n\n\ncomp &lt;- compute_average_effects(\n  model = mcompression,\n  data = dp,\n  x_variable = \"lncaprat\",\n  pred_type = \"response\",\n  mediator = \"deml\",\n  z_values = c(-10,  10),\n  quiet=TRUE\n)\n\n\ncomp &lt;- ggplot(comp, aes(x = x, y = median_prediction, color = factor(z))) +\n  geom_line() +\n  geom_ribbon(aes(ymin = median_prediction - 1.96 * median_se, ymax = median_prediction + 1.96 * median_se, fill = factor(z)), alpha = 0.2) +\n  labs(title = \"Capabilities and Democracy - Compression\",\n       x = \"ln(Capabilities Ratio)\",\n       y = \"Pr(Dispute)\",\n       color = \"Polity\",\n       fill = \"Polity\") +\n  scale_color_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  scale_fill_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\ncomp+int"
  },
  {
    "objectID": "interactions24.html#multiplicative-interactions",
    "href": "interactions24.html#multiplicative-interactions",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "coefficients on dummy variables represent intercept or level shifts; differential intercepts between groups on the dummy.\nstructural stability (same slope across groups) - relaxed with multiplicative interactions.\ninclude constituents variables in the model.\nconstituent interpretation is always conditional.\ninteraction coefficient interpretation is always conditional.\ninference is always conditional. To evaluate whether \\(\\beta_1 + \\beta_3 \\neq 0\\), we need to compute a standard error on \\(\\beta_1 + \\beta_3\\). Matt Golder’s web site is an excellent reference for anything interaction-related, and particularly for guidance on computing standard errors."
  },
  {
    "objectID": "interactions24.html#compression-and-interaction",
    "href": "interactions24.html#compression-and-interaction",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "In the simulation below notice how the effects of \\(x\\) on \\(y\\) are different due to compression effects - that is, the effect of \\(x\\) is greatest at \\(x=0, y=.5\\), and declines as \\(|x|\\) increases. Also, notice how additive changes to the intercept shift the curve left-right, but do not change the slope at \\(Pr(Y=1)\\). Multiplicative (interactive) changes change the derivative of the curve. Notice that compression exists in both the interactive and non-interactive settings."
  },
  {
    "objectID": "interactions24.html#whats-the-difference-compression.",
    "href": "interactions24.html#whats-the-difference-compression.",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "In nonlinear models, the effects of \\(x\\) variables on \\(Pr(y=1)\\) are increasingly compressed as \\(x\\beta\\) moves away from zero, and as \\(Pr(y=1)\\) moves towards its limits. Berry, DeMeritt, and Esarey (2010) seem to originate the term “compression” referring to the changing effects of \\(x\\) on \\(y\\) as \\(x\\beta\\) changes in monotonic link functions (e.g. logit, probit, etc.). Compression is describing what Nagler (1994) calls “inherent” interaction in nonlinear models; it describes why the effects of \\(x\\) depend on the values of the other \\(x\\) variables in the model.\nIn the linear model, the effects of \\(x\\) are unconditional unless we include an interaction; in the nonlinear model, the effects are always conditional due to compression, even without a multiplicative interaction. So we need to answer two questions:\n\nhow can we take advantage of “inherent interaction” due to compression?\nif models are inherently interactive, when or why do we need multiplicative interactions?\n\nFor the second question, this is a debate in the literature. Berry, DeMeritt, and Esarey (2010) suggest interaction terms are not always necessary; Rainey (2016) suggests otherwise.\nLet’s think about the probability space and quantities of interest as a way to understand compression."
  },
  {
    "objectID": "interactions24.html#examples",
    "href": "interactions24.html#examples",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "Using the democratic peace data, let’s look at two models. In the first, lets think about how democracy and shared borders intersect. We’ll examine a model including both variables but no interaction, and so examine compression effects. Then, let’s interact democracy and border and look at the effects. In the second case, we’ll do the same thing with democracy and the balance of power under the idea that differences in power may affect conflict differently for democrats and non democrats.\n\n\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n \ndp$deml_border &lt;- dp$deml*dp$border\n\ndemborderint &lt;- glm(dispute~deml+border+deml_border+caprat+jio, data=dp, family=binomial(link=\"logit\"))\n\ndembordercomp &lt;- glm(dispute~deml+border+caprat+jio, data=dp, family=binomial(link=\"logit\"))\n\nstargazer(dembordercomp, demborderint, type=\"html\", title=\"Democracy and Shared Borders\")\n\n\n\nDemocracy and Shared Borders\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\ndispute\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\ndeml\n\n\n-0.048***\n\n\n-0.067***\n\n\n\n\n\n\n(0.008)\n\n\n(0.012)\n\n\n\n\n\n\n\n\n\n\n\n\nborder\n\n\n1.143***\n\n\n1.338***\n\n\n\n\n\n\n(0.078)\n\n\n(0.118)\n\n\n\n\n\n\n\n\n\n\n\n\ndeml_border\n\n\n\n\n0.033**\n\n\n\n\n\n\n\n\n(0.015)\n\n\n\n\n\n\n\n\n\n\n\n\ncaprat\n\n\n-0.003***\n\n\n-0.003***\n\n\n\n\n\n\n(0.0004)\n\n\n(0.0004)\n\n\n\n\n\n\n\n\n\n\n\n\njio\n\n\n-0.024***\n\n\n-0.024***\n\n\n\n\n\n\n(0.003)\n\n\n(0.003)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-2.860***\n\n\n-2.966***\n\n\n\n\n\n\n(0.115)\n\n\n(0.128)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n20,142\n\n\n20,142\n\n\n\n\nLog Likelihood\n\n\n-3,341.355\n\n\n-3,338.786\n\n\n\n\nAkaike Inf. Crit.\n\n\n6,692.710\n\n\n6,689.571\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\nAnd let’s plot average effects for these two models.\n\n\ncode\nlibrary(averagemarginaleffects)\n\n# Compression\n# compute effects using the averagemarginaleffects package\n\ncompression &lt;- compute_average_effects(\n  model = dembordercomp,\n  data = dp,\n  x_variable = \"deml\",\n  pred_type = \"response\",\n  mediator = \"border\",\n  interaction = NULL,\n  quiet=TRUE\n)\n\n\n\n# plot the compression effect\n\ncomp_plot &lt;- ggplot(compression, aes(x = x, y = median_prediction, color = factor(z))) +\n  geom_line() +\n  geom_ribbon(aes(ymin = median_prediction - 1.96 * median_se, ymax = median_prediction + 1.96 * median_se, fill = factor(z)), alpha = 0.2) +\n  labs(title = \"Compression Effects\",\n       x = \"Polity\",\n       y = \"Pr(Dispute)\",\n       color = \"Shared Border\",\n       fill = \"Shared Border\") +\n  scale_color_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  scale_fill_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n# Interaction \n# compute effects using the averagemarginaleffects package\n\ninteraction &lt;- compute_average_effects(\n  model = demborderint,\n  data = dp,\n  x_variable = \"deml\",\n  interaction = list(vars = c(\"deml\", \"border\"), int_var = \"deml_border\"),\n  pred_type = \"response\",\n  mediator = NULL,\n  quiet=TRUE\n)\n\n# plot the interaction effect\n\nint_plot &lt;- ggplot(interaction, aes(x = x, y = median_prediction, color = factor(z))) +\n  geom_line() +\n  geom_ribbon(aes(ymin = median_prediction - 1.96 * median_se, ymax = median_prediction + 1.96 * median_se, fill = factor(z)), alpha = 0.2) +\n  labs(title = \"Interaction Effects\",\n       x = \"Polity\",\n       y = \"Pr(Dispute)\",\n       color = \"Shared Border\",\n       fill = \"Shared Border\") +\n  scale_color_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  scale_fill_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\ncomp_plot+int_plot\n\n\n\n\n\n\n\n\n\nThe predictions of the compression and interactive models are very similar in this case.\n\n\n\n\n\ncode\n#ln of caprat\ndp$lncaprat &lt;- log(dp$caprat)\n#interaction term, deml*lncaprat\ndp$deml_caprat &lt;- dp$deml*dp$lncaprat\n\n#model\n\nmcompression &lt;- glm(dispute~deml+lncaprat+border+jio, data=dp, family=binomial(link=\"logit\"))\n\nminteraction &lt;- glm(dispute~deml+lncaprat+deml_caprat+border+jio, data=dp, family=binomial(link=\"logit\"))\n\nstargazer(mcompression, minteraction, type=\"html\", title=\"Democracy and Capabilities\")\n\n\n\nDemocracy and Capabilities\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\ndispute\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\ndeml\n\n\n-0.045***\n\n\n-0.077***\n\n\n\n\n\n\n(0.008)\n\n\n(0.013)\n\n\n\n\n\n\n\n\n\n\n\n\nlncaprat\n\n\n-0.254***\n\n\n-0.178***\n\n\n\n\n\n\n(0.025)\n\n\n(0.033)\n\n\n\n\n\n\n\n\n\n\n\n\ndeml_caprat\n\n\n\n\n0.013***\n\n\n\n\n\n\n\n\n(0.004)\n\n\n\n\n\n\n\n\n\n\n\n\nborder\n\n\n0.919***\n\n\n0.893***\n\n\n\n\n\n\n(0.089)\n\n\n(0.090)\n\n\n\n\n\n\n\n\n\n\n\n\njio\n\n\n-0.026***\n\n\n-0.025***\n\n\n\n\n\n\n(0.003)\n\n\n(0.003)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-2.219***\n\n\n-2.425***\n\n\n\n\n\n\n(0.149)\n\n\n(0.165)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n20,142\n\n\n20,142\n\n\n\n\nLog Likelihood\n\n\n-3,344.418\n\n\n-3,339.020\n\n\n\n\nAkaike Inf. Crit.\n\n\n6,698.835\n\n\n6,690.039\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\n\n\ncode\nlibrary(averagemarginaleffects)\n\n\n# average effects\n\ncap_int &lt;- compute_average_effects(\n  model = minteraction,\n  data = dp,\n  x_variable = \"lncaprat\",\n  interaction = list(vars = c(\"lncaprat\", \"deml\"), int_var = \"deml_caprat\"),\n  pred_type = \"response\",\n  mediator = NULL,\n  z_values = c(-10,  10),\n  quiet=TRUE\n)\n\n#plot\n\nint &lt;- ggplot(cap_int, aes(x = x, y = median_prediction, color = factor(z))) +\n  geom_line() +\n  geom_ribbon(aes(ymin = median_prediction - 1.96 * median_se, ymax = median_prediction + 1.96 * median_se, fill = factor(z)), alpha = 0.2) +\n  labs(title = \"Interaction Effects\",\n       x = \"ln(Capabilities Ratio)\",\n       y = \"Pr(Dispute)\",\n       color = \"Polity\",\n       fill = \"Polity\") +\n  scale_color_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  scale_fill_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n\n\ncomp &lt;- compute_average_effects(\n  model = mcompression,\n  data = dp,\n  x_variable = \"lncaprat\",\n  pred_type = \"response\",\n  mediator = \"deml\",\n  z_values = c(-10,  10),\n  quiet=TRUE\n)\n\n\ncomp &lt;- ggplot(comp, aes(x = x, y = median_prediction, color = factor(z))) +\n  geom_line() +\n  geom_ribbon(aes(ymin = median_prediction - 1.96 * median_se, ymax = median_prediction + 1.96 * median_se, fill = factor(z)), alpha = 0.2) +\n  labs(title = \"Compression Effects\",\n       x = \"ln(Capabilities Ratio)\",\n       y = \"Pr(Dispute)\",\n       color = \"Polity\",\n       fill = \"Polity\") +\n  scale_color_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  scale_fill_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\ncomp+int\n\n\n\n\n\n\n\n\n\nIn this set of models, the predictions are substantially different. The slopes in the noninteractive model are very similar (unsurprisingly - see the simulation above), but those slopes over capabilities vary dramatically in the interactive model. In the interactive model, the chance of conflict in autocratic dyads declines precipitously as the capabilities ratio grows (as the pair becomes less balanced). In democratic pairs, the chances of conflict are lower and practically constant across the range of capabilities ratio, indicating democratic dyads fight less often than do autocratic pairs, and that the balance of capabilities matters little to their chances of conflict."
  },
  {
    "objectID": "choiceA24.html",
    "href": "choiceA24.html",
    "title": "Choice Models",
    "section": "",
    "text": "It’s common to have data where individuals choose among multiple, unordered outcomes:\n\nvoters choosing among candidates.\nconsumers choosing among products.\njob choice.\nforeign policy choice.\n\n\n\nThe multinomial and conditional logit models are robust, easy to estimate and interpret, and less punitive than some others if you violate assumptions.\n\nAny outcome can be ordered; not every outcome should be ordered.\nIf we use an ordered model on unordered data, the parameters are likely to be biased and not to make sense\nIf we use a unordered model on ordered data, we are neglecting to use some of the information in the y variable so our model may be inefficient, but the consequences are not as severe.\n\n\n\n\nFollowing Long (pp. 152ff):\nSuppose an unordered, discrete variable \\(y = j\\) where \\(j = 1,2 \\ldots\\). Let \\(Pr(y=j) = x\\beta_j +\\epsilon\\) so the probability of any value of \\(y\\) is a function of \\(x_i\\) and a vector of outcome-specific coefficients, \\(\\beta_j\\). Because these are probabilities, they cannot be negative, so transform \\(exp(x\\beta_j)\\) - but now the sum of these over all values of \\(j\\) exceed 1. So divide by the sum of all the categories, and now we sum to 1 : \\(\\frac{exp(x\\beta_j)}{\\sum_{j=1}^{J} exp(x\\beta_j)}\\)\n\nBut \\(\\frac{exp(x\\beta_j)}{\\sum_{j=1}^{J} exp(x\\beta_j)}\\) does not suggest a single set of solutions \\(\\beta_j\\), so is not identified.\nThe usual solution is to assume one \\(\\beta_j=0\\).\nRecalling that \\(j = 1, 2, \\ldots\\), we restrict the first category (\\(\\beta_1=0\\)), and rewrite \\(\\frac{exp(x\\beta_j)}{\\sum_{j=2}^{J} exp(x\\beta_j)}\\).\n\nThis means \\(y=1\\) is the reference category, so now \\(\\beta_j\\) is compared to \\(\\beta_1\\) as if the quantity is now \\(\\beta_j - \\beta_1\\). This is, after all, the comparison we usually make: that the effect of \\(x_i\\) is \\(\\beta_j\\) relative to the excluded category.\nThis leaves us with:\n\\[Pr(y=1) = \\frac{1}{1+ \\sum_{j=2}^{J} exp(x\\beta_j)} \\]\nfor \\(j=1\\), and for \\(j&gt;1\\), we have:\n\\[Pr(y=j) = \\frac{exp(x\\beta_j)}{\\sum_{j=2}^{J} exp(x\\beta_j)}\\]\nNote constraining \\(\\beta_1 = 0\\) is arbitrary - we can set any of the categories as the reference.\n\n\n\nAnother, more interesting way, to motivate models for choices over outcomes is from a rational choice framework. We consider individuals making choices over outcomes. Those choices are informed by their utilities for the outcomes.\n\n\n\nAn individual’s utility for each choice is given by\n\\[\nU_{i,j}= \\mu_{i,j} + \\varepsilon_{i,j}\n\\]\nwhere the subscripts indicate the \\(i^{th}\\) individual and the \\(j^{th}\\) outcome. Each individual’s utility for each outcome is a function of a systematic component, \\(\\mu_{i,j}\\) and a stochastic component \\(\\varepsilon_{i,j}\\). We can parameterize the systematic component as a function of some variables, ${i,j} =x{i,j} $ :\n\\[\nU_{i,j}=x_{i,j} \\beta  + \\varepsilon_{i,j} \\nonumber\n\\]\nSome variables, \\(x\\) have \\(\\beta\\) effect on \\(i\\)’s utility for outcome \\(j\\). Moreover, each individual has a complete set of preferences over the \\(J\\) outcomes and those preferences are transitive. Each individual makes choices to maximize their utility, thus comparing choices \\(j, k \\ldots J\\) in a pairwise fashion:\n\\[Pr(Y=j) = Pr(U_{i,j}&gt;U_{i,k})  \\\\\n      = Pr(x_{i,j} \\beta + \\varepsilon_{i,j} &gt;  x_{i,k}\\beta  + \\varepsilon_{i,k})\\\\\n       = Pr(\\varepsilon_{i,j}-\\varepsilon_{i,k} &gt; x_{i,k} \\beta -x_{i,j} \\beta )\n\\]\nIf the difference in the random components of \\(j\\) and any other outcome, \\(k\\) is greater than the difference between the systematic components, then \\(j\\) maximizes \\(i\\)’s utility. Looking at the last inequality above, we can see this will happen when \\(\\varepsilon_{i,j}\\) is big, when \\(\\beta x_{i,j}\\) is big, or both.\nTo estimate this, we have to make an assumption about how the errors are distributed. The way we’ve derived the model, the errors are actually the difference between the stochastic parts of outcomes \\(i, j\\):\n\\[\nF(\\varepsilon_{i,j}) = exp[-exp(-\\varepsilon_{i,j})] \\nonumber\n\\]\nassume \\(\\varepsilon_{i,j}\\)s are i.i.d Type 1 generalized extreme value (Gumbel); if the errors are i.i.d., their difference is logistic:\nThe probability \\(Y=j\\) is\n\\[\nPr(Y=j) = \\frac{e^{X_i \\beta_j }}{\\sum \\limits_{j=1}^{J} e^{X_i\\beta_j}} \\nonumber\n\\]\nAs above, the model is not identified since there is no reference category. So we typically constrain \\(\\beta_{i,j=1}=0\\) and estimate the remaining \\(J-1\\) categories:\n\\[\nPr(Y=j) = \\frac{e^{x_i \\beta_j }}{1+\\sum \\limits_{j=2}^{J} e^{x_i\\beta_k }} \\nonumber  \\\\\nPr(Y=1) = \\frac{1}{1+\\sum \\limits_{j=2}^{J} e^{\\beta_k x_i}} \\nonumber\n\\]\nIf \\(J=2\\), the model is the binary logit - so the binary logit is a special case of the CL or MNL models.\n\nThis motivation is nice because it links the likelihood function to a simple, formal model where individuals make choices over ordered preferences.\n\nindividual \\(i\\)\nchooses among \\(j\\) alternatives\nlet \\(d=1\\) if \\(y_i = j\\), else zero.\n\\(Pr(Y=j) = \\frac{e^{x_i \\beta_j }}{\\sum \\limits_{j=1}^{J} e^{x_i\\beta_k }}\\)\n\n\n\n\n\\[\n\\mathcal{L} ( \\beta ) = \\prod _ { i=1}^{N} \\prod_ { j=1}^{J}  Pr \\left( y _ {i } = j | x _ { i } \\right) ^ { d _ { i j } }\n\\]\n\\[\n\\ln \\mathcal{L} ( \\beta ) = \\sum _ { i=1}^{N} \\sum_ { j=1}^{J}  d _ { i j } \\ln Pr \\left( y _ { i } = j | x _ { i } \\right)\n\\]\n\\[\n\\ln \\mathcal{L} ( \\beta ) = \\sum _ { i=1}^{N}  \\sum_ { j=1}^{J} d _ { i j } \\ln \\left( \\frac { \\exp \\left( x _ { i }  \\beta_j \\right) } { \\sum _ { k } \\exp \\left( x _ { i  }  \\beta_j \\right) } \\right)\n\\]\n\n\n\nThe logistic CDF is \\[\nPr(Y=1)=\\frac{e^{x_i\\beta}}{1+e^{x_i\\beta}}\\nonumber\n\\]\nWe use this to represent a binary choice.\nExtending this to a trichotomous choice, say among chocolate, vanilla and strawberry icecream is easy:\n\\[\nPr(Y=chocolate)=\\frac{e^{x_i\\beta_c}}{1+e^{x_i\\beta_c}+e^{x_i\\beta_v}} \\nonumber\n\\]\nassuming strawberry is the reference category.\nThe n-chotomous choice is also easy:\n\\[\nPr(Y=j)=\\frac{e^{x_i\\beta_j}}{1+\\sum\\limits_{j=1}^{J}e^{x_i\\beta}} \\nonumber\n\\]\nYou should be getting the idea the binary logit model is a special case of the more general choice model with errors distributed i.i.d. logistic.\nYou should also see that interpretation is a straightforward extension of the binary logit.\n\n\n\nTo illustrate the MNL and CL models, I’m going to use data from a paper on foreign policy substitution. The \\(y\\) variable represents the foreign policy choice the US president makes given domestic conditions; it takes on 4 values distributed as follows:\n\n0 = no action = 61.13 percent the observations\n1 = MID only = 22.6 percent\n2 = GATT only = 11.8 percent\n3 = both MID and GATT = 4.3 percent\n\nThe data are structured with one observation for each US-month between 1945 and 1990, so for each observation, we observe the policy the president selects, 0, 1, 2, or 3. The variables of interest vary across observations (US-months) but not across outcomes; thus, the data structure and the nature of the independent variables suggest MNL is appropriate.\nThe main expectation here is that the extent of support in Congress for the President’s legislative agenda will affect foreign policy choice - stronger Presidents have greater latitude to choose more costly policies (e.g. military force). The data include a measure of presidential support in Congress, the unemployment rate, the president’s approval rating, and whether the president is in an election year.\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n\n\n\n\n\n\nunemp2\n\n\n0.089\n\n\n0.256**\n\n\n0.609***\n\n\n\n\n\n\n(0.077)\n\n\n(0.106)\n\n\n(0.157)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npressupp\n\n\n0.009\n\n\n-0.043***\n\n\n-0.035*\n\n\n\n\n\n\n(0.009)\n\n\n(0.012)\n\n\n(0.018)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npelect\n\n\n0.167\n\n\n0.278\n\n\n-0.521\n\n\n\n\n\n\n(0.265)\n\n\n(0.330)\n\n\n(0.602)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\napproval\n\n\n0.017*\n\n\n0.023*\n\n\n0.037*\n\n\n\n\n\n\n(0.010)\n\n\n(0.013)\n\n\n(0.021)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-2.996***\n\n\n-1.560\n\n\n-5.935***\n\n\n\n\n\n\n(1.058)\n\n\n(1.392)\n\n\n(2.282)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAkaike Inf. Crit.\n\n\n994.056\n\n\n994.056\n\n\n994.056\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\nRecalling the \\(y\\) variable has 4 categories, the model estimates 3 sets of coefficients, one for each category relative to the reference category. By default, the reference category is the one with the lowest value, in this case, no action. This is something we can change to facilitate interpretation. The coefficients are interpreted as the effect of the variable on the probability of choosing the category relative to the reference category. So you can see presidential support in Congress decreases the chances of GATT action or both military and GATT action relative to doing nothing.\n\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n0\n\n\n1\n\n\n2\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n(4)\n\n\n(5)\n\n\n(6)\n\n\n\n\n\n\n\n\nunemp2\n\n\n0.089\n\n\n0.256**\n\n\n0.609***\n\n\n-0.609***\n\n\n-0.520***\n\n\n-0.354**\n\n\n\n\n\n\n(0.077)\n\n\n(0.106)\n\n\n(0.157)\n\n\n(0.157)\n\n\n(0.164)\n\n\n(0.177)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npressupp\n\n\n0.009\n\n\n-0.043***\n\n\n-0.035*\n\n\n0.035*\n\n\n0.044**\n\n\n-0.008\n\n\n\n\n\n\n(0.009)\n\n\n(0.012)\n\n\n(0.018)\n\n\n(0.018)\n\n\n(0.019)\n\n\n(0.020)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npelect\n\n\n0.167\n\n\n0.278\n\n\n-0.521\n\n\n0.521\n\n\n0.688\n\n\n0.800\n\n\n\n\n\n\n(0.265)\n\n\n(0.330)\n\n\n(0.602)\n\n\n(0.602)\n\n\n(0.623)\n\n\n(0.644)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\napproval\n\n\n0.017*\n\n\n0.023*\n\n\n0.037*\n\n\n-0.037*\n\n\n-0.020\n\n\n-0.014\n\n\n\n\n\n\n(0.010)\n\n\n(0.013)\n\n\n(0.021)\n\n\n(0.021)\n\n\n(0.022)\n\n\n(0.023)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-2.996***\n\n\n-1.560\n\n\n-5.935***\n\n\n5.935***\n\n\n2.938\n\n\n4.373*\n\n\n\n\n\n\n(1.058)\n\n\n(1.392)\n\n\n(2.282)\n\n\n(2.282)\n\n\n(2.381)\n\n\n(2.517)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAkaike Inf. Crit.\n\n\n994.056\n\n\n994.056\n\n\n994.056\n\n\n994.056\n\n\n994.056\n\n\n994.056\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\nCompare the coefficients in the first 3 columns to those in the last 3 columns; notice their similarities but changed signs. Really, changing the reference category has only modest interpretive value. This is especially true because interpreting coefficients is full of pitfalls in the MNL model because, since the effects of variable on one outcome are relative to others, even signs can be misleading. It’s even more essential to compute MNL effects than in most ML cases.\n\n\n\n\nPredicted probabilities are straightforward to compute - they’re an easy extension of the binary logit model.\n\\[P(Y=j) =  \\frac{e^{\\beta_{j}'x_{i}}}{1+\\sum\\limits_{j=1}^{J}e^{\\beta_{j}'x_{i}}}\n\\\\\n\\text{so}~~ P(Y=1) = \\frac{e^{X_{1}\\beta_1}}{1 + e^{X_{1}\\beta_1} +\ne^{X_{2}\\beta_2} + e^{X_{3}\\beta_3}} \\nonumber\n\\]\n\n\nLet’s look at some predictions from the first model.\n\n\ncode\nX &lt;- as.matrix(expand.grid(\n  intercept= 1, \n  unemp2 = median(fpdata$unemp2, na.rm=TRUE),\n  pressupp = seq(43,93, length.out = 50),\n  pelect = 0,\n  approval = median(fpdata$approval, na.rm=TRUE)\n))\n\nxb &lt;- X %*% t(coef(mnl1))\nxb &lt;-data.frame(xb)\nprobs &lt;- exp(xb$X1)/(1+exp(xb$X1)+exp(xb$X2)+exp(xb$X3))\n\n# decompose vcov matrix\nv1 &lt;- as.matrix(vcov(mnl1)[1:5,1:5])\nv2 &lt;- as.matrix(vcov(mnl1)[6:10,6:10])\nv3 &lt;- as.matrix(vcov(mnl1)[11:15,11:15])\n# compute standard errors\nse1 &lt;- sqrt(diag(as.matrix(X) %*% v1 %*% t(X)))\nse2 &lt;- sqrt(diag(as.matrix(X) %*% v2 %*% t(X)))\nse3 &lt;- sqrt(diag(as.matrix(X) %*% v3 %*% t(X)))\n# compute  boundary components\nub1 &lt;- exp(xb$X1 + (1.96 * se1))\nlb1 &lt;- exp(xb$X1 - (1.96 * se1))\nub2 &lt;- exp(xb$X2 + (1.96 * se2))\nlb2 &lt;- exp(xb$X2 - (1.96 * se2))\nub3 &lt;- exp(xb$X3 + (1.96 * se3))\nlb3 &lt;- exp(xb$X3 - (1.96 * se3))\n# compute probabilities\npmid &lt;- exp(xb$X1)/(1+exp(xb$X1)+ exp(xb$X2) + exp(xb$X3))\npgatt &lt;- exp(xb$X2)/(1+exp(xb$X1)+ exp(xb$X2) + exp(xb$X3))\npboth &lt;- exp(xb$X3)/(1+exp(xb$X1)+ exp(xb$X2) + exp(xb$X3))\n# compute boundaries\npubmid &lt;- ub1/(1+ub1+ub2+ub3)\nplbmid &lt;- lb1/(1+lb1+lb2+lb3)\npubgatt &lt;- ub2/(1+ub1+ub2+ub3)\nplbgatt &lt;- lb2/(1+lb1+lb2+lb3)\npubboth &lt;- ub3/(1+ub1+ub2+ub3)\nplbboth &lt;- lb3/(1+lb1+lb2+lb3)\n# data frame \nprobs &lt;- data.frame(X,pmid, pgatt, pboth, pubmid, plbmid, pubgatt, plbgatt, pubboth, plbboth)\n\n#plot  \"#005A43\", \"#8C8C8C\"\n\nggplot(probs, aes(x = pressupp)) +\n  geom_ribbon(aes(ymin = plbmid, ymax = pubmid), fill = \"#005A43\", alpha = 0.2) +\n  geom_ribbon(aes(ymin = plbgatt, ymax = pubgatt), fill = \"#005A43\", alpha = 0.2) +\n  geom_ribbon(aes(ymin = plbboth, ymax = pubboth), fill = \"#005A43\", alpha = 0.2) +\n  geom_line(aes(y = pmid), color = \"#005A43\") +\n  geom_line(aes(y = pgatt), color = \"#005A43\") +\n  geom_line(aes(y = pboth), color = \"#005A43\") +\n  labs(x = \"Presidential Support\", y = \"Predicted Probability\", color = \"Choice\") +\n  theme_minimal()+\n  annotate(\"text\", x = 60, y = 0.3, label = \"Military Action\") +\n  annotate(\"text\", x = 75, y = 0.15, label = \"GATT\") +\n  annotate(\"text\", x = 50, y = 0.1, label = \"Both\")\n\n\n\n\n\n\n\nThe plot shows the predicted probabilities of each choice as a function of presidential support in Congress. You can see that as support increases, the probability of military action increases, while the probability of GATT action decreases. The probability of both military and GATT action is relatively low and doesn’t change much with support. There appears to be a tradeoff between military and economic action as the president finds greater success in Congress.\nIf you look at the code, you’ll notice I’ve computed the predictions by hand. The predict function works only partially and poorly with choice model implementations in R. In this plot, I’ve computed the ML standard errors (\\(\\sqrt(diag(XVX'))\\)). In the plot below, I’ve computed the delta method standard errors - the confidence bands a slightly larger, but the inferences are similar. In general, the delta method standard errors are preferable in choice models like this.\n\n\ncode\n# compute delta standard errors as p*(1-p)*(XVX')\n\ndeltamidub &lt;- pmid + (1.96 * (pmid*(1-pmid)*se1))\ndeltamidlb &lt;- pmid - (1.96 * (pmid*(1-pmid)*se1))\ndeltagattub &lt;- pgatt + (1.96 * (pgatt*(1-pgatt)*se2))\ndeltagattlb &lt;- pgatt - (1.96 * (pgatt*(1-pgatt)*se2))\ndeltabothub &lt;- pboth + (1.96 * (pboth*(1-pboth)*se3))\ndeltabothlb &lt;- pboth - (1.96 * (pboth*(1-pboth)*se3))\n\n# plot with delta method confidence intervals\n\nggplot(probs, aes(x = pressupp)) +\n  geom_ribbon(aes(ymin = deltamidlb, ymax = deltamidub), fill = \"#005A43\", alpha = 0.2) +\n  geom_ribbon(aes(ymin = deltagattlb, ymax = deltagattub), fill = \"#005A43\", alpha = 0.2) +\n  geom_ribbon(aes(ymin = deltabothlb, ymax = deltabothub), fill = \"#005A43\", alpha = 0.2) +\n  geom_line(aes(y = pmid), color = \"#005A43\") +\n  geom_line(aes(y = pgatt), color = \"#005A43\") +\n  geom_line(aes(y = pboth), color = \"#005A43\") +\n  labs(x = \"Presidential Support\", y = \"Predicted Probability\", color = \"Choice\") +\n  theme_minimal()+\n  annotate(\"text\", x = 60, y = 0.3, label = \"Military Action\") +\n  annotate(\"text\", x = 75, y = 0.15, label = \"GATT\") +\n  annotate(\"text\", x = 50, y = 0.1, label = \"Both\")\n\n\n\n\n\n\n\n\n\n\n\nOne thing you might have noticed but not thought about is the structure of the data we’ve been using for the MNL model. Each observation is an individual, that individual’s choice, and the characteristics of that individual. So in these data, each line is a US month, what foreign policy choice the US makes, and characteristics of the US.\nWhat if we have characteristics of the outcomes, say, that some foreign policy choices are more risky than others? How could those enter the model?\n\n\n\nThe CL model generalizes the MNL model by permitting characteristics of the outcomes to vary on the right side. - The CL likelihood is identical to the MNL likelhood. - Specified correctly, the two models are identical. - The difference? How the data are structured. - The CL is a logit with fixed effects denoting which group an observation belongs to.\n\n\n\nThe distinction between CL/MNL has to do with the nature of our expectations about how \\(X\\) influences the choices in \\(y\\). We can consider two types of effects (and the two types of data they would require).\n\n\n\nComparing CL and MNL\n\n\n\n\n\n\n\n\n\nIndependent Variable\nWith respect to cases\nWith respect to \\(Y\\)\nModel\n# of \\(\\widehat{\\beta}\\)s\n\n\n\n\n\n\n\n\n\nCharacteristics of the individual\nVary across cases (individuals)\nConstant across choices (\\(Y=j\\))\nMNL\n\\(j-1\\)\n\n\n\n\n\n\n\n\n\nCharacteristics of the outcome \\(j\\)\nConstant across cases (individuals)\nVary across outcomes \\(J=1 ~\\mbox{to}~ m\\)\nCL\n1\n\n\n\n\n\n\n\n\n\nIndividual and case characteristics\nVary across cases\nVary across outcomes \\(J=1 ~\\mbox{to}~ m\\)\nModified CL\n\\(\\beta_{x}\\), \\(\\beta_{0}\\) \\(\\beta_{x,0}\\)\n\n\n\n\n\n\n\n\n\n\n\n\nThe multinomial logit model allows \\(X\\) to vary across cases (individual observations), but not across outcomes.\nSuppose we model vote choice, and we believe that party affiliation and education influence whether an individual votes for Perot, Bush, or Clinton (in 1992). Party affiliation will vary across individual voter, but not across choice; put another way, a Democrat is a Democrat regardless of whether we are considering whether he votes for Bush (0), or Clinton (1), or Perot (2). Education, similarly, varies across individual but not across choice.\n\n\n\n\n\n\n\n\n\n\n\n\nVoter\nChoice\nParty ID\nEducation\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0\nR\n14\n\n\n\n\n2\n0\nR\n12\n\n\n\n\n3\n2\nD\n6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwhere we have on line of data for each individual/case/voter, choice represents the value of \\(y\\) that voter selects (0,1,2). Notice that our two independent variables do not vary across a voter’s alternatives or choice, but they do vary across voters; voter 1 is a smart republican while voter 3 is a not-so-smart democrat. These data are appropriate for a multinomial logit model; we’ll get \\(j-1\\) estimates of \\(\\beta\\) for each independent variable representing the effect of \\(x_{i}\\) on the probability \\(y=j\\) rather than \\(y=k\\).\nCL allows the independent variables to vary for each outcome in \\(y\\), and with a bit of tweaking, can also allow independent variables that vary across individuals but not outcomes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVoter\nAlternative\nChoice\nD\nParty ID\nEducation\nSpending\nCampaign Stops\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0\n1\n0\nR\n14\n11\n0\n\n\n1\n1\n1\n1\nR\n14\n45\n2\n\n\n1\n2\n1\n0\nR\n14\n54\n2\n\n\n2\n0\n0\n1\nD\n12\n11\n1\n\n\n2\n1\n0\n0\nD\n12\n45\n6\n\n\n2\n2\n0\n0\nD\n12\n54\n4\n\n\n3\n0\n2\n0\nD\n6\n11\n2\n\n\n3\n1\n2\n0\nD\n6\n45\n3\n\n\n3\n2\n2\n1\nD\n6\n54\n5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nthe data include variables specific to voters (constant across alternatives), variables specific to alternatives (constant across voters), and variables that vary across both.\nIn these data, voter is just an identification of the case, alternative lists each possible choice for each voter, choice identifies which alternative the voter selected, D is a dummy variable marking which alternative the voter chooses, party id is obvious, education is voter’s years of schooling; note that these last two vary across individual but not across individual choice. Spending measures the amount each candidate spent in the general election campaign; it varies across choice (candidate) but not across individual. Finally, campaign stops is the number of campaign visits a candidate made to a voter’s home town. This variable varies across both voter and alternative.\nYou’ll notice these are panel data - the units are the voters, and the panels are by alternatives. This is how the CL makes sense as a logit with fixed effects, grouped by unit, in this case voter.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVoter\nAlternative\nChoice\nD\nParty ID\nEducation\nSpending\nCampaign Stops\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0\n1\n0\nR\n14\n11\n0\n\n\n1\n1\n1\n1\nR\n14\n45\n2\n\n\n1\n2\n1\n0\nR\n14\n54\n2\n\n\n2\n0\n0\n1\nD\n12\n11\n1\n\n\n2\n1\n0\n0\nD\n12\n45\n6\n\n\n2\n2\n0\n0\nD\n12\n54\n4\n\n\n3\n0\n2\n0\nD\n6\n11\n2\n\n\n3\n1\n2\n0\nD\n6\n45\n3\n\n\n3\n2\n2\n1\nD\n6\n54\n5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice that we have information on each possible choice for each voter because the last two variables vary for each choice available to the voter. Thus, the data structure for the conditional model contains all the information in the multinomial data structure plus additional information regarding independent variables pertaining to each choice available to a voter or to specific both to choice and to individual.\nThe CL and MNL models are estimated identically - they have identical likelihood functions.\nWhat varies between the two are:\n\nthe information contained in the data - do the \\(x\\)s vary across \\(i\\) (thus, \\(x_i\\)) or across \\(j\\) (thus, \\(x_j\\)) or both (thus \\(x_{i,j}\\))?\nthe structure of the data - the CL model will by definition have \\(J\\) times as many observations as the MNL model.\n\nIf the disturbances are i.i.d. logistic, then\n\\[\nPr(Y=j) =  \\frac{e^{\\beta'z_{i,j}}}{\\sum\\limits_{j=1}^{J}e^{\\beta'z_{i,j}}}\\nonumber\n\\] where \\(z_{i,j}\\) has two components, \\(x_{i,j}\\) and \\(w_{i}\\) (following Greene’s notation). \\(x_{i,j}\\) varies across choices (\\(j\\)) and possibly across individuals (\\(i\\)) as well; \\(w_{i}\\) varies across individuals. We refer to variation across {choices} (\\(x_{i,j}\\)) as attributes of the choices; we refer to variation across individuals (\\(w_{i}\\)) as characteristics of the individuals.\nTaking the last equation and substituting \\(x_{i,j}\\) and \\(w_{i}\\) for \\(z\\), we get\n\\[\nPr(Y=j) =\\frac{e^{(x_{i,j}\\beta)} e^{({w_{i}\\gamma})}} {\\sum\\limits_{j=1}^{J}e^{(x_{i,j}\\beta)} e^{({w_{i}\\gamma})}}\n\\]\nand if \\(w_{i}\\) does not vary across individuals, it drops out of the probability leaving\n\\[\nPr(Y=j) =\\frac{e^{x_{j}\\beta}}{\\sum\\limits_{j=1}^{J}e^{x_{j}\\beta}}\\nonumber\n\\]\nwhich is the conditional logit because the variables \\(x\\) vary across outcomes, \\(j\\). The model estimates one parameter \\(\\beta\\) for \\(x_{j}\\), indicating the effect of \\(x_{j}\\) on the probablity of \\(Y\\). Note that for simplicity, I’ve removed the subscript \\(i\\) so this \\(x\\) varies only by outcome, not by individual; also note that it could vary by individual as well.\nIf \\(x\\) only varies across individuals, \\(i\\), and not across outcomes, \\(j\\), then the function is\n\\[\nPr(Y=j) =\\frac{e^{x_{i}\\beta_j}}{\\sum\\limits_{k=1}^{J}e^{x_{i}\\beta_{j}}}\n\\]\nso the model estimates a parameter \\(\\beta_{j}\\) for each outcome \\(j\\) based on a variable that varies across individuals, \\(i\\). Thus, we can observe the effect of \\(x_i\\) on the probability \\(Y=j\\). Note again that the model is identified by constraining one of the \\(\\beta\\)s to zero and making its associated outcome a reference or comparison category; thus, the \\(j=1\\) rather than zero in the summation limit. This is why we don’t actually estimate \\(j\\) \\(\\beta\\)s; we estimate \\(j-1\\) \\(\\beta\\)s in the multinomial or conditional logit model.\nAfter all this effort to distinguish the two models from one another, you’ll be disappointed to learn that the two models, properly specified are identical.\nThe LLF is :\n\\[\\ln \\mathcal{L} ( \\beta ) = \\sum _ { i=1}^{N}  \\sum_ { j=1}^{J} d _ { i j } \\ln \\left( \\frac { \\exp \\left( x _ { i j}  \\beta \\right) } { \\sum _ { k } \\exp \\left( x _ { i j }  \\beta \\right) } \\right)\n\\]\n\n\n\nSuppose that we want to estimate a model including a variable that varies across outcomes, but not across individuals (in this case, constant across US-months). Say we have a measure of risk inherent in any of the 4 foreign policy actions in the dv:\n\n\n\nForeign Policy Data Varying by \\(i\\) and \\(j\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nmonth\nchoices\nopted\nrisk\ncon1\ncon2\ncon3\nappc1\nappc2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1945\n1\n0\n1\n.25\n0\n0\n0\n0\n0\n\n\n1945\n1\n1\n0\n1\n1\n0\n0\n81.33334\n0\n\n\n1945\n1\n2\n0\n.5\n0\n1\n0\n0\n81.33334\n\n\n1945\n1\n3\n0\n1.25\n0\n0\n1\n0\n0\n\n\n1945\n2\n0\n1\n.25\n0\n0\n0\n0\n0\n\n\n1945\n2\n1\n0\n1\n1\n0\n0\n81.33334\n0\n\n\n1945\n2\n2\n0\n.5\n0\n1\n0\n0\n81.33334\n\n\n1945\n2\n3\n0\n1.25\n0\n0\n1\n0\n0\n\n\n1945\n3\n0\n1\n.25\n0\n0\n0\n0\n0\n\n\n1945\n3\n1\n0\n1\n1\n0\n0\n81.33334\n0\n\n\n1945\n3\n2\n0\n.5\n0\n1\n0\n0\n81.33334\n\n\n1945\n3\n3\n0\n1.25\n0\n0\n1\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince I’m interested in how an attribute specific variable (risk) influences foreign policy choice, I need to structure the data so it represents that variable against all possible choices; thus, for January 1945, the data have 4 observations, one for each outcome, and risk indicates the level of risk the president incurs with any single choice."
  },
  {
    "objectID": "choiceA24.html#unordered-choices",
    "href": "choiceA24.html#unordered-choices",
    "title": "Choice Models",
    "section": "",
    "text": "The multinomial and conditional logit models are robust, easy to estimate and interpret, and less punitive than some others if you violate assumptions.\n\nAny outcome can be ordered; not every outcome should be ordered.\nIf we use an ordered model on unordered data, the parameters are likely to be biased and not to make sense\nIf we use a unordered model on ordered data, we are neglecting to use some of the information in the y variable so our model may be inefficient, but the consequences are not as severe."
  },
  {
    "objectID": "choiceA24.html#probability-model",
    "href": "choiceA24.html#probability-model",
    "title": "Choice Models",
    "section": "",
    "text": "Following Long (pp. 152ff):\nSuppose an unordered, discrete variable \\(y = j\\) where \\(j = 1,2 \\ldots\\). Let \\(Pr(y=j) = x\\beta_j +\\epsilon\\) so the probability of any value of \\(y\\) is a function of \\(x_i\\) and a vector of outcome-specific coefficients, \\(\\beta_j\\). Because these are probabilities, they cannot be negative, so transform \\(exp(x\\beta_j)\\) - but now the sum of these over all values of \\(j\\) exceed 1. So divide by the sum of all the categories, and now we sum to 1 : \\(\\frac{exp(x\\beta_j)}{\\sum_{j=1}^{J} exp(x\\beta_j)}\\)\n\nBut \\(\\frac{exp(x\\beta_j)}{\\sum_{j=1}^{J} exp(x\\beta_j)}\\) does not suggest a single set of solutions \\(\\beta_j\\), so is not identified.\nThe usual solution is to assume one \\(\\beta_j=0\\).\nRecalling that \\(j = 1, 2, \\ldots\\), we restrict the first category (\\(\\beta_1=0\\)), and rewrite \\(\\frac{exp(x\\beta_j)}{\\sum_{j=2}^{J} exp(x\\beta_j)}\\).\n\nThis means \\(y=1\\) is the reference category, so now \\(\\beta_j\\) is compared to \\(\\beta_1\\) as if the quantity is now \\(\\beta_j - \\beta_1\\). This is, after all, the comparison we usually make: that the effect of \\(x_i\\) is \\(\\beta_j\\) relative to the excluded category.\nThis leaves us with:\n\\[Pr(y=1) = \\frac{1}{1+ \\sum_{j=2}^{J} exp(x\\beta_j)} \\]\nfor \\(j=1\\), and for \\(j&gt;1\\), we have:\n\\[Pr(y=j) = \\frac{exp(x\\beta_j)}{\\sum_{j=2}^{J} exp(x\\beta_j)}\\]\nNote constraining \\(\\beta_1 = 0\\) is arbitrary - we can set any of the categories as the reference."
  },
  {
    "objectID": "choiceA24.html#an-alternative-motivation",
    "href": "choiceA24.html#an-alternative-motivation",
    "title": "Choice Models",
    "section": "",
    "text": "Another, more interesting way, to motivate models for choices over outcomes is from a rational choice framework. We consider individuals making choices over outcomes. Those choices are informed by their utilities for the outcomes."
  },
  {
    "objectID": "choiceA24.html#discrete-choice-random-utility",
    "href": "choiceA24.html#discrete-choice-random-utility",
    "title": "Choice Models",
    "section": "",
    "text": "An individual’s utility for each choice is given by\n\\[\nU_{i,j}= \\mu_{i,j} + \\varepsilon_{i,j}\n\\]\nwhere the subscripts indicate the \\(i^{th}\\) individual and the \\(j^{th}\\) outcome. Each individual’s utility for each outcome is a function of a systematic component, \\(\\mu_{i,j}\\) and a stochastic component \\(\\varepsilon_{i,j}\\). We can parameterize the systematic component as a function of some variables, ${i,j} =x{i,j} $ :\n\\[\nU_{i,j}=x_{i,j} \\beta  + \\varepsilon_{i,j} \\nonumber\n\\]\nSome variables, \\(x\\) have \\(\\beta\\) effect on \\(i\\)’s utility for outcome \\(j\\). Moreover, each individual has a complete set of preferences over the \\(J\\) outcomes and those preferences are transitive. Each individual makes choices to maximize their utility, thus comparing choices \\(j, k \\ldots J\\) in a pairwise fashion:\n\\[Pr(Y=j) = Pr(U_{i,j}&gt;U_{i,k})  \\\\\n      = Pr(x_{i,j} \\beta + \\varepsilon_{i,j} &gt;  x_{i,k}\\beta  + \\varepsilon_{i,k})\\\\\n       = Pr(\\varepsilon_{i,j}-\\varepsilon_{i,k} &gt; x_{i,k} \\beta -x_{i,j} \\beta )\n\\]\nIf the difference in the random components of \\(j\\) and any other outcome, \\(k\\) is greater than the difference between the systematic components, then \\(j\\) maximizes \\(i\\)’s utility. Looking at the last inequality above, we can see this will happen when \\(\\varepsilon_{i,j}\\) is big, when \\(\\beta x_{i,j}\\) is big, or both.\nTo estimate this, we have to make an assumption about how the errors are distributed. The way we’ve derived the model, the errors are actually the difference between the stochastic parts of outcomes \\(i, j\\):\n\\[\nF(\\varepsilon_{i,j}) = exp[-exp(-\\varepsilon_{i,j})] \\nonumber\n\\]\nassume \\(\\varepsilon_{i,j}\\)s are i.i.d Type 1 generalized extreme value (Gumbel); if the errors are i.i.d., their difference is logistic:\nThe probability \\(Y=j\\) is\n\\[\nPr(Y=j) = \\frac{e^{X_i \\beta_j }}{\\sum \\limits_{j=1}^{J} e^{X_i\\beta_j}} \\nonumber\n\\]\nAs above, the model is not identified since there is no reference category. So we typically constrain \\(\\beta_{i,j=1}=0\\) and estimate the remaining \\(J-1\\) categories:\n\\[\nPr(Y=j) = \\frac{e^{x_i \\beta_j }}{1+\\sum \\limits_{j=2}^{J} e^{x_i\\beta_k }} \\nonumber  \\\\\nPr(Y=1) = \\frac{1}{1+\\sum \\limits_{j=2}^{J} e^{\\beta_k x_i}} \\nonumber\n\\]\nIf \\(J=2\\), the model is the binary logit - so the binary logit is a special case of the CL or MNL models.\n\nThis motivation is nice because it links the likelihood function to a simple, formal model where individuals make choices over ordered preferences.\n\nindividual \\(i\\)\nchooses among \\(j\\) alternatives\nlet \\(d=1\\) if \\(y_i = j\\), else zero.\n\\(Pr(Y=j) = \\frac{e^{x_i \\beta_j }}{\\sum \\limits_{j=1}^{J} e^{x_i\\beta_k }}\\)"
  },
  {
    "objectID": "choiceA24.html#random-utility",
    "href": "choiceA24.html#random-utility",
    "title": "Choice Models",
    "section": "",
    "text": "This motivation is nice because it links the likelihood function to a simple, formal model where individuals make choices over ordered preferences.\n\nindividual \\(i\\)\nchooses among \\(j\\) alternatives\nlet \\(d=1\\) if \\(y_i = j\\), else zero.\n\\(Pr(Y=j) = \\frac{e^{x_i \\beta_j }}{\\sum \\limits_{j=1}^{J} e^{x_i\\beta_k }}\\)"
  },
  {
    "objectID": "choiceA24.html#likelihood---multinomial-logit",
    "href": "choiceA24.html#likelihood---multinomial-logit",
    "title": "Choice Models",
    "section": "",
    "text": "\\[\n\\mathcal{L} ( \\beta ) = \\prod _ { i=1}^{N} \\prod_ { j=1}^{J}  Pr \\left( y _ {i } = j | x _ { i } \\right) ^ { d _ { i j } }\n\\]\n\\[\n\\ln \\mathcal{L} ( \\beta ) = \\sum _ { i=1}^{N} \\sum_ { j=1}^{J}  d _ { i j } \\ln Pr \\left( y _ { i } = j | x _ { i } \\right)\n\\]\n\\[\n\\ln \\mathcal{L} ( \\beta ) = \\sum _ { i=1}^{N}  \\sum_ { j=1}^{J} d _ { i j } \\ln \\left( \\frac { \\exp \\left( x _ { i }  \\beta_j \\right) } { \\sum _ { k } \\exp \\left( x _ { i  }  \\beta_j \\right) } \\right)\n\\]"
  },
  {
    "objectID": "choiceA24.html#extending-the-binary-model",
    "href": "choiceA24.html#extending-the-binary-model",
    "title": "Choice Models",
    "section": "",
    "text": "The logistic CDF is \\[\nPr(Y=1)=\\frac{e^{x_i\\beta}}{1+e^{x_i\\beta}}\\nonumber\n\\]\nWe use this to represent a binary choice.\nExtending this to a trichotomous choice, say among chocolate, vanilla and strawberry icecream is easy:\n\\[\nPr(Y=chocolate)=\\frac{e^{x_i\\beta_c}}{1+e^{x_i\\beta_c}+e^{x_i\\beta_v}} \\nonumber\n\\]\nassuming strawberry is the reference category.\nThe n-chotomous choice is also easy:\n\\[\nPr(Y=j)=\\frac{e^{x_i\\beta_j}}{1+\\sum\\limits_{j=1}^{J}e^{x_i\\beta}} \\nonumber\n\\]\nYou should be getting the idea the binary logit model is a special case of the more general choice model with errors distributed i.i.d. logistic.\nYou should also see that interpretation is a straightforward extension of the binary logit."
  },
  {
    "objectID": "choiceA24.html#mnl-estimation",
    "href": "choiceA24.html#mnl-estimation",
    "title": "Choice Models",
    "section": "",
    "text": "To illustrate the MNL and CL models, I’m going to use data from a paper on foreign policy substitution. The \\(y\\) variable represents the foreign policy choice the US president makes given domestic conditions; it takes on 4 values distributed as follows:\n\n0 = no action = 61.13 percent the observations\n1 = MID only = 22.6 percent\n2 = GATT only = 11.8 percent\n3 = both MID and GATT = 4.3 percent\n\nThe data are structured with one observation for each US-month between 1945 and 1990, so for each observation, we observe the policy the president selects, 0, 1, 2, or 3. The variables of interest vary across observations (US-months) but not across outcomes; thus, the data structure and the nature of the independent variables suggest MNL is appropriate.\nThe main expectation here is that the extent of support in Congress for the President’s legislative agenda will affect foreign policy choice - stronger Presidents have greater latitude to choose more costly policies (e.g. military force). The data include a measure of presidential support in Congress, the unemployment rate, the president’s approval rating, and whether the president is in an election year.\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n\n\n\n\n\n\nunemp2\n\n\n0.089\n\n\n0.256**\n\n\n0.609***\n\n\n\n\n\n\n(0.077)\n\n\n(0.106)\n\n\n(0.157)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npressupp\n\n\n0.009\n\n\n-0.043***\n\n\n-0.035*\n\n\n\n\n\n\n(0.009)\n\n\n(0.012)\n\n\n(0.018)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npelect\n\n\n0.167\n\n\n0.278\n\n\n-0.521\n\n\n\n\n\n\n(0.265)\n\n\n(0.330)\n\n\n(0.602)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\napproval\n\n\n0.017*\n\n\n0.023*\n\n\n0.037*\n\n\n\n\n\n\n(0.010)\n\n\n(0.013)\n\n\n(0.021)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-2.996***\n\n\n-1.560\n\n\n-5.935***\n\n\n\n\n\n\n(1.058)\n\n\n(1.392)\n\n\n(2.282)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAkaike Inf. Crit.\n\n\n994.056\n\n\n994.056\n\n\n994.056\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\nRecalling the \\(y\\) variable has 4 categories, the model estimates 3 sets of coefficients, one for each category relative to the reference category. By default, the reference category is the one with the lowest value, in this case, no action. This is something we can change to facilitate interpretation. The coefficients are interpreted as the effect of the variable on the probability of choosing the category relative to the reference category. So you can see presidential support in Congress decreases the chances of GATT action or both military and GATT action relative to doing nothing.\n\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n0\n\n\n1\n\n\n2\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n(4)\n\n\n(5)\n\n\n(6)\n\n\n\n\n\n\n\n\nunemp2\n\n\n0.089\n\n\n0.256**\n\n\n0.609***\n\n\n-0.609***\n\n\n-0.520***\n\n\n-0.354**\n\n\n\n\n\n\n(0.077)\n\n\n(0.106)\n\n\n(0.157)\n\n\n(0.157)\n\n\n(0.164)\n\n\n(0.177)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npressupp\n\n\n0.009\n\n\n-0.043***\n\n\n-0.035*\n\n\n0.035*\n\n\n0.044**\n\n\n-0.008\n\n\n\n\n\n\n(0.009)\n\n\n(0.012)\n\n\n(0.018)\n\n\n(0.018)\n\n\n(0.019)\n\n\n(0.020)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npelect\n\n\n0.167\n\n\n0.278\n\n\n-0.521\n\n\n0.521\n\n\n0.688\n\n\n0.800\n\n\n\n\n\n\n(0.265)\n\n\n(0.330)\n\n\n(0.602)\n\n\n(0.602)\n\n\n(0.623)\n\n\n(0.644)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\napproval\n\n\n0.017*\n\n\n0.023*\n\n\n0.037*\n\n\n-0.037*\n\n\n-0.020\n\n\n-0.014\n\n\n\n\n\n\n(0.010)\n\n\n(0.013)\n\n\n(0.021)\n\n\n(0.021)\n\n\n(0.022)\n\n\n(0.023)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-2.996***\n\n\n-1.560\n\n\n-5.935***\n\n\n5.935***\n\n\n2.938\n\n\n4.373*\n\n\n\n\n\n\n(1.058)\n\n\n(1.392)\n\n\n(2.282)\n\n\n(2.282)\n\n\n(2.381)\n\n\n(2.517)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAkaike Inf. Crit.\n\n\n994.056\n\n\n994.056\n\n\n994.056\n\n\n994.056\n\n\n994.056\n\n\n994.056\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\nCompare the coefficients in the first 3 columns to those in the last 3 columns; notice their similarities but changed signs. Really, changing the reference category has only modest interpretive value. This is especially true because interpreting coefficients is full of pitfalls in the MNL model because, since the effects of variable on one outcome are relative to others, even signs can be misleading. It’s even more essential to compute MNL effects than in most ML cases."
  },
  {
    "objectID": "choiceA24.html#interpretation",
    "href": "choiceA24.html#interpretation",
    "title": "Choice Models",
    "section": "",
    "text": "Predicted probabilities are straightforward to compute - they’re an easy extension of the binary logit model.\n\\[P(Y=j) =  \\frac{e^{\\beta_{j}'x_{i}}}{1+\\sum\\limits_{j=1}^{J}e^{\\beta_{j}'x_{i}}}\n\\\\\n\\text{so}~~ P(Y=1) = \\frac{e^{X_{1}\\beta_1}}{1 + e^{X_{1}\\beta_1} +\ne^{X_{2}\\beta_2} + e^{X_{3}\\beta_3}} \\nonumber\n\\]\n\n\nLet’s look at some predictions from the first model.\n\n\ncode\nX &lt;- as.matrix(expand.grid(\n  intercept= 1, \n  unemp2 = median(fpdata$unemp2, na.rm=TRUE),\n  pressupp = seq(43,93, length.out = 50),\n  pelect = 0,\n  approval = median(fpdata$approval, na.rm=TRUE)\n))\n\nxb &lt;- X %*% t(coef(mnl1))\nxb &lt;-data.frame(xb)\nprobs &lt;- exp(xb$X1)/(1+exp(xb$X1)+exp(xb$X2)+exp(xb$X3))\n\n# decompose vcov matrix\nv1 &lt;- as.matrix(vcov(mnl1)[1:5,1:5])\nv2 &lt;- as.matrix(vcov(mnl1)[6:10,6:10])\nv3 &lt;- as.matrix(vcov(mnl1)[11:15,11:15])\n# compute standard errors\nse1 &lt;- sqrt(diag(as.matrix(X) %*% v1 %*% t(X)))\nse2 &lt;- sqrt(diag(as.matrix(X) %*% v2 %*% t(X)))\nse3 &lt;- sqrt(diag(as.matrix(X) %*% v3 %*% t(X)))\n# compute  boundary components\nub1 &lt;- exp(xb$X1 + (1.96 * se1))\nlb1 &lt;- exp(xb$X1 - (1.96 * se1))\nub2 &lt;- exp(xb$X2 + (1.96 * se2))\nlb2 &lt;- exp(xb$X2 - (1.96 * se2))\nub3 &lt;- exp(xb$X3 + (1.96 * se3))\nlb3 &lt;- exp(xb$X3 - (1.96 * se3))\n# compute probabilities\npmid &lt;- exp(xb$X1)/(1+exp(xb$X1)+ exp(xb$X2) + exp(xb$X3))\npgatt &lt;- exp(xb$X2)/(1+exp(xb$X1)+ exp(xb$X2) + exp(xb$X3))\npboth &lt;- exp(xb$X3)/(1+exp(xb$X1)+ exp(xb$X2) + exp(xb$X3))\n# compute boundaries\npubmid &lt;- ub1/(1+ub1+ub2+ub3)\nplbmid &lt;- lb1/(1+lb1+lb2+lb3)\npubgatt &lt;- ub2/(1+ub1+ub2+ub3)\nplbgatt &lt;- lb2/(1+lb1+lb2+lb3)\npubboth &lt;- ub3/(1+ub1+ub2+ub3)\nplbboth &lt;- lb3/(1+lb1+lb2+lb3)\n# data frame \nprobs &lt;- data.frame(X,pmid, pgatt, pboth, pubmid, plbmid, pubgatt, plbgatt, pubboth, plbboth)\n\n#plot  \"#005A43\", \"#8C8C8C\"\n\nggplot(probs, aes(x = pressupp)) +\n  geom_ribbon(aes(ymin = plbmid, ymax = pubmid), fill = \"#005A43\", alpha = 0.2) +\n  geom_ribbon(aes(ymin = plbgatt, ymax = pubgatt), fill = \"#005A43\", alpha = 0.2) +\n  geom_ribbon(aes(ymin = plbboth, ymax = pubboth), fill = \"#005A43\", alpha = 0.2) +\n  geom_line(aes(y = pmid), color = \"#005A43\") +\n  geom_line(aes(y = pgatt), color = \"#005A43\") +\n  geom_line(aes(y = pboth), color = \"#005A43\") +\n  labs(x = \"Presidential Support\", y = \"Predicted Probability\", color = \"Choice\") +\n  theme_minimal()+\n  annotate(\"text\", x = 60, y = 0.3, label = \"Military Action\") +\n  annotate(\"text\", x = 75, y = 0.15, label = \"GATT\") +\n  annotate(\"text\", x = 50, y = 0.1, label = \"Both\")\n\n\n\n\n\n\n\nThe plot shows the predicted probabilities of each choice as a function of presidential support in Congress. You can see that as support increases, the probability of military action increases, while the probability of GATT action decreases. The probability of both military and GATT action is relatively low and doesn’t change much with support. There appears to be a tradeoff between military and economic action as the president finds greater success in Congress.\nIf you look at the code, you’ll notice I’ve computed the predictions by hand. The predict function works only partially and poorly with choice model implementations in R. In this plot, I’ve computed the ML standard errors (\\(\\sqrt(diag(XVX'))\\)). In the plot below, I’ve computed the delta method standard errors - the confidence bands a slightly larger, but the inferences are similar. In general, the delta method standard errors are preferable in choice models like this.\n\n\ncode\n# compute delta standard errors as p*(1-p)*(XVX')\n\ndeltamidub &lt;- pmid + (1.96 * (pmid*(1-pmid)*se1))\ndeltamidlb &lt;- pmid - (1.96 * (pmid*(1-pmid)*se1))\ndeltagattub &lt;- pgatt + (1.96 * (pgatt*(1-pgatt)*se2))\ndeltagattlb &lt;- pgatt - (1.96 * (pgatt*(1-pgatt)*se2))\ndeltabothub &lt;- pboth + (1.96 * (pboth*(1-pboth)*se3))\ndeltabothlb &lt;- pboth - (1.96 * (pboth*(1-pboth)*se3))\n\n# plot with delta method confidence intervals\n\nggplot(probs, aes(x = pressupp)) +\n  geom_ribbon(aes(ymin = deltamidlb, ymax = deltamidub), fill = \"#005A43\", alpha = 0.2) +\n  geom_ribbon(aes(ymin = deltagattlb, ymax = deltagattub), fill = \"#005A43\", alpha = 0.2) +\n  geom_ribbon(aes(ymin = deltabothlb, ymax = deltabothub), fill = \"#005A43\", alpha = 0.2) +\n  geom_line(aes(y = pmid), color = \"#005A43\") +\n  geom_line(aes(y = pgatt), color = \"#005A43\") +\n  geom_line(aes(y = pboth), color = \"#005A43\") +\n  labs(x = \"Presidential Support\", y = \"Predicted Probability\", color = \"Choice\") +\n  theme_minimal()+\n  annotate(\"text\", x = 60, y = 0.3, label = \"Military Action\") +\n  annotate(\"text\", x = 75, y = 0.15, label = \"GATT\") +\n  annotate(\"text\", x = 50, y = 0.1, label = \"Both\")"
  },
  {
    "objectID": "choiceA24.html#mnl",
    "href": "choiceA24.html#mnl",
    "title": "Choice Models",
    "section": "",
    "text": "One thing you might have noticed but not thought about is the structure of the data we’ve been using for the MNL model. Each observation is an individual, that individual’s choice, and the characteristics of that individual. So in these data, each line is a US month, what foreign policy choice the US makes, and characteristics of the US.\nWhat if we have characteristics of the outcomes, say, that some foreign policy choices are more risky than others? How could those enter the model?"
  },
  {
    "objectID": "choiceA24.html#conditional-logit",
    "href": "choiceA24.html#conditional-logit",
    "title": "Choice Models",
    "section": "",
    "text": "The CL model generalizes the MNL model by permitting characteristics of the outcomes to vary on the right side. - The CL likelihood is identical to the MNL likelhood. - Specified correctly, the two models are identical. - The difference? How the data are structured. - The CL is a logit with fixed effects denoting which group an observation belongs to."
  },
  {
    "objectID": "choiceA24.html#multinomial-v.-conditional-logit",
    "href": "choiceA24.html#multinomial-v.-conditional-logit",
    "title": "Choice Models",
    "section": "",
    "text": "The distinction between CL/MNL has to do with the nature of our expectations about how \\(X\\) influences the choices in \\(y\\). We can consider two types of effects (and the two types of data they would require).\n\n\n\nComparing CL and MNL\n\n\n\n\n\n\n\n\n\nIndependent Variable\nWith respect to cases\nWith respect to \\(Y\\)\nModel\n# of \\(\\widehat{\\beta}\\)s\n\n\n\n\n\n\n\n\n\nCharacteristics of the individual\nVary across cases (individuals)\nConstant across choices (\\(Y=j\\))\nMNL\n\\(j-1\\)\n\n\n\n\n\n\n\n\n\nCharacteristics of the outcome \\(j\\)\nConstant across cases (individuals)\nVary across outcomes \\(J=1 ~\\mbox{to}~ m\\)\nCL\n1\n\n\n\n\n\n\n\n\n\nIndividual and case characteristics\nVary across cases\nVary across outcomes \\(J=1 ~\\mbox{to}~ m\\)\nModified CL\n\\(\\beta_{x}\\), \\(\\beta_{0}\\) \\(\\beta_{x,0}\\)\n\n\n\n\n\n\n\n\n\n\n\n\nThe multinomial logit model allows \\(X\\) to vary across cases (individual observations), but not across outcomes.\nSuppose we model vote choice, and we believe that party affiliation and education influence whether an individual votes for Perot, Bush, or Clinton (in 1992). Party affiliation will vary across individual voter, but not across choice; put another way, a Democrat is a Democrat regardless of whether we are considering whether he votes for Bush (0), or Clinton (1), or Perot (2). Education, similarly, varies across individual but not across choice.\n\n\n\n\n\n\n\n\n\n\n\n\nVoter\nChoice\nParty ID\nEducation\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0\nR\n14\n\n\n\n\n2\n0\nR\n12\n\n\n\n\n3\n2\nD\n6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwhere we have on line of data for each individual/case/voter, choice represents the value of \\(y\\) that voter selects (0,1,2). Notice that our two independent variables do not vary across a voter’s alternatives or choice, but they do vary across voters; voter 1 is a smart republican while voter 3 is a not-so-smart democrat. These data are appropriate for a multinomial logit model; we’ll get \\(j-1\\) estimates of \\(\\beta\\) for each independent variable representing the effect of \\(x_{i}\\) on the probability \\(y=j\\) rather than \\(y=k\\).\nCL allows the independent variables to vary for each outcome in \\(y\\), and with a bit of tweaking, can also allow independent variables that vary across individuals but not outcomes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVoter\nAlternative\nChoice\nD\nParty ID\nEducation\nSpending\nCampaign Stops\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0\n1\n0\nR\n14\n11\n0\n\n\n1\n1\n1\n1\nR\n14\n45\n2\n\n\n1\n2\n1\n0\nR\n14\n54\n2\n\n\n2\n0\n0\n1\nD\n12\n11\n1\n\n\n2\n1\n0\n0\nD\n12\n45\n6\n\n\n2\n2\n0\n0\nD\n12\n54\n4\n\n\n3\n0\n2\n0\nD\n6\n11\n2\n\n\n3\n1\n2\n0\nD\n6\n45\n3\n\n\n3\n2\n2\n1\nD\n6\n54\n5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nthe data include variables specific to voters (constant across alternatives), variables specific to alternatives (constant across voters), and variables that vary across both.\nIn these data, voter is just an identification of the case, alternative lists each possible choice for each voter, choice identifies which alternative the voter selected, D is a dummy variable marking which alternative the voter chooses, party id is obvious, education is voter’s years of schooling; note that these last two vary across individual but not across individual choice. Spending measures the amount each candidate spent in the general election campaign; it varies across choice (candidate) but not across individual. Finally, campaign stops is the number of campaign visits a candidate made to a voter’s home town. This variable varies across both voter and alternative.\nYou’ll notice these are panel data - the units are the voters, and the panels are by alternatives. This is how the CL makes sense as a logit with fixed effects, grouped by unit, in this case voter.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVoter\nAlternative\nChoice\nD\nParty ID\nEducation\nSpending\nCampaign Stops\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0\n1\n0\nR\n14\n11\n0\n\n\n1\n1\n1\n1\nR\n14\n45\n2\n\n\n1\n2\n1\n0\nR\n14\n54\n2\n\n\n2\n0\n0\n1\nD\n12\n11\n1\n\n\n2\n1\n0\n0\nD\n12\n45\n6\n\n\n2\n2\n0\n0\nD\n12\n54\n4\n\n\n3\n0\n2\n0\nD\n6\n11\n2\n\n\n3\n1\n2\n0\nD\n6\n45\n3\n\n\n3\n2\n2\n1\nD\n6\n54\n5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice that we have information on each possible choice for each voter because the last two variables vary for each choice available to the voter. Thus, the data structure for the conditional model contains all the information in the multinomial data structure plus additional information regarding independent variables pertaining to each choice available to a voter or to specific both to choice and to individual.\nThe CL and MNL models are estimated identically - they have identical likelihood functions.\nWhat varies between the two are:\n\nthe information contained in the data - do the \\(x\\)s vary across \\(i\\) (thus, \\(x_i\\)) or across \\(j\\) (thus, \\(x_j\\)) or both (thus \\(x_{i,j}\\))?\nthe structure of the data - the CL model will by definition have \\(J\\) times as many observations as the MNL model.\n\nIf the disturbances are i.i.d. logistic, then\n\\[\nPr(Y=j) =  \\frac{e^{\\beta'z_{i,j}}}{\\sum\\limits_{j=1}^{J}e^{\\beta'z_{i,j}}}\\nonumber\n\\] where \\(z_{i,j}\\) has two components, \\(x_{i,j}\\) and \\(w_{i}\\) (following Greene’s notation). \\(x_{i,j}\\) varies across choices (\\(j\\)) and possibly across individuals (\\(i\\)) as well; \\(w_{i}\\) varies across individuals. We refer to variation across {choices} (\\(x_{i,j}\\)) as attributes of the choices; we refer to variation across individuals (\\(w_{i}\\)) as characteristics of the individuals.\nTaking the last equation and substituting \\(x_{i,j}\\) and \\(w_{i}\\) for \\(z\\), we get\n\\[\nPr(Y=j) =\\frac{e^{(x_{i,j}\\beta)} e^{({w_{i}\\gamma})}} {\\sum\\limits_{j=1}^{J}e^{(x_{i,j}\\beta)} e^{({w_{i}\\gamma})}}\n\\]\nand if \\(w_{i}\\) does not vary across individuals, it drops out of the probability leaving\n\\[\nPr(Y=j) =\\frac{e^{x_{j}\\beta}}{\\sum\\limits_{j=1}^{J}e^{x_{j}\\beta}}\\nonumber\n\\]\nwhich is the conditional logit because the variables \\(x\\) vary across outcomes, \\(j\\). The model estimates one parameter \\(\\beta\\) for \\(x_{j}\\), indicating the effect of \\(x_{j}\\) on the probablity of \\(Y\\). Note that for simplicity, I’ve removed the subscript \\(i\\) so this \\(x\\) varies only by outcome, not by individual; also note that it could vary by individual as well.\nIf \\(x\\) only varies across individuals, \\(i\\), and not across outcomes, \\(j\\), then the function is\n\\[\nPr(Y=j) =\\frac{e^{x_{i}\\beta_j}}{\\sum\\limits_{k=1}^{J}e^{x_{i}\\beta_{j}}}\n\\]\nso the model estimates a parameter \\(\\beta_{j}\\) for each outcome \\(j\\) based on a variable that varies across individuals, \\(i\\). Thus, we can observe the effect of \\(x_i\\) on the probability \\(Y=j\\). Note again that the model is identified by constraining one of the \\(\\beta\\)s to zero and making its associated outcome a reference or comparison category; thus, the \\(j=1\\) rather than zero in the summation limit. This is why we don’t actually estimate \\(j\\) \\(\\beta\\)s; we estimate \\(j-1\\) \\(\\beta\\)s in the multinomial or conditional logit model.\nAfter all this effort to distinguish the two models from one another, you’ll be disappointed to learn that the two models, properly specified are identical.\nThe LLF is :\n\\[\\ln \\mathcal{L} ( \\beta ) = \\sum _ { i=1}^{N}  \\sum_ { j=1}^{J} d _ { i j } \\ln \\left( \\frac { \\exp \\left( x _ { i j}  \\beta \\right) } { \\sum _ { k } \\exp \\left( x _ { i j }  \\beta \\right) } \\right)\n\\]"
  },
  {
    "objectID": "choiceA24.html#conditional-logit-estimation",
    "href": "choiceA24.html#conditional-logit-estimation",
    "title": "Choice Models",
    "section": "",
    "text": "Suppose that we want to estimate a model including a variable that varies across outcomes, but not across individuals (in this case, constant across US-months). Say we have a measure of risk inherent in any of the 4 foreign policy actions in the dv:\n\n\n\nForeign Policy Data Varying by \\(i\\) and \\(j\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nmonth\nchoices\nopted\nrisk\ncon1\ncon2\ncon3\nappc1\nappc2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1945\n1\n0\n1\n.25\n0\n0\n0\n0\n0\n\n\n1945\n1\n1\n0\n1\n1\n0\n0\n81.33334\n0\n\n\n1945\n1\n2\n0\n.5\n0\n1\n0\n0\n81.33334\n\n\n1945\n1\n3\n0\n1.25\n0\n0\n1\n0\n0\n\n\n1945\n2\n0\n1\n.25\n0\n0\n0\n0\n0\n\n\n1945\n2\n1\n0\n1\n1\n0\n0\n81.33334\n0\n\n\n1945\n2\n2\n0\n.5\n0\n1\n0\n0\n81.33334\n\n\n1945\n2\n3\n0\n1.25\n0\n0\n1\n0\n0\n\n\n1945\n3\n0\n1\n.25\n0\n0\n0\n0\n0\n\n\n1945\n3\n1\n0\n1\n1\n0\n0\n81.33334\n0\n\n\n1945\n3\n2\n0\n.5\n0\n1\n0\n0\n81.33334\n\n\n1945\n3\n3\n0\n1.25\n0\n0\n1\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince I’m interested in how an attribute specific variable (risk) influences foreign policy choice, I need to structure the data so it represents that variable against all possible choices; thus, for January 1945, the data have 4 observations, one for each outcome, and risk indicates the level of risk the president incurs with any single choice."
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Discrete Time Hazard Models",
    "section": "",
    "text": "consider what “memory” might look like in a binary time series setting.\nintroduce concepts underlying hazard models.\nunderstand the discrete time hazard model."
  },
  {
    "objectID": "test.html#memoryless",
    "href": "test.html#memoryless",
    "title": "Discrete Time Hazard Models",
    "section": "Memoryless",
    "text": "Memoryless\nBy construction, this model lacks any memory. \\(Pr(Y_{i,t}=1)\\) is a function of \\(X_{i,t}\\), but is independent of anything that happened prior to \\(t\\).\nThis means dyads that have been at peace for 2 years and for 22 years are treated as the same, varying only on the \\(X\\) variables.\nNote this is by choice - we’re assuming \\(y_t\\) has no bearing on \\(y_{t+1}\\), so there is no persistence or memory from period to period. We’ll encounter a range of models that are explicitly aimed at understanding this question: How does having survived up until now affect the chances of failing now?"
  },
  {
    "objectID": "test.html#data",
    "href": "test.html#data",
    "title": "Discrete Time Hazard Models",
    "section": "Data",
    "text": "Data\nThe democratic peace data is panel data - composed of cross sections observed over time. The \\(y\\) variable is binary, and measures a rare event - conflict. So the \\(y\\) variable for any particular panel is usually a string of zeros, occasionally punctuated by a one.\nWhat we’d really like to know is if/how strings of zeros affect the chances of a one at any given point in time. This is a question of hazards."
  },
  {
    "objectID": "test.html#time",
    "href": "test.html#time",
    "title": "Discrete Time Hazard Models",
    "section": "Time",
    "text": "Time\nWhat would data for mortality (or any sort of spells) look like? There are two basic types:\nSurvival:\n\\[Age = 1, 2, 3, 4, 5, \\ldots 87, 88, 89\\]\nor\nFailure:\n\\[Death = 0, 0, 0, 0, 0, \\ldots 0, 0, 1\\]\nNote that these convey the two parts of the hazard - how many periods the individual survives, and at what period the individual fails.\nThese also represent two ways to think about time: continuously:\n\\[Age = 1, 2, 3, 4, 5, \\ldots 87, 88, 89\\]\nor discretely:\n\\[Death = 0, 0, 0, 0, 0, \\ldots 0, 0, 1\\]\nIn truth, we almost always measure time discretely in the sense that failure can only happen in certain intervals (days, weeks, years, etc.), even though time is continuous and failure can happen in much smaller increments than these (e.g. minutes, seconds). Still, most treatments consider two types of hazard models:\n\nContinuous time models using data like \\(Age = 1, 2, 3, 4, 5, \\ldots 87, 88, 89\\) as the \\(y\\) variable.\nDiscrete time models using data like \\(Death = 0, 0, 0, 0, 0, \\ldots 0, 0, 1\\) as the \\(y\\) variable. These are usually estimated using a binomial LLF (e.g. the logit model)."
  },
  {
    "objectID": "test.html#hazard-models",
    "href": "test.html#hazard-models",
    "title": "Discrete Time Hazard Models",
    "section": "Hazard models",
    "text": "Hazard models\nA key feature of any hazard model is that the model accounts for both time until failure, and the realization of failure. In continuous or discrete time models, both of these are part of the estimation.\n\n\n\n\n\n\nNote\n\n\n\nAn aside on naming - models like these are interchangeably called “hazard models,” “survival models,” “duration models,” or “event history models.” They all refer to the same basic idea - modeling the time until an event occurs. They can sometimes indicate whether the quantity of interest is the hazard or survival - note that these are opposites in the sense that the hazard is the probability of failure at a particular point in time, while survival is the probability of surviving up to that point in time. “Event history” often refers to discrete time models."
  },
  {
    "objectID": "test.html#discrete-time---binary-time-series-cross-section-data",
    "href": "test.html#discrete-time---binary-time-series-cross-section-data",
    "title": "Discrete Time Hazard Models",
    "section": "Discrete Time - Binary Time Series Cross Section data",
    "text": "Discrete Time - Binary Time Series Cross Section data\nTime is continuous insofar as time units are infinitely divisible, but in practice, we measure time in discrete units like days, months, years, etc. In the democratic peace data above, we will have two dyads “fail” (have disputes) at year 3; but it’s nearly certain one of those dyads started its dispute before the other one. We group the data by these time intervals (years, in this case). So we are measuring time discretely (i.e, in years), but the underlying process is continuous. Moreover, the fact these can be grouped by failure time makes them grouped duration data.\nIt’s common to have binary \\(y\\) variables observed for cross sections over time - these are Binary Time Series Cross Section (BTSCS) data. This is the form the democratic peace data takes, and is a common form of data in the social sciences. BTSCS data are grouped duration data, and failure is measured in discrete time.\nHere’s an example of BTSCS data thinking of disputes in dyads over time.\n\n\n\n\nBTSCS data\n\n\n\n\n\n\n\n\n\nState\nYear\nDispute\nCensored\n\n\n\nUS-Cuba\n1960\n0\n0\n\n\n\nUS-Cuba\n1961\n1\n0\n\n\n\nUS-Cuba\n1962\n0\n0\n\n\n\nUS-Cuba\n1963\n0\n0\n\n\n\nUS-Cuba\n1964\n0\n0\n\n\n\nUS-Cuba\n1965\n0\n0\n\n\n\nUS-Cuba\n1966\n0\n0\n\n\n\nUS-Cuba\n1967\n1\n0\n\n\n\nUS-Cuba\n1968\n0\n0\n\n\n\nUS-Cuba\n1969\n0\n0\n\n\n\nUS-Cuba\n1970\n0\n1\n\n\n\nUS-UK\n1960\n0\n0\n\n\n\nUS-UK\n1961\n0\n0\n\n\n\nUS-UK\n1962\n0\n0\n\n\n\nUS-UK\n1963\n0\n0\n\n\n\nUS-UK\n1964\n0\n0\n\n\n\nUS-UK\n1965\n0\n0\n\n\n\nUS-UK\n1966\n0\n0\n\n\n\nUS-UK\n1967\n0\n0"
  },
  {
    "objectID": "test.html#terminology",
    "href": "test.html#terminology",
    "title": "Discrete Time Hazard Models",
    "section": "Terminology",
    "text": "Terminology\nLet’s begin thinking about terminology:\n\nwe observe at each point \\(t\\) whether a unit fails or not. Failure means experiencing the event of interest. In mortality studies, this is is death; in the democratic peace data, the event is a militarized dispute.\neach unit is at risk until it exits the data either because the period of observation ends, or because it fails and can only fail once. In mortality studies, an individual can only fail once; in the democratic peace data, a dyad can fail multiple times.\na unit survives some spell up to the point at which it fails. We can count these time periods to measure survival time.\nthe period a unit survives is called a spell; spells end at failure.\nwe have no idea what happened to these units prior to 1960; the units are left-censored.\nwe have no idea what happens to these units after 1970; the units are right censored. Any unit that does not experience the failure event during the period of study is right-censored."
  },
  {
    "objectID": "test.html#illustration",
    "href": "test.html#illustration",
    "title": "Discrete Time Hazard Models",
    "section": "Illustration",
    "text": "Illustration\nHere are different spells:\n\n\ncode\nlibrary(tidyverse)\nlibrary(highcharter)\n\n# Binghamton University colors\nbinghamton_colors &lt;- c(\"#005A43\", \"#8C2132\", \"#FFD100\", \"#000000\", \"#636466\")\n\n# Create dataframes for each case with updated labels\ncases &lt;- list(\n  list(x = c(3, 6), y = c(1, 1), name = \"uncensored\"),\n  list(x = c(-0.5, 2.5), y = c(2, 2), name = \"left censored\"),\n  list(x = c(2.8, 8), y = c(3, 3), name = \"fails at last period\"),\n  list(x = c(3.5, 10), y = c(4, 4), name = \"right censored\"),\n  list(x = c(5.5, 7), y = c(5, 5), name = \"uncensored\")\n)\n\n# Create the plot\nhc &lt;- highchart() %&gt;%\n  hc_chart(type = \"line\") %&gt;%\n  hc_xAxis(\n    title = list(text = \"time\"),\n    plotLines = list(\n      list(value = 2, width = 2, color = \"black\"),\n      list(value = 8, width = 2, color = \"black\")\n    ),\n    min = 0,\n    max = 10\n  ) %&gt;%\n  hc_yAxis(\n    title = list(text = \"case\"),\n    min = 0,\n    max = 5,\n    tickInterval = 1\n  ) %&gt;%\n  hc_plotOptions(\n    series = list(\n      lineWidth = 3,\n      marker = list(enabled = FALSE)\n    )\n  ) %&gt;%\n  hc_legend(enabled = FALSE)\n\n# Add each case as a separate series with Binghamton colors\nfor (i in seq_along(cases)) {\n  hc &lt;- hc %&gt;% hc_add_series(\n    data = list_parse2(data.frame(x = cases[[i]]$x, y = cases[[i]]$y)),\n    name = cases[[i]]$name,\n    color = binghamton_colors[i]\n  )\n}\n\n# Display the plot\nhc\n\n\n\n\n\n\n\nsome units survive through the end of the study; these units are right censored. That is, they do not fail during the period of observation.\nfailure is only observed per year; so failure is grouped by year; these are grouped duration data. We could, for instance, graph the density of failures at each point in time, effectively grouping them by failure time.\nthe probability of failing at \\(t\\), given survival til \\(t\\) is the hazard of failure; at any point in time, this is called the hazard rate, denoted \\(h(t)\\).\nin the democratic peace model above, \\(h(t)\\) does not depend on what happened at \\(t-1\\), so \\(h(t)\\) is constant over time or is time invariant, or is duration independent."
  },
  {
    "objectID": "test.html#survival-spells",
    "href": "test.html#survival-spells",
    "title": "Discrete Time Hazard Models",
    "section": "Survival Spells",
    "text": "Survival Spells\nWe can measure survival spells; time elapsed until failure or censoring. These are the same data as above, just re-formed so the units are different. Note the summed survival time is equal to the total time at risk. So for the US-Cuba dyad, the total time at risk is 11 years. Also, notice that the US-Cuba dyad is censored in 1970. It survives 3 years since its last dispute, but the end of that spell is our observation period, not another dispute.\n\n\n\n\nSpell data\n\n\n\n\n\n\n\n\n\n\nState\nYear\nDispute\nfail\ncensored\nsurvival\n\n\nUS-Cuba\n1961\n1\n1\n0\n2\n\n\nUS-Cuba\n1967\n1\n1\n0\n6\n\n\nUS-Cuba\n1970\n0\n0\n1\n3\n\n\nUS-UK\n1970\n0\n0\n1\n11"
  },
  {
    "objectID": "test.html#survival-time",
    "href": "test.html#survival-time",
    "title": "Discrete Time Hazard Models",
    "section": "Survival time",
    "text": "Survival time\nSurvival time: the time up to failure, in the interval \\(t_0, t_{\\infty}\\) such that \\(t \\in \\{1,2,3 \\ldots t_{\\infty} \\}\\)"
  },
  {
    "objectID": "test.html#failure",
    "href": "test.html#failure",
    "title": "Discrete Time Hazard Models",
    "section": "Failure",
    "text": "Failure\nThe probability of the failure event:\n\\[\\begin{aligned}\nf(t) = Pr(t_i=t) \\nonumber\n\\end{aligned}\\]\nThis is the density."
  },
  {
    "objectID": "test.html#cumulative-function",
    "href": "test.html#cumulative-function",
    "title": "Discrete Time Hazard Models",
    "section": "Cumulative Function",
    "text": "Cumulative Function\nWrite the cumulative probability of failure up to \\(t_i\\).\n\\[\\begin{aligned}\nF(t) = \\sum_{i=1}^{\\infty} f(t_i) \\nonumber\n\\end{aligned}\\]\nNow, consider the probability of surviving up until \\(t\\) - this is equal to 1 minus the CDF, so\n\\[S(t) = 1-F(t) = P(t_{i} \\geq t)\\]\nMost importantly, the conditional probability given by the probability of failing at \\(t_i\\) given survival up until \\(t_i\\):\n\\[\\begin{aligned}\nh(t)=Pr(t=t_i | t \\geq t_i) \\nonumber \\\\\n= \\frac{f(t)}{S(t)} \\nonumber\n\\end{aligned}\\]\nThis is the hazard rate."
  },
  {
    "objectID": "test.html#hazard-rate",
    "href": "test.html#hazard-rate",
    "title": "Discrete Time Hazard Models",
    "section": "Hazard Rate",
    "text": "Hazard Rate\n\\[\\begin{aligned}\nh(t)=Pr(t=t_i | t \\geq t_i) \\nonumber \\\\\n= \\frac{f(t)}{S(t)} \\nonumber\n\\end{aligned}\\]\nThe hazard rate is conceptually important because it explicitly relates the past to the present, thereby incorporating memory into the statistical model. The hazard is different from \\(Pr(y_t=1)\\) because it conditions on what has happened prior to \\(t\\)."
  },
  {
    "objectID": "test.html#discrete-time-ht",
    "href": "test.html#discrete-time-ht",
    "title": "Discrete Time Hazard Models",
    "section": "Discrete time h(t)",
    "text": "Discrete time h(t)\nWhat we have derived is the discrete time hazard function - time is measured in discrete units (e.g. years, not parts of years like months or days); some processes only make sense in discrete terms - e.g. a member of the US House can only be turned out by voters every two years, not before."
  },
  {
    "objectID": "test.html#density",
    "href": "test.html#density",
    "title": "Discrete Time Hazard Models",
    "section": "Density",
    "text": "Density\nSince the probability of survival at some value of \\(t\\) is the probability of survival at \\(t\\) given survival up to \\(t\\), the conditional probability of survival is 1 minus the hazard rate:\n\\[Pr(t_j&gt;t | t_j\\geq t) = 1 - h(t)\\]\nWe can rewrite the survivor function as a product of the probabilities of surviving up to \\(t\\):\n\\[S(t) = \\prod_{j=0}^{t} \\{1-h(t-j)\\}\\]\nWe can rewrite the density \\(f(t)\\):\n\\[f(t) = h(t)S(t)\\]"
  },
  {
    "objectID": "test.html#estimation",
    "href": "test.html#estimation",
    "title": "Discrete Time Hazard Models",
    "section": "Estimation",
    "text": "Estimation\nLet’s build a likelihood - as you might have guessed, it needs to involve \\(f(t)\\) and \\(S(t)\\) (failure and survival times) so we can estimate \\(h(t)\\).\n\\[ \\mathcal{L} = \\prod_{t_i\\leq t} f(t_i) \\prod_{t_i\\geq t} S(t_i) \\]\nthen, think of censoring where \\(y_{i,t}\\) indicates when, and whether a subject ever fails; if zero, censored, if one, uncensored (fails during our period of observation).\n\\[ \\mathcal{L} = \\prod_{t_i\\leq t} f(t_i)^{y_{i,t}} \\prod_{t_i\\geq t} S(t_i)^{1- y_{i,t}} \\]\nThis should be looking familiar.\nNow, substituting:\n\\[ \\mathcal{L} = \\prod_{i=1}^{N} \\Bigg\\{ h(t) \\prod_{j=1}^{t-1}   [1-h(t-i)] \\Bigg\\} ^{y_{i,t}} \\Bigg\\{ \\prod_{j=1}^{t}   [1-h(t-i)] \\Bigg\\}^{1- y_{i,t}}\\]\nAnd substitute an appropriate link density for \\(f(t)\\) and \\(S(t)\\), e.g., exponential,\n\\[f(t) = \\lambda(t) exp^{\\lambda(t)}\\] \\[S(t) = exp^{-\\lambda(t)}\\] \\[h(t) = \\lambda\\]\nWeibull: \\[f(t) = \\lambda p (\\lambda(t))^{p-1} exp^{-(\\lambda t)^p}\\] \\[S(t) = exp^{-(\\lambda t)^p}\\] \\[h(t) = \\lambda p (\\lambda t)^{p-1}\\]\netc …"
  },
  {
    "objectID": "test.html#the-problem",
    "href": "test.html#the-problem",
    "title": "Discrete Time Hazard Models",
    "section": "The problem",
    "text": "The problem\nThe standard logit/probit model in these data assumes the errors are i.i.d. - that the disturbances are uncorrelated. A somewhat more interesting observation is that the model assumes no relationship between the outcome at \\(t\\) and the outcome at \\(t-1, t-2 \\ldots t-k\\). So the observations on \\(y\\) arise independently of one another …almost as if each observation is an independent Bernoulli trial. If this isn’t true, the model is misspecified, and likely the parameter estimates are biased.\nIn the context of the democratic peace data, this means the probability of a dispute at any point in time is unrelated to how long that particular dyad has been at peace. Whether it’s been at peace for 1 year or 10 years has no bearing on the chances of conflict now. On its face, this is a heroic assumption."
  },
  {
    "objectID": "test.html#the-solution",
    "href": "test.html#the-solution",
    "title": "Discrete Time Hazard Models",
    "section": "The solution",
    "text": "The solution\nAt its root, this is a model specification issue - we think time since last dispute is probably related to the chances of a dispute today, but no such measure is in the model. BKT suggest including “survival time” as a right hand side variable. Doing so explicitly models the effect of surviving up til \\(t\\) on the probability of failing at \\(t\\).\nBKT suggest including nonlinear functions of survival time so the effect of time isn’t constrained to be monotonic. They suggested using cubic splines of survival time; Carter and Signorino (2010) later show polynomials for survival time are just as good and easier to compute/understand."
  },
  {
    "objectID": "test.html#the-result",
    "href": "test.html#the-result",
    "title": "Discrete Time Hazard Models",
    "section": "The result",
    "text": "The result\nThis fundamentally changed models on BTSCS data - the state of the art since then is to include survival time, thereby measuring “memory” in the \\(y\\) series. While most BTSCS models since Beck, Katz, and Tucker (1998) include survival time, relatively few interpret it; that’s okay insofar as the effect of survival might not be of theoretical interest. Most incorrectly interpret the predictions as probabilities - they are now conditional probabilities, $pr(fail | survival), so hazards.\n\nConstant \\(h(t)\\) - no memory\nRevisiting …in this model, \\(Pr(y_i=1) = F(x_i\\beta + \\beta_0)\\), \\(x_i \\beta\\) induces deviation from the constant or baseline level; but there is no temporal variation, temporal persistence, or memory. What happened last year has no bearing on what we observe this year. Repeating, it is as if these are Bernoulli trials.\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\ndpm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n\nstargazer(dpm1, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\n\ncode\n# at mean data over 30 observations\n\natmean &lt;- data.frame(stime=seq(1,34,1), deml=median(dp$deml), border=0, caprat=median(dp$caprat), ally=0)\n\npredictions &lt;- data.frame(atmean, predict(dpm1, newdata=atmean, type=\"response\", se=TRUE)) %&gt;% mutate(fit=round(fit, 2) )\n\n\nggplot(predictions, aes(x=stime, y=fit)) +\n  geom_line(color=\"#005A43\", size=1) +\n  labs(x=\"Time since last dispute\", y=\"Pr(Dispute)\") +\n  geom_ribbon(aes(ymin=fit-1.96*se.fit, ymax=fit+1.96*se.fit), fill=\"grey90\", alpha=0.4) +\n  theme_minimal() +\n  annotate(\"text\", x = 15, y = 0.041, label = \"Effect of Democracy\", color = \"red\", size = 3) +\n  theme(panel.grid.major = element_line(color = \"black\",\n                                        size = 0.25,\n                                        linetype = 1)) +\n  theme(panel.grid.minor = element_line(color = \"black\",\n                                        size = 0.15,\n                                        linetype = 1))\n\n\n\n\n\n\n\n\n\nThis is the case where\n\\[h_0(t) = \\frac{1}{1+ exp(\\beta_0)}\\]\nthe baseline hazard is the constant. Even with \\(x\\) variables, there is still no accounting for time - the \\(x\\) effects are only shifts around this baseline hazard.\n\\[h_0(t) = \\frac{1}{1+ exp(\\beta_0+ x'\\beta)}\\]\nthis is still a constant baseline hazard with the effects of \\(x\\) deviating around it.\n\n\nMeasuring survival time\n\ncount periods of survival up to failure. This is a counter of survival time. generate a binary variable for each survival period.\nEither include those survival dummies in the logit, or include the survival counter itself with polynomials, e.g. \\(t^2, t^3, \\ldots\\).\ninterpret those coefficients as baseline hazards for groups that survive to \\(t_i\\).\nwith all \\(x\\) variables set to zero, the probability of failure is now given by the constant and the appropriate dummy or counter coefficient.\nNote the quantity of interest is not constant across time: it’s a conditional probability; the probability of failing at \\(t\\) given the estimated probability of survival through \\(t-1\\) - \\(h(t)|S(t)\\).\n\n\nHere’s what the survival time counter looks like in the democratic peace data:\n\n\ncode\nsdpshort &lt;- survivaldp %&gt;% head(160) %&gt;% dplyr::select(dyad, year, dispute, stime)\n\nlibrary(kableExtra)\ntibble(sdpshort)%&gt;% \n    kable(\"html\", caption=\"Survival Time, Democratic Peace Data\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) \n\n\n\nSurvival Time, Democratic Peace Data\n\n\ndyad\nyear\ndispute\nstime\n\n\n\n\n2020\n1951\n0\n0\n\n\n2020\n1952\n0\n1\n\n\n2020\n1953\n0\n2\n\n\n2020\n1954\n0\n3\n\n\n2020\n1955\n0\n4\n\n\n2020\n1956\n0\n5\n\n\n2020\n1957\n0\n6\n\n\n2020\n1958\n0\n7\n\n\n2020\n1959\n0\n8\n\n\n2020\n1960\n0\n9\n\n\n2020\n1961\n0\n10\n\n\n2020\n1962\n0\n11\n\n\n2020\n1963\n0\n12\n\n\n2020\n1964\n0\n13\n\n\n2020\n1965\n0\n14\n\n\n2020\n1966\n0\n15\n\n\n2020\n1967\n0\n16\n\n\n2020\n1968\n0\n17\n\n\n2020\n1969\n0\n18\n\n\n2020\n1970\n0\n19\n\n\n2020\n1971\n0\n20\n\n\n2020\n1972\n0\n21\n\n\n2020\n1973\n0\n22\n\n\n2020\n1974\n1\n23\n\n\n2020\n1975\n1\n0\n\n\n2020\n1976\n0\n0\n\n\n2020\n1977\n0\n1\n\n\n2020\n1978\n0\n2\n\n\n2020\n1979\n1\n3\n\n\n2020\n1980\n0\n0\n\n\n2020\n1981\n0\n1\n\n\n2020\n1982\n0\n2\n\n\n2020\n1983\n0\n3\n\n\n2020\n1984\n0\n4\n\n\n2020\n1985\n0\n5\n\n\n2041\n1961\n0\n0\n\n\n2041\n1962\n0\n1\n\n\n2041\n1963\n1\n2\n\n\n2041\n1964\n0\n0\n\n\n2041\n1965\n0\n1\n\n\n2041\n1966\n0\n2\n\n\n2041\n1967\n0\n3\n\n\n2041\n1968\n0\n4\n\n\n2041\n1969\n0\n5\n\n\n2041\n1970\n0\n6\n\n\n2041\n1971\n0\n7\n\n\n2041\n1972\n0\n8\n\n\n2041\n1973\n0\n9\n\n\n2041\n1974\n0\n10\n\n\n2041\n1975\n0\n11\n\n\n2041\n1976\n0\n12\n\n\n2041\n1977\n0\n13\n\n\n2041\n1978\n0\n14\n\n\n2041\n1979\n0\n15\n\n\n2041\n1980\n0\n16\n\n\n2041\n1981\n0\n17\n\n\n2041\n1982\n0\n18\n\n\n2041\n1983\n0\n19\n\n\n2041\n1984\n0\n20\n\n\n2041\n1985\n0\n21\n\n\n2042\n1951\n0\n0\n\n\n2042\n1952\n0\n1\n\n\n2042\n1953\n0\n2\n\n\n2042\n1954\n0\n3\n\n\n2042\n1955\n0\n4\n\n\n2042\n1956\n0\n5\n\n\n2042\n1957\n0\n6\n\n\n2042\n1958\n0\n7\n\n\n2042\n1959\n1\n8\n\n\n2042\n1960\n0\n0\n\n\n2042\n1962\n0\n1\n\n\n2042\n1964\n0\n2\n\n\n2042\n1965\n0\n3\n\n\n2042\n1966\n0\n4\n\n\n2042\n1967\n0\n5\n\n\n2042\n1968\n0\n6\n\n\n2042\n1969\n0\n7\n\n\n2042\n1970\n0\n8\n\n\n2042\n1971\n0\n9\n\n\n2042\n1972\n0\n10\n\n\n2042\n1973\n0\n11\n\n\n2042\n1974\n0\n12\n\n\n2042\n1975\n0\n13\n\n\n2042\n1976\n0\n14\n\n\n2042\n1977\n0\n15\n\n\n2042\n1978\n0\n16\n\n\n2042\n1979\n0\n17\n\n\n2042\n1980\n0\n18\n\n\n2042\n1981\n0\n19\n\n\n2042\n1982\n0\n20\n\n\n2042\n1983\n0\n21\n\n\n2042\n1984\n0\n22\n\n\n2042\n1985\n0\n23\n\n\n2051\n1962\n0\n0\n\n\n2051\n1963\n0\n1\n\n\n2051\n1964\n0\n2\n\n\n2051\n1965\n0\n3\n\n\n2051\n1966\n0\n4\n\n\n2051\n1967\n0\n5\n\n\n2051\n1968\n0\n6\n\n\n2051\n1969\n0\n7\n\n\n2051\n1970\n0\n8\n\n\n2051\n1971\n0\n9\n\n\n2051\n1972\n0\n10\n\n\n2051\n1973\n0\n11\n\n\n2051\n1974\n0\n12\n\n\n2051\n1975\n0\n13\n\n\n2051\n1976\n0\n14\n\n\n2051\n1977\n0\n15\n\n\n2051\n1978\n0\n16\n\n\n2051\n1979\n0\n17\n\n\n2051\n1980\n0\n18\n\n\n2051\n1981\n0\n19\n\n\n2051\n1982\n0\n20\n\n\n2051\n1983\n0\n21\n\n\n2051\n1984\n0\n22\n\n\n2051\n1985\n0\n23\n\n\n2052\n1962\n0\n0\n\n\n2052\n1963\n0\n1\n\n\n2052\n1964\n0\n2\n\n\n2052\n1965\n0\n3\n\n\n2052\n1966\n0\n4\n\n\n2052\n1967\n0\n5\n\n\n2052\n1968\n0\n6\n\n\n2052\n1969\n0\n7\n\n\n2052\n1970\n0\n8\n\n\n2052\n1971\n0\n9\n\n\n2052\n1972\n0\n10\n\n\n2052\n1973\n0\n11\n\n\n2052\n1974\n0\n12\n\n\n2052\n1975\n0\n13\n\n\n2052\n1976\n0\n14\n\n\n2052\n1977\n0\n15\n\n\n2052\n1978\n0\n16\n\n\n2052\n1979\n0\n17\n\n\n2052\n1980\n0\n18\n\n\n2052\n1981\n0\n19\n\n\n2052\n1982\n0\n20\n\n\n2052\n1983\n0\n21\n\n\n2052\n1984\n0\n22\n\n\n2052\n1985\n0\n23\n\n\n2070\n1951\n0\n0\n\n\n2070\n1952\n0\n1\n\n\n2070\n1953\n0\n2\n\n\n2070\n1954\n0\n3\n\n\n2070\n1955\n0\n4\n\n\n2070\n1956\n1\n5\n\n\n2070\n1957\n0\n0\n\n\n2070\n1958\n0\n1\n\n\n2070\n1959\n0\n2\n\n\n2070\n1960\n0\n3\n\n\n2070\n1961\n0\n4\n\n\n2070\n1962\n0\n5\n\n\n2070\n1963\n0\n6\n\n\n2070\n1964\n0\n7\n\n\n2070\n1965\n0\n8\n\n\n2070\n1966\n0\n9\n\n\n2070\n1967\n0\n10\n\n\n2070\n1968\n0\n11\n\n\n2070\n1969\n0\n12\n\n\n\n\n\n\n\n\n\nMonotonic hazard\nSo how to deal with this, incorporating memory: thinking in terms of hazards rather than probabilities (i.e., conditional rather than unconditional probabilities), what if we measure survival time?\n\nThe binary \\(y\\) variable is an indicator of failure at \\(t\\); the model estimates \\(f(t)\\), which we’ve said is not especially informative since subjects might fail before \\(t\\).\nThink of the number of periods up to failure as the cumulative survival time, \\(S(t)\\).\n\nSee how we’re starting to construct the hazard rate by its parts.\nHere’s an example in the democratic peace data:\n\\(dispute = \\beta_0+ \\beta_1(survival)\\)\n\n\ncode\ndpm2 &lt;-glm(dispute ~ stime, family=binomial(link=\"logit\"), data=survivaldp )\n\nstargazer(dpm2, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Survival Time\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Time\n\n\n-0.265*** (0.010)\n\n\n\n\nConstant\n\n\n-1.513*** (0.045)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n6,208.082\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\nAnd here are predicted probabilities from the model.\n\n\ncode\natmean &lt;- data.frame(stime=seq(1,34,1))\n\npredictions &lt;- data.frame(atmean, predict(dpm2, newdata=atmean, type=\"response\", se=TRUE)) \n\nggplot(predictions, aes(x=stime, y=fit)) +\n  geom_line() +\n  labs(x=\"Time since last dispute\", y=\"Pr(Dispute)\") +\n  geom_ribbon(aes(ymin=fit-1.96*se.fit, ymax=fit+1.96*se.fit), fill=\"#005A43\", alpha=0.4) +\n  theme_minimal()+\n  theme(panel.grid.major = element_line(color = \"black\",\n                                        size = 0.25,\n                                        linetype = 1)) +\n  theme(panel.grid.minor = element_line(color = \"black\",\n                                        size = 0.15,\n                                        linetype = 1))\n\n\n\n\n\n\n\n\n\nWhere the baseline hazard is no longer constant - it can increase or decrease monotonically:\n\\[h_0(t) = \\frac{1}{1+ exp(\\beta_0 + \\gamma_0(t))}\\]\nthe baseline hazard is the constant plus the effect of survival time - the \\(x\\) effects are shifts around this baseline hazard.\n\\[h_0(t) = \\frac{1}{1+ exp(\\beta_0 + \\gamma_0(t)) + x'\\beta)}\\]\nThe baseline hazard is no longer constrained to be constant, though it can be if \\(\\gamma_0=0\\).\n\nThis model accounts for “memory” - the QI is now the hazard.\nThe hazard is not constrained to be constant, but is constrained to be monotonic.\nTo relax this, we can\n\ninclude dummy variables - these are discrete time indicators based on the counter.\ninclude cubic splines or lowess estimates - these are smoothed time functions based on the counter.\ninclude polynomials of the time counter.\n\n\n\n\nNon-monotonic hazard\n\n\ncode\nsurvivaldp &lt;- survivaldp %&gt;% mutate(stime2=stime^2, stime3=stime^3)\n\ndpm3 &lt;-glm(dispute ~ stime+stime2+stime3, family=binomial(link=\"logit\"), data=survivaldp )\n\nstargazer(dpm3, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Survival Time\", \"Survival Time2\", \"Survival Time3\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Time\n\n\n-0.830*** (0.035)\n\n\n\n\nSurvival Time2\n\n\n0.052*** (0.004)\n\n\n\n\nSurvival Time3\n\n\n-0.001*** (0.0001)\n\n\n\n\nConstant\n\n\n-0.950*** (0.048)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n5,742.615\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\n\ncode\natmean &lt;- data.frame(stime=seq(1,34,1), stime2=seq(1,34,1)^2, stime3=seq(1,34,1)^3)\n\npredictions &lt;- data.frame(atmean, predict(dpm3, newdata=atmean, type=\"response\", se=TRUE)) \n\nggplot(predictions, aes(x=stime, y=fit)) +\n  geom_line() +\n  labs(x=\"Time since last dispute\", y=\"Pr(Dispute)\") +\n  geom_ribbon(aes(ymin=fit-1.96*se.fit, ymax=fit+1.96*se.fit), fill=\"#005A43\", alpha=0.4) +\n  theme_minimal()+\n  theme(panel.grid.major = element_line(color = \"black\",\n                                        size = 0.25,\n                                        linetype = 1)) +\n  theme(panel.grid.minor = element_line(color = \"black\",\n                                        size = 0.15,\n                                        linetype = 1))\n\n\n\n\n\n\n\n\n\nThis last is a close approximation of a Cox proportional hazards model. The hazard is non monotonic; it nests the exponential (constant hazard), and the monotonic (Weibull) hazard, and is very general. Besides, it’s very easy to estimate and interpret.\n\n\nUnderstanding substantive variables in the hazard context\nThe survival variables now permit the baseline hazard to vary. The effects of \\(x\\) variables can be thought of as deviations from those baseline hazards. For example, think about the models presented above, but with democracy. The estimates on democracy will shift the baseline hazard up or down.\n\n\ncode\nsurvivaldp &lt;- survivaldp %&gt;% mutate(stime2=stime^2, stime3=stime^3)\n\ndpm4 &lt;-glm(dispute ~ stime+stime2+stime3+deml, family=binomial(link=\"logit\"), data=survivaldp )\n\nstargazer(dpm4, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Survival Time\", \"Survival Time2\", \"Survival Time3\", \"Democracy\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Time\n\n\n-0.823*** (0.035)\n\n\n\n\nSurvival Time2\n\n\n0.051*** (0.004)\n\n\n\n\nSurvival Time3\n\n\n-0.001*** (0.0001)\n\n\n\n\nDemocracy\n\n\n-0.065*** (0.007)\n\n\n\n\nConstant\n\n\n-1.298*** (0.065)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n5,650.709\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\n\ncode\ndpm4 &lt;-glm(dispute ~ stime+stime2+stime3+deml+caprat, family=binomial(link=\"logit\"), data=survivaldp )\n\n# copy estimation data for avg effects\ndppred &lt;- survivaldp\n\n# df for output\ndf &lt;- data.frame(time=seq(1,34,1))\nfor (d in seq(-10,10,10)) {\n    df[paste0(\"p\", d+10)] &lt;- NA\n  df[paste0(\"Se\", d+10)] &lt;- NA\n}\n\n# predictions\nfor (d in seq(-10,10,10)) {\n  dppred$deml &lt;- d\n for (t in seq(1,34,1)) {  \n  dppred$stime &lt;- t\n  dppred$stime2 &lt;- t^2 \n  dppred$stime3 &lt;- t^3\n  pred &lt;- predict(dpm4, newdata=dppred, type=\"response\", se=TRUE)\n  df[t, paste0(\"p\", d+10)] &lt;- mean(pred$fit, na.rm=TRUE)\n  df[t, paste0(\"Se\", d+10)] &lt;- mean(pred$se.fit, na.rm=TRUE)\n  df$time[t] &lt;- t\n}\n}\n\n# plot\nggplot(df, aes(x=time, y=p0)) +\n  geom_line(color=\"#005A43\") +\n  geom_ribbon(aes(ymin=p0-1.96*Se0, ymax=p0+1.96*Se0), fill=\"#6CC24A\", alpha=0.4) +\n  geom_line(aes(y=p10), color=\"#005A43\") +\n  geom_ribbon(aes(ymin=p10-1.96*Se10, ymax=p10+1.96*Se10), fill=\"#A7DA92\", alpha=0.4) +\n  geom_line(aes(y=p20), color=\"#005A43\") +\n  geom_ribbon(aes(ymin=p20-1.96*Se20, ymax=p20+1.96*Se20), fill=\"#005A43\", alpha=0.4) +\n  labs(x=\"Time since last dispute\", y=\"Pr(Dispute)\") +\n  theme_minimal()+\n  theme(panel.grid.major = element_line(color = \"black\",\n                                        size = 0.25,\n                                        linetype = 1)) +\n  theme(panel.grid.minor = element_line(color = \"black\",\n                                        size = 0.15,\n                                        linetype = 1))"
  },
  {
    "objectID": "test.html#an-alternative---transitions",
    "href": "test.html#an-alternative---transitions",
    "title": "Discrete Time Hazard Models",
    "section": "An alternative - transitions",
    "text": "An alternative - transitions\nWhat happens if we lag \\(y\\) as we might with a continuous variable (say, in OLS), such that the estimation model is\n\\[y_t = \\beta_0 + \\beta_1(x_1) + \\ldots + \\gamma(y_{t-1}) + \\varepsilon\\]\nWith binary time series data, lagging \\(y\\) would measure changes of state - these are a class known as transition models (there are a variety of these).\nIn general, these are interactive models where\n\\[Pr(y_i=1) = F(x_{i,t}\\beta + y_{i,t-1}*x_{i,t} \\gamma)\\]\nand \\(\\gamma\\) measures the difference in effect when \\(y_{i,t-1} = 0\\) (this is just \\(\\beta\\)), and when \\(y_{i,t-1}  = 1\\); denote this \\(\\alpha\\). So \\(\\gamma = \\beta - \\alpha\\). That difference indicates the conditional probability of state transitions from the state where \\(y\\) takes on one value, to the state where it takes on the other value.\nTransition models are useful, but measure something fundamentally different from the latent hazard rate, or the chances of failure given a history of survival. Put differently, lagging \\(y\\) in a binary variable model does not measure memory or persistence; it does not measure the extent to which the observed value today depends on the value yesterday; it does not measure how the latent probability of failure today depends on surviving through yesterday."
  },
  {
    "objectID": "choiceB24.html",
    "href": "choiceB24.html",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "",
    "text": "Models for nominal variables are interesting because they derive so neatly from utility theory - this is the ideal of EITM. But it also means that we have to consider how assumptions from theory manifest themselves in econometric models. This alone is an interesting topic, because we’d want theory assumptions accurately reflected in the statistical model."
  },
  {
    "objectID": "choiceB24.html#utility-theory",
    "href": "choiceB24.html#utility-theory",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "",
    "text": "Models for nominal variables are interesting because they derive so neatly from utility theory - this is the ideal of EITM. But it also means that we have to consider how assumptions from theory manifest themselves in econometric models. This alone is an interesting topic, because we’d want theory assumptions accurately reflected in the statistical model."
  },
  {
    "objectID": "choiceB24.html#independence-of-irrelevant-alternatives-iia",
    "href": "choiceB24.html#independence-of-irrelevant-alternatives-iia",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Independence of Irrelevant Alternatives (IIA)",
    "text": "Independence of Irrelevant Alternatives (IIA)\n\nBoth of the MNL and CL models assume IIA.\nIIA is a rationality assumption about individual behavior rather than an econometric assumption.\nThe econometric realization of IIA is i.i.d. - we assume the errors surrounding each value of \\(Y=j\\) for \\(j-1\\) alternatives are i.i.d\nThis means we do make an econometric assumption about homogeneity of choices and individuals."
  },
  {
    "objectID": "choiceB24.html#what-is-iia",
    "href": "choiceB24.html#what-is-iia",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "What is IIA?",
    "text": "What is IIA?\nOrdeshook (1986) has a well-known example.\n\nA diner goes into a restaurant and wants to order soup. He is offered either Chicken or Vegetable and he chooses chicken - thus, his preference ordering must be Chicken p Vegetable.\nSoon, the waiter returns to say they also have noodle soup; the diner changes his order from Chicken to Vegetable.\n\nYou can see that the introduction of noodle soup should have no effect on his ranking of Chicken above Vegetable.\nHe might prefer Chicken p Vegetable p Noodle or Chicken p Noodle p Vegetable, but the original ranking of Chicken p Vegetable must remain unaffected.\nThe addition of Noodle cannot change his pairwise ordering of Chicken and Vegetable."
  },
  {
    "objectID": "choiceB24.html#iia-in-the-mnl",
    "href": "choiceB24.html#iia-in-the-mnl",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "IIA in the MNL",
    "text": "IIA in the MNL\nIf the introduction of new alternatives changes the order of individual preferences, then IIA fails. In estimation, we consider the ratio of the probabilities of outcomes \\(j\\) to \\(k\\) where these are two categories in \\(Y\\); note how this relies explicitly on the notion that MNL is a set of binary logits, pairwise comparisons:\n\\[\n\\frac{Pr(Y=j)}{Pr(Y=k)} = \\frac{ \\frac{e^{x_{i}\\beta_{j}}}{\\sum\\limits_{j=1}^{J}e^{x_{i}\\beta_{j}}}}{\\frac{e^{x_{i}\\beta_{j}'}}{\\sum\\limits_{j=1}^{J}e^{x_{i}\\beta_{j}}}}\n=\\frac{e^{x_{i}\\beta_{j}'}}{{e^{x_{i}\\beta_{j}'}}} \\nonumber\n\\]\nConsideration of another outcome, \\(l\\), cannot influence this ratio. This ratio is the ratio of preferences for \\(j\\) to \\(k\\), all other options irrelevant."
  },
  {
    "objectID": "choiceB24.html#related-to-preference-transitivity",
    "href": "choiceB24.html#related-to-preference-transitivity",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Related to Preference Transitivity",
    "text": "Related to Preference Transitivity\nThink of this in terms of the transitivity requirement:\nSuppose alternatives \\(a,b,c\\) where \\(a&gt;b\\) – any of the following can be true: \\[\\begin{aligned}\nc&gt;a&gt;b \\nonumber \\\\\na&gt;c&gt;b \\nonumber \\\\\na&gt;b&gt;c \\nonumber\n\\end{aligned}\\]\n…because all retain \\(a&gt;b\\). Consideration of \\(c\\) cannot upset the relation of \\(a&gt;b\\).\nSo the ratio of \\(Pr(Y=j)\\) to \\(Pr(Y=k)\\) must be entirely independent of other alternatives; adding other choices to the outcome variable cannot change this ratio or IIA is violated.\nTo link this to the soup example above, if introducing noodle soup to the menu changes the ratio of \\(\\frac{P(chicken)}{P(vegetable)}\\), then it must do so by taking probability away disproportionately from one choice rather than the other. If the probability of one shifts while the other does not, then introducing noodle soup shifts the diner’s preference order."
  },
  {
    "objectID": "choiceB24.html#redblue-busses",
    "href": "choiceB24.html#redblue-busses",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Red/Blue Busses",
    "text": "Red/Blue Busses\nHere’s another example that appears in virtually every econometrics book. Suppose we want to study the how individuals are transported to work. Half of our sample takes a car, the other half takes a red bus from the Red Bus Co. Thus,\n\\[\\begin{aligned}\nPr(car)=.5 , Pr(red ~bus) = .5 \\nonumber \\\\~\\\\\n\\frac{Pr(car)=.5 }{Pr(red~ bus) = .5 } = 1 \\nonumber\n\\end{aligned}\\]\nThe ratio of the probability of driving a car to the probability of taking the red bus is 1. So no matter what, we cannot introduce another alternative that changes this ratio.\nA new bus company comes to town, the Blue Bus Co. It’s unlikely the car drivers will switch to the blue bus company - they’d have been riding the red bus all along. Thus, the Blue Bus Co. is going to compete with the Red Bus Co. and will draw customers disproportionately from the Red Bus Co. So,\n\\[\\begin{aligned}\nPr(car)=.5 , Pr(red~ bus) = .25,   Pr(blue ~bus) = .25 \\nonumber \\\\~\\\\\n\\frac{Pr(car)=.5 }{Pr(red ~bus) = .25 } = 2 \\nonumber\n\\end{aligned}\\]\nThe Blue Bus Co. drew from the probability of taking a red bus, but not from the probability of taking a car, so the ratio of the probability of taking a car to the probability of taking the red bus changed. Introducing the Blue Bus alternative violated IIA. In order not to have violated IIA, Blue Bus Co. would have had to have taken probability equally from both categories, car and red bus.\nIf we were estimating \\(Pr(Blue Bus|X)\\), the effects of \\(X\\) would have to increase or decrease the chances of choosing the Blue Bus proportionally to increasing or decreasing the chances of Red Busses or cars or rickshaws, or whatever, so that the ratio of the expected conditional probabilities would remain constant.\nFor this to happen, the unobservables that influence these choices would have to be independent of one another.\n\n\n\n\n\n\nAn important implication\n\n\n\nIf an alternative \\(j\\) is a near substitute for alternative \\(k\\), but not for alternatives \\(l,m, \\ldots z\\), then the alternative \\(j\\) is neither independent nor irrelevant. In other words, MNL and CL models may be inappropriate for questions about substitution because of the IIA assumption. This would be unfortunate since a lot of models of substitution are MNL."
  },
  {
    "objectID": "choiceB24.html#an-important-implication",
    "href": "choiceB24.html#an-important-implication",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "An important implication",
    "text": "An important implication\nIf an alternative \\(j\\) is a near substitute for alternative \\(k\\), but not for alternatives \\(l,m, \\ldots z\\), then the alternative \\(j\\) is neither independent nor irrelevant. In other words, MNL and CL models are often not appropriate for questions about substitution.\nThis is an unfortunate conclusion since a lot of models of substitution are MNL."
  },
  {
    "objectID": "choiceB24.html#empirical-implication",
    "href": "choiceB24.html#empirical-implication",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Empirical implication",
    "text": "Empirical implication\n\nThe error terms of outcomes \\(j\\) and \\(k\\) are not correlated by assumption. If the IIA assumption fails, then the error terms are correlated and probably heteroskedastic as well (so neither independent nor identically distributed).\nMoreover, the model is misspecified; we assume individuals making decision behave according to IIA but they don’t, so our model does not represent their behavior. In the end, estimates are biased and can be inefficient as well."
  },
  {
    "objectID": "choiceB24.html#relating-iia-to-i.i.d.",
    "href": "choiceB24.html#relating-iia-to-i.i.d.",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Relating IIA to i.i.d.",
    "text": "Relating IIA to i.i.d.\nIn the 1996 Presidential election, the public faced a choice among three candidates, Bill Clinton, Bob Dole and Ross Perot. Lucky voters ranked these three fine gentlemen and cast their votes. Suppose the distribution of preferences just between Clinton and Dole prior to Perot’s entry looked like this:\nClinton = .6 Dole = .4\nVoters preferred Clinton by a 3:2 ratio over Dole.\nSuppose, however, that once Ross Perot entered the race, he offered a morally founded and liberal platform such that he took virtually all of Clinton’s support, leaving Clinton with just 10% of the voters, so\nClinton = .1 Dole = .4 Perot = .5\nConsistent with the rational choice requirement, voters have shifted from one of the original alternatives to the new alterative, not to the other original choice. Inconsistent with the econometric requirement, the ratio of Clinton to Dole is not the same once Perot is in the race - it was originally 3:2, and now it is 1:4. So on the surface, it appears as if we have maintained the formal requirement and violated the econometric one.\nBut look more closely.\nIt is also true that voters now prefer Dole to Clinton by a 4 to 1 ratio, so in fact, the formal requirement also fails. Thus, it appears as if the formal and econometric versions of the requirement have some equivalence.\nThe choice between Clinton and Dole is correlated with the Perot option. The fact that Perot is an option shifts the probabilities for Clinton and Dole unevenly - to be clear, if Perot took the same proportion of the vote from each such that the ratio of Clinton to Dole was still 3:2, then IIA would be intact.\nThe factors that make voters choose Perot are correlated with the factors that make voters choose Clinton or Dole. If we include those factors in the model (instead of having them in the errors), we reduce the correlation of the errors. This is a specification problem."
  },
  {
    "objectID": "choiceB24.html#iia-in-reality",
    "href": "choiceB24.html#iia-in-reality",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "IIA in reality",
    "text": "IIA in reality\nAs Long & Freese point out, testing IIA is anything but straightfoward - they essentially advise against trusting tests. Paul Allison expresses similar skepticism of IIA tests.\nMy intuition is models violating IIA often don’t have different estimates than models accounting for it. why account for it? To be sure estimates are stable, and to evaluate model specification. correlation of the errors around the outcomes indicate some common factors predict those outcomes; they’re in the error because they’re excluded from the model."
  },
  {
    "objectID": "choiceB24.html#how-to-relax-iia-by-relaxing-i.i.d.",
    "href": "choiceB24.html#how-to-relax-iia-by-relaxing-i.i.d.",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "How to relax IIA? By relaxing i.i.d.",
    "text": "How to relax IIA? By relaxing i.i.d.\nmultinomial probit (choice specific variables like CL) - structured like CL, but probit, so normal, but the multivariate normal distribution with j-1 dimensions; estimates the correlation of the errors, thereby permitting non-independence. multivariate probit (case specific variables like MNL) - a set of simultaneous probit equations, \\(x\\) variables can differ across outcomes; measures correlations among errors, permitting non-independence. mixed logit - (choice or choice by case specific variables) - estimates fixed and random parameters; the latter estimates effects across individuals, permitting variation, and estimates that variation.\nHere’s an account of how different methods impose IIA or i.i.d. restrictions.\n\n\n\n\nModels & Assumptions\n\n\n\n\n\n\n\n\n\nModel\nIIA\ni. i. d.\nComment\nIndependent ariables\n\n\n\n\n\n\n\n\n\nMNL/CL\nYes\nYes\ni.i.d Logistic\n\\(X ~\\forall~ Y_i\\); CL varies\n\n\nMNP\nNo\nNo\nnot independent, but identical\n\\(X ~\\forall ~Y_i\\)\n\n\nMVP\nNo\nPartial\nnot independent, but identical\n\\(X_i ~\\text{for each} ~ Y_i\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are several approaches to relaxing the IIA assumption including the multinomial probit, the heteroskedastic extreme value model, and the multivariate probit. The two probit models, rooted in the standard normal distribution, take advantage of the multivariate normal distribution to allow for correlation among the errors. The HEV model allows the variance of the errors to vary across the choices, so relaxes the “identically distributed” assumption, though still requires the errors are independent. Let’s look at the multinomial probit and the multivariate probit in more detail."
  },
  {
    "objectID": "choiceB24.html#multivariate-probit",
    "href": "choiceB24.html#multivariate-probit",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Multivariate probit",
    "text": "Multivariate probit\nThe idea here is to estimate a system of equations, one for each outcome. The errors in these equations can be correlated, so the outcomes are not assumed independent. The model is a set of \\(J\\) probit equations, one for each outcome, \\(j\\). The \\(x\\) variables can vary across the equations, so the \\(x\\) variables for \\(y_1\\) can be different from the \\(x\\) variables for \\(y_2\\). The errors are correlated across the equations, so the errors for \\(y_1\\) are correlated with the errors for \\(y_2\\). The model estimates the correlation of the errors, \\(\\rho\\). This both measures the extent to which the outcomes are correlated and relaxes the IIA (i.i.d.) assumption by “explaining” the correlation.\nThinking of the foreign policy choice data we’ve been working with, we have so far conceived of the \\(y\\) variable as a single, categorical but unordered variable:\n\n0 = no action\n1 = MID only\n2 = GATT only\n3 = both MID and GATT\n\nInstead, we could define 2 dummy variables this way:\n\nMilitarized Interstate Dispute - 0=no, 1=yes\nGATT dispute - 0=no, 1=yes\n\nsuch that both variables can equal zero at the same time (indicating neither a MID nor a GATT dispute occurred) or they can both equal one at the same time (indicating both types of dispute occur at the same time). Note these represent the same information as the categorical variable. Now, we’ve got two \\(y\\) variables and we could estimate two probit equations:\n\\[\nP(MID=1) = \\Phi(\\beta_m x_i) \\nonumber \\\\\nP(GATT=1) = \\Phi(\\beta_g x_i) \\nonumber\n\\]\n\nThe binary variables option is an elegant way to capture all these possibilities.\nHaving two dependent variables implies two equations. -If we estimate the two equations as individual regressions, their errors are uncorrelated by construction; we impose i.i.d. by treating the alternatives as zero.\nIf we estimate these equations simultaneously, that is as a system, we must say something specific about their errors and the relationship among their errors.\nAs a set of equations, we can (perhaps should?) have different sets of \\(X\\) variables predicting the outcomes; \\(X\\) is restricted across outcomes in most of the choice models we’ve talked about.\n\nWhy probit? We take advantage of the dimensionality of the normal distribution, and use the multivariate (in this case, bivariate) normal distribution - so the outcomes are jointly determined, and the unobservables are correlated, \\(\\rho\\). In the single equation probit, we assume the errors are univariate normal.\n\n\ncode\nlibrary(mvProbit)\nfpdata &lt;- read_dta(\"/Users/dave/Documents/teaching/606J-mle/2024/topics/choicemodels/nominaldata.dta\")\n\nfpmvp &lt;- filter(fpdata, !is.na(unemp2) & !is.na(pressupp) & !is.na(pelect) & !is.na(approval))\n\nmvp &lt;- mvProbit &lt;- mvProbit(cbind(mid, gatt) ~ unemp2 +pressupp +pelect , data = as.data.frame(fpmvp), iterlim = 20, nGHK = 100)\n\nlibrary(kableExtra)\n\n# Extract coefficients and standard errors\ncoef_table &lt;- summary(mvp)$estimate\ncoef_df &lt;- as.data.frame(coef_table)\n\n# Add column for p-values\ncoef_df$p_value &lt;- 2 * (1 - pnorm(abs(coef_df$`t value`)))\n\nn_cols &lt;- ncol(coef_df)\n\n# Create appropriate column names based on the number of columns\nif (n_cols == 3) {\n  col_names &lt;- c(\"Estimate\", \"Std. Error\", \"z value\")\n} else if (n_cols == 4) {\n  col_names &lt;- c(\"Estimate\", \"Std. Error\", \"z value\", \"Pr(&gt;|z|)\")\n} else {\n  col_names &lt;- colnames(coef_df)  # Use existing column names if structure is unexpected\n}\n\n# Create the table\nkable(coef_df, \n      format = \"html\",\n      digits = 3,\n      col.names = col_names,\n      caption = \"MVP Model Results\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n                full_width = FALSE) %&gt;%\n  add_header_above(c(\" \" = 1, \"Coefficients\" = (n_cols ))) %&gt;%\n  row_spec(0, bold = TRUE)\n\n\nMVP Model Results\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficients\n\n\n\n\nEstimate\nStd. error\nt value\nPr(&gt; t)\np_value\n\n\n\n\nb_1_0\n-1.433\n0.486\n-2.948\n0.003\n0.003\n\n\nb_1_1\n0.062\n0.039\n1.576\n0.115\n0.115\n\n\nb_1_2\n0.008\n0.005\n1.605\n0.108\n0.108\n\n\nb_1_3\n-0.014\n0.147\n-0.096\n0.924\n0.924\n\n\nb_2_0\n-0.331\n0.523\n-0.633\n0.527\n0.527\n\n\nb_2_1\n0.155\n0.043\n3.609\n0.000\n0.000\n\n\nb_2_2\n-0.023\n0.006\n-3.993\n0.000\n0.000\n\n\nb_2_3\n-0.024\n0.171\n-0.142\n0.887\n0.887\n\n\nR_1_2\n-0.023\n0.094\n-0.245\n0.806\n0.806\n\n\n\n\n\n\nInterestingly, the main results from the multinomial models hold in general, but as the hypothesis test on \\(\\rho\\) (which is the correlation between the disturbance terms in the two equations) shows, the errors in the two equations are correlated significantly. Some interpret this as evidence IIA does not hold (see Alvarez & Nagler’s paper) and some economists suggest this is evidence of substitution.\n\n\ncode\nlibrary(mvProbit)\nlibrary(kableExtra)\n\nmvp &lt;- mvProbit &lt;- mvProbit(cbind(mid, gatt) ~ unemp2 +pressupp +pelect , data = as.data.frame(fpmvp), iterlim = 20, nGHK = 10)\n\n\nX &lt;- as.matrix(expand.grid(\n  intercept= 1, \n  unemp2 = median(fpdata$unemp2, na.rm=TRUE),\n  pressupp = seq(43,93, length.out = 50),\n  pelect = 0,\n  approval = median(fpdata$approval, na.rm=TRUE)\n))\n\n# Calculate expectations and marginal effects\neffects &lt;- data.frame(mvProbitExp(~ unemp2 +pressupp +pelect , coef=mvp$estimate, data=as.data.frame(X)), X)\n\n\n# plot V1 V2 over pressupp\n\nggplot(effects, aes(x=pressupp, y=V1)) +\n  geom_line() +\n  geom_line(aes(y=V2), color=\"red\") +\n  labs(title=\"Marginal Effects of Press Support on MID and GATT Disputes\",\n       x=\"Press Support\", y=\"Marginal Effect\") +\n  theme_minimal() +\n  theme(legend.position=\"none\")"
  },
  {
    "objectID": "choiceB24.html#closing-out-choice-models",
    "href": "choiceB24.html#closing-out-choice-models",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Closing out Choice Models",
    "text": "Closing out Choice Models\n\nMNL is essentially a collection of simultaneous binary logits.\nMNL and CL are identical models.\nOnly the data structure and what we can measure vary between MNL/CL.\nMNL has \\(n\\) observations, data only on characteristics of \\(i\\).\nCL has \\(n*j\\) observations, data on attributes of \\(j\\) (and perhaps of \\(i\\) as well).\nboth require IIA\nIIA requires that, for any pairwise comparison of outcomes, \\(i, j\\), the ratio of \\(Pr(i)/Pr(j)\\) must not change due to the existence of any other option, \\(k\\).\nIn the model, the ratio of those probabilities conditional on the \\(X_i\\) must be consistent.\nAny time we can conceive of the choice outcomes as substitutes, the ratio will certainly change; so IIA fails.\nIn the econometric model, we assume the disturbances around outcome \\(j\\) are uncorrelated with the disturbances around outcome \\(k\\). If IIA fails, this fails, and the errors are not i.i.d.\nSolutions usually involve relaxing the independence assumption.\nSimplest is to use the n-variate normal, dimensions correlated by \\(\\rho_{j,k}\\).\nOne model using this is the multinomial probit. It restricts the \\(x\\) to be the same across the outcomes (just like MNL).\nAnother using the normal is the multivariate probit (a set of simultaneous probit equations); the \\(x\\) can vary across outcomes.\nIn either case, computation on the n-variate normal gets very difficult with higher dimensions ($&gt;$2 or 3 outcomes). Usually done with simulations (this is what Stata does).\nEither model resolves IIA problem by permitting correlation of errors such that shifts in the ratio of probabilities is permissible.\n\nYou should have the idea that substitution is a major concept lurking beneath all these choice models, or certainly beneath all their applications.\n\nImagine that two choices, A and B. A is more expensive, but comes in lots of colors. B is cheaper, but only comes in black. Individuals will substitute one for the other based on how they evaluate the tradeoff between price and color.\nThose who really prefer color will choose A; those that really respond to price will choose B.\nIf a new alternative, C, were introduced and it was identical to A, it would draw entirely from individuals who preferred A in the first place, and would draw nothing from B.\n\nThis would violate IIA - the ratio\n\\[\\frac{\\partial(Pr(y_i))/\\partial(x)}{\\partial(Pr(y_j))/\\partial(x)} \\neq 1\\]\nBut suppose new alternative C offers some limited color options, and is cheaper than A. - Will C draw equally from A and B? Not necessarily. - Will C draw from A and B for different reasons? Yes.\nIn other words, an alternative, C, alters the (pairwise) choice probabilities between A and B at different rates, and for heterogeneous reasons. Some substitution to C occurs because of price, some because of color - there is no reason to believe movement to C is at equal rates with respect to price {and} with respect to color. If the rate of substitution from A to C is different from the rate of substitution from B to C, then IIA fails. If the rate of substitution from A to C is different between those who substitute based on price and those who substitute based on color, we have a different problem.\nThis restriction is known as the Invariant Proportion of Substitution (IPS), due to Steenburgh (2008).\nIPS: the restriction in essentially every choice model that:\n\\[\n\\frac{\\partial(Pr(y_i))/\\partial(x_{i,a})}{\\partial(Pr(y_j))/\\partial(x_{i,a})} = \\Psi_{i, a}  ~~\\forall a  \\nonumber\n\\]\nThe rate of substitution between \\(i,j\\) in this example does not depend at all on which attribute, \\(a\\) is changed or improved. Change in attribute \\(x_{i, a}\\) produces a constant rate of substitution, \\(\\Psi\\) regardless of which attribute actually changed.\nHow significant a restriction is IPS? Conceptually, perhaps it is quite restrictive - the rates of substitution cannot vary across attributes in any standard choice model (GEV, correlated normal). If we believe individual choices among A, B, and C are driven in different directions by different attributes of A, B, and C, then assuming this cannot happen is a problem.\nSolutions - few practical ones exist. The “mother” logit or “universal” logit (McFadden 1975) is possible, but some suggest it departs random utility conditions in other ways. One innovation is the “flexible substitution logit.” (Liu, Steenburgh, and Gupta 2011)\nThe punch lines are these:\n\nchoice models are very flexible.\nchoice models appeal because they arise so directly from random utility models.\nIIA is a significant restriction, but fairly easy to relax.\nIPS is a newly recognized restriction - how consequential it is is not really known."
  },
  {
    "objectID": "choiceB24.html#choice-models-and-theory",
    "href": "choiceB24.html#choice-models-and-theory",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "",
    "text": "Models for nominal variables are interesting because they derive so neatly from utility theory - this is the ideal of EITM. But it also means that we have to consider how assumptions from theory manifest themselves in econometric models. This alone is an interesting topic, because we’d want theory assumptions accurately reflected in the statistical model."
  },
  {
    "objectID": "countA24.html",
    "href": "countA24.html",
    "title": "Count Models I",
    "section": "",
    "text": "Counts of events are common in the social sciences, though one of the originators of count models, Lambert, was interested in an engineering question with respect to the number of mistakes a silicon chip press makes.\nA. C. Cameron and Trivedi (2013) list examples of event count variables:\n\npatents issued\nbank failures\naccident insurance claims\ncredit ratings\nPresidential appointments to the Supreme Court\n\nEvent count data have the following characteristics:\n\nthey are nonnegative - since we cannot have negative events, all values are greater than or equal to zero\nthey are discrete\n\nYou should note that these two points also characterize the binary, nominal variables we’ve considered.\nSuppose …}\nSuppose \\(y\\) is whether or not a voter turned out to vote in the 2016 general election in the US. For each voter, \\(i\\), we observe \\(0|1\\). The implied unit of analysis is the voter.\nNote that we could aggregate voters into groups by census tract, county, congressional district, state, etc. Our binary, perhaps binomial variable is now a discrete count of voters per unit (county, etc.).\nPer unit …\nWe are now observing aggregated Bernoulli events per some unit, sometimes time, sometimes space, sometimes other - examples include:\n\nnumber of cheaters caught per calculus exam.\nnumber of unique visitors to a web page per hour.\nnumber of snicker bars produced without nuts per 8-hour shift.\nnumber of violent social protests per country, per year.\nnumber of flu cases per family.\nnumber of failed coup attempts per year.\n\nRefer to the per unit as the exposure, \\(d\\).\nThis matters because …\nLike the binary variables, these aggregations are discrete and strictly non-negative. The exposure, \\(d\\), produces a rate at which these discrete events happen. Such a variable might look like this:\n\n\n\n\nSnicker Bar Failures per 8-hour shift\n\n\nFailures\nHours\n\n\n0\n8\n\n\n0\n8\n\n\n2\n8\n\n\n1\n8\n\n\n0\n8\n\n\n0\n8\n\n\n…\n…\n\n\n1\n8\n\n\n\n\n\n\n\nOf course, \\(d\\) is constant here, so will factor out.\n\n\nGenerally, if \\(d\\) is known and/or finite, the event variable in question should not be modeled as a count variable. If we know the number of voters, and the number of registered voters, we know the binomial “successes” and the number of trials. Model as a proportion, or as a logit where the unit of analysis is the registered voter, and the \\(y\\) variable indicates vote or not.\n\n\n\nIf \\(d\\) is unknown or infinite or infinitely divisible (like time, which is a very common \\(d\\)), events per \\(d\\) may well be counts. Such a \\(d\\) cannot be the denominator (trials) to produce a proportion, and cannot readily become the unit of analysis.\n\n\n\n\nSuppose events occur at some rate (so \\(j\\) events per \\(d\\)); label that rate \\(\\lambda\\); and the exposure, \\(d\\), is some period of time with length \\(d\\).\nThe probability of an event, \\(j\\) occurring during \\(d\\) is \\(\\lambda*d\\); the probability of no event during that period is \\((1-\\lambda)*d\\).\nAssume the probability of one event occurring is entirely independent of the probability that any other event occurs in the same period, \\(d\\).\nNow, we want to know the probability that \\(y=j\\) events occurring in any period \\(t\\) of length \\(d\\), where \\(j= 0 , 1, 2, 3 \\ldots \\infty\\).\n\n\\[\nPr(y_i=j) = \\frac{e^{-\\lambda d} * (\\lambda d)^{y_i}}{y_i !} \\nonumber\n\\]\n\n\n\nThis is the PDF; the CDF of a discrete distribution is the sum of the PDF.\n\\[\nPr(y_i=j) = \\frac{e^{-\\lambda d} * (\\lambda d)^{y_i}}{y_i !} \\nonumber\n\\]\nIf \\(d\\) is constant, normalized to one, it drops out:\n\\[\nPr(y_i=j) = \\frac{e^{-\\lambda}  \\lambda^{y_i}}{y_i !} \\nonumber\n\\]\nand we can interpret \\(\\lambda\\) as \\(E[Y]\\), the expected number of events per time interval.\n\n\n\nThis should give you the idea that quantities are very easy in the Poisson regression, and that there are three quantities we can generate:\n\nthe probability \\(y\\) takes on any particular number of events : \\[Pr(y_i=j) = \\frac{e^{-\\lambda}  \\lambda^{y_i}}{y_i !}\\].\nthe expected number of events given \\(X\\) : \\(\\hat{\\lambda}\\) which we parameterize as \\[\\hat{\\lambda}=exp(x\\widehat{\\beta})\\].\nthe cumulative probability of \\(j\\) events, so say \\[Pr(y \\leq 3) = \\sum\\limits_{j&lt;4}\\frac{e^{-\\lambda}  \\lambda^{y_i}}{y_i !}\\].\n\n\n\n\nFor the Poisson regression, we assume:\n\nthe probabilities of events in a period are independent of one another.\nthe expected count is equal to the variance; \\(E[Y]= var[Y]\\) - more specifically, \\(E[Y|X]= var[Y|X]\\)\nthe probability of an event, \\(\\lambda\\), in any given period, \\(d\\), is constant across \\(d\\) (i.e. constant rate of arrival)\n\n\n\n\nCredited to Simeon Denis Poisson in his study of jury decisions (wrongful convictions, in part), but first put to use by Ladislaus von Bortkiewicz in a study showing the number of Prussian soldiers kicked to death by mules or horses followed a Poisson distribution.\n\n\ncode\nlibrary(vcd)\ndata(\"VonBort\")\nhorse&lt;-data.frame(VonBort)\nhorsesum &lt;- horse %&gt;% group_by(deaths) %&gt;% count(deaths) %&gt;% ungroup()\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\nhchart(horsesum, \"column\", hcaes(x=deaths, y=n)) %&gt;% \n  hc_title(text=\"Deaths by Horse or Mule Kick\") %&gt;% \n  hc_subtitle(text=\"Prussian Army, 19th Century\") %&gt;% \n  hc_yAxis(title=\"Frequency\") %&gt;% \n  hc_colors(\"#005A43\") %&gt;%\n  hc_xAxis(title=\"Deaths\") %&gt;% \n  hc_legend(enabled=FALSE) %&gt;% \n  hc_tooltip(pointFormat = \"Deaths: {point.x}&lt;br&gt;Frequency: {point.y}\") %&gt;% \n  hc_credits(enabled=TRUE, text=\"Source: von Bortkiewicz, L. (1898). Das Gesetz der Kleinen Zahlen. Leipzig: Teubner.\") \n\n\n\n\n\n\n\n\n\nAnother example - we might observe the number of individuals who catch the flu during a winter out of the number of people in their respective families. Note three things:\n\n\\(j\\) occurs over \\(d\\); \\(d\\) is exposure which is the family size.\n\\(d\\) is not constant; some families are small, some are large.\nevents are not independent; the probability of one case of flu is related to the probability of another case of flu in the same exposure unit, \\(d\\).\n\nThe Poisson can accommodate the first two of these, but not the third. Actually, you should note that if family size is known, this is probably not a variable we should model as a count; we should compute the proportion instead.\n\n\n\n\n\n\nAn aside on events over time\n\n\n\nThe fact that \\(d\\) is usually some period of time, and that \\(\\lambda\\) is a rate of occurrence over time is interesting.\n\nthese models observe or count events per spell.\nwe can imagine dividing a spell, \\(d\\) into \\(j+1\\) segments of unspecified length; call these units \\(d_{t,j}\\).\nwe could count the number of time units between \\(j=1, j=2, \\ldots j=j\\), or count the time units within each \\(d_{t,j}\\).\nso instead of counting events, we could count time between events.\n\n\n\n\n\n\nWhat sort of model does this \\(y\\) variable imply?\nSuppose data that look like this:\n\n\n\n\nEvents per year\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nevents\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1945\n0\n\n\n\n\n\n\n\n\n\n\n1946\n3\n\n\n\n\n\n\n\n\n\n\n1947\n1\n\n\n\n\n\n\n\n\n\n\n1948\n0\n\n\n\n\n\n\n\n\n\n\n1949\n0\n\n\n\n\n\n\n\n\n\n\n1950\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe same data conceived differently:\n\n\n\n\nMonths since last event\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nmonths\nevent\nevent month\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1945\n12\n0\n.\n\n\n\n\n\n\n\n\n1946\n15\n1\nMarch\n\n\n\n\n\n\n\n\n1946\n6\n1\nSept\n\n\n\n\n\n\n\n\n1946\n1\n1\nOct\n\n\n\n\n\n\n\n\n1947\n4\n1\nFeb\n\n\n\n\n\n\n\n\n1948\n20\n0\n.\n\n\n\n\n\n\n\n\n1949\n32\n0\n.\n\n\n\n\n\n\n\n\n1950\n35\n1\nMarch\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonths since last event\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nmonth\nevent\nevent month\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1945\n1\n0\n\n\n\n\n\n\n\n\n\n1945\n2\n0\n\n\n\n\n\n\n\n\n\n1945\n3\n0\n\n\n\n\n\n\n\n\n\n⋮\n⋮\n⋮\n\n\n\n\n\n\n\n\n\n1946\n1\n0\n\n\n\n\n\n\n\n\n\n1946\n2\n0\n\n\n\n\n\n\n\n\n\n1946\n3\n1\nMarch\n\n\n\n\n\n\n\n\n1946\n4\n0\n\n\n\n\n\n\n\n\n\n1946\n5\n0\n\n\n\n\n\n\n\n\n\n1946\n6\n0\n\n\n\n\n\n\n\n\n\n1946\n7\n0\n\n\n\n\n\n\n\n\n\n1946\n8\n0\n\n\n\n\n\n\n\n\n\n1946\n9\n1\nSept\n\n\n\n\n\n\n\n\n1946\n10\n1\nOct\n\n\n\n\n\n\n\n\n1946\n11\n0\n\n\n\n\n\n\n\n\n\n1946\n12\n0\n\n\n\n\n\n\n\n\n\n1947\n1\n0\n\n\n\n\n\n\n\n\n\n⋮\n⋮\n⋮\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\nPr(y_i=j) = \\frac{e^{-\\lambda}  \\lambda^{y_i}}{y_i !} \\nonumber\n\\]\nAgain, emphasizing two assumptions:\n\nmean-variance equality; note there is only one parameter.\nevent independence within spell, \\(d\\).\n\n\n\n\n\n\ncode\n#plot 4 poisson distributions at lambda values of .1, 1, 5, 10\n\nx &lt;- 0:20\ny1 &lt;- dpois(x, lambda = .1)\ny2 &lt;- dpois(x, lambda = 1)\ny3 &lt;- dpois(x, lambda = 5)\ny4 &lt;- dpois(x, lambda = 10)\n\ndf &lt;- data.frame(x = rep(x, 4), y = c(y1, y2, y3, y4), lambda = rep(c(.1, 1, 5, 10), each = 21))\n\nggplot(df, aes(x = x, y = y, color = factor(lambda))) +\n  geom_line() +\n  labs(title = \"Poisson Distributions\", x = \"Events\", y = \"Probability\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"red\", \"blue\", \"green\", \"purple\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\nSince \\(\\lambda\\) must be nonnegative, the exponential function is an obvious candidate as the link function.\n\\[\n\\lambda = E[Y] = e^{x_{i} \\beta_k} \\nonumber\n\\\\]\nThe exponential link is also the reason the poisson model is also known as the exponential poisson regression.\n\n\n\nThe Poisson regression model is easy to estimate in ML.\n\\[\n\\ln L (\\lambda|Y) = \\ln\\prod_{i=1}^{n} f(Y|\\lambda) \\nonumber \\\\\n=\\sum_{i=1}^{n} \\ln \\left(\\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!}\\right) \\nonumber \\\\\n= -n\\lambda+\\sum_{i=1}^{n}y_i\\ln\\lambda - \\sum_{i=1}^{n}ln(y_i!)\\nonumber\n\\]\n\n\n\nAll the usual techniques apply (at-mean; average; simulated):\n\n\\(exp(\\beta_k)\\) is the incident rate ratio (IRR) - similar to odds ratios in the logistic model.\n\\(E[Y]=\\lambda = e^{x_{i} \\beta_k}\\) - this is the expected number of events per period, conditional on \\(x \\beta\\).\n$Pr(y_i=j) = $ - the PDF (like any) describes the probabilities of all possible values of \\(y\\). So we could examine the probability \\(y=2\\); or \\(Pr(y&lt;3)\\), etc.\n\\(Pr(y_i &lt; j) = \\sum_{y=0}^{j}[ \\frac{e^{-\\lambda}  \\lambda^{y_i}}{y_i !}]\\) - since this is a discrete distribution, the CDF is the sum of the PDF up to the value of interest."
  },
  {
    "objectID": "countA24.html#things-that-are-not-counts",
    "href": "countA24.html#things-that-are-not-counts",
    "title": "Count Models I",
    "section": "",
    "text": "Generally, if \\(d\\) is known and/or finite, the event variable in question should not be modeled as a count variable. If we know the number of voters, and the number of registered voters, we know the binomial “successes” and the number of trials. Model as a proportion, or as a logit where the unit of analysis is the registered voter, and the \\(y\\) variable indicates vote or not."
  },
  {
    "objectID": "countA24.html#things-that-might-be-counts",
    "href": "countA24.html#things-that-might-be-counts",
    "title": "Count Models I",
    "section": "",
    "text": "If \\(d\\) is unknown or infinite or infinitely divisible (like time, which is a very common \\(d\\)), events per \\(d\\) may well be counts. Such a \\(d\\) cannot be the denominator (trials) to produce a proportion, and cannot readily become the unit of analysis."
  },
  {
    "objectID": "countA24.html#toward-the-poisson",
    "href": "countA24.html#toward-the-poisson",
    "title": "Count Models I",
    "section": "",
    "text": "Suppose events occur at some rate (so \\(j\\) events per \\(d\\)); label that rate \\(\\lambda\\); and the exposure, \\(d\\), is some period of time with length \\(d\\).\nThe probability of an event, \\(j\\) occurring during \\(d\\) is \\(\\lambda*d\\); the probability of no event during that period is \\((1-\\lambda)*d\\).\nAssume the probability of one event occurring is entirely independent of the probability that any other event occurs in the same period, \\(d\\).\nNow, we want to know the probability that \\(y=j\\) events occurring in any period \\(t\\) of length \\(d\\), where \\(j= 0 , 1, 2, 3 \\ldots \\infty\\).\n\n\\[\nPr(y_i=j) = \\frac{e^{-\\lambda d} * (\\lambda d)^{y_i}}{y_i !} \\nonumber\n\\]"
  },
  {
    "objectID": "countA24.html#poisson-pdf",
    "href": "countA24.html#poisson-pdf",
    "title": "Count Models I",
    "section": "",
    "text": "This is the PDF; the CDF of a discrete distribution is the sum of the PDF.\n\\[\nPr(y_i=j) = \\frac{e^{-\\lambda d} * (\\lambda d)^{y_i}}{y_i !} \\nonumber\n\\]\nIf \\(d\\) is constant, normalized to one, it drops out:\n\\[\nPr(y_i=j) = \\frac{e^{-\\lambda}  \\lambda^{y_i}}{y_i !} \\nonumber\n\\]\nand we can interpret \\(\\lambda\\) as \\(E[Y]\\), the expected number of events per time interval."
  },
  {
    "objectID": "countA24.html#poisson-qis",
    "href": "countA24.html#poisson-qis",
    "title": "Count Models I",
    "section": "",
    "text": "This should give you the idea that quantities are very easy in the Poisson regression, and that there are three quantities we can generate:\n\nthe probability \\(y\\) takes on any particular number of events : \\[Pr(y_i=j) = \\frac{e^{-\\lambda}  \\lambda^{y_i}}{y_i !}\\].\nthe expected number of events given \\(X\\) : \\(\\hat{\\lambda}\\) which we parameterize as \\[\\hat{\\lambda}=exp(x\\widehat{\\beta})\\].\nthe cumulative probability of \\(j\\) events, so say \\[Pr(y \\leq 3) = \\sum\\limits_{j&lt;4}\\frac{e^{-\\lambda}  \\lambda^{y_i}}{y_i !}\\]."
  },
  {
    "objectID": "countA24.html#assumptions",
    "href": "countA24.html#assumptions",
    "title": "Count Models I",
    "section": "",
    "text": "For the Poisson regression, we assume:\n\nthe probabilities of events in a period are independent of one another.\nthe expected count is equal to the variance; \\(E[Y]= var[Y]\\) - more specifically, \\(E[Y|X]= var[Y|X]\\)\nthe probability of an event, \\(\\lambda\\), in any given period, \\(d\\), is constant across \\(d\\) (i.e. constant rate of arrival)"
  },
  {
    "objectID": "countA24.html#poisson-distribution",
    "href": "countA24.html#poisson-distribution",
    "title": "Count Models I",
    "section": "",
    "text": "Credited to Simeon Denis Poisson in his study of jury decisions (wrongful convictions, in part), but first put to use by Ladislaus von Bortkiewicz in a study showing the number of Prussian soldiers kicked to death by mules or horses followed a Poisson distribution.\n\n\ncode\nlibrary(vcd)\ndata(\"VonBort\")\nhorse&lt;-data.frame(VonBort)\nhorsesum &lt;- horse %&gt;% group_by(deaths) %&gt;% count(deaths) %&gt;% ungroup()\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\nhchart(horsesum, \"column\", hcaes(x=deaths, y=n)) %&gt;% \n  hc_title(text=\"Deaths by Horse or Mule Kick\") %&gt;% \n  hc_subtitle(text=\"Prussian Army, 19th Century\") %&gt;% \n  hc_yAxis(title=\"Frequency\") %&gt;% \n  hc_colors(\"#005A43\") %&gt;%\n  hc_xAxis(title=\"Deaths\") %&gt;% \n  hc_legend(enabled=FALSE) %&gt;% \n  hc_tooltip(pointFormat = \"Deaths: {point.x}&lt;br&gt;Frequency: {point.y}\") %&gt;% \n  hc_credits(enabled=TRUE, text=\"Source: von Bortkiewicz, L. (1898). Das Gesetz der Kleinen Zahlen. Leipzig: Teubner.\")"
  },
  {
    "objectID": "countA24.html#rates-of-occurence",
    "href": "countA24.html#rates-of-occurence",
    "title": "Count Models I",
    "section": "",
    "text": "Another example - we might observe the number of individuals who catch the flu during a winter out of the number of people in their respective families. Note three things:\n\n\\(j\\) occurs over \\(d\\); \\(d\\) is exposure which is the family size.\n\\(d\\) is not constant; some families are small, some are large.\nevents are not independent; the probability of one case of flu is related to the probability of another case of flu in the same exposure unit, \\(d\\).\n\nThe Poisson can accommodate the first two of these, but not the third. Actually, you should note that if family size is known, this is probably not a variable we should model as a count; we should compute the proportion instead.\n\n\n\n\n\n\nAn aside on events over time\n\n\n\nThe fact that \\(d\\) is usually some period of time, and that \\(\\lambda\\) is a rate of occurrence over time is interesting.\n\nthese models observe or count events per spell.\nwe can imagine dividing a spell, \\(d\\) into \\(j+1\\) segments of unspecified length; call these units \\(d_{t,j}\\).\nwe could count the number of time units between \\(j=1, j=2, \\ldots j=j\\), or count the time units within each \\(d_{t,j}\\).\nso instead of counting events, we could count time between events."
  },
  {
    "objectID": "countA24.html#counts-and-hazards",
    "href": "countA24.html#counts-and-hazards",
    "title": "Count Models I",
    "section": "",
    "text": "What sort of model does this \\(y\\) variable imply?\nSuppose data that look like this:\n\n\n\n\nEvents per year\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nevents\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1945\n0\n\n\n\n\n\n\n\n\n\n\n1946\n3\n\n\n\n\n\n\n\n\n\n\n1947\n1\n\n\n\n\n\n\n\n\n\n\n1948\n0\n\n\n\n\n\n\n\n\n\n\n1949\n0\n\n\n\n\n\n\n\n\n\n\n1950\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe same data conceived differently:\n\n\n\n\nMonths since last event\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nmonths\nevent\nevent month\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1945\n12\n0\n.\n\n\n\n\n\n\n\n\n1946\n15\n1\nMarch\n\n\n\n\n\n\n\n\n1946\n6\n1\nSept\n\n\n\n\n\n\n\n\n1946\n1\n1\nOct\n\n\n\n\n\n\n\n\n1947\n4\n1\nFeb\n\n\n\n\n\n\n\n\n1948\n20\n0\n.\n\n\n\n\n\n\n\n\n1949\n32\n0\n.\n\n\n\n\n\n\n\n\n1950\n35\n1\nMarch\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonths since last event\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nmonth\nevent\nevent month\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1945\n1\n0\n\n\n\n\n\n\n\n\n\n1945\n2\n0\n\n\n\n\n\n\n\n\n\n1945\n3\n0\n\n\n\n\n\n\n\n\n\n⋮\n⋮\n⋮\n\n\n\n\n\n\n\n\n\n1946\n1\n0\n\n\n\n\n\n\n\n\n\n1946\n2\n0\n\n\n\n\n\n\n\n\n\n1946\n3\n1\nMarch\n\n\n\n\n\n\n\n\n1946\n4\n0\n\n\n\n\n\n\n\n\n\n1946\n5\n0\n\n\n\n\n\n\n\n\n\n1946\n6\n0\n\n\n\n\n\n\n\n\n\n1946\n7\n0\n\n\n\n\n\n\n\n\n\n1946\n8\n0\n\n\n\n\n\n\n\n\n\n1946\n9\n1\nSept\n\n\n\n\n\n\n\n\n1946\n10\n1\nOct\n\n\n\n\n\n\n\n\n1946\n11\n0\n\n\n\n\n\n\n\n\n\n1946\n12\n0\n\n\n\n\n\n\n\n\n\n1947\n1\n0\n\n\n\n\n\n\n\n\n\n⋮\n⋮\n⋮"
  },
  {
    "objectID": "countA24.html#back-to-the-poisson",
    "href": "countA24.html#back-to-the-poisson",
    "title": "Count Models I",
    "section": "",
    "text": "\\[\nPr(y_i=j) = \\frac{e^{-\\lambda}  \\lambda^{y_i}}{y_i !} \\nonumber\n\\]\nAgain, emphasizing two assumptions:\n\nmean-variance equality; note there is only one parameter.\nevent independence within spell, \\(d\\)."
  },
  {
    "objectID": "countA24.html#poisson-distributions",
    "href": "countA24.html#poisson-distributions",
    "title": "Count Models I",
    "section": "",
    "text": "code\n#plot 4 poisson distributions at lambda values of .1, 1, 5, 10\n\nx &lt;- 0:20\ny1 &lt;- dpois(x, lambda = .1)\ny2 &lt;- dpois(x, lambda = 1)\ny3 &lt;- dpois(x, lambda = 5)\ny4 &lt;- dpois(x, lambda = 10)\n\ndf &lt;- data.frame(x = rep(x, 4), y = c(y1, y2, y3, y4), lambda = rep(c(.1, 1, 5, 10), each = 21))\n\nggplot(df, aes(x = x, y = y, color = factor(lambda))) +\n  geom_line() +\n  labs(title = \"Poisson Distributions\", x = \"Events\", y = \"Probability\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"red\", \"blue\", \"green\", \"purple\")) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "countA24.html#parameterizing-lambda",
    "href": "countA24.html#parameterizing-lambda",
    "title": "Count Models I",
    "section": "",
    "text": "Since \\(\\lambda\\) must be nonnegative, the exponential function is an obvious candidate as the link function.\n\\[\n\\lambda = E[Y] = e^{x_{i} \\beta_k} \\nonumber\n\\\\]\nThe exponential link is also the reason the poisson model is also known as the exponential poisson regression."
  },
  {
    "objectID": "countA24.html#estimation",
    "href": "countA24.html#estimation",
    "title": "Count Models I",
    "section": "",
    "text": "The Poisson regression model is easy to estimate in ML.\n\\[\n\\ln L (\\lambda|Y) = \\ln\\prod_{i=1}^{n} f(Y|\\lambda) \\nonumber \\\\\n=\\sum_{i=1}^{n} \\ln \\left(\\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!}\\right) \\nonumber \\\\\n= -n\\lambda+\\sum_{i=1}^{n}y_i\\ln\\lambda - \\sum_{i=1}^{n}ln(y_i!)\\nonumber\n\\]"
  },
  {
    "objectID": "countA24.html#quantities-of-interest",
    "href": "countA24.html#quantities-of-interest",
    "title": "Count Models I",
    "section": "",
    "text": "All the usual techniques apply (at-mean; average; simulated):\n\n\\(exp(\\beta_k)\\) is the incident rate ratio (IRR) - similar to odds ratios in the logistic model.\n\\(E[Y]=\\lambda = e^{x_{i} \\beta_k}\\) - this is the expected number of events per period, conditional on \\(x \\beta\\).\n$Pr(y_i=j) = $ - the PDF (like any) describes the probabilities of all possible values of \\(y\\). So we could examine the probability \\(y=2\\); or \\(Pr(y&lt;3)\\), etc.\n\\(Pr(y_i &lt; j) = \\sum_{y=0}^{j}[ \\frac{e^{-\\lambda}  \\lambda^{y_i}}{y_i !}]\\) - since this is a discrete distribution, the CDF is the sum of the PDF up to the value of interest."
  },
  {
    "objectID": "countA24.html#what-causes-extra-poissonness",
    "href": "countA24.html#what-causes-extra-poissonness",
    "title": "Count Models I",
    "section": "What causes extra poissonness?",
    "text": "What causes extra poissonness?\nExtra poissonness is that there is excess variance (that the ratio of mean to variance is less than one). What causes it?\nInteresting processes:\n\nover dispersion (perhaps due to heterogeneous spells).\npositive contagion such that one event increases the chances of another event in the same spell.\nheterogeneous events - some events are “bigger” than others, e.g. militarized disputes.\ndual processes producing spells with zero events and those with more than zero events.\n\n\nPositive Contagion\nImagine we were modeling the number of family members who catch the flu each winter. It’s not hard to see that if one person catches the flu, the probability someone else catches the flu increases; thus, the probabilities of events are not independent of one another and there is a (literal) contagion across events. If events are not independent, then it is very likely that the variance in \\(Y\\) will be quite different from the mean, and violating the mean-variance equality assumption has bad effects on inference.\nHere are (fake) data on flu cases per household:\n\n\\(\\left(\n\\begin{array}{cccccccccccccc}\n0 &0 &0 &5 &  0 & 0 & 6 & 0 & 3 & 0 & 2 & 2 & 0 \\\\\n\\end{array} \\right)\\)\n\nThe observed mean is 1.39 and the variance is 4.42, obviously suggesting that the mean-variance equality requirement is not met. Note this is different from the conditional expectations of the mean and variance and thus is not a formal test of the mean-variance equality restriction - for that, see C. A. Cameron and Trivedi (1990). My quick test in this nonparametric setting, however, suggests these data suffer positive contagion which manifests as overdispersion; the occurrence of one event increases the probability of more events.\n\n\n\n\n\n\nNote\n\n\n\nBad news, friends\nIf we model these data using a Poisson model because the Poisson restricts the mean to be equal to the variance. Think about the practical result - the variance will be restricted to be smaller than it actually is, so it will be underestimated. If we underestimate the variance, the estimated standard errors will be smaller than they really should be, so we will conclude coefficients are statistically significant when they really are not - this is bad and it’s Type I error time.\n\n\n\n\nNegative contagion\nWhat if, instead of modeling the number of family members with the flu, we are modeling the number of high school students a teacher catches cheating during an exam. When the teacher nails the first student, the others are going to become much more cautious; this means that fewer may actually cheat and the ones who do cheat will be very careful. Thus, catching one reduces the probability of catching a second student. This is called negative contagion; it manifests as underdispersion.\nSo what are the consequences of negative contagion? If we estimate the Poisson model and thus constrain the variance to be equal to the mean and therefore larger than it actually is, we overestimate the variance and thus overestimate our standard errors. As a result, we’re likely to make Type II errors, failing to find relationships that actually exist in the data.\nIn reality, negative contagion or underdispersion is relatively rare in political science (and social science) data. Most econometric work develops models for overdispersion (like the negative binomial model) - models like the continuous parameter binomial regression (and others) will measure underdispersion."
  },
  {
    "objectID": "countA24.html#alpha-indicates-poor-model-specification",
    "href": "countA24.html#alpha-indicates-poor-model-specification",
    "title": "Count Models I",
    "section": "\\(\\alpha\\) indicates poor model specification?",
    "text": "\\(\\alpha\\) indicates poor model specification?\nIf the overdispersion arises from unobserved heterogeneity, then the problem really could be one of model specification; if we could specify the sources of that heterogeneity in the data, we could include those variables in the model and account explicitly for the factors leading to overdispersion.\nThus, as we change the specification of the model, evidence of overdispersion disappears. So, \\(\\alpha \\neq 0|X_{c}\\) but \\(\\alpha = 0|X_{f}\\).\nEmphasizing …\nThe ancillary parameter \\(\\alpha\\), like many such parameters, soaks up heterogeneity we are failing to measure adequately in the model. Such parameters are almost always best thought of as indicators of poor model specification.\nIf \\(\\alpha\\) is different from zero, we should be asking what variables (excluded) would explain the heterogeneity in the data - the heterogeneity in these models will be due to excess variance, most probably given by outlying values of \\(y\\). Because the left limit is zero, outliers are higher counts.\nSo what variables would predict the higher (extra poisson) counts?\n\n\n\n\n\n\nAdvice\n\n\n\nAlways estimate the NB first. If \\(\\alpha=0\\), the model is Poisson, but inefficient because you estimated an additional, unnecessary parameter. So now, estimate and report the Poisson. Say clearly in the text that you report the Poisson because the NB showed no evidence of overdispersion."
  },
  {
    "objectID": "countA24.html#nb-example",
    "href": "countA24.html#nb-example",
    "title": "Count Models I",
    "section": "NB Example",
    "text": "NB Example\n\n\n\n\n\n\nAdvice\n\n\n\nAlways estimate the NB first. If \\(\\alpha=0\\), the model is Poisson, but inefficient because you estimated an additional, unnecessary parameter. So now, estimate and report the Poisson. Say clearly in the text that you report the Poisson because the NB showed no evidence of overdispersion."
  },
  {
    "objectID": "countA24.html#a-test-for-overdispersion",
    "href": "countA24.html#a-test-for-overdispersion",
    "title": "Count Models I",
    "section": "A test for overdispersion",
    "text": "A test for overdispersion\nC. A. Cameron and Trivedi (1990) propose a regression based test for overdispersion. It’s computed as follows.\n\ncompute \\(\\lambda\\) in-sample\ngenerate estimated variance, u (see code below)\nregress u on \\(\\lambda\\)\nthe null hypothesis is equidispersion; rejecting the null is evidence of overdispersion in \\(y|X\\).\n\nWhile this process is instructive, really there’s no need to do this because we can estimate the negative binomial to measure \\(\\alpha\\).\n\n\ncode\n# Predict lambda (expected counts) in-sample\nrain$lambda &lt;- predict(poisson, type = \"response\", newdata=rain)\n\n# Generate estimated variance, u\nrain$u &lt;- ((rain$violent_events_no_onset - rain$lambda)^2 - rain$violent_events_no_onset) /\n          (rain$lambda * sqrt(2))\n\n# Regress u on lambda\noverdispersion_test &lt;- lm(u ~ lambda, data = rain)\n\n# Print the summary of the regression\nstargazer(overdispersion_test, type = \"html\")\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nu\n\n\n\n\n\n\n\n\nlambda\n\n\n0.173***\n\n\n\n\n\n\n(0.030)\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n0.964***\n\n\n\n\n\n\n(0.209)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n765\n\n\n\n\nR2\n\n\n0.042\n\n\n\n\nAdjusted R2\n\n\n0.041\n\n\n\n\nResidual Std. Error\n\n\n5.233 (df = 763)\n\n\n\n\nF Statistic\n\n\n33.676*** (df = 1; 763)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\nThe regression indicates we can reject the null hypothesis of equidispersion, so we have some evidence of overdispersion in the model."
  },
  {
    "objectID": "countA24.html#negative-binomial-example",
    "href": "countA24.html#negative-binomial-example",
    "title": "Count Models I",
    "section": "Negative Binomial Example",
    "text": "Negative Binomial Example\nHere’s a negative binomial regression, specified exactly as the Poisson model above.\n\n\ncode\nrain &lt;- read_dta(\"/Users/dave/Documents/teaching/606J-mle/2022/slides/L10_count1/code/H_S_JPR_491_Replication_Revised.dta\")\n\n#filter to remove ccode 520 (Somalia)\nrain &lt;- filter(rain, ccode != 520)\n\n# Estimate the model\n\nnb &lt;- glm.nb(violent_events_no_onset ~violent_events_no_onset_l+ GPCP_precip_mm_deviation_sd + GPCP_precip_mm_deviation_sd_sq + GPCP_precip_mm_deviation_sd_l + GPCP_precip_mm_deviation_sd_l_sq + polity2 + polity2_sq + log_pop_pwt + log_pop_pwt_fd + log_rgdpch_pwt + grgdpch_pwt +incidence + ttrend , data = rain)\n\n# 1/nb$theta\n\n# table using stargazer\n\nstargazer(nb, type = \"html\")\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nviolent_events_no_onset\n\n\n\n\n\n\n\n\nviolent_events_no_onset_l\n\n\n0.079***\n\n\n\n\n\n\n(0.006)\n\n\n\n\n\n\n\n\n\n\nGPCP_precip_mm_deviation_sd\n\n\n0.012\n\n\n\n\n\n\n(0.044)\n\n\n\n\n\n\n\n\n\n\nGPCP_precip_mm_deviation_sd_sq\n\n\n0.085***\n\n\n\n\n\n\n(0.028)\n\n\n\n\n\n\n\n\n\n\nGPCP_precip_mm_deviation_sd_l\n\n\n0.043\n\n\n\n\n\n\n(0.047)\n\n\n\n\n\n\n\n\n\n\nGPCP_precip_mm_deviation_sd_l_sq\n\n\n0.022\n\n\n\n\n\n\n(0.034)\n\n\n\n\n\n\n\n\n\n\npolity2\n\n\n-0.003\n\n\n\n\n\n\n(0.010)\n\n\n\n\n\n\n\n\n\n\npolity2_sq\n\n\n-0.005**\n\n\n\n\n\n\n(0.002)\n\n\n\n\n\n\n\n\n\n\nlog_pop_pwt\n\n\n0.444***\n\n\n\n\n\n\n(0.045)\n\n\n\n\n\n\n\n\n\n\nlog_pop_pwt_fd\n\n\n-4.617*\n\n\n\n\n\n\n(2.532)\n\n\n\n\n\n\n\n\n\n\nlog_rgdpch_pwt\n\n\n-0.106*\n\n\n\n\n\n\n(0.057)\n\n\n\n\n\n\n\n\n\n\ngrgdpch_pwt\n\n\n-0.016***\n\n\n\n\n\n\n(0.006)\n\n\n\n\n\n\n\n\n\n\nincidence\n\n\n-0.039\n\n\n\n\n\n\n(0.108)\n\n\n\n\n\n\n\n\n\n\nttrend\n\n\n0.007\n\n\n\n\n\n\n(0.010)\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-2.851***\n\n\n\n\n\n\n(0.613)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n765\n\n\n\n\nLog Likelihood\n\n\n-1,374.122\n\n\n\n\ntheta\n\n\n1.203*** (0.119)\n\n\n\n\nAkaike Inf. Crit.\n\n\n2,776.244\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\nIn the negative binomial model, \\(\\theta\\) is the inverse of the variance parameter, \\(\\alpha\\):\n\\[ \\alpha = \\frac{1}{\\theta} \\nonumber \\]\nso for the model above,\n\\[ \\alpha = \\frac{1}{1.203} = .831 \\nonumber \\]\nNote that \\(\\theta\\) is statistically different from zero, so we can reject the null hypothesis of equidispersion. Put differently, the disperson parameter is not zero, indicating there is overdispersion."
  },
  {
    "objectID": "countA24.html#negative-binomial-predictions",
    "href": "countA24.html#negative-binomial-predictions",
    "title": "Count Models I",
    "section": "Negative Binomial Predictions",
    "text": "Negative Binomial Predictions\n\n\ncode\n# predictions using library(averagemarginaleffects)\n\nrain &lt;- read_dta(\"/Users/dave/Documents/teaching/606J-mle/2022/slides/L10_count1/code/H_S_JPR_491_Replication_Revised.dta\")\n\n#filter to remove ccode 520 (Somalia)\nrain &lt;- filter(rain, ccode != 520)\n\n# Estimate the model\n\nnb &lt;- glm.nb(violent_events_no_onset ~violent_events_no_onset_l+ GPCP_precip_mm_deviation_sd + GPCP_precip_mm_deviation_sd_sq + GPCP_precip_mm_deviation_sd_l + GPCP_precip_mm_deviation_sd_l_sq + polity2 + polity2_sq + log_pop_pwt + log_pop_pwt_fd + log_rgdpch_pwt + grgdpch_pwt +incidence + ttrend , data = rain)\n\n# #identify the estimation sample, copy df\n# rain$used &lt;- TRUE\n# rain$used[na.action(nb)] &lt;- FALSE\n# df &lt;- rain %&gt;%  filter(used==\"TRUE\")\n# \n# avgeffects &lt;-compute_average_effects(nb, \n#   data=df, \n#   x_variable = \"GPCP_precip_mm_deviation_sd\",\n#   polynomial = list(vars = c(\"GPCP_precip_mm_deviation_sd\",        \"GPCP_precip_mm_deviation_sd_sq\"), order = 2),\n#   num_points = 60, pred_type = \"link\")\n# \n# avgeffects$ub &lt;- exp(avgeffects$median_prediction + 1.96 * avgeffects$median_se)\n# avgeffects$lb &lt;- exp(avgeffects$median_prediction - 1.96 * avgeffects$median_se)\n# avgeffects$ey &lt;- exp(avgeffects$median_prediction)\n# \n# # plot\n# \n# ggplot(avgeffects, aes(x = x)) +\n#   geom_ribbon(aes(ymin = lb, ymax = ub), alpha = 0.2) +\n#   geom_line(aes(y = ey)) +\n#   labs(x = \"Deviation in rainfall\", y = \"Expected Count\")\n\n\n#identify the estimation sample, copy df\nrain$used &lt;- TRUE\nrain$used[na.action(nb)] &lt;- FALSE\ndf &lt;- rain %&gt;%  filter(used==\"TRUE\")\n\n# loop over values of rain\n\ntemp&lt;-NULL\nxb&lt;-0\nse&lt;-0\ni=1\nfor (r in seq(-3, 3, .1)) {\n  df$GPCP_precip_mm_deviation_sd &lt;- r\n  df$GPCP_precip_mm_deviation_sd_sq &lt;- r^2\n  temp &lt;- data.frame(predict(nb, newdata = df, type = \"link\", se.fit = TRUE))\n  xb[i] &lt;- median(temp$fit, na.rm = TRUE)\n  se[i] &lt;- median(temp$se.fit, na.rm = TRUE)\n  i=i+1\n}\n\npreds &lt;- data.frame(x = seq(-3, 3, .1), xb = xb, se = se)\npreds$ey &lt;- exp(preds$xb)\npreds$ub &lt;- exp(preds$xb + 1.96 * preds$se)\npreds$lb &lt;- exp(preds$xb - 1.96 * preds$se)\n\n# plot\n\nggplot(preds, aes(x = x)) +\n  geom_ribbon(aes(ymin = lb, ymax = ub), alpha = 0.2) +\n  geom_line(aes(y = ey)) +\n  labs(x = \"Deviation in rainfall\", y = \"Expected Violent Events\")"
  },
  {
    "objectID": "countB24.html",
    "href": "countB24.html",
    "title": "Count Models I",
    "section": "",
    "text": "Not enough?"
  },
  {
    "objectID": "countB24.html#poisson-processes",
    "href": "countB24.html#poisson-processes",
    "title": "Count Models I",
    "section": "Poisson Processes",
    "text": "Poisson Processes\n\nconstant rate of arrival of events, \\(\\lambda\\).\n\\(\\lambda\\) is the expected number of events per period, \\(d\\).\nevent probability per period is \\(\\lambda d\\)\nevents arrive independently of one another in each period.\nconstant rate of arrival and event independence imply mean-variance equality."
  },
  {
    "objectID": "countB24.html#extra-poisson-processes",
    "href": "countB24.html#extra-poisson-processes",
    "title": "Count Models II",
    "section": "Extra-poisson processes",
    "text": "Extra-poisson processes\n\nThe occurrence of one event increases/decreases the chances of a second event.\nThis might occur due to positive/negative contagion.\nThe result is observations of \\(y\\) with larger/smaller observed numbers of events.\nThis increases/decreases the mean relative to the variance; this is over/under dispersion.\nThe negative binomial measures the ``extra’’ variance, \\(\\alpha\\). If \\(\\alpha=0\\), the variance is \\(exp(0)*\\lambda=\\lambda\\), so the variance is Poisson."
  },
  {
    "objectID": "countB24.html#trials",
    "href": "countB24.html#trials",
    "title": "Count Models II",
    "section": "Trials",
    "text": "Trials\nIf we have events coded as a binary variable indicating successes and failures, and we know the number of trials, the data are binomial, and probably should {not} be aggregated and treated as Poisson.\nRelatedly, we often assume the number of trials, \\(M\\) is infinite - i.e., an infinite number of events {could} occur in each period. But sometimes, we know \\(M\\), and that it is {not} infinite, and that no number of events greater than \\(M\\) could take place in period \\(d\\). Think of this known number of trials as {exposure} or the maximum number of times an event could possibly occur in a period.\nIf exposure, is neither infinite nor constant, it’s important to control for it in the model:\n\\[E[Y] = exp(x\\beta + \\ln(exposure))\\]\nwhere the coefficient on exposure is fixed at 1. Note this reduces to \\(\\lambda*M\\) or the rate of event arrival given the number of trials. The rate is now proportional to trials or exposure.\\ ~\\\nFor instance, suppose we model the number of deaths by terror attack per attack. Exposure for attacks in crowded markets would be different from exposure for attacks in relatively deserted roadsides. Alternatively, exposure for number of flu cases per household would be limited by the size of the household."
  },
  {
    "objectID": "countB24.html#incidence-rate-ratios",
    "href": "countB24.html#incidence-rate-ratios",
    "title": "Count Models II",
    "section": "Incidence Rate Ratios",
    "text": "Incidence Rate Ratios\nNote that the estimate \\(\\hat{\\lambda}\\) is a rate. That rate measured at two values of a variable, say \\(X=0, X=1\\) can be written as a ratio (the Incidence Rate Ratio, IRR):\n\\[\\frac{\\hat{\\lambda}|X=1}{\\hat{\\lambda}|X=0} = \\frac{x\\hat{\\beta}+(X=1)*\\hat{\\beta_x}}{x\\hat{\\beta}+(X=0)*\\hat{\\beta_x}} = exp(\\hat{\\beta_x})\\]\nwhich we can interpret as the change in the incidence rate, \\(\\lambda\\) given a change in \\(X\\) from zero to one. This is an easy interpretation, useful to report for count models."
  },
  {
    "objectID": "countB24.html#quantities-of-interest",
    "href": "countB24.html#quantities-of-interest",
    "title": "Count Models II",
    "section": "Quantities of Interest",
    "text": "Quantities of Interest\n\nthe rate of event occurrence, \\(\\lambda = exp(x\\beta)\\). -\\(Pr(Y=y) = \\frac{exp(-\\lambda) * \\lambda^y_i}{y_i !}\\)\nconfidence bounds given by \\(\\lambda \\pm 1.96*s.e.\\) or by something similar for the Poisson probability."
  },
  {
    "objectID": "countB24.html#unobserved-heterogeneity",
    "href": "countB24.html#unobserved-heterogeneity",
    "title": "Count Models II",
    "section": "Unobserved Heterogeneity",
    "text": "Unobserved Heterogeneity\nA problem in any model is unobserved heterogeneity. In nonlinear models, the problem is acute because misspecification may have to do with\n\nmissing variables.\noutliers on \\(y\\).\nfunctional form of \\(x\\) or \\(\\beta\\).\nassumed homogeneity in observations on \\(y\\).\nassumed homogeneity in the rate of change across \\(y\\).\nassumed homogeneity in DGP with respect to \\(y\\).\n\nEvent count and hazard models provide a variety of mechanisms for thinking about these problems, for accounting for them mechanically, and for modeling heterogeneity."
  },
  {
    "objectID": "countB24.html#heterogeneity-in-event-data",
    "href": "countB24.html#heterogeneity-in-event-data",
    "title": "Count Models II",
    "section": "Heterogeneity in Event Data",
    "text": "Heterogeneity in Event Data\nLet’s talk about things we might be concerned with in events data:\n\nfrequency distribution of events.\nsimilarity of events within period.\nsimilarity of periods, not just in length but in exposure for events.\ndifferences in reasons for non occurrence.\ncorrelation of events within period."
  },
  {
    "objectID": "countB24.html#heterogeneity-in-failuresurvival-time-data",
    "href": "countB24.html#heterogeneity-in-failuresurvival-time-data",
    "title": "Count Models II",
    "section": "Heterogeneity in Failure/Survival Time Data",
    "text": "Heterogeneity in Failure/Survival Time Data\nAnd compare those things to what we might be concerned about in time to failure (hazard) data:\n\ndistribution of time periods between events/failures - label these ``failure times.’’\nsimilarity of failure times.\nsimilarity of failure events.\ncensored cases; why some cases never have failure events.\ncorrelation of failure probabilities within period."
  },
  {
    "objectID": "countB24.html#heterogeneity",
    "href": "countB24.html#heterogeneity",
    "title": "Count Models II",
    "section": "Heterogeneity",
    "text": "Heterogeneity\nNotice in both lists that the problems all lie in heterogeneity either in events, periods between events, or latent probabilities of events."
  },
  {
    "objectID": "countB24.html#different-dgps-indicate-different-mechanisms",
    "href": "countB24.html#different-dgps-indicate-different-mechanisms",
    "title": "Count Models I",
    "section": "Different DGPs indicate different mechanisms",
    "text": "Different DGPs indicate different mechanisms\n\nDifferent distributions implies different data generating processes and that we need to think about models with two parts, potentially two equations.\nThis sort of thinking characterizes event count, hazard, censored, selection, and seemingly unrelated regression models.\nThe focus here is on count models where the observed count is generated by multiple processes."
  },
  {
    "objectID": "countB24.html#motivating-models",
    "href": "countB24.html#motivating-models",
    "title": "Count Models I",
    "section": "Motivating models",
    "text": "Motivating models\nThe formal motivations of these models is straightforward enough. Motivating a mechanism, however, is more complicated and more important. The common empirical indicator of multiple DGPs is excess zeros. But excess zeros does not guarantee two processes are at work, nor that we know what they are even if they do.\nShort version - use these if your theory indicates parallel processes at work, not merely because the data are dominated by zeros."
  },
  {
    "objectID": "countB24.html#mechanics",
    "href": "countB24.html#mechanics",
    "title": "Count Models II",
    "section": "Mechanics",
    "text": "Mechanics\nSuppose that we have count data that contains a large number of zeros.\n\nThe zeros reduce both the mean and the variance.\nThe variance is likely to be larger than the mean due to any positive counts.\nAn overabundance of zeros can produce overdispersion; a large conditional variance relative to the mean.\n\nThis sounds like garden-variety unobserved heterogeneity; we haven’t sufficiently modeled the zeros in the data. While this might be true, another possibility exists."
  },
  {
    "objectID": "countB24.html#split-populations",
    "href": "countB24.html#split-populations",
    "title": "Count Models II",
    "section": "Split Populations",
    "text": "Split Populations\nSuppose that two different processes actually generate the data we observe in our event count variable. One process determines whether or not we will ever have any chance of observing positive (nonzero) numbers of events; that is, it determines \\(Pr(y_i=0)\\).\nThe second process determines how many events we will observe, given that we have any probability of observing nonzero events, \\(Pr(y_i=j)\\).\nThe generality of this allows us to distinguish two basic models. If our story tells us to expect,\n\\[\nPr(y_i&gt;j, \\forall j&gt;0)|Pr(y_i&gt;0)\n\\]\nthen we have something like the hurdle poisson model. On the other hand, if our story says\n\\[\nPr(y_i&gt;j, \\forall j \\geq 0|Pr(y_i&gt;0)\n\\]\nthen we have something like the zero inflated model. I’m going to focus on the zero inflated models.\nLet’s consider two stories to illustrate (the first drawn from Green via Zorn (1998, 372), the other from the IR literature).\nSuppose we want to know how many times individuals have fished in a particular lake. Some individuals will answer that they have never fished in that lake because they’ve simply never gone there, preferring other fishing holes. These individuals are at risk for having fished there, but haven’t for exogenous reasons. Other individuals, however, will respond that they’ve never fished in the lake and probably never will because they don’t fish, don’t own fishing gear, and hate the water. These folks are never at risk of fishing there any positive number of times - they will always receive zeros in the data.\nThe problem is that the data have plenty of zeros (“No, I’ve never fished there”), but the zeros can mean two substantively different things - “I could have, but didn’t” for some reason (I live too far away, etc.) and “I hate fish.” Some process determines whether or not an individual crosses the threshold from “always-zero” into the “possibly, but not inevitably, positive” zone. A second process determines how often an individual actually does fish there, given that they fish at all.\nHere’s a second example. The international conflict literature talks at some length about the “opportunity and willingness” to engage in military conflict and claims these are “jointly necessary conditions” for interstate conflict. Typically, IR scholars are adept at identifying variables that represent different facets of “willingness” to fight, but deal less well with whether or not states have the opportunity to fight. Indeed, there’s an empirical literature on the “opportunity” to fight addressing the issue of “political relevance.”\nBut think about the possibility that two processes generate the data on whether or not or how often states fight one another. One process makes pairs of states either eligible to fight or not; the other process determines how often those states fight, given that they are eligible to do so. Put another way, one process determines whether a dyad has the opportunity to fight, and another process determines the extent to which they are willing to fight and how many disputes the dyad actually has."
  },
  {
    "objectID": "countB24.html#dual-regimes",
    "href": "countB24.html#dual-regimes",
    "title": "Count Models II",
    "section": "Dual Regimes",
    "text": "Dual Regimes\nTwo regimes generate the observed data. Treating those two data generating processes masks substantial heterogeneity, and so misspecifies the model in structural ways. If we fail to acknowledge the two regimes, we treat them as if they’re the same. So in the case of excess zeros, we treat those zeros as if they’re all generated by the same process.\nThe underlying problem in either of these examples and in these dual regime models more generally is that even though we might theorize on the two processes, ultimately we cannot look at the zeros in the data and say, “these zeros occur because these states simply lack the opportunity to fight, while those zeros occur because even though states had the opportunity to fight, they chose not to.”’’” In other words, we cannot observe the exact split in the zeros, between the two regimes or processes.\nIn the count context, we turn to zero inflated count models - these are one type of split population model."
  },
  {
    "objectID": "countB24.html#zero-inflated-event-counts",
    "href": "countB24.html#zero-inflated-event-counts",
    "title": "Count Models I",
    "section": "Zero Inflated Event Counts",
    "text": "Zero Inflated Event Counts\nSuppose we have an event count variable \\(Y\\), but we believe the variable might be produced by two different processes. Then,\n\\[\ny_{i}=p_{i}^{*}y_{i}^{*} \\nonumber\n\\]\nIn this formulation, \\(p_{i}\\) is the probability of any individual makes the transition from the state where he can only experience a zero, to the state where he is eligible to experience a positive number of events; \\(p_{i}^{*}\\) is a dichotmous variable indicating whether or not the individual makes the transition - note that this is very much the intuition underlying binary variable models like probit and logit. \\(y_{i}^{*}\\) is the Poisson distributed event count {for those individuals who make the transition}. Thus, the mean of \\(y_{i}^{*}\\), \\(\\lambda_{i}\\) is conditional on \\(p_{i}\\).\\\nLess formally, consider \\(p_{i}^{*}\\) and \\(Y_{i}^{*}\\) as two dependent variables about which we can hypothesize; the first is dichotomous and we can model it using probit or logit; the second is Poisson and we can use Poisson regression (or even negative binomial regression if there are other sources of overdispersion).\nSuppose we believe that a set of variables, \\(z\\) produce \\(p_{i}^{*}\\); let’s label the probability of being a zero (not making the transition) as \\(\\psi\\) and we can treat this either as probit,\n\\[\n\\psi = \\Phi(z_{i} \\gamma) \\nonumber\n\\]\nor logit\n\\[\n\\psi = \\frac{e^{z_{i}\\gamma}}{1+e^{z_{i}\\gamma}} \\nonumber\n\\]\nNow consider \\(y_{i}^{*}\\) which is Poisson distributed and we believe it is a function of some variables, \\(X\\);\n\\[\nP(Y_{i}=y) = \\frac{e^{(-\\lambda)} \\lambda^{y}}{{y!}} \\nonumber\n\\] and\n\\[\n\\lambda_{i} = e^{X_{i} \\beta_{i}} \\nonumber\n\\]\nNow, let’s combine these\n\\[\nP(y_{i}=0|x_{i})=\\psi_{i}+(1-\\psi_{i})e^{-\\lambda_{i}} \\nonumber \\\\ \\nonumber \\\\\nP(Y=y_{i}|x_{i})=(1-\\psi_{i})\\frac{e^{-\\lambda_{i}}\\lambda_{i}^{y_{i}}}{y_{i}!}~\n\\mbox{for}~ y_{i}&gt;0  \\nonumber\n\\]\nThis is the Zero Inflated Poisson (ZIP) model. The conditional mean of \\(Y\\), \\(\\lambda\\), is adjusted for the zeros in the model - the conditional mean is adjusted by a factor of \\(\\psi\\), the probability of {not} experiencing the transition. We can also extend this logic to the negative binomial (ZINB) model, but I’m going to stick with the Poisson for examples below. Long provides a good treatment of the both of these models and their interpretations."
  },
  {
    "objectID": "countB24.html#examples",
    "href": "countB24.html#examples",
    "title": "Count Models I",
    "section": "Examples",
    "text": "Examples\nIn a widely cited paper, Fordham (1998) argues that US uses of force abroad for political purposes will vary with the political economy concerns of US presidents; those PE concerns vary with party. Republican presidents can better address inflation (a concern of their core constituents) via macroeconomic policy than they can unemployment. Democrats can better deal with unemployment while satisfying their core constituents’ preferences. This means US presidents of different parties should react to economic conditions differently. Democratic presidents should use force less when they face unemployment, more when they face inflation. Republican presidents should use force less when they face inflation, more when they face unemployment.\nThe \\(y\\) variable in this paper is US uses of force per quarter; let’s take a look at the data.\n\n\ncode\nlibrary(haven)\n\n# read in the data\n\nfordham &lt;- read_dta(\"/Users/dave/Documents/teaching/606J-mle/2022/slides/L11_count2/code/fordham98jcr.dta\")\n\nforce &lt;- fordham %&gt;% group_by(force) %&gt;% summarise(n=n())\n\n# highcharter aggregated frequency bars of force variable \n\nhighchart() %&gt;% hc_chart(type = \"column\") %&gt;% hc_title(text = \"Uses of Force\") %&gt;% hc_xAxis(categories = force$force) %&gt;% hc_yAxis(title = list(text = \"Frequency\")) %&gt;% hc_add_series(name = \"Uses of Force\", data = force$n) %&gt;% hc_tooltip(valueDecimals = 0) %&gt;% hc_plotOptions(column = list(colorByPoint = TRUE)) %&gt;% hc_colors(c(\"#005A43\")) \n\n\n\n\n\n\nHere’s a Poisson model similar to what Fordham reports (left column) and a negative binomial model (right column) for comparison.\n\n\ncode\n# Poisson model\n\nmod1 &lt;- glm(force ~ unemp + cpi + war + wecycle + pecycle + demunemp + demcpi, data=fordham, family = poisson(link = \"log\"))\n\n# negative binomial model, MASS library\n\nmod1nb &lt;- glm.nb(force ~ unemp + cpi + war + wecycle + pecycle + demunemp + demcpi, data=fordham)\n\nstargazer(list(mod1, mod1nb), type=\"html\")\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nforce\n\n\n\n\n\n\nPoisson\n\n\nnegative\n\n\n\n\n\n\n\n\n\nbinomial\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nunemp\n\n\n0.150**\n\n\n0.150**\n\n\n\n\n\n\n(0.060)\n\n\n(0.063)\n\n\n\n\n\n\n\n\n\n\n\n\ncpi\n\n\n-0.087**\n\n\n-0.087**\n\n\n\n\n\n\n(0.040)\n\n\n(0.041)\n\n\n\n\n\n\n\n\n\n\n\n\nwar\n\n\n-0.542*\n\n\n-0.542*\n\n\n\n\n\n\n(0.281)\n\n\n(0.290)\n\n\n\n\n\n\n\n\n\n\n\n\nwecycle\n\n\n0.965***\n\n\n0.974***\n\n\n\n\n\n\n(0.307)\n\n\n(0.320)\n\n\n\n\n\n\n\n\n\n\n\n\npecycle\n\n\n0.023\n\n\n0.024\n\n\n\n\n\n\n(0.237)\n\n\n(0.247)\n\n\n\n\n\n\n\n\n\n\n\n\ndemunemp\n\n\n-0.100*\n\n\n-0.101*\n\n\n\n\n\n\n(0.051)\n\n\n(0.053)\n\n\n\n\n\n\n\n\n\n\n\n\ndemcpi\n\n\n0.082\n\n\n0.083\n\n\n\n\n\n\n(0.057)\n\n\n(0.059)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-0.785*\n\n\n-0.787*\n\n\n\n\n\n\n(0.427)\n\n\n(0.446)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n184\n\n\n184\n\n\n\n\nLog Likelihood\n\n\n-202.887\n\n\n-203.646\n\n\n\n\ntheta\n\n\n\n\n10.327 (16.120)\n\n\n\n\nAkaike Inf. Crit.\n\n\n421.773\n\n\n423.292\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\nRecall the dispersion parameter is given by\n\\[\n\\alpha = \\frac{1}{\\hat{\\theta}} \\nonumber\n\\]\nIn this case, 0.097 - very small in magnitude, and not statistically different from zero, indicating the Poisson is the correct model. Despite the abundance of zeros, the count variable does not have much in the way of outliers at its upper bound, constraining the variance.\nHere are predictions from the Poisson regression similar to what he reports:\n\n\ncode\n#predict uses of force over unemp from min to max; all other variables at medians\n\npredictiondatadem &lt;- data.frame(unemp=seq(min(fordham$unemp), max(fordham$unemp), length.out=100), cpi=median(fordham$cpi), war=0, wecycle=0, pecycle=0, demunemp=seq(min(fordham$unemp), max(fordham$unemp), length.out=100), demcpi=median(fordham$demcpi))\n\npredictiondatarep &lt;- data.frame(unemp=seq(min(fordham$unemp), max(fordham$unemp), length.out=100), cpi=median(fordham$cpi), war=0, wecycle=0, pecycle=0, demunemp=0, demcpi=0)\n\npreds &lt;- data.frame(predict(mod1, newdata=predictiondatadem, type=\"response\", se=TRUE))\n\npreds &lt;- data.frame(preds, predict(mod1, newdata=predictiondatarep, type=\"response\", se=TRUE), unemp=predictiondatadem$unemp)\n\npreds &lt;- preds %&gt;% rename(dem=fit, demse=se.fit, rep=fit.1, repse=se.fit.1)\n\nggplot(preds, aes(x=unemp, y=dem, ymin=dem-demse, ymax=dem+demse)) + geom_line(color=\"blue\") + geom_ribbon(aes(ymin=dem-demse, ymax=dem+demse), fill=\"blue\", alpha=.2) + geom_line(aes(x=unemp, y=rep), color=\"red\") + geom_ribbon(aes(ymin=rep-repse, ymax=rep+repse), fill=\"red\", alpha=.2) + labs(title=\"Uses of Force\", x=\"Unemployment\", y=\"Expected Uses of Force\") + theme_minimal() + theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\n\nConsistent with Fordham’s expectations, Democratic presidents are less reactive to unemployment in terms of using force abroad than are Republicans. As unemployment mounts, Republican presidents are more likely to use force abroad, while Democrats may be more likely to use macroeconomic policy because they can do so while satisfying their core constituents."
  },
  {
    "objectID": "countB24.html#opportunities-to-use-force-abroad",
    "href": "countB24.html#opportunities-to-use-force-abroad",
    "title": "Count Models II",
    "section": "Opportunities to use force abroad",
    "text": "Opportunities to use force abroad\nOne of the enduring problems in studies examining this very question is whether the opportunity to use force is always present or if it varies in some way. Some formal work argues that, because potential US targets (scapegoats) can observe when the US president is in need of a rally of some sort, those targets can behave uncontroversially in order to avoid becoming targets. As a result, just when US presidents most need a target, they are least likely to find one - thus, opportunities are not constant and they are not randomly determined.\nIn other words, opportunities to use force may be endogenous to the same variables that determine the likelihood of using force.\nTaking a look at Fordham’s dependent variable, we can find some evidence that a second process (determining opportunities) might be at work:\n\n\ncode\n# kableextra table of force frequency and percentages rounded to 2 decimal places\nlibrary(kableExtra)\n\nfordham %&gt;% group_by(force) %&gt;% summarise(n=n(), percent=round(n()/nrow(fordham)*100, 2)) %&gt;% kable(\"html\") %&gt;% kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), full_width = F)\n\n\n\n\n\n\nforce\nn\npercent\n\n\n\n\n0\n95\n51.63\n\n\n1\n57\n30.98\n\n\n2\n21\n11.41\n\n\n3\n9\n4.89\n\n\n4\n1\n0.54\n\n\n5\n1\n0.54\n\n\n\n\n\n\n\n\nMore than 50% of the observations are zeros - is this because US leaders choose not to use force more than half the time, or because they cannot use force? This is unobserved, but regardless, the overabundance of zeros is likely to pull the mean down relative to the variance and thus produce over-dispersion.\nSuppose we model these two processes even though they are unobserved. According to existing work, some of the same variables that make presidents use force abroad (the economy, etc.) will also influence the availability of opportunities to use force, and thus will influence the likelihood of a zero. Consider the following model:\n\n\ncode\n# zero inflated poisson model using pscl\nlibrary(pscl)\n\nmod2 &lt;- zeroinfl(force ~ unemp + cpi + war + wecycle + pecycle + demunemp + demcpi  | unemp  + war + wecycle + pecycle + demunemp +approval +vietsynd   , data=fordham, dist=\"poisson\", link=\"logit\")\n\nlibrary(modelsummary)\nmodelsummary(mod2)\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  count_(Intercept)\n                  -1.263   \n                \n                \n                                   \n                  (0.466)  \n                \n                \n                  count_unemp      \n                  0.223    \n                \n                \n                                   \n                  (0.066)  \n                \n                \n                  count_cpi        \n                  -0.078   \n                \n                \n                                   \n                  (0.040)  \n                \n                \n                  count_war        \n                  -0.614   \n                \n                \n                                   \n                  (0.285)  \n                \n                \n                  count_wecycle    \n                  0.881    \n                \n                \n                                   \n                  (0.306)  \n                \n                \n                  count_pecycle    \n                  0.001    \n                \n                \n                                   \n                  (0.244)  \n                \n                \n                  count_demunemp   \n                  -0.037   \n                \n                \n                                   \n                  (0.054)  \n                \n                \n                  count_demcpi     \n                  0.127    \n                \n                \n                                   \n                  (0.056)  \n                \n                \n                  zero_(Intercept) \n                  -15.845  \n                \n                \n                                   \n                  (12.166) \n                \n                \n                  zero_unemp       \n                  2.101    \n                \n                \n                                   \n                  (1.357)  \n                \n                \n                  zero_war         \n                  -7.129   \n                \n                \n                                   \n                  (706.556)\n                \n                \n                  zero_wecycle     \n                  -3.442   \n                \n                \n                                   \n                  (242.892)\n                \n                \n                  zero_pecycle     \n                  -3.636   \n                \n                \n                                   \n                  (3.309)  \n                \n                \n                  zero_demunemp    \n                  1.431    \n                \n                \n                                   \n                  (0.821)  \n                \n                \n                  zero_approval    \n                  -0.153   \n                \n                \n                                   \n                  (0.083)  \n                \n                \n                  zero_vietsynd    \n                  2.268    \n                \n                \n                                   \n                  (1.458)  \n                \n                \n                  Num.Obs.         \n                  184      \n                \n                \n                  R2               \n                  0.133    \n                \n                \n                  R2 Adj.          \n                  0.093    \n                \n                \n                  AIC              \n                  417.4    \n                \n                \n                  BIC              \n                  468.8    \n                \n                \n                  RMSE             \n                  0.86     \n                \n        \n      \n    \n\n\nInterpretation, as usual, is relatively straightforward in terms of direction and significance, but there is one important twist. Recall that \\(\\psi\\) is the probability of not making the transition. In other words, \\(\\psi\\) is the probability of observing a zero - this is exactly backwards from how we’d normally interpret logit or probit coefficients (note the default in pscl is the logit link). So, as unemployment increases, so does the likelihood of observing a zero for Republican presidents - for Democrats, the effect is even greater. This may be evidence that foreign states seek to avoid trouble with the US when unemployment goes up - opportunities to use force become more scarce.\nAs usual, we want to interpret the magnitudes of these effects and, as is the case in any event count, we might want to predict \\(E[Y]\\) or \\(P(Y=y_{i})\\). You’ll be happy to know that the computing the expected value is very simple:\n\\[\nE[Y] = \\lambda - \\lambda \\psi \\nonumber \\\\ \\nonumber \\\\\n= e^{X_{i}\\beta_{i}} - e^{X_{i}\\beta_{i}} \\psi \\nonumber\n\\]\nand computing \\(P(Y=y_{i})\\) is also straightforward:\n\\[\nP(y_{i}=0|x_{i})=\\psi_{i}+(1-\\psi_{i})e^{-\\lambda_{i}} \\nonumber \\\\ \\nonumber \\\\\nP(Y=y_{i}|x_{i})=(1-\\psi_{i})\\frac{e^{-\\lambda_{i}}\\lambda_{i}^{y_{i}}}{y_{i}!}~\n\\mbox{for}~ y_{i}&gt;0  \\nonumber\n\\] Let’s generate predictions for the expected uses of force using the zero inflated model.\n\n\ncode\n# predict expected uses of force using predictiondatadem and predictiondatarep\n\npredictiondatadem &lt;- data.frame(predictiondatadem, approval=55, vietsynd=0)\n\npredictiondatarep &lt;- data.frame(predictiondatarep, approval=55, vietsynd=0)\n\npreds2 &lt;- data.frame(demforce =predict(mod2, newdata=predictiondatadem, type=\"response\"))\n\npreds2 &lt;- data.frame(preds2, repforce=predict(mod2, newdata=predictiondatarep, type=\"response\"), unemp=predictiondatadem$unemp)\n\nggplot(preds2, aes(x=unemp, y=demforce)) + geom_line(color=\"blue\") + geom_line(aes(x=unemp, y=repforce), color=\"red\") + labs(title=\"Uses of Force\", x=\"Unemployment\", y=\"Expected Uses of Force\") + theme_minimal() + theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\n\nAs unemployment increases, uses of force diverge between the two parties even more than in Fordham’s original model. Foreign states may avoid trouble with the US when unemployment goes up, making opportunities to use force more scarce, though Republican presidents, unable to use macroeconomic policy to address unemployment, seem to seek out opportunities to use force. Democrats, both because there are fewer opportunities and fewer incentives to use force under growing unemployment, see uses of force decline."
  },
  {
    "objectID": "countB24.html#comment",
    "href": "countB24.html#comment",
    "title": "Count Models II",
    "section": "Comment",
    "text": "Comment\nThe zero altered models are mainly estimatable in R using either pscl or glmmTMB. The former is written by political scientists including Simon Jackman. The latter is a package aimed at multi-level estimation (e.g. random effects). Results from the two do not always correspond. Moreover, models that converge easily in Stata do not always converge in R - I’m making no judgment regarding what’s right here."
  },
  {
    "objectID": "countB24.html#what-do-we-mean-exactly-by-heterogeneity",
    "href": "countB24.html#what-do-we-mean-exactly-by-heterogeneity",
    "title": "Count Models II",
    "section": "What do we mean exactly by heterogeneity?",
    "text": "What do we mean exactly by heterogeneity?\n\nevents/periods are qualitatively different such that we cannot expect the same process to produce them all.\nput differently, events are drawn from different distributions of events; different means, variances, or both.\nif the means are different, \\(\\beta\\) will be attenuated.\nif the variances are different, the model suffers something like heteroskedasticity.\neven if events are drawn from the same distribution, they may come from different parts of the distribution. % - if \\(x\\) variables are not correlated (sufficient to explain) the whole distribution, some events are unmodeled and skew conditional expected mean and variance.\n\n\nEvents/Periods from Different Distributions\n\nWorld War II, Vietnam, Franco-Prussian, Football - vary on magnitude, participants, rules, weaponry.\nInterstate war, Civil war, Extra-systemic war - different units.\nRelevant, irrelevant dyads - imperfect measurement; split outcomes; continuous latent variable.\nPatients visiting general practitioners in US - varies with insurance and wages.\nTaylor Swift concerts attended - varies in times on tour, proximity to event\nVisits to Disney World per year (time between visits) - proximity and number of children as exposure.\n\n\n\nDifferent Parts of Distribution\n\nEffect of divided govt - different pre-cold war, cold war.\nPower status changes as states become major or super powers, others decline\n\n\n\nDifferent DGPs indicate different mechanisms\n\nDifferent distributions implies different data generating processes and that we need to think about models with two parts, potentially two equations.\nThis sort of thinking characterizes event count, hazard, censored, selection, and seemingly unrelated regression models.\nThe focus here is on count models where the observed count is generated by multiple processes."
  },
  {
    "objectID": "haz-cont24.html",
    "href": "haz-cont24.html",
    "title": "Continuous Time Hazards",
    "section": "",
    "text": "The Plan"
  },
  {
    "objectID": "haz-cont24.html#continuous-time-models",
    "href": "haz-cont24.html#continuous-time-models",
    "title": "Continuous Time Hazards",
    "section": "Continuous Time Models",
    "text": "Continuous Time Models\nContinuous time models aggregate the duration data into observations that indicate the total amount of time that has elapsed thus far. Two variations on the continuous time models depend exclusively on whether the independent variables vary over time or not. These are known as time-varying covariate (TVC) and non-time-varying covariate models (NTVC).\nData for an NTVC model might appear as follows, where we observe each individual once, we know how long that individual lasted before experiencing the hazard, we know whether or not the observation “failed” or experienced the hazard (if not, the observation is censored), and we have an \\(X\\) variable:\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndividual\nDuration\nFailure\nX\n\n\n\n\n\n\n\n\n\n\n\n\n1\n6\n1\n1.4\n\n\n\n\n2\n3\n1\n.12\n\n\n\n\n3\n2\n1\n.6\n\n\n\n\n4\n5\n1\n.5\n\n\n\n\n\n\n\n\n\n\n\n\nn\n14\n0\n2.1\n\n\n\n\n\n\n\nOn the other hand, the same data for TVC would appear as follows:\\\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndividual\nDuration\nFailure\nX\n\n\n\n\n\n\n\n\n\n\n\n\n1\n1\n0\n1.4\n\n\n\n\n1\n2\n0\n1.3\n\n\n\n\n1\n3\n0\n1.5\n\n\n\n\n1\n4\n0\n1.8\n\n\n\n\n1\n5\n0\n2.4\n\n\n\n\n1\n6\n1\n0.4\n\n\n\n\n2\n1\n0\n1.12\n\n\n\n\n2\n2\n0\n0.12\n\n\n\n\n2\n3\n1\n0.92\n\n\n\n\n3\n1\n0\n0.65\n\n\n\n\n3\n2\n1\n0.62\n\n\n\n\n4\n1\n0\n1.5\n\n\n\n\n4\n2\n0\n1.25\n\n\n\n\n4\n3\n0\n1.45\n\n\n\n\n4\n4\n0\n2.5\n\n\n\n\n4\n5\n1\n0.5\n\n\n\n\n…\n…\n…\n…"
  },
  {
    "objectID": "haz-cont24.html#a-general-estimation-approach",
    "href": "haz-cont24.html#a-general-estimation-approach",
    "title": "Continuous Time Hazards",
    "section": "A general estimation approach",
    "text": "A general estimation approach\n\nQuantity of interest is either expected duration or hazard.\nIn either case, failure depends on survival function.\nIn either case, some observations fail, others are censored - failure if unobserved but nonzero probability. Recall censoring is almost always due to decisions the analyst makes.\nIn some (many) social science treatments, observations can fail more than once (multiple failure).\nIn some cases, there is more than one way to fail (competing risks)."
  },
  {
    "objectID": "haz-cont24.html#quantities",
    "href": "haz-cont24.html#quantities",
    "title": "Continuous Time Hazards",
    "section": "Quantities",
    "text": "Quantities\nSuppose a variable, \\(t\\), measuring duration. The cumulative probability of surviving up to \\(t\\) is :\n\\[\nF(t)= \\int_0^t f(u)d(u) = Pr (T\\leq t) \\nonumber  \n\\]\nThe density is the probability of failure at any point in time:\n\\[\nf(t) = \\frac{dF(t)}{d(t)} \\nonumber\n\\]\nSurvivor function - probability a unit survives to \\(t\\); proportion of units surviving to \\(t\\):\n\\[\nS(t)= 1-F(t) = Pr(T \\geq t) \\nonumber\n\\]\nHazard rate - rate of failure at \\(t\\) given survival to \\(t\\):\n\\[\nh(t)= \\frac{f(t)}{S(t)} \\nonumber\n\\]\nAll together now:\n\\[\\begin{aligned}\nF(t)= \\int_0^t f(u)d(u) = Pr (T\\leq t) \\nonumber  \\\\\nf(t) = \\frac{dF(t)}{d(t)} \\nonumber \\\\\nS(t)= 1-F(t) = Pr(T \\geq t) \\nonumber \\\\ \\nonumber \\\\\nh(t)= \\frac{f(t)}{S(t)} \\nonumber\n\\end{aligned}\\]"
  },
  {
    "objectID": "haz-cont24.html#censoring",
    "href": "haz-cont24.html#censoring",
    "title": "Continuous Time Hazards",
    "section": "Censoring",
    "text": "Censoring\n\\[\nS(t)= 1-F(t) = Pr(T \\geq t) \\nonumber \\\\\n  S(t)= 1- \\int_0^t f(u)d(u)   \\nonumber \\\\ \\nonumber \\\\\nS(t)= \\int_t^{t=C_i} f(u)d(u)  \\nonumber  \n\\]\n\nAll Uncensored (using B-S&J notation) }\n\\[\n\\mathcal{L} =  \\prod_i^n f(t_i) \\nonumber\n\\]\n\n\nCensoring (using B-S&J notation)}\nDefine \\(t^*\\) is the maximum observed duration for censored observations. Suppose two types of observations such that\n\\[\\delta_{i} = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{if $t_i\\leq t^* $} \\\\\n         0, & \\mbox{if $t_i &gt;  t^* $}\n         \\end{array}\n     \\right.\n     \\]\nwhere \\(\\delta_i = 1\\) if the observation \\(i\\) fails during the period of study.\n\n\nGeneral LF with censoring (using B-S&J notation)}\n\\[\n\\mathcal{L} =  \\prod_{i=1}^n \\Bigg\\{ f(t_i) \\Bigg\\} ^{\\delta_i} \\Bigg\\{ S(t_i) \\Bigg\\} ^{1-\\delta_i}  \\nonumber\n\\]\n\nuncensored cases provide information about failure probability (hazard), and about survival probability.\nuncensored cases contribute their density; recall density is exact probability of an event, failure in this case.\ncensored cases provide information about survival probability.\n\nTaking logs and adding covariates:\n\\[\n\\ln \\mathcal{L} =  \\sum_{i=1}^n \\bigg\\{ \\delta \\ln(f(t| x \\beta)) + (1-\\delta) \\ln(S(t_i | x\\beta)) \\bigg\\} \\nonumber\n\\]\n\n\nAsides\nThis likelihood should remind you of the LF for binary response models. - Two processes at work here: survival (continuous); failure (binary). - Think of survival as counting periods until failure; cumulative probability of survival. - Think of failure as the occurrence of events; probability survival ends. - Think of censoring as the absence of events; it’s the probability that failure always equals zero in-sample."
  },
  {
    "objectID": "haz-cont24.html#weibull-hazard",
    "href": "haz-cont24.html#weibull-hazard",
    "title": "Continuous Time Hazards",
    "section": "Weibull hazard",
    "text": "Weibull hazard\n\\[ h(t) = exp(\\beta)*p *(exp(\\beta) * t) ^{p-1}\\]\nwhere \\(\\beta\\) is the PH formulated estimate. The PH/AFT equivalence is:\n\\[ \\beta_{ph} = \\frac{- \\beta_{aft}}{1/p} \\]\nThe hazard then for AFT coefficients is :\n\\[ h(t) = exp(\\frac{ -\\beta}{1/p} ) *p* (exp(\\frac{ -\\beta}{1/p}) * t) ^{p-1}\\]"
  },
  {
    "objectID": "haz-cont24.html#exponential-hazards",
    "href": "haz-cont24.html#exponential-hazards",
    "title": "Continuous Time Hazards",
    "section": "Exponential hazards",
    "text": "Exponential hazards\nLet \\(h(t) = \\lambda\\). The hazard is constant with respect to time, varying only with changes in \\(x\\) variables, which we’ve not yet entered here.\n\nthe density is \\(f(t) = \\lambda exp(-\\lambda t)\\); the probability of failure is not cumulative, but varies only with values of \\(t\\).\nthe model requires that, for each period of time, the baseline hazard is the same, deviations arising only from the value of \\(t\\) and any covariates \\(x\\).\ncovariates - since the hazard (often denoted \\(\\lambda\\)) has to be positive, the effects have to be bounded, so it’s pretty natural to let \\(x\\) variables enter exponentially such that \\(h(t) = \\lambda = exp(x \\beta)\\).\n\nRewriting \\(f(t)\\):\n\\[\nf(t) = \\lambda exp(-\\lambda t) \\nonumber \\\\  \\nonumber \\\\\n= exp(x\\beta) exp(-exp(x\\beta) t) \\nonumber\n\\]\nand \\(S(t) = exp(-\\lambda t)\\) or with covariates, \\(S(t) = exp(-exp(x\\beta) t)\\)"
  },
  {
    "objectID": "haz-cont24.html#exponential-llf",
    "href": "haz-cont24.html#exponential-llf",
    "title": "Continuous Time Hazards",
    "section": "Exponential LLF",
    "text": "Exponential LLF\nThe LLF with censoring is:\n\\[\n\\ln \\mathcal{L} =  \\sum_{i=1}^n \\bigg\\{ \\delta \\ln(f(t| x \\beta)) + (1-\\delta) \\ln(S(t_i | x\\beta)) \\bigg\\} \\nonumber \\\\\n=  \\sum_{i=1}^n \\bigg\\{ \\delta \\bigg[ \\ln(exp(x\\beta) exp(-e^{(x\\beta)} t)) \\bigg] + (1-\\delta) \\ln(exp(-e^{(x\\beta)} t)) \\bigg\\} \\nonumber \\\\\n=  \\sum_{i=1}^n \\bigg\\{ \\delta \\bigg[ x\\beta (-exp(x\\beta) t)) \\bigg]+ (1-\\delta) (-exp(x\\beta) t)) \\bigg\\} \\nonumber\n\\]\nLooking at the exponential LLF, \\[\n=  \\sum_{i=1}^n \\bigg\\{ \\delta  \\bigg[ x\\beta (-exp(x\\beta) t)) \\bigg] + (1-\\delta) (-exp(x\\beta) t)) \\bigg\\} \\nonumber\n\\]\nyou should note:\n\nthe absence of memory; the effects of \\(x\\beta\\) are evaluated \\(at\\) points in time, \\(t\\), but not with respect to what has happened previously.\nthis is what we mean when we say the baseline hazard is constant. another way to think about this is the baseline hazard “resets” at each \\(t\\); no memory."
  },
  {
    "objectID": "haz-cont24.html#weibull-hazards",
    "href": "haz-cont24.html#weibull-hazards",
    "title": "Continuous Time Hazards",
    "section": "Weibull hazards",
    "text": "Weibull hazards\nIn the Weibull model,\n\\[\nh(t)= \\lambda p (\\lambda t)^{p-1} \\nonumber\n\\]\n\nthe baseline hazard varies by \\(p\\), and thus does not have to remain constant over time.\nif \\(p=1\\), then the model reduces to the exponential model.\n-else, \\(p\\) determines whether the hazard is increasing or decreasing at a faster rate; values greater than 1 indicate an increasing hazard rate and values less than 1 indicate a declining hazard.\n\n\nWeibull parts\n\nHazard:\n\\[\nh(t)= \\lambda p (\\lambda t)^{p-1} \\nonumber\n\\]\n\n\nDensity:\n\\[\nf(t)= \\lambda p (\\lambda t)^{p-1} (exp(-\\lambda t)^{p})\\nonumber\n\\]\n\n\nSurvivor:\n\\[\nS(t)= exp(- \\lambda t)^p \\nonumber\n\\]\nAs above, LLF combines the density and the survivor functions. Note again, the exponential is the special case where p=1."
  },
  {
    "objectID": "haz-cont24.html#hazard-ratios",
    "href": "haz-cont24.html#hazard-ratios",
    "title": "Continuous Time Hazards",
    "section": "Hazard Ratios",
    "text": "Hazard Ratios\nHazard ratios are formed by the hazards for two different values of an \\(x\\) variable; so the comparison of two observations that vary on that \\(x\\) variable.\n\\[\n\\mbox{haz. ratio}_k  = \\frac{h(t)|x_k = 1}{h(t)|x_k = 0} \\nonumber \\\\\n= \\frac{exp(\\beta_0+ x_1\\beta_1+ \\ldots + x_k \\beta_k(1))}{exp(\\beta_0+ x_1\\beta_1+ \\ldots + x_k \\beta_k(0))} \\nonumber \\\\\n=\\frac{exp(\\beta_k(1))}{exp(\\beta_k(0))} \\nonumber \\\\\n= exp(\\beta_k) \\nonumber\n\\]\nThe interpretation of \\(exp(\\beta_k)\\) is therefore relative to a one unit change in \\(x_k\\)."
  },
  {
    "objectID": "haz-cont24.html#weibull-model",
    "href": "haz-cont24.html#weibull-model",
    "title": "Continuous Time Hazards",
    "section": "Weibull model",
    "text": "Weibull model\n\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\ndp &lt;- read.csv(\"/Users/dave/Documents/teaching/606J-mle/2024/MLEfall24/dptime.csv\")\n\n#dp &lt;- dp %&gt;% group_by(dyad) %&gt;% mutate(time = 1:n())\n\ndp &lt;- dp %&gt;% mutate(pyears = pyears+1)\n\n# Required libraries\nlibrary(survival)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Fit Weibull model\nweib_model &lt;- survreg(Surv(pyears, dispute) ~ border + deml + caprat,\n                      data = dp,\n                      dist = \"weibull\")\n\nexp_model &lt;- survreg(Surv(pyears, dispute) ~ border + deml + caprat,\n                      data = dp,\n                      dist = \"exponential\")\n\nmodelsummary(weib_model, stars = TRUE)\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  9.485*** \n                \n                \n                             \n                  (0.238)  \n                \n                \n                  border     \n                  -1.993***\n                \n                \n                             \n                  (0.142)  \n                \n                \n                  deml       \n                  0.158*** \n                \n                \n                             \n                  (0.013)  \n                \n                \n                  caprat     \n                  0.004*** \n                \n                \n                             \n                  (0.001)  \n                \n                \n                  Log(scale) \n                  0.614*** \n                \n                \n                             \n                  (0.030)  \n                \n                \n                  Num.Obs.   \n                  20990    \n                \n                \n                  AIC        \n                  11303.9  \n                \n                \n                  BIC        \n                  11343.7  \n                \n                \n                  RMSE       \n                  3.4e+17  \n                \n        \n      \n    \n\n\n# Create prediction data frame\nnew_data &lt;- expand_grid(\n  deml = seq(-10, 10, by = 0.1),  # Finer grid for smoother lines\n  border = c(0, 1),\n  caprat = mean(dp$caprat)  # Using mean caprat for predictions\n)\n\n# Get predictions\nnew_data$predicted_time &lt;- predict(weib_model, \n                                   newdata = new_data, \n                                   type = \"response\")\n\n# Get standard errors for confidence intervals\npred_se &lt;- predict(weib_model, \n                   newdata = new_data, \n                   type = \"response\", \n                   se.fit = TRUE)\n\n# Calculate confidence intervals\nnew_data$lower_ci &lt;- pred_se$fit - 1.96 * pred_se$se.fit\nnew_data$upper_ci &lt;- pred_se$fit + 1.96 * pred_se$se.fit\n\n\n# Create main plot\nggplot(new_data, aes(x = deml)) +\n  # Add confidence intervals with same color\n  geom_ribbon(aes(ymin = lower_ci, \n                  ymax = upper_ci,\n                  group = factor(border)),\n              fill = \"grey50\",\n              alpha = 0.3) +\n  # Add predicted lines with different colors\n  geom_line(aes(y = predicted_time, \n                color = factor(border)),\n            linewidth = 1) +\n  # Customize colors and labels\n  scale_color_manual(values = c(\"#2C3E50\", \"#E74C3C\"),\n                     name = \"Border Status\",\n                     labels = c(\"No Border\", \"Border\")) +\n  # Add labels and title\n  labs(x = \"Democracy Level\",\n       y = \"Predicted Duration\",\n       title = \"Weibull Model: Predicted Duration by Democracy Level and Border Status\",\n       subtitle = \"Showing predictions with 95% confidence intervals\") +\n  # Customize theme\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 10, color = \"gray50\"),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_line(color = \"gray90\"),\n    panel.grid.major.y = element_line(color = \"gray90\"),\n    axis.title = element_text(face = \"bold\"),\n    legend.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\nThe hazard ratio of border sharing to non-contiguous states is \\(exp(1.22)=3.39\\); border sharing states are more than three times as likely to ``fail’’ or their hazard is three times as high as non-contiguous states."
  },
  {
    "objectID": "haz-cont24.html#hazard-ratios-proportional-hazards",
    "href": "haz-cont24.html#hazard-ratios-proportional-hazards",
    "title": "Continuous Time Hazards",
    "section": "Hazard Ratios, Proportional Hazards",
    "text": "Hazard Ratios, Proportional Hazards\nOne important thing to notice here is that the hazards are proportional and independent of time. This is true in the exponential and Weibull models. More on this shortly."
  },
  {
    "objectID": "haz-cont24.html#accelerated-failure-time-aft",
    "href": "haz-cont24.html#accelerated-failure-time-aft",
    "title": "Continuous Time Hazards",
    "section": "Accelerated Failure Time (AFT)",
    "text": "Accelerated Failure Time (AFT)\nAlternatively, we can think of the model as a regression of the log of duration on a set of covariates (a log-linear regression). Thus,\n\\[\nt_i = exp(x \\beta^*) * u \\nonumber \\\\\n\\ln(u) = \\epsilon \\nonumber \\\\\n\\epsilon = \\ln(t_i) - x \\beta^* \\nonumber\n%t_{i} = e^{\\lambda_{0} + \\lambda_{1}X} \\ast \\sigma u \\nonumber\n\\]\nwhere \\(\\beta^*\\) indicates covariate effects of a different from than in the hazard formulation. This formulation has residuals; the difference between observed and residual enter the LLF and form the basis for the MLEs.\nThe model resembles a linear model and actually has an error term where the error can take on any positive value (thus suggesting we can generate meaningful residuals). The two derivations of the Weibull model are equivalent because \\(p=\\frac{1}{\\sigma}\\) and \\(\\beta= \\frac{-\\lambda}{\\sigma}\\).\n\nthe coefficients are equivalent though scaled differently.\n\nThe hazard motivation produces what Stata calls log relative hazard estimates.\nThe regression-based motivation produces accelerated failure time coefficients.\n\nThough how we interpret them is different, they are substantively the same. - AFT coefficients are relative to duration or survival time; hazard coefficients are relative to the hazard rate. - AFTs and h(t) coefficients will always be in opposite directions.\nThe relationship between the AFT and log-hazard formulations is as follows (for the Weibull):\nThe hazard for the Weibull model is:\n\\[\nh(t)= \\lambda p (\\lambda t)^{p-1} \\nonumber\n\\]\nRecalling \\(\\lambda=exp(x\\widehat{\\beta})\\), let’s rewrite the hazard:\n\\[\nh(t)= exp(x\\widehat{\\beta}) p (exp(x\\widehat{\\beta}) t)^{p-1} \\nonumber\n\\]\nwhere \\(\\beta\\) is the PH formulated estimate. The PH/AFT equivalence is:\n\\[ \\beta_{ph} = \\frac{- \\beta_{aft}}{1/p} \\]\nTo compute the Weibull hazard for Weibull AFT coefficients:\n\\[  \nh(t)= exp(\\frac{ -\\beta}{1/p} ) * p * (exp(\\frac{ -\\beta}{1/p} ) * t)^{p-1} \\nonumber\n\\]"
  },
  {
    "objectID": "haz-cont24.html#proportional-hazards",
    "href": "haz-cont24.html#proportional-hazards",
    "title": "Continuous Time Hazards",
    "section": "Proportional Hazards",
    "text": "Proportional Hazards\nProportional hazards is the idea the ratio of two hazards are a fixed proportion regardless of time. The Weibull/exponential models are PH; so is the Cox PH model which we’ll discuss next. For Cox, the hazard rate for an individual observation is (see BS & J, pp 48 ff):\n\\[\nh_i(t) = h_0(t) exp(X \\beta) \\nonumber\n\\]\nThe hazard ratio is:\n\\[\n\\frac{h_i(t)}{h_0(t)} =  exp(\\beta(x_i-x_j) \\nonumber\n\\]\nwhich is independent of time, and therefore a fixed proportion regardless of time."
  },
  {
    "objectID": "haz-cont24.html#semi-parametric-alism",
    "href": "haz-cont24.html#semi-parametric-alism",
    "title": "Continuous Time Hazards",
    "section": "Semi-parametric-alism?",
    "text": "Semi-parametric-alism?\nWhat does it mean for the model to be semi-parametric? It’s that in derivation, we do not assume anything specific about the functional form of the hazard or event times, though the model is based on a parametric regression - hence, “semi.” The model depends on:\n\nindependence of observations.\nunique failure times - no two individuals fail simultaneously.\nproportionality"
  },
  {
    "objectID": "haz-cont24.html#issues",
    "href": "haz-cont24.html#issues",
    "title": "Continuous Time Hazards",
    "section": "Issues",
    "text": "Issues\nTwo main issues you need to be aware of in proportional hazards models are:\n\nties or non-unique failure times - what happens in this model when individuals fail at the same instant?\nproportional hazards assumption - what does it mean; what happens if it fails; how can we know\n\nThese are two (among several) important issues - I’m treating them superficially here because of time; don’t let that persuade you these lack importance. B-S&J treat them in much greater detail.\n\nTies\nIf events (failures) are measured to occur at the same time, those events are ties.\nWhat happens if (when) two individuals fail simultaneously? It’s reasonable to assume we never have ties if we think time is continuous (infinitely divisible); but since we can’t measure failures over time at infinitely small intervals, this assumption doesn’t really hold.\nThere are three main approaches for handling ties.\n\nBreslow - adjusts the denominator of the LF for the possible orderings of tied events (Stata’s default).\nEfron - an improvement to Breslow - the default in R’s survival library.\nExact methods - more complex, more accurate.\n\nWhich to choose? In general, if you have few ties, these all produce similar results so it doesn’t matter. If you have a lot of ties, prefer Efron or Exact, though Exact is very computationally demanding.\nOne other possibility in data with lots of ties is that the failure events either occur in discrete time (they can only occur at certain intervals), or we observe events at certain time points such that they appear discrete. If this is the case, it may make sense to recast the data as discrete, and estimate a discrete time model.\n\n\nNon-Proportionality\nThe models we’ve talked about thus far (exponential, Weibull, Cox) are proportional hazards models.\nAs noted above, proportional hazards imply the hazard ratio for two individuals is constant over time. So two individuals may have different hazards, but the ratio of the hazard for \\(i\\) to the hazard for \\(j\\) does not vary with \\(t\\). This means the effect of any covariate \\(x\\) is invariant to time. Though this assumption is different, it should remind you of the proportionality representation of IIA in choice models.\nIf the hazards are non proportional, the estimates and standard errors are both wrong, so our inferences are as well.\n\nWhy would hazards be non proportional?\n\na covariate has a diminishing or threshold effect on survival - perhaps \\(x\\) increases survival, but only up to some point after which it has no effect - e.g. drug resistance. Hazards for two individuals, one treatment, one control, will converge at some point as the treatment individual gets smaller and smaller effect from the drug, relative to the control individual.\na covariate measures learning such that it decreases the difference between an individual learning something and an individual who already learned.\na covariate increases the disparity in survival between individuals over time, so their hazards diverge. -hazards for individuals converge and then cross; a common example is the survival effects of surgery versus radiation or drug treatment for cancer patients. The hazard for surgery patients is high at first, but declines; for radiation/drug patients, the hazards are low at first, but increase with time. In the long run, surgery patients’ hazards usually crosses beneath that of radiation patients.\n\nIn all these cases, the hazards for individuals change over time at different rates or in different directions. This means the ratio of their hazards is not constant over time. So variation over time is the common factor here and implies tests for non proportionality and solutions to it.\nIn what follows, I’m focusing only on the Cox model."
  },
  {
    "objectID": "haz-cont24.html#ties",
    "href": "haz-cont24.html#ties",
    "title": "Continuous Time Hazards",
    "section": "Ties",
    "text": "Ties\nIf events (failures) are measured to occur at the same time, those events are ties.\nWhat happens if (when) two individuals fail simultaneously? It’s reasonable to assume we never have ties if we think time is continuous (infinitely divisible); but since we can’t measure failures over time at infinitely small intervals, this assumption doesn’t really hold.\nThere are three main approaches for handling ties.\n\nBreslow - adjusts the denominator of the LF for the possible orderings of tied events (Stata’s default).\nEfron - an improvement to Breslow - the default in R’s survival library.\nExact methods - more complex, more accurate.\n\nWhich to choose? In general, if you have few ties, these all produce similar results so it doesn’t matter. If you have a lot of ties, prefer Efron or Exact, though Exact is very computationally demanding.\nOne other possibility in data with lots of ties is that the failure events either occur in discrete time (they can only occur at certain intervals), or we observe events at certain time points such that they appear discrete. If this is the case, it may make sense to recast the data as discrete, and estimate a discrete time model."
  },
  {
    "objectID": "haz-cont24.html#proportionality",
    "href": "haz-cont24.html#proportionality",
    "title": "Continuous Time Hazards",
    "section": "Proportionality",
    "text": "Proportionality\nThe models we’ve talked about thus far (exponential, Weibull, Cox) are proportional hazards models. What exactly is proportionality?\nProportional hazards imply the hazard ratio for two individuals is constant over time. So two individuals may have different hazards, but the ratio of the hazard for \\(i\\) to the hazard for \\(j\\) does not vary with \\(t\\). This means the effect of any covariate \\(x\\) is invariant to time. Though this assumption is different, it should remind you of the proportionality representation of IIA in choice models."
  },
  {
    "objectID": "haz-cont24.html#violating-ph",
    "href": "haz-cont24.html#violating-ph",
    "title": "Continuous Time Hazards",
    "section": "Violating PH",
    "text": "Violating PH\nIf the hazards are non proportional, the estimates and standard errors are both wrong, so our inferences are as well."
  },
  {
    "objectID": "haz-cont24.html#why-would-hazards-be-non-proportional",
    "href": "haz-cont24.html#why-would-hazards-be-non-proportional",
    "title": "Continuous Time Hazards",
    "section": "Why would hazards be non proportional?",
    "text": "Why would hazards be non proportional?\n\na covariate has a diminishing or threshold effect on survival - perhaps \\(x\\) increases survival, but only up to some point after which it has no effect - e.g. drug resistance. Hazards for two individuals, one treatment, one control, will converge at some point as the treatment individual gets smaller and smaller effect from the drug, relative to the control individual.\na covariate measures learning such that it decreases the difference between an individual learning something and an individual who already learned."
  },
  {
    "objectID": "haz-cont24.html#why-would-hazards-be-non-proportional-1",
    "href": "haz-cont24.html#why-would-hazards-be-non-proportional-1",
    "title": "Continuous Time Hazards",
    "section": "Why would hazards be non-proportional?",
    "text": "Why would hazards be non-proportional?\n\na covariate increases the disparity in survival between individuals over time, so their hazards diverge. -hazards for individuals converge and then cross; a common example is the survival effects of surgery versus radiation or drug treatment for cancer patients. The hazard for surgery patients is high at first, but declines; for radiation/drug patients, the hazards are low at first, but increase with time. In the long run, surgery patients’ hazards usually crosses beneath that of radiation patients.\n\nIn all these cases, the hazards for individuals change over time at different rates or in different directions. This means the ratio of their hazards is not constant over time. So variation over time is the common factor here and implies tests for non proportionality and solutions to it.\nIn what follows, I’m focusing only on the Cox model."
  },
  {
    "objectID": "haz-cont24.html#tests-for-ph-in-the-cox-model",
    "href": "haz-cont24.html#tests-for-ph-in-the-cox-model",
    "title": "Continuous Time Hazards",
    "section": "Tests for PH in the Cox model",
    "text": "Tests for PH in the Cox model\nThere are several, I’m only presenting two residual-based methods appropriate for the Cox model:\n\nPlot the Schoenfeld residuals against some function of time.\nGenerate a test statistic measuring the relationship between the Schoenfeld residuals and time.\n\n\nSchoenfeld Residuals\nThe math here is difficult and not informative.\n\nThe idea is that we can get residuals for each individual, at each event time, for each covariate (by differentiating the Cox LL w.r.t. \\(\\beta\\)). Each residual is the estimated difference between the observed and predicted hazard for each variable, individual, time.\nUsing the Schoenfeld residuals (or their scaled version), we can either plot them over time, or generate a test statistic. The latter, due to Grambsch and Therneau’s (1994) article, produces what amounts to a correlation between the residuals for each variable and time; it’s \\(\\chi^2\\) distributed, so hypothesis testing is simple. The null is that there is no systematic relationship between the residuals and time. They also have a “global” test - it evaluates all the residuals for the model.\nIn Stata, the command after stcox is estat phtest, detail. In R, use the cox.zph function in the survival package.\n\n\ncode\n# Fit Cox model\n\ncox_model &lt;- coxph(Surv(length, censor) ~ oadm + demosumb + sumpopbg + rterrain, \n                   data = war)\nmodelsummary(cox_model, stars = TRUE)\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  oadm    \n                  0.268  \n                \n                \n                          \n                  (0.434)\n                \n                \n                  demosumb\n                  0.056* \n                \n                \n                          \n                  (0.028)\n                \n                \n                  sumpopbg\n                  0.000  \n                \n                \n                          \n                  (0.000)\n                \n                \n                  rterrain\n                  0.755  \n                \n                \n                          \n                  (0.589)\n                \n                \n                  Num.Obs.\n                  77     \n                \n                \n                  AIC     \n                  522.0  \n                \n                \n                  BIC     \n                  531.4  \n                \n                \n                  RMSE    \n                  1.00   \n                \n        \n      \n    \n\n\n\ncode\n# Check proportional hazards assumption\n\ncoxphtest &lt;- cox.zph(cox_model)\ncoxphtest\n\n\n         chisq df     p\noadm     3.368  1 0.066\ndemosumb 0.802  1 0.371\nsumpopbg 0.335  1 0.563\nrterrain 1.261  1 0.261\nGLOBAL   6.352  4 0.174\n\n\nWe can also plot the residuals in the same package:\n\n\ncode\n# Plot Schoenfeld residuals\n\nggcoxzph(cox.zph(cox_model))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is a great book - and these scholars have done a lot to develop and extend survival models.\n\n\n\n\n\nWhat if my hazards are non proportional?\nIf the hazards for some \\(x\\) vary with time, then why not control for their interaction. After all, NPH means that the effect of \\(x\\) changes as \\(t\\) changes - and when that’s what we expect, we interact stuff, right?\n\nWe usually use \\(ln(t) * x\\); the interaction coefficient measures the marginal effect of \\(x\\) at time \\(t\\). We do not include the constituent term \\(ln(t)\\)."
  },
  {
    "objectID": "haz-cont24.html#what-if-my-hazards-are-non-proportional",
    "href": "haz-cont24.html#what-if-my-hazards-are-non-proportional",
    "title": "Continuous Time Hazards",
    "section": "What if my hazards are non proportional?",
    "text": "What if my hazards are non proportional?\nIf the hazards for some \\(x\\) vary with time, then why not control for their interaction. After all, NPH means that the effect of \\(x\\) changes as \\(t\\) changes - and when that’s what we expect, we interact stuff, right?\n\nWe usually use \\(ln(t) * x\\); the interaction coefficient measures the marginal effect of \\(x\\) at time \\(t\\). We do not include the constituent term \\(ln(t)\\)."
  },
  {
    "objectID": "haz-cont24.html#war-duration-example",
    "href": "haz-cont24.html#war-duration-example",
    "title": "Continuous Time Hazards",
    "section": "War duration example",
    "text": "War duration example\nHere are two models using data from (bennettstam?); their paper was one of the early applications of the continuous time hazard models in the discipline.\n\nwar &lt;- read.csv(\"/Users/dave/Documents/teaching/606J-mle/2022/slides/L12_hazards2/code/one_per_war.csv\")\n\n\n\n# First, create the Cox model\n# Assuming your data is in a dataframe called 'df'\ncox_model &lt;- coxph(Surv(length, censor) ~ oadm + demosumb + sumpopbg + rterrain, \n                   data = war)\n\n# Create Weibull model\nweib_model &lt;- survreg(Surv(length, censor) ~ oadm + demosumb + sumpopbg + rterrain, \n                      data = war, \n                      dist = \"weibull\")\n\nmodelsummary(list(cox_model, weib_model))\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n              \n        \n        \n        \n                \n                  oadm       \n                  0.268  \n                  -0.584 \n                \n                \n                             \n                  (0.434)\n                  (0.661)\n                \n                \n                  demosumb   \n                  0.056  \n                  -0.072 \n                \n                \n                             \n                  (0.028)\n                  (0.042)\n                \n                \n                  sumpopbg   \n                  0.000  \n                  0.000  \n                \n                \n                             \n                  (0.000)\n                  (0.000)\n                \n                \n                  rterrain   \n                  0.755  \n                  -1.190 \n                \n                \n                             \n                  (0.589)\n                  (0.895)\n                \n                \n                  (Intercept)\n                         \n                  3.007  \n                \n                \n                             \n                         \n                  (0.439)\n                \n                \n                  Log(scale) \n                         \n                  0.435  \n                \n                \n                             \n                         \n                  (0.087)\n                \n                \n                  Num.Obs.   \n                  77     \n                  77     \n                \n                \n                  AIC        \n                  522.0  \n                  543.4  \n                \n                \n                  BIC        \n                  531.4  \n                  557.5  \n                \n                \n                  RMSE       \n                  1.00   \n                  24.28  \n                \n        \n      \n    \n\n\n\nHere are predictions from the two models. You’ll note (as you would have in the table above) the effects are in opposite directions. This is because the Weibull model is parameterized as AFTE, while the Cox model is parameterized as PH.\n\n\ncode\n# Create prediction data\npred_data &lt;- data.frame(\n  demosumb = 0:17,\n  oadm = 0,\n  sumpopbg = 159139,\n  rterrain = 0.34\n)\n\n# Get predictions for Cox model\ncox_preds &lt;- predict(cox_model, \n                     newdata = pred_data, \n                     type = \"risk\")\n\n# Get predictions for Weibull model\nweib_preds &lt;- predict(weib_model, \n                      newdata = pred_data, \n                      type = \"response\")\n\n# Combine predictions into a data frame for plotting\nplot_data &lt;- data.frame(\n  demosumb = pred_data$demosumb,\n  cox = cox_preds,\n  weibull = weib_preds\n)\n\n# Create the plot\n\nw &lt;- ggplot(plot_data, aes(x = demosumb)) +\n  geom_line(aes(y = weibull, color = \"Weibull Model\"), size = 1) +\n  labs(title = \"Weibull Model\",\n       x = \"demosumb\",\n       y = \"Predicted Risk\") +\n  theme_minimal()\n\nc &lt;- ggplot(plot_data, aes(x = demosumb)) +\n  geom_line(aes(y = cox, color = \"Cox Model\"), size = 1) +\n  labs(title = \"Cox model\",\n       x = \"demosumb\",\n       y = \"Predicted Risk\") +\n  theme_minimal()\n\npatchwork::wrap_plots(w, c)"
  },
  {
    "objectID": "haz-cont24.html#coxs-proportional-hazards-model",
    "href": "haz-cont24.html#coxs-proportional-hazards-model",
    "title": "Continuous Time Hazards",
    "section": "Cox’s Proportional Hazards Model",
    "text": "Cox’s Proportional Hazards Model\nThe Cox model is one of the most general continuous time hazard models. It is semi-parametric and estimates a partial likelihood function. This means that it makes no specific distributional assumptions about the shape of the baseline hazard. The model is conceptually very similar to the discrete time (logit) model.\nRepeating, the Cox hazard for an individual is:\n\\[\nh_i(t) = h_0(t) exp(x \\beta) \\nonumber\n\\]\nbut unlike the parametric models above, the baseline, \\(h_0(t)\\) remains an unknown in the Cox model; it is not estimated in the model, and therefore, we make no assumptions about its shape. Hence, the model is semi-parametric because there is no parameterization of the baseline hazard - there is no constant. Since the hazards change with \\(x\\) variables, you can see why the hazards for one individual to another are especially important - so proportional hazards are central to this model insofar as they only make sense relative to one another.\nThe Cox assumes a baseline hazard \\(h_{0}\\) that shifts depending on covariates, \\(X\\). Covariates enter the model exponentially:\n\\[\nh(t)=h_{0}(t)exp(X \\beta) \\nonumber\n\\]\nNow, imagine what happens when \\(X\\) is equal to zero (so the null or constant-only model) - we’re left with:\n\\[\nh(t)=h_{0}(t) \\nonumber\n\\]\nNote that the value of \\(h_{0}\\) doesn’t matter because the effects of \\(x_i\\) are relative to \\(x_j\\). If \\(X\\) is not zero, the effects of \\(X\\) on the baseline probability are proportional to changes in \\(X\\).\nThus, the interpretation of the model is simply the relative effect of \\(X\\) on the hazard. That is, a change in \\(x\\) from \\(i\\) to \\(j\\) has effect \\(exp(\\hat{\\beta})\\).\nThe model is especially flexible (just as in the discrete case) because it makes no assumptions about a distribution. This is the principle virtue of the Cox model and the main reason it has become the preferred model in a lot of political science (and other) research.\n\nSemi-parametric-alism?\nWhat does it mean for the model to be semi-parametric? It’s that in derivation, we do not assume anything specific about the functional form of the hazard or event times, though the model is based on a parametric regression - hence, “semi.” The model depends on:\n\nindependence of observations.\nunique failure times - no two individuals fail simultaneously.\nproportionality\n\n\n\nWar duration example\nHere are two models using data from Bennett and Stam (1996); their paper was one of the early applications of the continuous time hazard models in the discipline.\n\n\ncode\nwar &lt;- read.csv(\"/Users/dave/Documents/teaching/606J-mle/2022/slides/L12_hazards2/code/one_per_war.csv\")\n\n\n\n# First, create the Cox model\n# Assuming your data is in a dataframe called 'df'\ncox_model &lt;- coxph(Surv(length, censor) ~ oadm + demosumb + sumpopbg + rterrain, \n                   data = war)\n\n# Create Weibull model\nweib_model &lt;- survreg(Surv(length, censor) ~ oadm + demosumb + sumpopbg + rterrain, \n                      data = war, \n                      dist = \"weibull\")\n\nmodelsummary(list(cox_model, weib_model))\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n              \n        \n        \n        \n                \n                  oadm       \n                  0.268  \n                  -0.584 \n                \n                \n                             \n                  (0.434)\n                  (0.661)\n                \n                \n                  demosumb   \n                  0.056  \n                  -0.072 \n                \n                \n                             \n                  (0.028)\n                  (0.042)\n                \n                \n                  sumpopbg   \n                  0.000  \n                  0.000  \n                \n                \n                             \n                  (0.000)\n                  (0.000)\n                \n                \n                  rterrain   \n                  0.755  \n                  -1.190 \n                \n                \n                             \n                  (0.589)\n                  (0.895)\n                \n                \n                  (Intercept)\n                         \n                  3.007  \n                \n                \n                             \n                         \n                  (0.439)\n                \n                \n                  Log(scale) \n                         \n                  0.435  \n                \n                \n                             \n                         \n                  (0.087)\n                \n                \n                  Num.Obs.   \n                  77     \n                  77     \n                \n                \n                  AIC        \n                  522.0  \n                  543.4  \n                \n                \n                  BIC        \n                  531.4  \n                  557.5  \n                \n                \n                  RMSE       \n                  1.00   \n                  24.28  \n                \n        \n      \n    \n\n\n\nHere are predictions from the two models. You’ll note (as you would have in the table above) the effects are in opposite directions. This is because the Weibull model is parameterized as AFTE, while the Cox model is parameterized as PH.\n\n\ncode\n# Create prediction data\npred_data &lt;- data.frame(\n  demosumb = 0:17,\n  oadm = 0,\n  sumpopbg = 159139,\n  rterrain = 0.34\n)\n\n# Get predictions for Cox model\ncox_preds &lt;- predict(cox_model, \n                     newdata = pred_data, \n                     type = \"risk\")\n\n# Get predictions for Weibull model\nweib_preds &lt;- predict(weib_model, \n                      newdata = pred_data, \n                      type = \"response\")\n\n# Combine predictions into a data frame for plotting\nplot_data &lt;- data.frame(\n  demosumb = pred_data$demosumb,\n  cox = cox_preds,\n  weibull = weib_preds\n)\n\n# Create the plot\n\nw &lt;- ggplot(plot_data, aes(x = demosumb)) +\n  geom_line(aes(y = weibull, color = \"Weibull Model\"), size = 1) +\n  labs(title = \"Weibull Model\",\n       x = \"demosumb\",\n       y = \"Predicted Risk\") +\n  theme_minimal()\n\nc &lt;- ggplot(plot_data, aes(x = demosumb)) +\n  geom_line(aes(y = cox, color = \"Cox Model\"), size = 1) +\n  labs(title = \"Cox model\",\n       x = \"demosumb\",\n       y = \"Predicted Risk\") +\n  theme_minimal()\n\npatchwork::wrap_plots(w, c)"
  },
  {
    "objectID": "haz-cont24.html#non-proportionality",
    "href": "haz-cont24.html#non-proportionality",
    "title": "Continuous Time Hazards",
    "section": "Non-Proportionality",
    "text": "Non-Proportionality\nThe models we’ve talked about thus far (exponential, Weibull, Cox) are proportional hazards models.\nAs noted above, proportional hazards imply the hazard ratio for two individuals is constant over time. So two individuals may have different hazards, but the ratio of the hazard for \\(i\\) to the hazard for \\(j\\) does not vary with \\(t\\). This means the effect of any covariate \\(x\\) is invariant to time. Though this assumption is different, it should remind you of the proportionality representation of IIA in choice models.\nIf the hazards are non proportional, the estimates and standard errors are both wrong, so our inferences are as well.\n\nWhy would hazards be non proportional?\n\na covariate has a diminishing or threshold effect on survival - perhaps \\(x\\) increases survival, but only up to some point after which it has no effect - e.g. drug resistance. Hazards for two individuals, one treatment, one control, will converge at some point as the treatment individual gets smaller and smaller effect from the drug, relative to the control individual.\na covariate measures learning such that it decreases the difference between an individual learning something and an individual who already learned.\na covariate increases the disparity in survival between individuals over time, so their hazards diverge. -hazards for individuals converge and then cross; a common example is the survival effects of surgery versus radiation or drug treatment for cancer patients. The hazard for surgery patients is high at first, but declines; for radiation/drug patients, the hazards are low at first, but increase with time. In the long run, surgery patients’ hazards usually crosses beneath that of radiation patients.\n\nIn all these cases, the hazards for individuals change over time at different rates or in different directions. This means the ratio of their hazards is not constant over time. So variation over time is the common factor here and implies tests for non proportionality and solutions to it.\nIn what follows, I’m focusing only on the Cox model.\n\n\nTests for PH in the Cox model\nThere are several, I’m only presenting two residual-based methods appropriate for the Cox model:\n\nPlot the Schoenfeld residuals against some function of time.\nGenerate a test statistic measuring the relationship between the Schoenfeld residuals and time.\n\n\n\nSchoenfeld Residuals\nThe math here is difficult and not informative.\n\nThe idea is that we can get residuals for each individual, at each event time, for each covariate (by differentiating the Cox LL w.r.t. \\(\\beta\\)). Each residual is the estimated difference between the observed and predicted hazard for each variable, individual, time.\nUsing the Schoenfeld residuals (or their scaled version), we can either plot them over time, or generate a test statistic. The latter, due to Grambsch and Therneau’s (1994) article, produces what amounts to a correlation between the residuals for each variable and time; it’s \\(\\chi^2\\) distributed, so hypothesis testing is simple. The null is that there is no systematic relationship between the residuals and time. They also have a “global” test - it evaluates all the residuals for the model.\nIn Stata, the command after stcox is estat phtest, detail. In R, use the cox.zph function in the survival package.\n\n\ncode\n# Fit Cox model\n\ncox_model &lt;- coxph(Surv(length, censor) ~ oadm + demosumb + sumpopbg + rterrain, \n                   data = war)\nmodelsummary(cox_model, stars = TRUE)\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  oadm    \n                  0.268  \n                \n                \n                          \n                  (0.434)\n                \n                \n                  demosumb\n                  0.056* \n                \n                \n                          \n                  (0.028)\n                \n                \n                  sumpopbg\n                  0.000  \n                \n                \n                          \n                  (0.000)\n                \n                \n                  rterrain\n                  0.755  \n                \n                \n                          \n                  (0.589)\n                \n                \n                  Num.Obs.\n                  77     \n                \n                \n                  AIC     \n                  522.0  \n                \n                \n                  BIC     \n                  531.4  \n                \n                \n                  RMSE    \n                  1.00   \n                \n        \n      \n    \n\n\n\ncode\n# Check proportional hazards assumption\n\ncoxphtest &lt;- cox.zph(cox_model)\ncoxphtest\n\n\n         chisq df     p\noadm     3.368  1 0.066\ndemosumb 0.802  1 0.371\nsumpopbg 0.335  1 0.563\nrterrain 1.261  1 0.261\nGLOBAL   6.352  4 0.174\n\n\nWe can also plot the residuals in the same package:\n\n\ncode\n# Plot Schoenfeld residuals\n\nggcoxzph(cox.zph(cox_model))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is a great book - and these scholars have done a lot to develop and extend survival models.\n\n\n\n\n\nWhat if my hazards are non proportional?\nIf the hazards for some \\(x\\) vary with time, then why not control for their interaction. After all, NPH means that the effect of \\(x\\) changes as \\(t\\) changes - and when that’s what we expect, we interact stuff, right?\n\nWe usually use \\(ln(t) * x\\); the interaction coefficient measures the marginal effect of \\(x\\) at time \\(t\\). We do not include the constituent term \\(ln(t)\\)."
  }
]