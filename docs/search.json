[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Maximum Likelihood, Fall 2024",
    "section": "",
    "text": "This is the course website for PLSC 606J, Maximum Likelihood Estimation, Fall 2024. The course is an advanced course in data science focusing on maximum likelihood methods and their applications in political science.\n\nSyllabus\nSlides\nCode\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site:\nCreated with Quarto.\nAbout me:\nProfessor of political science, PhD Florida State 1999. I’ve taught this class for a long time, but never the same way twice. I study models of political violence, collect data on protests and repression.\n\n\n\n Back to top"
  },
  {
    "objectID": "mlesyllabus24.html#seminar-description",
    "href": "mlesyllabus24.html#seminar-description",
    "title": "MLE Syllabus",
    "section": "Seminar Description",
    "text": "Seminar Description\nThis 4 credit hour seminar is a survey of maximum likelihood methods and their applications to empirical political questions. It presumes students have a detailed and intuitive knowledge of least squares, probability theory, basic skills in scalar and matrix algebra, and a basic understanding of calculus. The course will deal mainly in understanding the principles of maximum likelihood estimation, under what conditions we move away from least squares, and what particular models are appropriate given observed data. The seminar will focus on application and interpretation of ML models and linking theory to statistical models. The course emphasizes coding and data viz in R and Stata.\nThe class meets one time per week for three hours. My office hours are designed to be homework help hours where I’ll work in the grad lab with any of you who are working on the exercises. The most productive pathway for this class is for you to get in the habit of working together, and those office hours are a good time for this."
  },
  {
    "objectID": "mlesyllabus24.html#course-purpose",
    "href": "mlesyllabus24.html#course-purpose",
    "title": "MLE Syllabus",
    "section": "Course Purpose",
    "text": "Course Purpose\nThis seminar fulfills the advanced quantitative methods requirement in the Ph.D. curriculum. The method of maximum likelihood underlies a majority of quantitative models in Political Science; this class teaches students to be astute consumers of such models, and how to implement and interpret ML models. These are crucial skills for dissertations in Political Science, and for producing publishable quantitative research."
  },
  {
    "objectID": "mlesyllabus24.html#learning-objectives",
    "href": "mlesyllabus24.html#learning-objectives",
    "title": "MLE Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nStudents will encounter an array of maximum likelihood models in this course. By the end of the course, students will have mastered the theory of maximum likelihood sufficient to write and program likelihood functions in ; they will be able to choose, estimate, and interpret appropriate models, model specifications, and model evaluation tools given their data; and they will be able to produce sophisticated quantities of interest (e.g. predicted probabilities, expected values, confidence intervals) via a variety of techniques including simulation and end point transformation. Students will also be able to present model findings verbally and graphically."
  },
  {
    "objectID": "mlesyllabus24.html#class-meetings-office-hours-assignments",
    "href": "mlesyllabus24.html#class-meetings-office-hours-assignments",
    "title": "MLE Syllabus",
    "section": "Class Meetings, Office Hours, Assignments",
    "text": "Class Meetings, Office Hours, Assignments\nThe course will meet this fall entirely in-person in the Social Science Experiment Lab on Wednesdays 9:40am-12:40pm.\nOffice hours are Mondays 1:30pm-3:30pm. I’ll likely hold these in the grad work room to help with your assignments. For an appointment, email me and we’ll sort out a time.\nAll assignments should be turned in on Brightspace - please submit ::\n\nPDFs generated from LaTeX or R Markdown (Quarto).\nannotated R scripts.\nwhere necessary, data.\n\nAssignments should be instantly replicable - running the code file should produce all models, tables, plots, etc."
  },
  {
    "objectID": "mlesyllabus24.html#reading",
    "href": "mlesyllabus24.html#reading",
    "title": "MLE Syllabus",
    "section": "Reading",
    "text": "Reading\nThe reading material for the course is important because it often demonstrates application of various MLE models; seeing how folks apply these and how they motivate their applications is really informative, something you cannot miss. We often won’t directly discuss the readings, but don’t let that imply they’re not important. If I get the sense we’re not keeping up with reading, expect the syllabus to change to incorporate quizzes or other accountability measures.\nReading for the course will consist of several books and articles (listed by week below). The books listed below also have Amazon links - you’ll find most of these cheaper used online.\n\nRequired\n\nBox-Steffensmeier, Janet and Jones, Brad. 2004. Event History Modeling. Cambridge. ISBN 0521546737\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Sage Publications Inc. ISBN 0803973748\nWard, Michael D. and John S. Ahlquist. 2018 Maximum Likelihood for Social Science. Cambridge. ISBN 978-1316636824.\n\n\n\nRecommended\nUseful, but not required (though some required reading in the first one):\n\nGary King. 1998. Unifying Political Methodology. University of Michigan Press. ISBN 0472085549\nJ. Scott Long. 2014. Regression Models for Categorical Dependent Variables Using Stata. 3rd Ed. Stata Press. ISBN 1597181110 (this book is good for practical/applied examples even if R is your primary language)\n\nGary King’s book is regarded as seminal in developing ML applications in political science. Scott Long’s is a similarlyaccessible treatment of a host of ML models and applications (and the Stata book is a great applied companion). Together, these two books are probably the most important on the syllabus as they are both accessible, but comprehensive and technical enough to be useful. Ward and Ahlquist’s book is a new overview of applied ML in a political science setting. Box-Steffensmeier and Jones is a thorough and accessible treatment of hazard models in a variety of empirical settings.\n\n\nAdditional Resources\nOther useful books include:\n\nCameron, A. Colin and Trivedi, Pravin K. 1998. Regression Analysis of Count Data. Cambridge. ISBN 0521635675\nMaddala, Gregory. 1983. Limited Dependent and Qualitative Variables in Econometrics. Cambridge. ISBN 0521338255\nPaul D Allison - Event History Analysis : Regression for Longitudinal Event Data. Sage Publications Inc. ISBN 0803920555\nTim Futing Liao - Interpreting Probability Models : Logit, Probit, and Other Generalized Linear Models. Sage Publications Inc. ISBN 0803949995\nJohn H Aldrich and Forrest D Nelson - Linear Probability, Logit, and Probit Models. Sage Publications Inc. ISBN 0803921330\nFred C Pampel - Logistic Regression : A Primer. Sage Publications Inc. ISBN 0761920102\nVani Kant Borooah - Logit and Probit : Ordered and Multinomial Models. Sage Publications Inc. ISBN 0761922423\nScott R Eliason - Maximum Likelihood Estimation : Logic and Practice. Sage Publications Inc. ISBN 0803941072\nRichard Breen - Regression Models : Censored, Sample Selected, or Truncated Data. Sage Publications Inc. ISBN 0803957106\nKrishnan Namboodiri - Matrix Algebra : An Introduction. ISBN 0803920520"
  },
  {
    "objectID": "mlesyllabus24.html#this-is-the-most-important-section-of-the-syllabus",
    "href": "mlesyllabus24.html#this-is-the-most-important-section-of-the-syllabus",
    "title": "Maximum Likelihood Syllabus",
    "section": "This is the most important section of the syllabus",
    "text": "This is the most important section of the syllabus\n\nReading\nIt will rarely be the case that we discuss all the readings in seminar. Their purposes are two-fold. First, the technical readings (King, Long, etc.) are to frame the derivations of the models and techniques. These are important because to understand how to interpret a model, you need a technical understanding of its origins. This is not to say you need to be able to derive these yourselves, or to perform complex mathematical computations. It is to say that if you carefully read about the models themselves, you will certainly find their interpretations easier. The mantra for this course is this - interpretation is everything. Without some understanding of how these models arise, you will find interpretation hard.\nSecond, the applied readings (i.e. articles) provide just that - application in a political, economic, or social context, and application in terms of interpretation of results. Some applications are nicely done, some less so. Not only are these useful examples of what to do and of what not to do, they will provide really useful models for your own efforts to apply these statistical models. Interpretation is everything.\nSo reading is up to you, and is crucial. As much as I hate to say this, if I get the sense at any point during the term you are not reading, I will absolutely begin weekly reading quizzes; or perhaps require weekly papers on the readings. Making them up will suck. Taking them will suck. Grading them will suck. There really isn’t much reading, and there’s just no excuse not to do it."
  },
  {
    "objectID": "mlesyllabus24.html#how-to-read",
    "href": "mlesyllabus24.html#how-to-read",
    "title": "Maximum Likelihood Syllabus",
    "section": "How to Read",
    "text": "How to Read\nReading academic stuff is a bit different from most other reading. Especially since you’re reading large amounts in graduate school (and the rest of your academic lives), it’s important to think about what you’re trying to accomplish. With journal articles, the goal has to be to understand the novel claim the paper makes, to understand why that novel claim is interesting or novel (at least according to the author), to understand how the author assesses the evidence, and to understand what that evidence tells us. Most importantly, you should have two answers to each of these - what the author says, and what you think. The author may tell you the argument is important for some reason, and you might disagree - you might think it’s unimportant, or important for a different reason, or wrong, or whatever.\nReading technical stuff is yet another category, but equally important. The best way to read math models is to read them aloud (yes, out loud) for three reasons. First, it forces you to slow down as you read it - you talk slower than your eyes move. Second, it forces you to deal with every element of the model - in silence, your mind will just skim right over it to the next set of English words. Third, it engages two senses reading silently does not - vocalization and hearing. \\\nReading out loud is not going to make everything crystal clear - but it will open some pathways whereby math language isn’t completely foreign. Most importantly, doing so bridges the gap between the math and the English accounts that normally follow, and make those English accounts easier to make sense of."
  },
  {
    "objectID": "mlesyllabus24.html#course-requirements-and-grades",
    "href": "mlesyllabus24.html#course-requirements-and-grades",
    "title": "MLE Syllabus",
    "section": "Course Requirements and Grades",
    "text": "Course Requirements and Grades\nThe seminar requires the following:\n\nProblem sets - 60% total\nMechanism papers - 40%\n\nPlease note that all written assignments must be submitted as PDFs either compiled in LaTeX or in R markdown (Quarto).\nYou’ll complete a series of problem sets, mostly applied. How many will depend on how things move along during the term. Regarding the problem sets - the work you turn in for the problem sets should clearly be your own, but I urge you to work together - doing so is a great way to learn and to overcome problems.\nThe mechanism papers are a series of three short papers you’ll write during the semester aimed at learning to identify and describe causal mechanisms, then at producing a causal mechanism. More on these early in the term.\nA word about completeness - attempt everything. To receive a passing grade in the course, you must finish all elements of the course, so all problem sets, all exams, papers, etc. To complete an element, you must at least attempt all parts of the element - so if a problem set has 10 problems, you must attempt all 10 or the assignment is incomplete, you’ve not completed every element of the course, and you cannot pass. I realize there may be problems you have trouble with and even get wrong, but you must try - the bottom line is don’t turn in incomplete work.\nGrades will be assigned on the following scale:\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\nRange\nGrade\nRange\n\n\n\n\nA\n94-100%\nC+\n77-79%\n\n\nA-\n90–93%\nC\n73-76%\n\n\nB+\n87–89%\nC-\n70-72%\n\n\nB\n83-86%\nD\n60-69%\n\n\nB-\n80-82%\nF\n&lt;60%"
  },
  {
    "objectID": "mlesyllabus24.html#course-policies",
    "href": "mlesyllabus24.html#course-policies",
    "title": "MLE Syllabus",
    "section": "Course Policies",
    "text": "Course Policies\n\nAttendance\nAttendance is expected, and is essential if you’re to succeed in this class.\n\n\nAcademic Integrity\nIdeas are the currency in academic exchange, so acknowledging where ideas come from is important. Acknowledging the sources of ideas also helps us identify an idea’s lineage which can be important for understanding how that line of thought has developed, and toward promoting future growth. As graduate students, you should have a good understanding of academic honesty and best practices. Here are details of Binghamton’s honesty policy."
  },
  {
    "objectID": "mlesyllabus24.html#course-schedule",
    "href": "mlesyllabus24.html#course-schedule",
    "title": "MLE Syllabus",
    "section": "Course Schedule",
    "text": "Course Schedule\nWeek 1, Aug 21 – Binary \\(y\\) Variables I - probit/logit, QI\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 1, 2, 4\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 3.    \n\nWeek 2, Aug 28 – Likelihood Theory and ML Estimation\n\nGary King. 1998. Unifying Political Methodology. Chapter 1-4\nJ. Scott Long. 1997. *Regression Models for Categorical and Limited Dependent Variables}. Chapters 1-2.\n\nWeek 3, Sept 4 – Binary \\(y\\) Variables II - symmetry, fit, diagnostics, prediction\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 3, 5, 6, 7\n\n\n\nNagler (1994)\nKing and Zeng (2001)\nFranklin and Kosaki (1989)\nC. Zorn (2005)\n\nWeek 4, Sept 11 – Binary \\(y\\) Variables III (discrete hazards)\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 11 \nBeck, Katz, and Tucker (1998)\nCarter and Signorino (2010)\n\nWeek 5, Sept 18 – Binary \\(y\\) Variables IV - variance, order\n\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 5.\nFranklin (1991)\nAlvarez and Brehm (1995)\nClark and Nordstrom (2005)\n\nWeek 6, Sept 25 – Assumptions and Specification - interactions, functional form, measurement of \\(y\\)\n\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 5.\nClark, Nordstrom, and Reed (2008)\nClarke and Stone (2008)\nBerry, Golder, and Milton (2012)\nBrambor, Clark, and Golder (2006)\n\nWeek 7, Oct 2 – No class, Yom Kippur\nWeek 8, Oct 9 – Choice Models I (Unordered \\(y\\) Variables) - MNL, MNP, CL (IIA)\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 9\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 6.\nAlvarez and Nagler (1998)\nLacy and Burden (1999)\nC. J. W. Zorn (1996)\n\nWeek 9, Oct 16– Choice Models II (Unordered Dependent Variables continued, and systems of eqs, ordered)\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 8\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 5.\nFranklin and Kosaki (1989)\n\nWeek 10, Oct 23 – Event Count Models I - poisson, dispersion\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 10\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 8.1, 8.2.\nGowa (1998)\nFordham (1998)\n\nWeek 11, Oct 30 – Event Count Models II - negative binomial, zero-altered\n\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 8.3-8.7.\nC. J. W. Zorn (1998)\nClark (2003)\n\nWeek 12, Nov 6 – Continuous Time Hazard Models I - parametric, semi-parametric models\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 11\nJanet Box-Steffensmeier and Brad Jones. 2004. Event History Modeling. Chs. 1-4\nJ. M. Box-Steffensmeier, Arnold, and Zorn (1997)\n\nWeek 13, Nov 13 – Continuous Time Hazard Models II - parametric models, special topics\n\nJanet Box-Steffensmeier and Brad Jones. 2004. Event History Modeling. Chs. 5-11\nC. J. W. Zorn (2000)\nBennett and Stam (1996)\nJ. Box-Steffensmeier, Reiter, and Zorn (2003)\n\nWeek 14, Nov 20 – Censored/Truncated Variables, Samples - selection models - J. Scott Long. 1997. *Regression Models for Categorical and Limited Dependent Variables}. Chapter 7\n\nReed (2000)\nSignorino (1999)\nTimpone (1998)\n\nWeek 15, Nov 27 – no class, Thanksgiving\nWeek 16, Dec 4 – Review"
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "likelihood24.html",
    "href": "likelihood24.html",
    "title": "Likelihood",
    "section": "",
    "text": "Motivating MLE\n\n\n\nML, OLS?\n\nWhy do we turn to maximum likelihood instead of OLS, especially given the simplicity and robustness of OLS?\nOur data can’t meet the OLS assumptions; \\(y\\) is often limited* such that we observe \\(y\\) but really want to measure \\(y^*\\).\n\\(y^*\\) is often a continuous (unlimited) variable we wish we could measure - if we could, we’d use OLS to estimate its correlates.\nOLS asks us to make the data satisfy the model. MLE asks us to build a model based on the data. Since our data often don’t satisfy the OLS assumptions, MLE offers a flexible alternative.\n\n\n\n\nLimited observability\nFor example, we might be interested in what leads citizens to vote in elections.\n\nwe observe an individual either does (1) or does not (0) turn out to vote.\nwhat we’d like to know is the probability that individual votes.\n\nOur ability to observe the latent variable, \\(y^*\\) is limited.\n\n\n\nLimited observability\n\n\ncode\nz &lt;- seq(-5,5,.1)\nl &lt;- seq(0,1,.01)\ns1 &lt;- 1/(1+exp(-z))\ns2 &lt;- pnorm(z)\n\ndf &lt;- data.frame(z=z, l=l, s1=s1, s2=s2)\n\nggplot(df, aes(x=z, y=l)) + \n  geom_line(aes(x=z, y=l), color=\"black\")+\n  geom_line(aes(x=z, y=s1), color=\"red\")+\n  geom_line(aes(x=z, y=s2), color=\"green\")+\n  labs(title=\"Rates of change\", x=\"z\", y=\"F(z)\")+\n  theme_minimal()+\n  transition_reveal(z)\n\n\n\n\n\n\n\n\n\n\n\ncode\n# Generate data\nx &lt;- seq(-5, 5, by = 0.1)\nnormal_cdf &lt;- pnorm(x)\nlogistic_cdf &lt;- plogis(x)\nlinear &lt;- (x + 5) / 10\n\n# Create highchart\nhc &lt;- highchart() %&gt;%\n  hc_chart(type = \"line\") %&gt;%\n  hc_title(text = \"Comparison of CDFs and Linear Function\") %&gt;%\n  hc_xAxis(title = list(text = \"x\"), min = -5, max = 5) %&gt;%\n  hc_yAxis(title = list(text = \"Probability / Value\"), min = 0, max = 1) %&gt;%\n  hc_tooltip(shared = TRUE, crosshairs = TRUE) %&gt;%\n  hc_legend(align = \"center\", verticalAlign = \"bottom\", layout = \"horizontal\")\n\n# Add series with animation\nhc &lt;- hc %&gt;%\n  hc_add_series(\n    data = lapply(seq_along(x), function(i) list(x[i], normal_cdf[i])),\n    name = \"Normal CDF\",\n    animation = list(duration = 2000)\n  ) %&gt;%\n  hc_add_series(\n    data = lapply(seq_along(x), function(i) list(x[i], logistic_cdf[i])),\n    name = \"Logistic CDF\",\n    animation = list(duration = 20000)\n  ) %&gt;%\n  hc_add_series(\n    data = lapply(seq_along(x), function(i) list(x[i], linear[i])),\n    name = \"Linear Function\",\n    animation = list(duration = 8000)\n  )\n\nhc\n\n\n\n\n\n\n\n\n\nLimited observability\n\n\n\n\n\n\n\n\n\n\n\n\nML\nML asks us to think of the data as given and to imagine the model that might best represent the Data Generating Process.\n\nIn OLS, the model is fixed - the assumptions about \\(\\epsilon\\) are given.\nIn ML, the data are fixed - we construct a model that reflects the nature of the data.\nWhat we assume about the unobservables is informed by what we know about the observables, the \\(y\\) variable.\nA useful way to describe \\(y\\) is to characterize its observed distribution.\nOnce we know the distribution of \\(y\\), we can begin building a model based on that distribution.\n\n\n\n\nLimited Dependent Variables\nWe are limited in what we can observe and/or measure in \\(y\\). E.g., we observe an individual voting or not; we cannot observe the chances an individual votes.\n\nThe \\(y\\) variable has both observed and latent qualities; label the latent variable \\(\\widetilde{y}\\).\nOften, we are more interested in this latent variable. We are more interested in the latent chance of voting than the observed behavior.\n\\(\\widetilde{y}\\) is the variable we wish we could measure.\n\n\n\n\nLimited Dependent Variables\n\nThe latent and observed variables are often distributed differently. Observed voting \\(y=(0,1)\\); latent variable \\(Pr(y=1)\\).\nLinking \\(X\\) variables to the latent \\(\\widetilde{y}\\) requires rescaling; this is often because changes in \\(\\widetilde{y}\\) given \\(X\\) are nonlinear.\nBesides, \\(\\widetilde{y}\\) is unobserved - so we have to generate it from the model.\nTo link \\(X\\) with \\(\\widetilde{y}\\) or \\(y\\), we assume their relationship follows some distribution; we call this the link distribution.\n\n\n\n\nThe big point\nOur data are often limited, and not suitable for OLS. OLS asks us to transform data to make it meet the model assumptions (this is Generalized Least Squares); MLE builds the model assumptions based on the data.\n\n\n\nBuilding an ML estimator\nWhat do we need to build an ML estimator?\n\n\\(Y\\) variable distribution.\nuse that distribution to write a function connecting \\(x\\beta\\) (the linear estimates), to \\(Y\\).\nuse another another function to link the linear prediction, \\(x\\beta\\) to \\(\\tilde{y}\\), to map the linear prediction onto the latent variable we wish we could measure.\n\n\n\n\nWarning\nI’m foreshadowing here, so don’t come unglued if the next slide seems foreign.\n\n\n\nFor example\n\n\\(Y = (0,1)\\), does an individual vote or not. This is binary, discrete, let’s say binomial.\n\\(\\tilde{Y}\\) is the latent probability an individual votes. Wish we could measure this, use OLS. :(\nWrite a function based on \\(Y\\): \\(\\pi^y (1-\\pi)^{(1-y)}\\) , the binomial.\n\\(\\pi\\) is given by some \\(X\\) variables we think affect voting - so \\(\\pi = x\\beta\\).\nSo substitute for \\(\\pi\\) \\((x\\beta)^y (1-x\\beta)^{(1-y)}\\)\nNow, link the linear prediction to \\(\\tilde{Y}\\) - map \\(x\\beta\\) onto the Pr(vote). How about using the Standard Normal CDF \\(\\Phi\\)?\n\n\\(\\Phi(x\\beta)^y  \\Phi(1-x\\beta)^{(1-y)}\\) - this is now the core of the Probit.\n\n\n\nComing up next\nThat’s what we want to be able to do - so in the coming parts, we’ll:\n\nreview ML theory, using Gary King’s notation.\nbuild a likelihood function, and compare to OLS.\nlook at the technology of ML - how do we actually get estimates from a likelihood function?\n\n\n\n\nLikelihood Theory\nProbability and Likelihood differ from one another principally by how they treat the data and model in relation to one another.\nProbability theory presumes some given model (or set of parameters) and seeks to estimate the data, given those parameters.\n\n\n\nLikelihood\nKing (pp. 9, 14) puts it this way:\n\\[Y\\sim f(y|\\theta,\\alpha)\\]\nand\n\\[\\theta = G(X,\\beta)\\]\nso our data, \\(Y\\) has a probability distribution given by parameters \\(\\theta, \\alpha\\), and \\(\\theta\\) is a function of some variables, \\(X\\) and their parameters, \\(\\beta\\).\n\n\n\nLikelihood\nAll of this comprises the model in King’s lingo, so a basic probability statement appears as:\n\\[Pr(y|\\mathcal{M}) \\equiv Pr(\\mathrm{data|model})\\]\nThis is a conditional probability resting on two conditions:\n\nThe data are random and unknown.\nThe model is known.\n\nUh oh. The model is known? The data aren’t? This works in many probability settings. What is the probability of rolling 3 threes in 6 rolls of a six-sided die? What is the probability you draw an ace from a standard deck of cards? In cases like these, the model is known even before we observe events (data).\n\n\n\nLikelihood\nIn cases like these, the model’s parameters are known and fixed. In our applications, the model and its parameters are the unknowns, but the events or data are known, fixed, and given.\nSuppose I flip a coin to decide whether I roll a 20-sided die, or a 6-sided die. You do not observe any of this - I only tell you I rolled a 4 - which die did I roll? This is the problem we try to deal with in ML - what is the data generating process that most likely produced the observed data. Unlike the simple die roll, the model isn’t known. Instead, there are many possible models that could have produced the data.\n\n\n\nInverse Probability\nGiven these conditions, the more sensible model would be:\n\\[Pr(\\mathcal{M}|y)\\]\nThis is the inverse probability model - but it requires knowledge (or strong assumptions) regarding an important element of the (unknown) model, \\(\\theta\\). Even Bayes can’t really do this.\n\n\n\nLikelihood\nLikelihood estimates the model, \\(\\mathcal{M}\\) given the data, but assumes that \\(\\theta\\) can take on different values, representing different (competing) hypothetical models.\nThe set of \\(\\theta\\)’s are the competing models or data generating processes that could have produced the observed data set.\n\n\n\nLikelihood\nKeeping with King’s notation, the likelihood axiom is:\n\\[L(\\tilde{\\theta}|y,\\mathcal{M}*)\\equiv L(\\tilde{\\theta}|y) \\] \\[=k(y)Pr(y|\\tilde{\\theta})\\] \\[\\propto Pr(y|\\tilde{\\theta})\\]\nwhere \\(\\tilde{\\theta}\\) represents the hypothetical value of \\(\\theta\\) (rather than its true value).\n\n\n\nLikelihood\nThe term \\(k(y)\\) (known as the “constant of proportionality”) is a constant across all the hypothetical values of \\(\\tilde{\\theta}\\) (and thus drops out) but is the key to the likelihood axiom; it represents the functional form through which the data (\\(y\\)) shape \\(\\tilde{\\theta}\\) and thus allow us to estimate the likelihood as a measure of relative (instead of absolute) uncertainty; our uncertainty in this case is relative to the other possible functions of \\(y\\) and the hypothetical values of \\(\\tilde{\\theta}\\).\nAs King (p. 61) puts it , \\(k(y)\\) “measures the relative likelihood of a specified hypothetical model \\(\\tilde{\\beta}\\) producing the data we observed.”\nIt turns out that the likelihood of observing the data is proportional to the probability of observing the data.\n\n\n\nTake Away Points\n\nWe want to know the probability (the model) of observing some data; if we find the model parameters with the highest likelihood of generating the observed data, we also know the probability because the two are proportional.\nLikelihoods are always negative; they do not have a scale or specific meaning; they do not transform into probabilities or anything familiar.\nWe find the parameter values that produce the largest likelihood values; those parameters that maximize the likelihood then can be translated into sensible quantities - they can be mapped onto \\(\\tilde{y}\\), giving us a measure of the variable we wish we had.\nIn OLS, we compute parameter estimates that minimize the sum (squared) distance of all the observed points to the predicted points - we minimize the errors in this fashion.\nThe technology of MLE is trial and error - choose some values for \\(\\tilde{\\theta}\\), compute the likelihood, then repeat. Compare all the likelihoods - the values of \\(\\tilde{\\theta}\\) that produced the highest likelihood value are the ones that most likely generated the observed data.\n\n\n\n\nLikelihood: a linear model\nSuppose you have data, \\(Y\\), where:\n\\[Y\\sim N(\\mu,\\sigma^{2})\\] \\[E(Y)=\\mu\\]\n\\[var(Y)=\\sigma^{2}\\]\n\\(Y\\) is distributed normally with appropriate distribution parameters that measure the moments of \\(Y\\).\nGiven these data, we want to know what values of \\(\\mu, \\sigma^2\\) most likely produced the data.\n\n\n\nWriting the Likelihood\nSince we’ve determined \\(Y\\) is normal, write the Normal PDF.\n\\[Pr(Y=y_{i})=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{\\left[\\frac{-(y_{i}-\\mu_{i})^{2}}{2\\sigma^{2}}\\right]}\\]\n\n\n\nWriting the Likelihood\nWe’re interested in the joint probability of sample the observations, the probability the data result from a particular Data Generating Process. Assuming the observations in the data are independent of one another, the joint density is equal to the product of the marginal probabilities:\n\\[Pr(A~ \\mathrm{and}~ B)=Pr(A)\\cdot Pr(B)\\]\nso the joint probability of \\(Y\\) is given by\n\\[Pr(Y=y_{i} \\forall i) = L(Y|\\mu, \\sigma^{2})= \\prod\\limits_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{\\left[\\frac{-(y_{i}-\\mu_{i})^{2}}{2\\sigma^{2}}\\right]}\\]\n\n\n\nWriting the Likelihood\nBut this assumes the parameters are given - they’re the unknowns. Since the likelihood is proportional to the probability of the data given the parameters, we can write a likelihood function:\n\\[\\mathcal{L}(\\mu, \\sigma^{2}|Y) = k(y)\\prod\\limits_{i=1}^{n} f_{normal}(Y|\\mu, \\sigma^{2}) \\propto \\prod\\limits_{i=1}^{n} f_{normal}(Y|\\mu, \\sigma^{2})\\]\n\\[= \\prod\\limits_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}}e^{\\left[\\frac{-(y_{i}-\\mu_{i})^{2}}{2\\sigma^{2}}\\right]}\\]\n\n\n\nWriting the Log Likelihood\nAdding is easier than multiplying; since we can transform the likelihood function by any monotonic form, we can take its natural log to replace the products with summations:\n\\[\\ln \\mathcal{L} (\\mu, \\sigma^{2}|Y) = \\ln \\prod\\limits_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{\\left[\\frac{-(y_{i}-\\mu_{i})^{2}}{2\\sigma^{2}}\\right]}\\]\n\\[= \\sum \\ln \\left[\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{\\left[\\frac{-(y_{i}-\\mu_{i})^{2}}{2\\sigma^{2}}\\right]} \\right]\\]\n\\[=\\sum\\left( -\\frac{1}{2}(\\ln(2\\pi))-\\frac{1}{2}(\\ln(\\sigma^{2}))-\\frac{1}{2\\sigma^{2}}\\left[\\sum\\limits_{i=1}^{n}(y_{i}-\\mu)^{2}\\right] \\right)\\]\n\n\n\nThe Linear model\nIt should be pretty evident this is the linear model.\n\nwe started with data that looked normal; continuous, unbounded, infinitely differentiable.\nassuming normality, we wrote a LLF in terms of the distribution parameters \\(\\mu, \\sigma^2\\).\nbut we want \\(\\mu\\) itself to vary with some \\(X\\) variables; \\(y\\) isn’t just characterized by a grand mean, but by a set of of conditional means given by \\(X\\).\nso we need to parameterize the model - write the distribution parameter as a function of some variables.\ngenerally, this is to declare \\(\\theta = F(x\\beta)\\).\nin the linear/normal case, to declare \\(\\mu=F(x\\beta)\\). Of course, \\(F\\) is linear, we just write \\(\\mu=x\\beta\\).\nin the LLF, substitute \\(x\\beta\\) for \\(\\mu\\), and now we’re taking the difference between \\(y\\) and \\(x\\beta\\), squaring those differences, and weighting them by the variance.\n\n\nTo put it all together \\(\\ldots\\)\n\\[\\ln L(\\mu, \\sigma^{2}|Y) = \\ln \\prod\\limits_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} exp \\left[\\frac{-(y-x\\beta)^{2}}{2\\sigma^{2}}\\right] \\] \\[= \\sum \\ln \\left\\{\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} exp \\left[\\frac{-(y-x\\beta)^{2}}{2\\sigma^{2}}\\right]\\right\\}\\]\n\\[= \\sum\\left(-\\frac{1}{2}(\\ln(2\\pi)) -\\frac{1}{2}(\\ln(\\sigma^{2})) -\\frac{1}{2\\sigma^{2}}\\left[\\sum\\limits_{i=1}^{n}(y-x\\beta)^{2}\\right] \\right)\\]\nDoes this look familiar? A lot like deriving OLS, eh?\n\n\n\nLinear regression in ML\nThis is the Normal (linear) log-likelihood function. It presumes some data, \\(\\mathbf{Y}\\) and some unknowns \\(\\mathbf{\\beta, \\sigma^2}\\). You should note the kernel of the function is the sum of the squared differences of \\(y\\) and \\(x\\beta\\).\n\\[\\ln\\mathcal{L} =\\sum\\left(-\\frac{1}{2}(\\ln(2\\pi)) -\\frac{1}{2}(\\ln(\\sigma^{2})) -\\frac{1}{2\\sigma^{2}}\\left[\\sum\\limits_{i=1}^{n}(y-x\\beta)^{2}\\right] \\right)\\]\nIt turns out if we take the derivative of this function with respect to \\(\\beta\\) and \\(\\sigma^2\\), the result is the ML estimator, and the OLS estimator. They’re the same.\n\n\nLinearity\nThis model is linear - our estimates, \\(x\\beta\\) map directly onto \\(y\\) - there is no latent variable, \\(\\tilde{y}\\) to map onto - so \\(x\\beta = \\widehat{y}\\).\nNote that this is not because \\(y\\) is normal. Rather, the fact that \\(y\\) contains a lot of information and is not limited makes it more likely \\(y\\) is normal.\nThough ML is most often used in cases where \\(y\\) is limited (so OLS is inappropriate), it’s important to see that we can estimate the linear model using either technology (OLS, ML) and get the same estimates.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "likelihood24.html#how-would-you-characterize-the-variable-measuring-deaths-by-mule-kick",
    "href": "likelihood24.html#how-would-you-characterize-the-variable-measuring-deaths-by-mule-kick",
    "title": "Likelihood",
    "section": "How would you characterize the variable measuring deaths by mule kick?",
    "text": "How would you characterize the variable measuring deaths by mule kick?\n\nVariable measures events.\nEvents are discrete, not continuous.\nAre events correlated or independent?\nVariable is bounded.\nWould OLS be appropriate?\nWhat specific problems do we need to accommodate here?\n\nGiven what we know about the dependent variable itself, we can begin to think about an appropriate model:\n\nThe model must accommodate a discrete dependent variable.\nWe might have two quantities of interest - the expected number of events (\\(E[Y]\\)), and the probability of any given number of events (\\(Pr(Y=j)\\)).\nWe need to make sure that the \\(X\\) variables we think cause increases or decreases in the number of deaths cannot produce nonsensical effects (predictions less than zero, for instance).\n\nSo we need to think about two different things here - the distribution of \\(\\epsilon\\) based on the observed distribution of \\(y\\), and the link between the \\(X\\) variables and \\(\\widetilde{y}\\), the latent quantity of interest.\n\nWhat distribution might describe the frequency of mule kick deaths?\nNeeds to be discrete.\nNeeds to characterize rare events - at most we see about four per period, so relatively rare.\nNeeds to have a lower bound at zero (since we can’t observe negative numbers of deaths), and upper bound at \\(+\\infty\\)\n\n\n\\[Pr(Y=y_{i})=\\frac{e^{-\\lambda}\\lambda^{y_{i}}}{y_{i}!}\\]"
  },
  {
    "objectID": "likelihood24.html#the-poisson-distribution",
    "href": "likelihood24.html#the-poisson-distribution",
    "title": "Likelihood",
    "section": "The Poisson Distribution",
    "text": "The Poisson Distribution\n\n\ncode\n#simulate and plot the poisson distribution at 3 different values of the mean\n\nset.seed(123)\nn &lt;- 1000\nlambda &lt;- c(1, 5, 10)\npoisson &lt;- rpois(n, lambda[1])\npoisson2 &lt;- rpois(n, lambda[2])\npoisson3 &lt;- rpois(n, lambda[3])\n\ndf &lt;- data.frame(poisson, poisson2, poisson3)\n\nggplot(df, aes(x=poisson)) + \n  geom_histogram(aes(y=..density..), bins=20, fill=\"blue\", alpha=0.5) + \n  geom_density(aes(y=..density..), color=\"blue\") + \n  geom_histogram(data=df, aes(x=poisson2, y=..density..), bins=20, fill=\"red\", alpha=0.5) + \n  geom_density(data=df, aes(x=poisson2, y=..density..), color=\"red\") + \n  geom_histogram(data=df, aes(x=poisson3, y=..density..), bins=20, fill=\"green\", alpha=0.5) + \n  geom_density(data=df, aes(x=poisson3, y=..density..), color=\"green\") + \n  labs(title=\"Poisson Distribution\", x=\"Deaths\", y=\"Density\") + \n  theme_minimal() + \n  theme(legend.position=\"none\")\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\nThinking in terms of the data, let’s write a likelihood function, the joint probability for all \\(i\\) observations in the sample, \\(n\\):\n\\[\nL(\\lambda)= \\prod_{i=1}^{n} \\left[\\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!} \\right]\n\\]\nTake the natural log of the likelihood function:\n\\[\n\\ln L(\\lambda)= \\ln \\left\\{\\prod_{i=1}^{n} \\left[\\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!} \\right] \\right\\}\\nonumber \\\\\n\\ln L(\\lambda)= \\sum_{i=1}^{n} \\left[-\\lambda + y_i \\ln(\\lambda) - \\ln(y_i!) \\right]\n\\]\nWhat about the \\(X\\) variables? Parameterize the model with respect to those variables such that they influence the mean, \\(\\lambda\\). So let’s make \\(\\lambda\\) a function of the \\(X\\) variables and their effects, \\(\\beta\\), such that,\n\\[\nE[Y|X]=\\lambda =F(\\beta X)\n\\]\nwhere \\(F(\\beta X)\\) is some link function, \\(F\\) correctly scaling \\(\\beta X\\) to \\(y\\), and \\(\\widetilde{y}\\). Remembering that we cannot have negative predictions, suppose we let \\(F\\) be the exponential distribution which is nonnegative, unbounded to the right, and nonlinear with respect to changes at the limit.\nPutting all this together using the exponential distribution as the link, we have:\n\\[\nE[Y|X]=\\lambda =e^{X\\beta} \\\\\n\\ln L(\\lambda)= \\ln \\left\\{\\prod_{i=1}^{n} \\left[\\frac{e^{-e^{\\beta X}}  e^{(\\beta X)^{y_i}}}{y_i!} \\right] \\right\\} \\\\\n\\ln L(\\lambda)= \\sum_{i=1}^{n} \\left[-e^{\\beta X} + y_i \\ln(\\beta X) - \\ln(y_i!) \\right]\n\\]\nLet’s derive another LLF - note that we start with the data and move toward the model. Suppose we have data on the number of civil wars in Africa over a ten year period, and the data are as follows:\n\\(Y\\) = {5 0 1 1 0 3 2 3 4 1}\n\\[\nY \\sim f_{poisson}(\\lambda)=  \\frac{e^{-\\lambda}\\lambda^{y_{i}}}{y_{i}!}\\nonumber\n\\]\nThe likelihood is given by the joint density\n(\\(L(y_{1},y_{2},y_{3}\\ldots y_{10})\\)):\n\\[\nL(\\lambda|Y) = \\prod\\limits_{i=1}^{10} f(y_{i},\\lambda)=  \\prod\\limits_{i=1}^{10} \\frac{e^{-\\lambda}\\lambda^{y_{i}}}{y_{i}!}\\nonumber \\\\ \\nonumber \\\\\n= \\frac{e^{-10\\lambda}\\lambda^{\\sum{y_{i}}}}{\\prod y_{i}!}\\nonumber \\\\ \\nonumber \\\\\n= \\frac{e^{-10\\lambda}\\lambda^{20}}{207360}\\nonumber\n\\]\nHere’s what we know so far:\nwe begin with some data, \\(Y\\). We want to find the model most likely responsible for generating these data. consider the distribution of \\(y\\) - describe that distribution. write a log-likelihood function for the sample; it should characterize the distribution of \\(y\\), and should link the linear prediction (\\(x\\beta\\)) with the quantity of interest, \\(\\tilde{y}\\).\nSo, we have data, and we have a likelihood function - what do we do with them?"
  },
  {
    "objectID": "likelihood24.html#estimation-technology-ols",
    "href": "likelihood24.html#estimation-technology-ols",
    "title": "Likelihood",
    "section": "Estimation Technology: OLS",
    "text": "Estimation Technology: OLS\nRecall that the technology of OLS is to assume a normally distributed error term, minimize the sum of those squared errors analytically using calculus."
  },
  {
    "objectID": "likelihood24.html#estimation-technology-mle",
    "href": "likelihood24.html#estimation-technology-mle",
    "title": "Likelihood",
    "section": "Estimation Technology: MLE",
    "text": "Estimation Technology: MLE\nThe technology of ML is to maximize the LLF with respect to \\(\\beta\\). We can do this in several different ways:\n\nanalytic methods - use calculus. Some/many models do not have analytical or closed form solutions.\nnumerical methods - use an algorithm to estimate starting values for \\(\\theta\\), then hill climb until the first derivative is zero, and the second derivative is negative. This is iterative, trying values, looking at the derivatives. This is what nearly all ML estimation uses - there are many different algorithms for doing this. \n\n\nAnalytic Methods\nWith some functions, we can solve for the unknowns directly:\n\\[\\begin{aligned}\n\\ln L(\\lambda|Y) =\\ln \\left[ \\prod\\limits_{i=1}^{N} \\frac{e^{-\\lambda}\\lambda^{y_{i}}}{y_{i}!}\\right]\\nonumber \\\\ \\nonumber \\\\\n=-N \\lambda+ \\sum(y_{i}) \\ln(\\lambda) - \\sum(\\ln(y_{i}!)) \\nonumber\n\\end{aligned}\\]\nTaking the derivative with respect to \\(\\lambda\\) and setting equal to zero:\n\\[\n\\frac{\\partial \\ln L}{\\partial \\lambda}=-N \\lambda+ \\sum(y_{i}) \\ln(\\lambda) - \\sum(\\ln(y_{i}!)) \\nonumber \\\\ \\nonumber  \\\\\n0=-N + \\frac{\\sum y_{i}}{\\lambda} \\nonumber\\\\ \\nonumber \\\\\n\\widehat{\\lambda}= \\frac{\\sum y_{i}}{N} \\nonumber\n\\]\nThis is just the sample mean of course:\n\\[\n\\ln L(\\lambda|Y) = \\ln \\left[ \\frac{e^{-10\\lambda}\\lambda^{20}}{207360}\\right] \\nonumber \\\\\n= -10 \\lambda+ 20 \\ln(\\lambda) - \\ln(207360) \\nonumber\n\\]\nand now find the derivative of the log-likelihood, again with respect to \\(\\lambda\\):\n\\[\n\\frac{\\partial \\ln L}{\\partial \\lambda}= -10 \\lambda+ 20 \\ln(\\lambda) - \\ln(207360) \\nonumber\n=-10 + \\frac{20}{\\lambda} \\nonumber\\\\ \\nonumber \\\\\n\\widehat{\\lambda}= \\frac{20}{10} \\nonumber \\\\\n\\widehat{\\lambda}= 2 \\nonumber\n\\]\nso the value of \\(\\lambda\\) that maximizes the likelihood of observing the civil war data is 2.\nWith some functions, we can solve for the unknowns directly. Stick with me these next couple slides b/c they end in an important point about MLE-OLS."
  },
  {
    "objectID": "likelihood24.html#normal-linear-llf",
    "href": "likelihood24.html#normal-linear-llf",
    "title": "Likelihood",
    "section": "Normal (linear) LLF:",
    "text": "Normal (linear) LLF:\n\\[\n= -\\frac{N}{2}(\\ln(2\\pi)) -\\frac{N}{2}(\\ln(\\sigma^{2})) -\\frac{1}{2\\sigma^{2}}\\left[\\sum\\limits_{i=1}^{n}(y_{i}-\\mu)^{2}\\right] \\nonumber\n\\]\nNotice \\(N\\) in the numerator; recall the rule of summation that \\(\\sum\\limits_{i=1}^{n}a= n\\cdot a\\).\nNow, take the derivative of the log-likelihood with respect to each of the parameters in turn (ignoring constant terms and terms that pertain exclusively to the other parameter).\n\\[\n\\frac{\\partial \\ln L}{\\partial \\mu}= \\frac{1}{\\sigma^{2}}\\sum(y_{i}-\\mu)=0 \\nonumber\\\\\n=\\sum(y_{i}-\\mu) = \\sum y_{i}- \\sum \\mu  \\nonumber\\\\\n=\\sum y_{i}- N \\mu = 0 \\nonumber \\\\\n\\mu=\\frac{\\sum y_{i}}{N} = \\widehat{y}\\nonumber\n\\]\nwe can also solve for \\(\\sigma^{2}\\), getting\n\\[\n\\frac{\\partial \\ln L}{\\partial \\sigma^{2}}= -\\frac{N}{2 \\sigma^{2}}+\\frac{1}{2 \\sigma^{4}} +\\sum(y_{i}-\\mu)=0 \\nonumber\\\\ \\nonumber\\\\\n=-\\frac{N}{2}\\sigma^{2}+\\frac{1}{2}\\sum(y_{i}-\\bar{y})^{2}= 0 \\nonumber\\\\ \\nonumber\\\\\n\\ldots\n\\widehat{\\sigma^{2}}=\\frac{\\sum(y_{i}-\\bar{y})^{2}}{N} \\nonumber\n\\]\nThis is a biased estimator of \\(\\sigma^{2}\\); \\(\\sigma^{2}\\) is underestimated because the denominator should be \\(N-1\\).\nThe same thing in matrix notation:\n\\[ln\\mathcal{L}(y | X, \\beta, \\sigma^2) = -\\frac{N}{2} ln(2\\pi) - \\frac{N}{2} ln(\\sigma^2) -\\frac{1}{2} \\left[ \\frac{(y-X\\beta)'(y-X\\beta)}{\\sigma^2} \\right] \\]\nrewriting to isolate the parameters:\n\\[\nln\\mathcal{L}(y | X, \\beta, \\sigma^2) = -\\frac{N}{2} ln(2\\pi) - \\frac{N}{2} ln(\\sigma^2) -\\frac{1}{2\\sigma^2} \\left[ yy'- 2y' X\\beta +\\beta' X' X\\beta) \\right]\n\\]\nTake derivatives of \\(\\ln \\mathcal{L}\\) w.r.t. \\(\\beta\\) and \\(\\sigma^2\\) (and skipping a lot here):\n\\[\\frac{\\partial ln \\mathcal{L}}{\\partial \\beta} = \\frac{1}{\\sigma^2} (X'y - X'X \\beta)\\]\nsetting equal to zero …\n\\[ \\frac{1}{\\sigma^2} (X'y - X'X \\beta) = 0\\] \\[X'X \\beta = X'y\\] \\[\\widehat{\\beta} = (X'X)^{-1} X' y \\]\n…going through the same thing for \\(\\sigma^2\\) gives us:\n\\[\\widehat{\\sigma^2} = \\frac{e'e}{N}\\]\nSo aside from seeing how analytic methods work, we have also seen that the BLUE OLS estimator is the ML estimator for \\(\\beta\\), and that the variance estimate in ML is biased downward (the denominator is always too large by \\(k-1\\)). This difference disappears in large samples.\nWhy do we leave OLS if these are the same? Because this is the rare case defined by normal data which both satisfies the OLS requirement for a normal disturbance, and permits MLE estimation with a normal LLF. With non-normal data, OLS and ML estimators diverge quite a lot."
  },
  {
    "objectID": "likelihood24.html#numerical-methods",
    "href": "likelihood24.html#numerical-methods",
    "title": "Likelihood",
    "section": "Numerical Methods",
    "text": "Numerical Methods\nNumerical methods are computationally intensive ways to plug in possible parameter values, generate a log likelihood, and then use calculus to evaluate whether the that value is a maximum. We use numerical methods when no analytic or ``closed form’’ solution exists.\nDo this by evaluating:\n\nthe first derivative of the LLF - by finding the point on the function where a tangent line has a slope equal to zero, we know we’ve found an inflection point.\nthe second derivative of the LLF - if the rate of change in the function at the very next point is increasing, it’s a minimum; decreasing, it’s a maximum.\n\nthe Hessian matrix - the matrix of second derivatives - tells us the curvature of the LLF, or the rate of change.\n\nSuppose that we have the event count data reported above representing civil wars in Africa, and that we want to compute the likelihood of \\(\\lambda|Y\\). We can compute the likelihood using numerical methods; one specific technique is a grid search procedure. Just as we might try different values for \\(x\\) when graphing a function, \\(f(x)\\) in algebra, we will insert possible values for \\(\\lambda\\) into the log-likelihood function in such a way that we can identify an apparent maximum (a value for \\(\\lambda\\) for which the log-likelihood is at its largest compared to contiguous values of \\(\\lambda\\)). Put another way, we take a guess at the value of \\(\\lambda\\), compute the log-likelihood, and take another guess at \\(\\lambda\\), compute the log-likelihood and compare the two estimates of the likelihood; we repeat this process until a pattern emerges such that we can discern a maximum value.\nThe log-likelihood function for the poisson distributed data on civil wars is\n\\[\n\\ln L(\\lambda|Y)= \\ln \\left[\\frac{e^{-10\\lambda}\\lambda^{20}}{207360}\\right] \\nonumber  \\\\ \\nonumber \\\\\n= -10 \\lambda+ 20 \\ln(\\lambda) - \\ln(207360) \\nonumber\n\\]\nSuppose we make some guesses regarding the value of \\(\\lambda\\), plug them into the function and compare the resulting values of the log-likelihood:\n\n\ncode\n#iterate over lambda, create data frame of lambda and log-likelihood\nlambda &lt;- seq(0.1, 3.5, by=0.1)\nllf &lt;- NULL\nfor (i in 1:length(lambda)){\n  L &lt;- -10*lambda[i] + 20*log(lambda[i]) - log(207360)\n  llf &lt;- data.frame(rbind(llf, c(lambda=lambda[i], ll=L)))\n}\n\n#highchart with reference line at maximum value of the log-likelihood\nhighchart() %&gt;% \n  hc_add_series(llf, \"line\", hcaes(x=lambda, y=ll)) %&gt;% \n  hc_title(text=\"Log-Likelihood Estimates\") %&gt;% \n  hc_subtitle(text=\"Civil Wars in Africa\") %&gt;% \n hc_xAxis(title = list(text = \"Lambda\"), plotLines = list(list(value = 2, color=\"red\"))) %&gt;%\n  hc_yAxis(title = list(text = \"log-likelihood\"), plotLines = list(list(value = max(llf$ll), color=\"red\"))) %&gt;%\n  hc_tooltip(pointFormat = \"Lambda: {point.x}&lt;br&gt;Log-Likelihood: {point.y}\") %&gt;% \n  hc_colors(\"#005A43\") \n\n\n\n\n\n\nWe can see that the largest value of the likelihood is where \\(\\lambda\\) = 2; similarly, 2 is the value of \\(\\lambda\\) that maximizes the log-likelihood as well. And not surprisingly, notice that we have arrived at the same solution we produced in the analytic example above."
  },
  {
    "objectID": "likelihood24.html#grid-search",
    "href": "likelihood24.html#grid-search",
    "title": "Likelihood",
    "section": "Grid Search",
    "text": "Grid Search\nGrid search is another method for maximization. The process is to iteratively try values for the parameters of interest, refining those values as the log-likelihood gets larger and larger. In general, we plug in values for the parameters, compute the likelihood, then identify the largest LL value - the parameters that produce that value are our answer.\nThis method is instructive for how numerical methods work, but not practical in most applications with more than a couple of unknowns."
  },
  {
    "objectID": "likelihood24.html#how-numerical-methods-work",
    "href": "likelihood24.html#how-numerical-methods-work",
    "title": "Likelihood",
    "section": "How numerical methods work",
    "text": "How numerical methods work\n\nChoose starting values of \\(\\beta\\) (sometimes from OLS) to estimate the log-likelihood.\nTake the derivative of the log-likelihood with respect to the parameters to find the gradient}. The gradient (or the gradient matrix, a \\(kxk\\) matrix) tells us the direction of the slope of a line tangent to the curve at the point of the log-likelihood estimate.\nIf the gradient is positive (if the matrix is positive definite), then \\(ln \\mathcal{L}\\) is increasing in \\(\\beta\\) - the slope is increasing, so increase our estimate of \\(\\beta\\) and try again.\nIf the gradient is negative (if the matrix is negative definite), the \\(ln \\mathcal{L}\\) is decreasing in \\(\\beta\\) - the slope is decreasing, so we’ve passed the maximum; choose a smaller value for \\(\\beta\\) and try again.\nAs the log-likelihood approaches the maximum, the gradient approaches zero - the slope of the line tangent to the curve at the point of the log-likelihood estimate is approaching zero, indicating we’re reaching the maximum of the function. Stop the search and evaluate the estimates of \\(\\beta\\) that produced the zero gradient.\nThroughout this process, we need to evaluate the second derivatives in order to figure out the rate at which the slope is changing; this helps us tell how close or far we are from the maximum. The second derivative describes the curvature of the LLF, or the rate of change.\nThe matrix of second derivatives (the Hessian matrix) or its approximation also provide the source of our estimates of the variance, and thus the standard errors.\n\nThe first derivative tells us the direction in which the function is changing. This is obviously important since we’re trying to find the maximum.\nThink of this as trying to figure out when you’re exactly at the top of a hill. The slope (the grade, the gradient) is positive while you’re climbing to the top, it’s zero at the top, and it’s negative on the way down the other side.\nBut is the hill flat or steep? If it’s flat, then the change in the slope between point A and point B is likely to be very small - this, of course, can make it difficult to know exactly when we’re at the top (the maximum). On the other hand, if the hill is very steep, the change in the slope between two points is pretty substantial. Put another way, the rate of change in the slope is larger (faster) the steeper the slope; it’s smaller (slower) the flatter the slope.\nThis matters to maximization because the second derivatives tell us how big (or small) a step we should take up the hill as we try to find the top. Suppose that the function is very flat; as indicated above, the change in the slope between two points would be small, so we can take larger steps in order to try to find the maximum. The second derivatives would tell us that the rate of change is very small, so we should take larger steps.\nThe software performing the estimation will choose the next value of \\(\\beta\\) a bit further away from the last value it tried. On the other hand, if the second derivatives are large so the rate of change is fast, we want to take relatively small steps so we don’t step right over the maximum. In any case, that’s the intuition for why we need to know the matrix of second derivatives."
  },
  {
    "objectID": "likelihood24.html#variance-covariance-matrix",
    "href": "likelihood24.html#variance-covariance-matrix",
    "title": "Likelihood",
    "section": "Variance-Covariance matrix",
    "text": "Variance-Covariance matrix\nEstimating the second derivatives can be a real nightmare in estimation, but it’s important not only for finding the maximum of the function (and therefore in estimating the \\(\\beta\\)s), but for computing the variance-covariance matrix as well. Here are our options:\n\nFind the Hessian. The Hessian is a \\(kxk\\) matrix of the second derivatives of the log-likelihood function with respect to \\(\\beta\\), where the second derivatives are on the main diagonal. Commonly estimated using the Newton-Raphson algorithm.\nFind the information matrix. This is the negative of the expected value of the Hessian matrix, computed using the method of scoring.\n\nOuter product approximation, where we sum the squares of the first derivatives (thus avoiding the second derivatives all together). This is computed using the Berndt, Hall, Hall, and Hausman} or BHHH algorithm."
  },
  {
    "objectID": "likelihood24.html#latent-variable-motivation",
    "href": "likelihood24.html#latent-variable-motivation",
    "title": "Likelihood",
    "section": "Latent Variable Motivation",
    "text": "Latent Variable Motivation\nThere are a couple of (related) ways to motivate the model. Let’s assume a latent quantity we’re interested, denoted \\(y^*\\), but our observations of \\(y\\) are limited to successes (\\(y_i=1\\)) and failures (\\(y_i=0\\)).\n\\[\ny^{*}=\\mathbf{x_i \\beta}+\\epsilon_i \\nonumber\n\\]\nfor \\(y^{*}\\), the latent variable,\n\\[\ny_{i} = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{if $y^{*}_{1}&gt;\\kappa$} \\\\\n         0, & \\mbox{if $y^{*}_{1} \\leq \\kappa$}\n         \\end{array}\n     \\right.\n\\]\nwhere \\(\\kappa\\) is an unobserved threshold.\nMake probabilities statements,\n\\[\nPr(y_i=1) = Pr(y^{*}_{1}&gt;\\kappa) \\nonumber \\\\ \\nonumber \\\\\n=Pr(\\mathbf{x_i \\beta}+\\epsilon_i&gt;\\kappa) \\nonumber \\\\ \\nonumber \\\\\n=Pr(\\epsilon_i&gt;\\kappa-\\mathbf{x_i \\beta}) \\nonumber\n\\]\nNormalizing \\(\\kappa=0\\),\n\\[\nPr(y_i=1)=Pr(\\epsilon_i&gt;-\\mathbf{x_i \\beta}) \\nonumber\n\\]\n\\[\nPr(y_i=1)=Pr(\\epsilon_i&gt;-\\mathbf{x_i \\beta}) \\nonumber \\\\ \\nonumber \\\\\n=1-F(-\\mathbf{x_i \\beta}) \\nonumber\n\\]\nassuming \\(F\\) is symmetric, with unit variance,\n\\[\n\\pi_i= Pr(y_i=1)=1-F(-\\mathbf{x_i \\beta}) = F(\\mathbf{x_i \\beta}) \\\\   \\nonumber \\\\\n1-\\pi_i=Pr(y_i=0)= 1-F(\\mathbf{x_i \\beta})\n\\]"
  },
  {
    "objectID": "likelihood24.html#binomial-likelihood-function",
    "href": "likelihood24.html#binomial-likelihood-function",
    "title": "Likelihood",
    "section": "Binomial Likelihood Function",
    "text": "Binomial Likelihood Function\nThe observed data are binary, assumed binomial (\\(\\pi\\)), \\(y_i=0,1\\). The likelihood function must have two parts, one for cases where \\(y_i=0\\), the other for cases where \\(y_i=1\\). Recalling that \\(\\pi=F(\\mathbf{x_i \\beta})\\), and \\(1-\\pi= 1-F(\\mathbf{x_i \\beta})\\),\n\\[\nPr(y_1,y_2,y_3 \\ldots y_n) =  \\prod_{y=1}F(\\mathbf{x_i \\beta}) \\prod_{y=0}[1-F(\\mathbf{x_i \\beta})]\\nonumber\n\\]\nThis is the joint probability we observe all the data, \\(Y\\), simultaneously. We can rewrite this as the likelihood of observing the data given \\(\\beta\\),\n\\[\n\\mathcal{L} (Y|\\beta) =  \\prod_{i=1}^{N} [F(\\mathbf{x_i \\beta})]^{y_i} [1-F(\\mathbf{x_i \\beta})]^{1-y_i}\\nonumber\n\\]\nAnd take the natural log\n\\[\n\\ln(\\mathcal{L} (Y|\\beta)) = \\ln( \\prod_{i=1}^{N} [F(\\mathbf{x_i \\beta})]^{y_i} [1-F(\\mathbf{x_i \\beta})]^{1-y_i})\\nonumber \\\\ \\nonumber \\\\\n= \\sum_{i=1}^{N} y_i \\ln F(\\mathbf{x_i \\beta})+ (1-y_i) \\ln[1-F(\\mathbf{x_i \\beta})] \\nonumber\n\\]\nNote the two parts of the LLF corresponding to the limited observations in the data, 0,1."
  },
  {
    "objectID": "likelihood24.html#choosing-a-link",
    "href": "likelihood24.html#choosing-a-link",
    "title": "Likelihood",
    "section": "Choosing a Link",
    "text": "Choosing a Link\nLet’s make this a probit model by assuming the link to the latent variable is standard normal, so \\(F(\\cdot)\\sim N_{i.i.d.}(0,1)\\):\n\\[\n\\ln \\mathcal{L} (Y|\\beta) = \\sum_{i=1}^{N} y_i \\ln \\Phi(\\mathbf{x_i \\beta})+ (1-y_i) \\ln[1-\\Phi(\\mathbf{x_i \\beta})] \\nonumber\n\\]\nLogit would look like this:\n\\[\n\\ln \\mathcal{L} (Y|\\beta) = \\sum_{i=1}^{N} \\left\\{ y_i \\ln \\left(\\frac{1}{1+e^{-\\mathbf{x_i \\beta}}}\\right)+ (1-y_i) \\ln \\left[1-\\left(\\frac{1}{1+e^{-\\mathbf{x_i \\beta}}}\\right)\\right] \\right\\}\\nonumber\n\\]"
  },
  {
    "objectID": "likelihood24.html#estimation",
    "href": "likelihood24.html#estimation",
    "title": "Likelihood",
    "section": "Estimation",
    "text": "Estimation\nIdeally, we’d like just to estimate by finding the values of \\(\\beta\\) that maximize the log-likelihood function, and do so analytically (i.e., using calculus). This is what we do in OLS, though with respect to minimizing the sum of the squared residuals. But because the solution is non-linear in \\(\\beta\\), there is no closed form or simple analytic solution.\nAs a result, ML models produce estimates of \\(\\beta\\) by using numerical optimization methods. These are generally iterative attempts to narrow down the range in which the maximum lies by plugging in different values of \\(\\beta\\) until the range is so small, we can safely say we’ve maximized the function using those values of \\(\\beta\\)."
  },
  {
    "objectID": "likelihood24.html#maximization",
    "href": "likelihood24.html#maximization",
    "title": "Likelihood",
    "section": "Maximization",
    "text": "Maximization\nSuppose we have binary data that look like this:\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\nOur question is what is the distribution parameter most likely responsible for having generated these observed data.\nWe need to plug a hypothetical value for the distribution parameter into the log-likelihood function, compute log-likelihoods for each observation, then do the same thing with other hypothetical values. Whichever value produces the biggest log-likelihoods is the value most likely responsible for producing the data we have.\nWhat do we mean by distribution parameter? Well, in the LLF below, we’ve referred to our unknown as \\(F(\\mathbf{x_i \\beta})\\), but we really mean we need an estimate of the parameter \\(\\Theta\\) which represents the effects of the \\(X\\)s via the functional form we’ve imposed by assuming a distribution of \\(\\epsilon\\). In this particular case (for simplicity) we don’t have any \\(X\\) variables.\n\\[\n\\ln(\\mathcal{L} (Y|\\Theta)) = \\sum_{i=1}^{N} y_i \\ln F(\\mathbf{\\Theta})+ (1-y_i) \\ln[1-F(\\mathbf{\\Theta})] \\nonumber\n\\]\nHere’s what we’ll do:\n\nchoose some hypothetical values of \\(\\Theta\\); since this is binary, and our latent variable a probability, let’s choose values from .2 to .8.\ncompute \\(N\\) log-likelihoods for each value of \\(\\Theta\\); N=20, and we have 7 values of \\(\\Theta\\).\nsum the \\(N\\) log-likelihoods for each value of \\(\\Theta\\); so we’ll end up with 7 summed log-likelihoods.\nevaluate the summed log-likelihoods, and see which is largest.\ndeclare the value of \\(\\Theta\\) that produced that largest summed log-likelihood as the parameter most likely to have generated the data.\n\nLet \\(\\Theta=.2\\):\n\\[\n\\ln(\\mathcal{L}_{i=1} (Y|\\Theta)) = 0 *\\ln (.2)+ (1-0) \\ln[1-.2] =-0.2231\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=2} (Y|\\Theta)) = 0 *\\ln (.2)+ (1-0) \\ln[1-.2]=-0.2231 \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=3} (Y|\\Theta)) = 0 *\\ln (.2)+ (1-0) \\ln[1-.2]=-0.2231 \\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=11} (Y|\\Theta)) = 1 *\\ln (.2)+ (1-1) \\ln[1-.2] =-1.609\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=12} (Y|\\Theta)) = 1 *\\ln (.2)+ (1-1) \\ln[1-.2]=-1.609 \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=13} (Y|\\Theta)) = 1 *\\ln (.2)+ (1-1) \\ln[1-.2] =-1.609\\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\sum_{i=1}^{20}=-18.32100 \\nonumber\n\\]\nLet \\(\\Theta=.3\\):\n\\[\n\\ln(\\mathcal{L}_{i=1} (Y|\\Theta)) = 0 *\\ln (.3)+ (1-0) \\ln[1-.3] =-0.3567\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=2} (Y|\\Theta)) = 0 *\\ln (.3)+ (1-0) \\ln[1-.3]=-0.3567 \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=3} (Y|\\Theta)) = 0 *\\ln (.3)+ (1-0) \\ln[1-.3]=-0.3567 \\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=11} (Y|\\Theta)) = 1 *\\ln (.3)+ (1-1) \\ln[1-.3] =-1.204\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=12} (Y|\\Theta)) = 1 *\\ln (.3)+ (1-1) \\ln[1-.3]=-1.204\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=13} (Y|\\Theta)) = 1 *\\ln (.3)+ (1-1) \\ln[1-.3] =-1.204\\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\sum_{i=1}^{20}=-15.60700 \\nonumber\n\\]\nLet \\(\\Theta=.4\\):\n\\[\n\\ln(\\mathcal{L}_{i=1} (Y|\\Theta)) = 0 *\\ln (.4)+ (1-0) \\ln[1-.5] =-0.5108\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=2} (Y|\\Theta)) = 0 *\\ln (.4)+ (1-0) \\ln[1-.5]=-0.5108 \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=3} (Y|\\Theta)) = 0 *\\ln (.4)+ (1-0) \\ln[1-.5]=-0.5108 \\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=11} (Y|\\Theta)) = 1 *\\ln (.4)+ (1-1) \\ln[1-.4] =-0.9163\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=12} (Y|\\Theta)) = 1 *\\ln (.4)+ (1-1) \\ln[1-.4]=-0.9163\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=13} (Y|\\Theta)) = 1 *\\ln (.4)+ (1-1) \\ln[1-.4] =-0.9163\\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\sum_{i=1}^{20}=-14.2711636 \\nonumber\n\\]\nLet \\(\\Theta=.5\\):\n\\[\n\\ln(\\mathcal{L}_{i=1} (Y|\\Theta)) = 0 *\\ln (.5)+ (1-0) \\ln[1-.5] =-0.6931\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=2} (Y|\\Theta)) = 0 *\\ln (.5)+ (1-0) \\ln[1-.5]=-0.6931 \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=3} (Y|\\Theta)) = 0 *\\ln (.5)+ (1-0) \\ln[1-.5]=-0.6931 \\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=11} (Y|\\Theta)) = 1 *\\ln (.5)+ (1-1) \\ln[1-.5] =-0.6931\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=12} (Y|\\Theta)) = 1 *\\ln (.5)+ (1-1) \\ln[1-.5]=-0.6931\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=13} (Y|\\Theta)) = 1 *\\ln (.5)+ (1-1) \\ln[1-.5] =-0.6931\\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\sum_{i=1}^{20}=-13.8629436 \\nonumber\n\\]\nLet’s compare what we have so far:\n\\[llf_{.2}&lt;llf_{.3}&lt;llf_{.4}&lt;llf_{.5}\\]\nYou can probably see some symmetry here due to the fact that half the data are ones, half zeros, so completing:\\~\\\n\\(llf_{.2}&lt;llf_{.3}&lt;llf_{.4}&lt;llf_{.5}&gt;llf_{.6}&gt;llf_{.7}&gt;llf_{.8}\\) \\~\\\nSo \\(\\Theta=.5\\) produces the largest log-likelihood (-13.86) and thus is the parameter most likely to have produced the observed data."
  },
  {
    "objectID": "binarymodels24.html",
    "href": "binarymodels24.html",
    "title": "Binary Response Models",
    "section": "",
    "text": "How can we model a binary \\(y\\) variable?\nDoes OLS (the linear probability model) work sufficiently well?\nHow can we build a maximum likelihood model?"
  },
  {
    "objectID": "binarymodels24.html#linear-probability-model",
    "href": "binarymodels24.html#linear-probability-model",
    "title": "Binary Response Models",
    "section": "Linear Probability Model",
    "text": "Linear Probability Model\nThe linear probability model (LPM) is the OLS linear regression with a binary dependent variable.\nThe main justification for the LPM is that OLS is unbiased (by Gauss Markov). But \\(\\ldots\\)\n\npredictions are nonsensical (linear, unbounded, measures of \\(\\hat{y}\\) rather than \\(y^*\\)).\ndisturbances are non-normal, and heteroskedastic.\nrelation or mapping of \\(x\\beta\\) and \\(y\\) are the wrong functional form (linear)."
  },
  {
    "objectID": "binarymodels24.html#linear-model",
    "href": "binarymodels24.html#linear-model",
    "title": "Binary Response Models",
    "section": "",
    "text": "The model we’ve worked with so far is\n\\[y_i=F(\\beta_0 +  \\beta_1 X_1 + \\beta_2 X_2 \\ldots + \\beta_k X_k + \\epsilon_i) \\]\nwhere \\(F\\) is a linear function, so\n\\[\\hat{y_i}=\\hat{\\beta_0} +  \\hat{\\beta_1} X_1 + \\hat{\\beta_2} X_2 \\ldots + \\hat{\\beta_k} X_k \\nonumber\\\\ \\nonumber\\\\\n=x\\hat{\\beta} \\]\n\\(F\\) is linear, so the model is linear in parameters. \\(x\\hat{\\beta}\\) is the linear prediction and is the quantity of interest, the conditional expected value of \\(y\\).\nNow, consider a situation where \\(y\\) is binary, such that,\n\\[  y = \\left\\{ \\begin{array}{ll}\n         1 \\\\\n         0  \n         \\end{array}\n     \\right. \\]\nand\n\\[ y^* = \\left\\{ \\begin{array}{ll}\n          \\pi_{i}\\\\\n          1-\\pi_{i}\n         \\end{array}\n     \\right. \\]\nwhere \\(y_i^*\\) is what we wish we could measure (say, the probability \\(y_i=1\\)), though we can only measure \\(y_i\\). Now, \\(y^*\\) is going to be our principle quantity of interest.\nSuppose the regression model\n\\[y_i =F(\\beta_0 +  \\beta_1 X_1 + \\beta_2 X_2 \\ldots + \\beta_k X_k + \\epsilon_i) \\]\nwhere \\(F\\) is a nonlinear function relating the linear prediction, \\(x\\beta\\) to \\(y_i^*\\).\n\\[\\widehat{y^*} = F(x\\widehat{\\beta})=F(\\hat{\\beta_0} +  \\hat{\\beta_1} X_1 + \\hat{\\beta_2} X_2 \\ldots + \\hat{\\beta_k} X_k ) \\]\n\\(F\\) is a nonlinear function, so the model is nonlinear in parameters. \\(x\\hat{\\beta}\\) is the linear prediction, but it is not the quantity of interest. Instead, the quantity of interest is \\(F(x\\hat{\\beta})\\) which is equal to \\(y_i^*\\).\n\n\n\n\n\n\nImportant Concept\n\n\n\nThe difference between this model and the OLS linear model is simply that we must transform the linear prediction, \\(x\\hat{\\beta}\\), by \\(F\\) in order to produce predictions. Put differently, we want to map our linear prediction, \\(x\\beta\\) onto \\(y^*\\).}"
  },
  {
    "objectID": "binarymodels24.html#limited-dependent-variables",
    "href": "binarymodels24.html#limited-dependent-variables",
    "title": "Binary Response Models",
    "section": "Limited Dependent Variables",
    "text": "Limited Dependent Variables\nWhy would we measure \\(y_i\\) rather than \\(y_i^*\\)?\nLimited dependent variables are usually limited in the sense that we cannot observe the range of the variable or the characteristic of the variable we want to observe. We are limited to observing \\(y_i\\), and so must estimate \\(y_i^*\\)."
  },
  {
    "objectID": "binarymodels24.html#examples-of-limited-dvs",
    "href": "binarymodels24.html#examples-of-limited-dvs",
    "title": "Binary Response Models",
    "section": "Examples of Limited DVs",
    "text": "Examples of Limited DVs\n\nbinary variables: 0=peace, 1=war; 0=vote, 1=don’t vote.\nunordered or nominal categorical variables: type of car you prefer: Honda, Toyota, Ford, Buick; policy choices; party or candidate choices.\nordered variables that take on few values: some survey responses.\ndiscrete count variables; number of episodes of scarring torture in a country-year, 0, 1, 2, 3, …, \\(\\infty\\); the number of flawed computer chips produced in a factory in a shift; the number of times a person has been arrested; the number of self-reported extramarital affairs; number of visits to your primary care doctor.\ntime to failure; how long a civil war lasts; how long a patient survives disease; how long a leader survives in office."
  },
  {
    "objectID": "binarymodels24.html#binary-dependent-variables",
    "href": "binarymodels24.html#binary-dependent-variables",
    "title": "Binary Response Models",
    "section": "Binary dependent variables",
    "text": "Binary dependent variables\nGenerally, we conceive of a binary variable as being the observable manifestation of some underlying, latent, unobserved continuous variable.\nIf we could adequately observe (and measure) the underlying continuous variable, we’d use some form of OLS regression to analyze that variable."
  },
  {
    "objectID": "binarymodels24.html#linear-probability-model-1",
    "href": "binarymodels24.html#linear-probability-model-1",
    "title": "Binary Response Models",
    "section": "Linear Probability Model",
    "text": "Linear Probability Model\nThe linear probability model (LPM) is the OLS linear regression with a binary dependent variable.\nThe main justification for the LPM is OLS is unbiased (by Gauss Markov). But \\(\\ldots\\)\n\npredictions are nonsensical (linear, unbounded, measures of \\(\\hat{y}\\) rather than \\(y^*\\)).\ndisturbances are non-normal, and heteroskedastic.\nrelation or mapping of \\(x\\beta\\) and \\(y\\) are the wrong functional form (linear)."
  },
  {
    "objectID": "binarymodels24.html#running-example---democratic-peace-data",
    "href": "binarymodels24.html#running-example---democratic-peace-data",
    "title": "Binary Response Models",
    "section": "Running example - Democratic Peace data",
    "text": "Running example - Democratic Peace data\nAs a running example, I’ll use the Democratic Peace data to estimate logit and probit models. These come from Oneal and Russett (1997)’s well-known study in ISQ. The units are dyad-years; the \\(y\\) variable is the presence or absence of a militarized dispute, and the \\(x\\) variables include a measure of democracy (the lowest of the two Polity scores in the dyad), and a set of controls. The principle expectation here is that as the lowest democracy score in the dyad increases, the probability of a militarized dispute decreases.\n\nPredictions out of bounds\nThis figured plots the predictions from a logit and OLS model. Unsurprisingly, the logit predictions are probabilities, so are in the \\([0,1]\\) interval. The OLS predictions are not, and are often out of bounds.\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\nlogitpreds &lt;- predict(m1, type=\"response\")\n\nmols &lt;-lm(dispute ~ border+deml+caprat+ally, data=dp )\nolspreds &lt;- predict(mols)\n\ndf &lt;- data.frame(logitpreds, olspreds, dispute=as.factor(dp$dispute))\n\nggplot(df, aes(x=logitpreds, y=olspreds, color=dispute)) + \n  geom_point()+\n  labs(title=\"Predictions from Logit and OLS\", x=\"Logit Predictions\", y=\"OLS Predictions\")+\n  geom_hline(yintercept=0)+\n  theme_minimal() +\n  scale_color_manual(values=c(\"#005A43\", \"#6CC24A\")) +\n  annotate(\"text\", x=.05, y=-.05, label=\"2,147 Predictions out of bounds\", color=\"red\")\n\n\n\n\n\n\n\n\n\nHere’s the distribution of predictions from the OLS model - you’ll note the modal density is around .04 (which is the sample frequency of \\(y\\).), but that a substantial and long tail are negative, so out of probability bounds.\n\n\ncode\nggplot(df, aes(x=olspreds)) + \n  geom_density(alpha=.5)+\n  labs(title=\"Density of OLS Predictions\", x=\"Predictions\", y=\"Density\")+\n  theme_minimal()+\ngeom_vline(xintercept=0, linetype=\"dashed\")\n\n\n\n\n\n\n\n\n\n\n\nHeteroskedastic Residuals\nThe residuals from the OLS model are heteroskedastic, and the distribution is not normal. In fact, the distribution appears more binomial, clustered around zero and one. This shouldn’t be surprising since the \\(y\\) variable only takes on values of zero and one, and since we compute the residuals by \\(u = y - \\hat{y}\\).\n\n\ncode\ndf &lt;- data.frame(df, mols$residuals)\n \nggplot(df, aes(x=mols.residuals, color=dispute)) + \n  geom_density()+\n  labs(title=\"Density of OLS Residuals\", x=\"Residuals\", y=\"Density\")+\n  theme_minimal()+\n    scale_color_manual(values=c(\"#005A43\", \"#6CC24A\")) +\n  geom_vline(xintercept=0, linetype=\"dashed\")"
  },
  {
    "objectID": "binarymodels24.html#when-is-the-lpm-reasonable",
    "href": "binarymodels24.html#when-is-the-lpm-reasonable",
    "title": "Binary Response Models",
    "section": "When is the LPM Reasonable?",
    "text": "When is the LPM Reasonable?\nThe best answer is never.\n\nThere seems to be a mild trend in the discipline to rehabilitate the LPM though it’s not clear why - that is, it’s hard to find statements about the advantages of doing so in any particular setting, or about the disadvantages of estimating a logit or probit model that would lead us to prefer the LPM.\n\nOLS is a rockin’ estimator, but it’s just not well suited to limited \\(y\\) variables. Efforts to rehabilitate the LPM are like putting lipstick on a pig."
  },
  {
    "objectID": "binarymodels24.html#on-linearity",
    "href": "binarymodels24.html#on-linearity",
    "title": "Binary Response Models",
    "section": "On Linearity",
    "text": "On Linearity\nIn the linear model, \\(\\hat{y_i}=x_i\\beta\\). This makes sense because \\(y = y^*\\). Put differently, \\(y\\) is continuous, unbounded, (assumed) normal, and is an “unlimited” measure of the concept we intend to measure.\nIn binary models, \\(y \\neq y^*\\), because our observation of \\(y\\) is limited such that we can only observe its presence or absence. We have two different realizations of the same variable: \\(y\\) is the limited but observed variable; \\(y^*\\) is the unlimited variable we want to measure, but cannot because it is unobservable.\nThe goal of these models is to use \\(y\\) in the regression in order to get estimates of \\(y^*\\). Those estimates of \\(y^*\\) are our principle quantity of interest in the binary variable model."
  },
  {
    "objectID": "binarymodels24.html#linking-xwidehatbeta-and-y",
    "href": "binarymodels24.html#linking-xwidehatbeta-and-y",
    "title": "Binary Response Models",
    "section": "Linking \\(x\\widehat{\\beta}\\) and \\(y^*\\)",
    "text": "Linking \\(x\\widehat{\\beta}\\) and \\(y^*\\)\nWe can produce the linear prediction, \\(x\\widehat{\\beta}\\), but we need to transform it to produce estimates of \\(y^*\\). To do so, we use a link function to map \\(x_i\\beta\\) onto the probability space, \\(y^*\\). This means \\(\\widehat{y_i} \\neq x\\widehat{\\beta}\\). Instead,\n\\[y^* = F(x_i\\beta)\\]\nWhere \\(F\\) is a continuous, sigmoid probability CDF. This is how we get estimates of our quantity of interest, \\(y^*\\)."
  },
  {
    "objectID": "binarymodels24.html#non-linear-change-in-pry1-across-values-of-x",
    "href": "binarymodels24.html#non-linear-change-in-pry1-across-values-of-x",
    "title": "Binary Response Models",
    "section": "Non linear change in Pr(y=1) across values of \\(x\\)",
    "text": "Non linear change in Pr(y=1) across values of \\(x\\)\nIn the LPM, the relationship between \\(Pr(y=1)\\) and \\(X\\) is linear, so the rate of change toward \\(Pr(y=1)\\) is constant across all values of \\(X\\).\nThis means that the rate of change approaching one (or approaching zero) is exactly the same as the rate of change anywhere else in the distribution.\nFor example, this means that the change from .99 to 1.00 is just as likely as the change from .50 to .51; is this sensible for a bounded latent variable (probability)?"
  },
  {
    "objectID": "binarymodels24.html#linear-and-sigmoid-functions-at-limits",
    "href": "binarymodels24.html#linear-and-sigmoid-functions-at-limits",
    "title": "Binary Response Models",
    "section": "Linear and Sigmoid Functions at Limits",
    "text": "Linear and Sigmoid Functions at Limits\n\n\ncode\nz &lt;- seq(-5,5,.1)\nl &lt;- seq(0,1,.01)\ns1 &lt;- 1/(1+exp(-z))\ns2 &lt;- pnorm(z)\n\nggplot() + \n  geom_line(aes(x=z, y=l), color=\"black\")+\n  geom_line(aes(x=z, y=s1), color=\"red\")+\n  geom_line(aes(x=z, y=s2), color=\"green\")+\n  labs(title=\"Linear and Sigmoid Functions\", x=\"z\", y=\"F(z)\")+\n  theme_minimal()+\n  annotate(\"text\", x=0, y=.75, label=\"Normal\", color=\"green\")+\n  annotate(\"text\", x=-3, y=.1, label=\"Logistic\", color=\"red\")"
  },
  {
    "objectID": "binarymodels24.html#non-constant-change-in-pry1-across-values-of-z",
    "href": "binarymodels24.html#non-constant-change-in-pry1-across-values-of-z",
    "title": "Binary Response Models",
    "section": "Non constant change in Pr(y=1) across values of \\(z\\)",
    "text": "Non constant change in Pr(y=1) across values of \\(z\\)\nAnimating the change in \\(Pr(y=1)\\) across values of \\(z\\) for the linear and sigmoid functions.\n\n\ncode\nlibrary(gganimate)\n\nz &lt;- seq(-5,5,.1)\nl &lt;- seq(0,1,.01)\ns1 &lt;- 1/(1+exp(-z))\ns2 &lt;- pnorm(z)\n\ndf &lt;- data.frame(z=z, l=l, s1=s1, s2=s2)\n\nggplot(df, aes(x=z, y=l)) + \n  geom_line(aes(x=z, y=l), color=\"black\")+\n  geom_line(aes(x=z, y=s1), color=\"#005A43\")+\n  geom_line(aes(x=z, y=s2), color=\"#6CC24A\")+\n  labs(title=\"Rates of change\", x=\"z\", y=\"F(z)\")+\n  theme_minimal()+\n  transition_reveal(z)"
  },
  {
    "objectID": "binarymodels24.html#a-nonlinear-model-for-binary-data",
    "href": "binarymodels24.html#a-nonlinear-model-for-binary-data",
    "title": "Binary Response Models",
    "section": "A nonlinear model for binary data",
    "text": "A nonlinear model for binary data\nSo \\(y\\) is binary, and we’ve established the linear model is not appropriate. The observed variable, \\(y\\), appears to be binomial (iid):\n\\[ y \\sim f_{binomial}(\\pi_i)\\]\n\\[ y = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{} \\pi_{i}\\\\\n         0, & \\mbox{} 1-\\pi_{i}\n         \\end{array}\n     \\right. \\]\n\\[ \\pi_i = F(x_i\\widehat{\\beta}) \\] \\[1- \\pi_i=1-F(x_i\\widehat{\\beta})\\]"
  },
  {
    "objectID": "binarymodels24.html#binomial-llf",
    "href": "binarymodels24.html#binomial-llf",
    "title": "Binary Response Models",
    "section": "Binomial LLF",
    "text": "Binomial LLF\nWrite the binomial density:\n\\[ Pr(y=1| \\pi) = \\pi_i^{y_i} (1-\\pi_i)^{1-y_i} \\]\nWrite the joint probability as a likelihood:\n\\[\\mathcal{L} (\\pi |\\ y) = \\prod \\limits_{i=1}^{n} \\left[ \\pi_i^{y_i} (1-\\pi_i)^{1-y_i}\\right]\\]\nWrite the log-likelihood:\n\\[\\ln \\mathcal{L} (\\pi| \\ y) = \\sum \\limits_{i=1}^{n}  \\left[ y_i \\ln ( \\pi_i) +  (1-y_i) \\ln(1-\\pi_i)\\right]\\]"
  },
  {
    "objectID": "binarymodels24.html#parameterize-the-distribution-parameter-pi_i",
    "href": "binarymodels24.html#parameterize-the-distribution-parameter-pi_i",
    "title": "Binary Response Models",
    "section": "Parameterize the Distribution parameter \\(\\pi_i\\)",
    "text": "Parameterize the Distribution parameter \\(\\pi_i\\)\nParameterize \\(\\pi_i\\):\n\\[\\pi_i= F(x \\beta)\\]\nThis is the binomial log-likelihood function.\n\\[\\ln \\mathcal{L} (\\pi| \\ y) = \\sum \\limits_{i=1}^{n}  \\left[ y_i \\ln (F(x_i\\widehat{\\beta})) +  (1-y_i) \\ln(1-F(x_i\\widehat{\\beta}))\\right]\\]\nBut we need to fill in \\(F\\), the link function."
  },
  {
    "objectID": "binarymodels24.html#link-function",
    "href": "binarymodels24.html#link-function",
    "title": "Binary Response Models",
    "section": "Link Function",
    "text": "Link Function\nWe parameterized \\(\\pi_i\\):\n\\[\\pi_i= F(x \\beta)\\]\nand now need to choose an appropriate link function for \\(F\\) such that:\n\nour prediction of \\(\\widehat{\\pi_i}\\) is bounded [0,1].\n\\(x_i \\widehat{\\beta}\\) can range over the interval \\([-\\infty, +\\infty]\\) and map onto the [0,1] interval.\n\nThere’s a large number of sigmoid shaped probability functions that will satisfy these needs.\n\n\n\n\n\n\nImportant Concept\n\n\n\nThe link function maps or transforms the linear prediction on the sigmoid probability space, and obeys the bounds of 0,1.\n\n\nThe most commonly used link functions are the standard normal (probit)}\n\\[Pr(y_i=1 | X) = \\Phi(x_i\\widehat{\\beta}) \\]\nand the logistic (logit) CDFs.\n\\[Pr(y_i=1 | X) = \\frac{1}{1+exp^{-(x_i\\widehat{\\beta})}} \\]\nHere are the logistic and Normal CDFs:\n\n\ncode\nz &lt;- seq(-5,5,.1)\nl &lt;- seq(0,1,.01)\ns1 &lt;- 1/(1+exp(-z))\ns2 &lt;- pnorm(z)\n\ndf &lt;- data.frame(z=z, l=l, s1=s1, s2=s2)\n\nggplot(df, aes(x=z, y=l)) + \n  geom_line(aes(x=z, y=l), color=\"black\", linetype=\"dashed\" )+\n  geom_line(aes(x=z, y=s1), color=\"#005A43\")+\n  geom_line(aes(x=z, y=s2), color=\"#6CC24A\")+\n  labs(title=\"Logistic and Normal CDFs\", x=expression(x*beta), y=\"Pr(y=1)\")+\n  theme_minimal() +\n  annotate(\"text\", x=1.3, y=.7, label=\"logistic\", color=\"black\")+\n  annotate(\"text\", x=-.2, y=.15, label=\"normal\", color=\"black\")\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nNote the sigmoid functions approach the limits at decreasing rates; the fastest rate of change is at \\(y=.5\\), a point around which the curves are symmetric. The point \\(y=.5\\) is the transition point below which we’d predict a zero, above which we’d predict a one if we were interested in classifying cases into zeros and ones. Classification is a common use for models like these, say distinguishing spam from non-spam emails, or predicting the presence or absence of a disease. More on this later."
  },
  {
    "objectID": "binarymodels24.html#probit-and-logit-llfs",
    "href": "binarymodels24.html#probit-and-logit-llfs",
    "title": "Binary Response Models",
    "section": "Probit and Logit LLFs",
    "text": "Probit and Logit LLFs\nProbit - link between \\(x\\hat{\\beta}\\) and \\(Pr(y=1)\\) is standard normal CDF: \\[\n\\ln \\mathcal{L} (Y|\\beta) = \\sum_{i=1}^{N} y_i \\ln \\Phi(\\mathbf{x_i \\beta})+ (1-y_i) \\ln[1-\\Phi(\\mathbf{x_i \\beta})] \\nonumber\n\\]\nLogit (logistic CDF):\n\\[\n\\ln \\mathcal{L} (Y|\\beta) = \\sum_{i=1}^{N} \\left\\{ y_i \\ln \\left(\\frac{1}{1+e^{-\\mathbf{x_i \\beta}}}\\right)+ (1-y_i) \\ln \\left[1-\\left(\\frac{1}{1+e^{-\\mathbf{x_i \\beta}}}\\right)\\right] \\right\\}\\nonumber\n\\]"
  },
  {
    "objectID": "binarymodels24.html#non-constant-change-in-pry1-across-values-of-z-1",
    "href": "binarymodels24.html#non-constant-change-in-pry1-across-values-of-z-1",
    "title": "Binary Response Models",
    "section": "Non constant change in Pr(y=1) across values of \\(z\\)",
    "text": "Non constant change in Pr(y=1) across values of \\(z\\)"
  },
  {
    "objectID": "binarymodels24.html#sigmoid-link-functions",
    "href": "binarymodels24.html#sigmoid-link-functions",
    "title": "Binary Response Models",
    "section": "Sigmoid link functions",
    "text": "Sigmoid link functions\n\nreturn probabilities in the appropriate bounds, mapping \\(x\\beta\\) onto \\(\\tilde{y}\\).\npermit different rates of change in \\(\\tilde{y}\\) across \\(x\\). The marginal effect of \\(x\\) is not \\(\\widehat{\\beta}\\) (more on this shortly).\n“compress” effects of extreme values of \\(x\\) on \\(Pr(y=1)\\).\nsuggest questions about symmetry and transition probabilities (i.e. is .5 always a sensible transition from the probability of zero to probability of one?)."
  },
  {
    "objectID": "binarymodels24.html#logit",
    "href": "binarymodels24.html#logit",
    "title": "Binary Response Models",
    "section": "Logit",
    "text": "Logit\n\\[\\Lambda(x\\widehat{\\beta}) = \\frac{exp(-x\\widehat{\\beta})}{1+exp(x\\widehat{\\beta})} \\] ~\\ \\[= \\frac{1}{1+exp(-x\\widehat{\\beta})}\\]"
  },
  {
    "objectID": "binarymodels24.html#probit",
    "href": "binarymodels24.html#probit",
    "title": "Binary Response Models",
    "section": "Probit",
    "text": "Probit\n\\[F(x\\widehat{\\beta}) =  \\Phi(x\\widehat{\\beta})\\]\nSome models have multiple quantities, eg count models} The Poisson event count, for instance:\nExpected value of \\(Y|X\\) - this would be the expected number of events:\n\\[E[Y|X] = \\widehat{\\lambda} = exp(x\\widehat{\\beta})\\]\nProbability \\(Y=y_i\\) for each value of \\(Y\\), e.g., the probability of observing \\(i=2\\) events:\n\\[Pr(Y = y_i) =  \\frac{(x\\widehat{\\beta})^{y_i} \\cdot exp(x\\widehat{\\beta}) }{y!}   \\]"
  },
  {
    "objectID": "binarymodels24.html#marginal-effects-linear-model",
    "href": "binarymodels24.html#marginal-effects-linear-model",
    "title": "Binary Response Models",
    "section": "Marginal Effects, Linear Model",
    "text": "Marginal Effects, Linear Model\nIn the linear model, the marginal effect of \\(x\\) is \\(\\widehat{\\beta}\\). That is, the effect of a one unit change in \\(x\\) on \\(y\\) is \\(\\widehat{\\beta}\\).\n\\[\n\\frac{\\partial \\widehat{y}}{\\partial x_k}= \\frac{\\partial x \\widehat{\\beta}}{\\partial x_k} \\nonumber \\\\  \\nonumber \\\\\n= \\widehat{\\beta} \\nonumber\n\\]\nThe marginal effect is constant with respect to \\(x_k\\)."
  },
  {
    "objectID": "binarymodels24.html#marginal-effects-nonlinear-model",
    "href": "binarymodels24.html#marginal-effects-nonlinear-model",
    "title": "Binary Response Models",
    "section": "Marginal Effects, Nonlinear Model",
    "text": "Marginal Effects, Nonlinear Model\nIn the nonlinear model, the marginal effect of \\(x_k\\) depends on where \\(x\\widehat{\\beta}\\) lies with respect to the probability distribution \\(F(\\cdot)\\).\n\\[\n\\frac{\\partial Pr(y=1)}{\\partial x_k}= \\frac{\\partial F(x\\widehat{\\beta})}{\\partial x_k} \\nonumber \\\\  \\nonumber \\\\\n=  \\frac{\\partial F(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} \\cdot \\frac{\\partial (x\\widehat{\\beta})}{\\partial x_k}  \\nonumber\n\\]\nBoth of these terms simplify …\nRemember that\n\\[\n\\frac{\\partial (x\\widehat{\\beta})}{\\partial x} = \\widehat{\\beta} \\nonumber\n\\]\nand \\[\n\\frac{\\partial F(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} = f(x\\widehat{\\beta}) \\nonumber\n\\]\nwhere the derivative of the CDF is the PDF.\nPutting these together gives us:\n\\[\n\\frac{\\partial F(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} = f(x\\widehat{\\beta}) \\widehat{\\beta} \\nonumber\n\\]\nThis is \\(\\widehat{\\beta}\\) weighted by or measured at the ordinate on the PDF - the ordinate is the height of the PDF associated with a value of the \\(x\\) axis (an abscissa)."
  },
  {
    "objectID": "binarymodels24.html#logit-marginal-effects",
    "href": "binarymodels24.html#logit-marginal-effects",
    "title": "Binary Response Models",
    "section": "Logit Marginal Effects",
    "text": "Logit Marginal Effects\nFor logit,\n\\[\n\\frac{\\partial \\Lambda(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} = \\lambda(x\\widehat{\\beta}) \\widehat{\\beta} \\nonumber\n\\]\nRecall that \\(\\Lambda\\) is the logit CDF (\\(1/(1+exp(-x_i\\widehat{\\beta}))\\)), and \\(\\lambda\\) is the logit PDF (\\(1/(1+exp(-x_i\\widehat{\\beta}))^2\\)).\nAlso, remember that\n\\[\\frac{e^{x_i\\widehat{\\beta}}}{1+e^{x_i\\widehat{\\beta}}} = \\frac{1}{1+e^{-x_i\\widehat{\\beta}}}\\]"
  },
  {
    "objectID": "binarymodels24.html#logit-marginal-effect",
    "href": "binarymodels24.html#logit-marginal-effect",
    "title": "Binary Response Models",
    "section": "Logit Marginal Effect",
    "text": "Logit Marginal Effect\n\\[\n\\begin{align}\n\\frac{\\partial \\Lambda(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} = \\lambda(x\\widehat{\\beta}) \\widehat{\\beta} \\\\\n= \\frac{e^{x_i\\widehat{\\beta}}}{(1+e^{x_i\\widehat{\\beta}})^2} \\widehat{\\beta}  \\\\\n=\\frac{e^{x_i\\widehat{\\beta}}}{1+e^{x_i\\widehat{\\beta}}} \\frac{1}{1+e^{x_i\\widehat{\\beta}}} \\widehat{\\beta}  \\\\\n=\\Lambda(x_i\\widehat{\\beta}) \\frac{1+e^{x_i\\widehat{\\beta}}-e^{x_i\\widehat{\\beta}}}{1+e^{x_i\\widehat{\\beta}}} \\widehat{\\beta}   \\\\\n=\\Lambda(x_i\\widehat{\\beta}) 1-\\frac{e^{x_i\\widehat{\\beta}}}{1+e^{x_i\\widehat{\\beta}}} \\widehat{\\beta}  \\\\\n=\\Lambda(x_i\\widehat{\\beta}) (1-\\Lambda(x_i\\widehat{\\beta})) \\widehat{\\beta}  \n\\end{align}\n\\]"
  },
  {
    "objectID": "binarymodels24.html#logit-marginal-effect-maximum",
    "href": "binarymodels24.html#logit-marginal-effect-maximum",
    "title": "Binary Response Models",
    "section": "Logit Marginal Effect (Maximum)",
    "text": "Logit Marginal Effect (Maximum)\nThis is cool because\n\\[\\frac{\\partial \\Lambda(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} = \\lambda(x\\widehat{\\beta}) \\widehat{\\beta}\\\\\n=\\Lambda(x_i\\widehat{\\beta}) (1-\\Lambda(x_i\\widehat{\\beta})) \\widehat{\\beta}  \\]\nmeans the marginal effect is \\(\\widehat{\\beta}\\) weighted by the probability of \\(y=1\\) times the probability of \\(y=0\\). Since the largest value this can take on is \\(Pr(y_i=1)=0.5 \\cdot Pr(y_i=0)=0.5= 0.25\\), then the maximum marginal effect is \\(0.25 \\widehat{\\beta}\\)."
  },
  {
    "objectID": "binarymodels24.html#visualizing-logit-marginal-effects",
    "href": "binarymodels24.html#visualizing-logit-marginal-effects",
    "title": "Binary Response Models",
    "section": "Visualizing Logit Marginal Effects",
    "text": "Visualizing Logit Marginal Effects\n\n\ncode\nz &lt;- seq(-5,5,.1)\np &lt;- plogis(z)\nd &lt;- dlogis(z)\n\ndf &lt;- data.frame(z=z, p=p, d=d)\n\n#plot pdf and cdf with reference line at y=.39\nggplot(df, aes(x=z)) + \n  geom_line(aes(y=d), color=\"black\")+\n  geom_line(aes(y=p), color=\"red\")+\n  geom_hline(yintercept=.25, linetype=\"dashed\")+\n  labs(title=\"Logistic PDF and CDF\", x=\"z\", y=\"F(z)\")+\n  theme_minimal()"
  },
  {
    "objectID": "binarymodels24.html#probit-marginal-effects-maximum",
    "href": "binarymodels24.html#probit-marginal-effects-maximum",
    "title": "Binary Response Models",
    "section": "Probit Marginal Effects (Maximum)",
    "text": "Probit Marginal Effects (Maximum)\nIn the probit model, this is simply\n\\[\n\\frac{\\partial \\Phi(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} = \\phi(x\\widehat{\\beta}) \\widehat{\\beta} \\nonumber\n\\]\nThe ordinate at the maximum of the standard normal PDF is 0.3989 - rounding to 0.4, we can say that the maximum marginal effect of any \\(\\widehat{\\beta}\\) in the probit model is \\(0.4\\widehat{\\beta}\\).\nThe ordinate is at the maximum where \\(z=0\\); recall this is the standard normal, so \\(x_i\\widehat{\\beta}=z\\). When \\(z=0\\),\n\\[Pr(z)=\\frac{1}{\\sqrt{2 \\pi}} \\exp \\left[\\frac{-(z)^{2}}{2}\\right] \\nonumber \\\\ \\nonumber\\\\\n=\\frac{1}{\\sqrt{2 \\pi}} \\nonumber\\\\\n\\approx .4 \\nonumber \\]"
  },
  {
    "objectID": "binarymodels24.html#visualizing-probit-marginal-effects",
    "href": "binarymodels24.html#visualizing-probit-marginal-effects",
    "title": "Binary Response Models",
    "section": "Visualizing Probit Marginal Effects",
    "text": "Visualizing Probit Marginal Effects\n\n\ncode\nz &lt;- seq(-5,5,.1)\np &lt;- pnorm(z)\nd &lt;- dnorm(z)\n\ndf &lt;- data.frame(z=z, p=p, d=d)\n\n#plot pdf and cdf with reference line at y=.39\nggplot(df, aes(x=z)) + \n  geom_line(aes(y=d), color=\"black\")+\n  geom_line(aes(y=p), color=\"red\")+\n  geom_hline(yintercept=.3989, linetype=\"dashed\")+\n  labs(title=\"Standard Normal PDF and CDF\", x=\"z\", y=\"F(z)\")+\n  theme_minimal()"
  },
  {
    "objectID": "binarymodels24.html#logit-odds-interpretation",
    "href": "binarymodels24.html#logit-odds-interpretation",
    "title": "Binary Response Models",
    "section": "Logit Odds Interpretation",
    "text": "Logit Odds Interpretation\nThe odds are given by the probability an event occurs divided by the probability it does not:\n\\[\n\\Omega(X) = \\frac{Pr(y=1)}{1-Pr(y=1)} \\nonumber\n= \\frac{\\Lambda(X\\widehat{\\beta})}{(1-\\Lambda(X\\widehat{\\beta}))} \\nonumber\n\\]"
  },
  {
    "objectID": "binarymodels24.html#logit-log-odds",
    "href": "binarymodels24.html#logit-log-odds",
    "title": "Binary Response Models",
    "section": "Logit Log-odds",
    "text": "Logit Log-odds\nLogging …\n\\[\\ln \\Omega(X) = \\ln \\left(\\frac{\\Lambda(X\\widehat{\\beta})}{(1-\\Lambda(X\\widehat{\\beta}))}\\right) =X\\widehat{\\beta} \\]\n\\[\n\\frac{\\partial \\ln \\Omega}{\\partial X} = \\widehat{\\beta} \\nonumber\n\\]\nWhich shows the change in the log-odds given a change in \\(X\\) is constant (and therefore linear). This quantity is sometimes called “the logit.”"
  },
  {
    "objectID": "binarymodels24.html#logit-odds-ratios",
    "href": "binarymodels24.html#logit-odds-ratios",
    "title": "Binary Response Models",
    "section": "Logit Odds Ratios",
    "text": "Logit Odds Ratios\nOdds ratios are very useful:\n\\[\n\\frac{ \\Omega x_k + 1}{\\Omega x_k} =exp(\\widehat{\\beta_k}) \\nonumber\n\\]\ncomparing the difference in odds between two values of \\(x_k\\); note the change in value does not have to be 1.\n\\[\n\\frac{ \\Omega x_k + \\iota}{\\Omega x_k} =exp(\\widehat{\\beta_k}* \\iota) \\nonumber\n\\]\nNot only is it simple to exponentiate \\(\\widehat{\\beta_k}\\), but the interpretation is that \\(x\\) increases/decreases \\(Pr(y=1)\\) by that factor, \\(exp(\\widehat{\\beta_k})\\), and more usefully, that:\n\\[\n100*(exp(\\widehat{\\beta_k})-1) \\nonumber\n\\]\nis the percentage change in the odds given a one unit change in \\(x_k\\).\nSo a logit coefficient of .226\n\\[\n100*(exp(.226)-1) =25.36 \\nonumber\n\\]\nProduces a 25.36% increase in the odds of \\(y\\) occurring."
  },
  {
    "objectID": "binarymodels24.html#logit-odds-ratios-1",
    "href": "binarymodels24.html#logit-odds-ratios-1",
    "title": "Binary Response Models",
    "section": "Logit Odds Ratios",
    "text": "Logit Odds Ratios\nNot only is it simple to exponentiate \\(\\widehat{\\beta_k}\\), but the interpretation is that \\(x\\) increases/decreases \\(Pr(y=1)\\) by that factor, \\(exp(\\widehat{\\beta_k})\\), and more usefully, that:\n\\[\n100*(exp(\\widehat{\\beta_k})-1) \\nonumber\n\\]\nis the percentage change in the odds given a one unit change in \\(x_k\\).\nSo a logit coefficient of .226\n\\[\n100*(exp(.226)-1) =25.36 \\nonumber\n\\]\nProduces a 25.36% increase in the odds of \\(y\\) occurring."
  },
  {
    "objectID": "binarymodels24.html#marginal-effects-at-means-mem",
    "href": "binarymodels24.html#marginal-effects-at-means-mem",
    "title": "Binary Response Models",
    "section": "Marginal Effects at Means (MEM)",
    "text": "Marginal Effects at Means (MEM)\nMEMs are what they sound like - effects with independent variables set at central tendencies.\n\nestimate model.\ncreate out of sample data.\nvary \\(x\\) of interest; set all other \\(x\\) variables to appropriate central tendencies - hence the “at Means.”\ngenerate QIs in out of sample data."
  },
  {
    "objectID": "binarymodels24.html#average-marginal-effects-ame",
    "href": "binarymodels24.html#average-marginal-effects-ame",
    "title": "Binary Response Models",
    "section": "Average Marginal Effects (AME)",
    "text": "Average Marginal Effects (AME)\nAverage Marginal Effects are in-sample but create a counterfactual for a variable of interest, assuming the entire sample looks like that case.\nFor instance, suppose a model of wages with covariates for education and gender. We might ask the question what would the predictions look like if the entire sample were male, but otherwise looked as it does? Alternatively, what would the predictions look like if the entire sample were female, but all other variables the same as they appear in the estimation data?\nTo answer these, we’d change the gender variable to male, generate \\(x{\\widehat{\\beta}}\\) for the entire sample, and take the average, then repeat with the gender variable set to female.\nTo generate AMEs,\n\nestimate model.\nin estimation data, set variable of interest to a particular value for the entire estimation sample.\ngenerate QIs (expected values, standard errors).\ntake average of QIs, and save.\nrepeat for all values of variable of interest, and plot."
  },
  {
    "objectID": "binarymodels24.html#uncertainty-standard-errors-of-linear-predictions",
    "href": "binarymodels24.html#uncertainty-standard-errors-of-linear-predictions",
    "title": "Binary Response Models",
    "section": "Uncertainty: Standard Errors of Linear Predictions",
    "text": "Uncertainty: Standard Errors of Linear Predictions\nConsider the linear prediction\n\\[X \\widehat{\\beta} \\]\nunder maximum likelihood theory:\n\\[var(X \\widehat{\\beta}) = \\mathbf{X V X'} \\]\nan \\(N x N\\) matrix, where \\(V\\) is the var-cov matrix of \\({\\widehat{\\beta}}\\). The main diagonal contains the variances of the \\(N\\) predictions. The standard errors are:\n\\[se(X \\widehat{\\beta}) = \\sqrt{diag(\\mathbf{X V X'})} \\]\nwhich is an \\(N x 1\\) vector."
  },
  {
    "objectID": "binarymodels24.html#uncertainty-delta-method",
    "href": "binarymodels24.html#uncertainty-delta-method",
    "title": "Binary Response Models",
    "section": "Uncertainty: Delta Method",
    "text": "Uncertainty: Delta Method\nThe ML method is appropriate for monotonic functions of \\(X \\widehat{\\beta}\\), e.g. logit, probit. In other models (e.g., multinomial logit), the function is not monotonic in \\(X \\widehat{\\beta}\\) so we use the Delta Method - this creates a linear approximation of the function. Greene (2012: 693ff) gives this as a general derivation of the variance:\n\\[Var[F(X \\widehat{\\beta})] = f(\\mathbf{x'\\widehat{\\beta}})^2 \\mathbf{x' V x} \\]\nWhere this would generate variances for whatever \\(F(X \\widehat{\\beta})\\) is, perhaps a predicted probability."
  },
  {
    "objectID": "binarymodels24.html#uncertainty-standard-errors-of-p-in-logit",
    "href": "binarymodels24.html#uncertainty-standard-errors-of-p-in-logit",
    "title": "Binary Response Models",
    "section": "Uncertainty: Standard Errors of \\(p\\) in Logit",
    "text": "Uncertainty: Standard Errors of \\(p\\) in Logit\nBy delta transformation is given by:\n\\[F(X \\widehat{\\beta}) * (1-F(X \\widehat{\\beta}) * \\mathbf(X V X')\\]\n\\[ = f(X \\widehat{\\beta})  * \\mathbf(X V X')\\]\nor\n\\[ p * (1-p) * stdp\\]"
  },
  {
    "objectID": "binarymodels24.html#uncertainty-simulating-confidence-intervals-i",
    "href": "binarymodels24.html#uncertainty-simulating-confidence-intervals-i",
    "title": "Binary Response Models",
    "section": "Uncertainty: Simulating confidence intervals, I",
    "text": "Uncertainty: Simulating confidence intervals, I\n\ndraw a sample with replacement of size \\(\\tilde{N}\\) from the estimation sample.\nestimate the model parameters in that bootstrap sample.\nusing the bootstrap estimates, generate quantities of interest (e.g. \\(x\\widehat{\\beta}\\)) repeat \\(j\\) times.\ncollect all these bootstrap QIs and use either percentiles or standard deviations to measure uncertainty."
  },
  {
    "objectID": "binarymodels24.html#uncertainty-simulating-confidence-intervals-ii",
    "href": "binarymodels24.html#uncertainty-simulating-confidence-intervals-ii",
    "title": "Binary Response Models",
    "section": "Uncertainty: Simulating confidence intervals, II",
    "text": "Uncertainty: Simulating confidence intervals, II\n\nestimate the model.\ngenerate a large sample distribution of parameters (e.g. using drawnorm).\ngenerate quantities of interest for the distribution of parameters.\nuse either percentiles or standard deviations of the QI to measure uncertainty."
  },
  {
    "objectID": "binarymodels24.html#standard-errors-for-predicted-probabilities",
    "href": "binarymodels24.html#standard-errors-for-predicted-probabilities",
    "title": "Binary Response Models",
    "section": "Standard Errors for Predicted Probabilities",
    "text": "Standard Errors for Predicted Probabilities\n\\[\n\\operatorname { Var } \\left[ \\operatorname { Pr } \\left( Y _ { i } = 1 \\right) \\right) ] = \\left[ \\frac { \\partial F \\left( \\mathbf { X } _ { i } \\hat { \\boldsymbol { \\beta } } \\right) } { \\partial \\hat { \\boldsymbol { \\beta } } } \\right] ^ { \\prime } \\hat { \\mathbf { v } } \\left[ \\frac { \\partial F \\left( \\mathbf { X } _ { i } \\hat { \\boldsymbol { \\beta } } \\right) } { \\partial \\hat { \\boldsymbol { \\beta } } } \\right]\n\\]\n\\[\n= \\left[ f \\left( \\mathbf { X } _ { i } \\hat { \\boldsymbol { \\beta } } \\right) \\right] ^ { 2 } \\mathbf { X } _ { i } ^ { \\prime } \\hat { \\mathbf { V } } \\mathbf { X } _ { i }\n\\]\n\\[s.e. (Pr(y=1)) = \\sqrt{\\left[ f \\left( \\mathbf { X } _ { i } \\hat { \\boldsymbol { \\beta } } \\right) \\right] ^ { 2 } \\mathbf { X } _ { i } ^ { \\prime } \\hat { \\mathbf { V } } \\mathbf { X } _ { i }}\\]"
  },
  {
    "objectID": "binarymodels24.html#uncertainty-ses-of-predictions-for-linear-combinations",
    "href": "binarymodels24.html#uncertainty-ses-of-predictions-for-linear-combinations",
    "title": "Binary Response Models",
    "section": "Uncertainty: SEs of Predictions for linear combinations",
    "text": "Uncertainty: SEs of Predictions for linear combinations\nA common circumstance that requires joint hypothesis tests is the case of polynomials (which are themselves interactions):\n\\[y = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_1 + \\widehat{\\beta}_2 x_{1}^2  + \\varepsilon \\]\nThe question is whether \\(\\widehat{\\beta}_1 = \\widehat{\\beta}_2  = 0\\) - the marginal effect is:\n\\[ \\widehat{\\beta}_1 + 2 \\widehat{\\beta}_2x_1\\]\nand requires the standard error for \\(\\widehat{\\beta}_1+\\widehat{\\beta}_2\\), which is:\n\\[ \\sqrt{var(\\widehat{\\beta}_1) + 4x_{1}^{2}var(\\widehat{\\beta}_2) +  4x_1 cov(\\widehat{\\beta}_1, \\widehat{\\beta}_2)  }\\]"
  },
  {
    "objectID": "binarymodels24.html#uncertainty-cis---end-point-transformation",
    "href": "binarymodels24.html#uncertainty-cis---end-point-transformation",
    "title": "Binary Response Models",
    "section": "Uncertainty: CIs - End Point Transformation",
    "text": "Uncertainty: CIs - End Point Transformation\nGenerate upper and lower bounds using either ML or Delta standard errors, such that\n\\[F(X \\widehat{\\beta} - c*s.e.) \\leq F(X \\widehat{\\beta}) \\leq F(X \\widehat{\\beta} + c* s.e.)\\]\n\nestimate the model, generate the linear prediction, and the standard error of the linear prediction using the either ML or Delta.\ngenerate linear boundary predictions, \\(x{\\widehat{\\beta}} \\pm c * \\text{st. err.}\\) where \\(c\\) is a critical value on the normal, eg. \\(z=1.96\\).\ntransform the linear prediction and the upper and lower boundary predictions by \\(F(\\cdot)\\).\nWith ML standard errors, EPT boundaries will obey distributional boundaries (ie, won’t fall outside 0-1 interval for probabilities); the linear end point predictions are symmetric, though they will not be symmetric in nonlinear models.\nWith delta standard errors, bounds may not obey distributional boundaries."
  },
  {
    "objectID": "binarymodels24.html#marginal-effects-in-the-nonlinear-model",
    "href": "binarymodels24.html#marginal-effects-in-the-nonlinear-model",
    "title": "Binary Response Models",
    "section": "Marginal Effects in the Nonlinear Model",
    "text": "Marginal Effects in the Nonlinear Model\n\n\ncode\nz &lt;- seq(-5,5,.1)\nncdf &lt;- pnorm(z)\nnpdf &lt;- dnorm(z)\nlcdf &lt;- plogis(z)\nlpdf &lt;- dlogis(z)\n\ndf &lt;- data.frame(ncdf=ncdf, npdf=npdf, lcdf=lcdf, lpdf=lpdf, z=z)\n\n#plot pdf and cdf with reference line at y=.39\n# ggplot(df, aes(x=z)) + \n#   geom_line(aes(y=npdf), color=\"black\")+\n#   geom_line(aes(y=ncdf), color=\"black\")+\n#   geom_line(aes(y=lpdf), color=\"blue\")+\n#   geom_line(aes(y=lcdf), color=\"blue\")+\n#   geom_hline(yintercept=.3989, linetype=\"dashed\")+\n#   geom_hline(yintercept=.25, linetype=\"dashed\")+\n#   labs(title=\"Standard Normal and Logistic PDFs and CDFs\", x=\"z\", y=\"F(z)\")+\n#   theme_minimal()\n# \n\nhighchart() %&gt;%\n  hc_add_series(df, \"line\", hcaes(x = z, y = npdf)) %&gt;%\n  hc_add_series(df, \"line\", hcaes(x = z, y = ncdf)) %&gt;%\n  hc_add_series(df, \"line\", hcaes(x = z, y = lpdf)) %&gt;%\n  hc_add_series(df, \"line\", hcaes(x = z, y = lcdf)) %&gt;%\n  hc_add_theme(hc_theme_flat()) %&gt;%\n  hc_xAxis(title = list(text = \"z\")) %&gt;%\n  hc_yAxis(title = list(text = \"F(z)\")) %&gt;%\n  hc_add_theme(hc_theme_flat()) %&gt;%\n  hc_legend(enabled = FALSE)"
  },
  {
    "objectID": "llfmax.html",
    "href": "llfmax.html",
    "title": "Writing the Likelihood",
    "section": "",
    "text": "The goal here is to walk through the process of writing a likelihood function based on the data we observe, then programming that likelihood function and optimizing it with respect to the data.\n\n\nLet’s recall the motivation for turning to ML. The data we have are not suitable for OLS because our observation of the data is limited. For instance, we observe a binary variable \\([0,1]\\) rather than the underlying continuous probability beneath it. If we could observe and measure that probability, we’d do so and perhaps use OLS to estimate a model. But since we only observe \\([0,1]\\), our observation is limited.\nOLS is poorly suited to model this variable especially if one of our goals is to generate predictions of that underlying probability. That is, one of our goals might be to estimate a measure of the \\(y\\) variable we wish we could measure directly.\nMLE is better suited to this task - it asks us what the data generating process is that produced the observed data, and to build a model appropriate to the limited observation of that \\(y\\) variable.\nTo do so, we need to:\n\ndescribe the observed distribution of \\(y\\)\nconsider what we wish we could measure - this points to the key quantities of interest we want to derive from the model.\ndescribe \\(y\\) according to a probability distribution; write that distribution\nwrite a log-likelihood function appropriate to that probability distribution.\nparameterize the log-likelihood function such that we have a link distribution to map the linear prediction, \\(x\\beta\\), onto the probability space of \\(y\\).\n\n\n\n\nLet’s start with a \\(y\\) variable as follows:\n\n\ncode\n#binary y variable\nset.seed(8675309)\nn &lt;- 1000\nX &lt;- matrix(rnorm(n * 2), ncol = 2)\ntrue_beta &lt;- c(-1, 0.5, -0.5)\nz &lt;- cbind(1, X) %*% true_beta\nprob &lt;- 1 / (1 + exp(-z))\ny &lt;- rbinom(n, 1, prob)\n# y &lt;- as.factor(y)\n\nlibrary(kableExtra)\ntable(y)%&gt;%\n  as.data.frame() %&gt;%\n  rename( \"count\" = \"Freq\") %&gt;%\n  mutate(y = ifelse(y == 0, \"0\", \"1\")) %&gt;%\n  kable(\"html\") %&gt;%\n  kable_styling(\"striped\", full_width = F) %&gt;%\n  column_spec(1, border_right = T) %&gt;%\n  collapse_rows(columns = 1, valign = \"top\")\n\n\n\n\n\n\ny\ncount\n\n\n\n\n0\n707\n\n\n1\n293\n\n\n\n\n\n\n\n\nThe variable, \\(y\\), takes on values of zero and one - it appears binomial; let’s write this in terms of the binomial distribution parameter \\(\\pi\\), so it takes on the value of one with probability \\(\\pi\\) and zero with probability \\(1-\\pi\\).\n\\[ y_i = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{} \\pi_{i}\\\\\n         0, & \\mbox{} 1-\\pi_{i}\n         \\end{array}\n     \\right.\\]\nThe likelihood of a single observation is:\n\\[ L(\\pi_i|y) = \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\]\nThis is a statement of the likelihood that any particular value of \\(\\pi\\) generated an observation. We want the likelihood any value of \\(\\pi\\) generated the entire dataset - this is the joint likelihood of all the observations.\n\\[ L(\\pi|y) = \\prod_{i=1}^{n} \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\] recalling that a joint probability is the product of individual probabilities.\nLet’s take the log of this likelihood function - the natural log makes computation easier. Even for computers, this reduces the computational intensity especially with respect to extremely small decimals.\n\\[ \\ln L(\\pi|y) = \\sum_{i=1}^{n} y_i \\ln(\\pi_i) + (1-y_i) \\ln(1-\\pi_i) \\]\nWe can take this a step further and parameterize \\(\\pi\\) as a function of \\(X\\beta\\) where \\(X\\) is the matrix of predictors and \\(\\beta\\) is the vector of coefficients such that\n\\[ \\pi_i = F({X_i\\beta}) \\]\nSo our estimate of the binomial probability is a function of the linear predictor \\(X\\beta\\). We map \\(X\\beta\\) onto the probability space \\(\\pi\\) using a link function, \\(F\\). The most common link functions are the logistic (logit model), and the standard normal (probit model). Let’s write the logit link:\n\\[ \\pi_i = \\frac{1}{1 + e^{-(X_i\\beta)}} \\]\nand now let’s write this all in the log-likelihood function:\n\\[ \\ln L(\\beta|y) = \\sum_{i=1}^{n} y_i \\ln(\\frac{1}{1 + e^{-(X_i\\beta)}}) + (1-y_i) \\ln(1-\\frac{1}{1 + e^{-(X_i\\beta)}}) \\]\n\n\n\nLet’s start with a simple maximization procedure called\nThe next step is to write a function that calculates the log-likelihood for a given value of \\(\\pi\\) and the observed data \\(y\\). Do this over and over for different values of \\(\\pi\\) and find the value of \\(\\pi\\) that maximizes the log-likelihood. First, let’s do this using the maxLik library in R.\n\nlibrary(maxLik)\n# Write the binomial log-likelihood function\nlog_likelihood &lt;- function(pi, y) {\n  sum(y * log(pi) + (1 - y) * log(1 - pi))\n}\n\n#maximize the function using maxLik\n\nm1m &lt;- maxLik(y=y, log_likelihood, start = c(0.5), method = \"NR\") #Newton-Raphson method, starting value of 0.5\n\nsummary(m1m)\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 3 iterations\nReturn code 2: successive function values within tolerance limit (tol)\nLog-Likelihood: -604.816 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(&gt; t)    \n[1,]  0.29300    0.01441   20.34  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\nNotice the value of \\(\\pi\\) that maximizes the log-likelihood is the same as the sample mean of \\(y\\), which is 0.293. The function recovers the true value of \\(\\pi\\) in this case.\n\n\n\nNumerical optimization is a field aimed at finding “best” outcomes or answers depending on some set of criteria - often, the “best” is the maximum or minimum of some function - that’s the case in maximum likelihood.\n\n\nOne way to accomplish this is to use a grid search, where we calculate the log-likelihood for a range of values of \\(\\pi\\) and find the value that maximizes the log-likelihood. We can repeat this to the desired level of specificity, progressively narrowing the grid. In this example, we’ll just do one search out to 3 decimal places. The steps are these:\n\ngenerate the range of candidate values of our parameter \\(\\pi\\) to plug into the log-likelihood function.\nplug each value into the log-likelihood function to compute the log-likelihood for that value.\nidentify which value of \\(\\pi\\) maximizes the log-likelihood.\n\nHere’s code to do this:\n\n# generate a vector of values of pi \n\npi_trials &lt;- seq(0, 1, by = 0.001)\n\n# Calculate log-likelihood for each pi value\n\nll_values &lt;- sapply(pi_trials, log_likelihood, y = y)\n\n# Find the pi value that maximizes the log-likelihood\n\npi_hat &lt;- pi_trials[which.max(ll_values)]\nprint(pi_hat)\n\n[1] 0.293\n\n\nYou’ll note we recover the sample mean just as we did above using the maxlik function. Let’s plot the log-likelihood against the values of \\(\\pi\\) to visualize the maximum.\n\n\ncode\n# Plot log-likelihood against pi\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(data.frame(pi = pi_trials, ll = ll_values), \"line\", hcaes(x = pi, y = ll)) %&gt;%\n  hc_title(text = \"What value of Pi maximizes the log-likelihood?\") %&gt;%\n  hc_xAxis(title = list(text = \"Pi\"), \n           plotLines = list(\n    list(color =\"red\", value = pi_hat ))) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_legend(enabled = FALSE) %&gt;%\n  hc_colors(bucolors)\n\n\n\n\n\n\n\n\n\nThere are lots of ways to optimize a function - in statistical software, most of them derive from Newton’s iterative method. A very common algorithm is the Newton-Raphson method. This is what maxlik used in the example above. Newton-Raphson is an iterative process that starts with an initial guess for the parameter \\(\\pi\\) and updates it in the direction of the maximum until it converges to the maximum. At each step, it computes the first and second derivatives of the likelihood function, then uses these to update the parameter estimate. The update rule is:\n\\[ \\pi_{new} = \\pi_{old} - \\frac{g(\\pi)}{H(\\pi)} \\]\nSo the new estimate of the parameter is the old estimate minus the first derivative (gradient) of the log-likelihood function divided by the Hessian (second derivative) of the log-likelihood function. Here, \\(g(\\pi)\\) is the gradient of the log-likelihood function and \\(H(\\pi)\\) is the Hessian of the log-likelihood function. The algorithm iterates this process until the change in \\(\\pi\\) is sufficiently small, below a specified threshold.\nLet’s write the Newton-Raphson algorithm ourselves:\n\n\n\n\n# use same data generated above; same LLF as above, aiming to recover pi\n\n#write the gradient - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(pi, y) {\n  sum(y / pi - (1 - y) / (1 - pi))\n}\n\n#write the Hessian - matrix of partial second derivatives of the log-likelihood function.\n\nhessian &lt;- function(pi, y) {\n  sum(-y / pi^2 - (1 - y) / (1 - pi)^2)\n}\n\n#declare a starting value for pi\n\npi &lt;- 0.5\n\n#set convergence criteria - when new pi is within 1e-6 of old pi, stop iterating\n\ntol &lt;- 1e-6\n\n#initialize iteration counter, set maximum iterations; how many times will we do this before we stop if we don't converge prior? \n\niter &lt;- 0\nmax_iter &lt;- 100\n\n#iterate the Newton-Raphson algorithm\n\nfor (iter in 1:max_iter) {\n  # Compute gradient  \n  grad &lt;- gradient(pi, y)\n  \n  # Compute Hessian\n  hess &lt;- hessian(pi, y)\n  \n  # Update pi\n  pi_new &lt;- pi - (grad / hess)\n  \n  pi &lt;- pi_new\n  \n  # Check for convergence\n  if (abs(pi_new - pi) &lt; tol) {\n    break\n  }\n  \n  iter &lt;- iter + 1\n}\n\n#after convergence, print the result\n\nprint(pi)\n\n[1] 0.293\n\n\nSo what’s gone on here? We have 1000 observations of a binary variable; maximum likelihood is answering the question “what is the value of the parameter \\(\\pi\\) that makes the observed data most likely?” To get that answer, we have tried candidate values of \\(\\pi\\) to see which one maximizes the log-likelihood function.\nWe’ve maximized the function two ways - using a grid search process, and using an optimization algorith implement in the maxLik package; it uses the Newton-Raphson method to find the maximum of the likelihood function. The start argument is the initial guess for the parameter \\(\\pi\\). The method argument specifies the optimization method, in this case, Newton-Raphson.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s make this more realistic in terms of resembling the models we want to run by adding an intercept term and multiple predictors to the model - the likelihood now has to account for those \\(X\\) variables and maximize with respect to them.\n\ngenerate some data on \\(y\\), and \\(X\\)\nwrite the likelihood\nwrite the Newton-Raphson algorithm\napply to the data\n\n\n# Generate binary y, and two X variables for regression; coefficients are -1, 0.5, -0.5\n\nset.seed(8675309)\nn &lt;- 10000\nX &lt;- matrix(rnorm(n * 2), ncol = 2)\ntrue_beta &lt;- c(-1, 0.5, -0.5)\nz &lt;- cbind(1, X) %*% true_beta\nprob &lt;- 1 / (1 + exp(-z))\ny &lt;- rbinom(n, 1, prob)\n\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson iterative algorithm\n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # Add intercept term to X if not already present\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#write the gradient function - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- plogis(X %*% beta)\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  k &lt;- ncol(X)\n  \n# set starting values for k columns of data (betas)\n  beta &lt;- rep(0, k) \n  \n# Store gradients and log-likelihoods for analysis\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n# iterate NR \n  for (iter in 1:max_iter) {\n    # compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # compute Hessian\n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # check for convergence\n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n    \n    # compute standard errors for this model by taking the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  }\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#html table comparing glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  column_spec(3, border_right = T) %&gt;%\n  column_spec(4, border_right = T) %&gt;%\n  collapse_rows(columns = 1:3, valign = \"top\")\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-0.9531826\n-0.9531826\n0.0236756\n0.0236748\n\n\nX1\n0.4938831\n0.4938831\n0.0238303\n0.0238295\n\n\nX2\n-0.4719703\n-0.4719703\n0.0240294\n0.0240286\n\n\n\n\n\n\n\n\nAlright, lots of fun here. Now, let’s use the program above to estimate a logit model using the democratic peace data.\n\n\n\n\ndp &lt;- read.csv(\"/users/dave/documents/teaching/606J-mle/2022/exercises/ex1/dp.csv\", header=TRUE)\n\n# for the log_likelihood function below, define y as the variable \"dispute\" in the dp data frame, and x1, x2, and x3 as the variables \"caprat\", \"border\", and \"deml\" in the dp data frame, respectively.\n\nX &lt;- as.matrix(dp[, c(\"caprat\", \"border\", \"deml\")])\ny &lt;- as.vector(dp$dispute)\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson \n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # Add intercept term to X if not already present\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#define the gradient function -  partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- 1 / (1 + exp(-X %*% beta))\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  p &lt;- ncol(X)\n  \n  # Initialize beta\n  beta &lt;- rep(0, p)\n  \n  # Store gradients and log-likelihoods\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n  for (iter in 1:max_iter) {\n    # Compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # Compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # Compute Hessian - 2nd partial derivatives \n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # Update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # Check for convergence - end if change in beta is less than tolerance value \n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n  }\n\n#compute standard errors for this model; take the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\nlogit$coefficients\n\n               [,1]\n       -3.666760193\ncaprat -0.002687365\nborder  0.945976154\ndeml   -0.078401814\n\nlogit$st.errors\n\n                   caprat       border         deml \n0.0721818683 0.0003920263 0.0738842477 0.0067789319 \n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#html table comparing glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:3, valign = \"top\")\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-3.6667602\n-3.6667602\n0.0721816\n0.0721819\n\n\nXcaprat\n-0.0026874\n-0.0026874\n0.0003920\n0.0003920\n\n\nXborder\n0.9459761\n0.9459762\n0.0738841\n0.0738842\n\n\nXdeml\n-0.0784018\n-0.0784018\n0.0067789\n0.0067789\n\n\n\n\n\n\n\n# extract LL from glm \n# library(stats)  \n# logLik(glm_fit)\n\nWhat did we just do?\n\nWe wrote a log-likelihood function for a logistic regression model with multiple predictors.\nTo maximize the log-likelihood, we implemented a Newton-Raphson algorithm in the following steps:\n\nDefined the gradient and Hessian functions. The gradient is the vector of first partial derivatives of the log-likelihood, and the Hessian is the matrix of second partial derivatives.\nInitialized the beta coefficients and stored gradients and log-likelihoods.\nIterated the algorithm to update beta values.\nChecked for convergence based on the change in beta.\n\nOnce the model converged, we computed standard errors for the estimated coefficients by taking the square root of the main diagonal elements of the inverse negative Hessian.\n\n\n\n\n\n#plot gradient convergence\ngradients &lt;- do.call(cbind, logit$gradient_history)\ngradients &lt;- as.data.frame(gradients) %&gt;%\n  mutate(coef=c(\"Intercept\", \"caprat\", \"border\", \"deml\")) \n\ngradients &lt;- gradients %&gt;% \n  pivot_longer(-coef, names_to = \"iteration\", values_to = \"value\")\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(gradients, \"line\", hcaes(x=iteration, y=value, group=coef, color=coef)) %&gt;%\n  hc_title(text = \"Gradient Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Gradient\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\n\n\n#plot ll convergence\n\nlls &lt;- as.data.frame(logit$ll_history) %&gt;%\n  mutate(iteration=1:nrow(.))\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(lls, \"line\", hcaes(x=iteration, y=`logit$ll_history`)) %&gt;%\n  hc_title(text = \"Log-Likelihood Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_colors(bucolors)"
  },
  {
    "objectID": "llfmax.html#optimization",
    "href": "llfmax.html#optimization",
    "title": "Writing the Likelihood",
    "section": "",
    "text": "Numerical optimization is a field aimed at finding “best” outcomes or answers depending on some set of criteria - often, the “best” is the maximum or minimum of some function - that’s the case in maximum likelihood.\n\n\nOne way to accomplish this is to use a grid search, where we calculate the log-likelihood for a range of values of \\(\\pi\\) and find the value that maximizes the log-likelihood. We can repeat this to the desired level of specificity, progressively narrowing the grid. In this example, we’ll just do one search out to 3 decimal places. The steps are these:\n\ngenerate the range of candidate values of our parameter \\(\\pi\\) to plug into the log-likelihood function.\nplug each value into the log-likelihood function to compute the log-likelihood for that value.\nidentify which value of \\(\\pi\\) maximizes the log-likelihood.\n\nHere’s code to do this:\n\n# generate a vector of values of pi \n\npi_trials &lt;- seq(0, 1, by = 0.001)\n\n# Calculate log-likelihood for each pi value\n\nll_values &lt;- sapply(pi_trials, log_likelihood, y = y)\n\n# Find the pi value that maximizes the log-likelihood\n\npi_hat &lt;- pi_trials[which.max(ll_values)]\nprint(pi_hat)\n\n[1] 0.293\n\n\nYou’ll note we recover the sample mean just as we did above using the maxlik function. Let’s plot the log-likelihood against the values of \\(\\pi\\) to visualize the maximum.\n\n\ncode\n# Plot log-likelihood against pi\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(data.frame(pi = pi_trials, ll = ll_values), \"line\", hcaes(x = pi, y = ll)) %&gt;%\n  hc_title(text = \"What value of Pi maximizes the log-likelihood?\") %&gt;%\n  hc_xAxis(title = list(text = \"Pi\"), \n           plotLines = list(\n    list(color =\"red\", value = pi_hat ))) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_legend(enabled = FALSE) %&gt;%\n  hc_colors(bucolors)\n\n\n\n\n\n\n\n\n\nThere are lots of ways to optimize a function - in statistical software, most of them derive from Newton’s iterative method. A very common algorithm is the Newton-Raphson method. This is what maxlik used in the example above. Newton-Raphson is an iterative process that starts with an initial guess for the parameter \\(\\pi\\) and updates it in the direction of the maximum until it converges to the maximum. At each step, it computes the first and second derivatives of the likelihood function, then uses these to update the parameter estimate. The update rule is:\n\\[ \\pi_{new} = \\pi_{old} - \\frac{g(\\pi)}{H(\\pi)} \\]\nSo the new estimate of the parameter is the old estimate minus the first derivative (gradient) of the log-likelihood function divided by the Hessian (second derivative) of the log-likelihood function. Here, \\(g(\\pi)\\) is the gradient of the log-likelihood function and \\(H(\\pi)\\) is the Hessian of the log-likelihood function. The algorithm iterates this process until the change in \\(\\pi\\) is sufficiently small, below a specified threshold.\nLet’s write the Newton-Raphson algorithm ourselves:\n\n\n\n\n# use same data generated above; same LLF as above, aiming to recover pi\n\n#write the gradient - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(pi, y) {\n  sum(y / pi - (1 - y) / (1 - pi))\n}\n\n#write the Hessian - matrix of partial second derivatives of the log-likelihood function.\n\nhessian &lt;- function(pi, y) {\n  sum(-y / pi^2 - (1 - y) / (1 - pi)^2)\n}\n\n#declare a starting value for pi\n\npi &lt;- 0.5\n\n#set convergence criteria - when new pi is within 1e-6 of old pi, stop iterating\n\ntol &lt;- 1e-6\n\n#initialize iteration counter, set maximum iterations; how many times will we do this before we stop if we don't converge prior? \n\niter &lt;- 0\nmax_iter &lt;- 100\n\n#iterate the Newton-Raphson algorithm\n\nfor (iter in 1:max_iter) {\n  # Compute gradient  \n  grad &lt;- gradient(pi, y)\n  \n  # Compute Hessian\n  hess &lt;- hessian(pi, y)\n  \n  # Update pi\n  pi_new &lt;- pi - (grad / hess)\n  \n  pi &lt;- pi_new\n  \n  # Check for convergence\n  if (abs(pi_new - pi) &lt; tol) {\n    break\n  }\n  \n  iter &lt;- iter + 1\n}\n\n#after convergence, print the result\n\nprint(pi)\n\n[1] 0.293\n\n\nSo what’s gone on here? We have 1000 observations of a binary variable; maximum likelihood is answering the question “what is the value of the parameter \\(\\pi\\) that makes the observed data most likely?” To get that answer, we have tried candidate values of \\(\\pi\\) to see which one maximizes the log-likelihood function.\nWe’ve maximized the function two ways - using a grid search process, and using an optimization algorith implement in the maxLik package; it uses the Newton-Raphson method to find the maximum of the likelihood function. The start argument is the initial guess for the parameter \\(\\pi\\). The method argument specifies the optimization method, in this case, Newton-Raphson.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s make this more realistic in terms of resembling the models we want to run by adding an intercept term and multiple predictors to the model - the likelihood now has to account for those \\(X\\) variables and maximize with respect to them.\n\ngenerate some data on \\(y\\), and \\(X\\)\nwrite the likelihood\nwrite the Newton-Raphson algorithm\napply to the data\n\n\n# Generate binary y, and two X variables for regression; coefficients are -1, 0.5, -0.5\n\nset.seed(8675309)\nn &lt;- 10000\nX &lt;- matrix(rnorm(n * 2), ncol = 2)\ntrue_beta &lt;- c(-1, 0.5, -0.5)\nz &lt;- cbind(1, X) %*% true_beta\nprob &lt;- 1 / (1 + exp(-z))\ny &lt;- rbinom(n, 1, prob)\n\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson iterative algorithm\n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # Add intercept term to X if not already present\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#write the gradient function - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- plogis(X %*% beta)\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  k &lt;- ncol(X)\n  \n# set starting values for k columns of data (betas)\n  beta &lt;- rep(0, k) \n  \n# Store gradients and log-likelihoods for analysis\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n# iterate NR \n  for (iter in 1:max_iter) {\n    # compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # compute Hessian\n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # check for convergence\n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n    \n    # compute standard errors for this model by taking the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  }\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#html table comparing glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  column_spec(3, border_right = T) %&gt;%\n  column_spec(4, border_right = T) %&gt;%\n  collapse_rows(columns = 1:3, valign = \"top\")\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-0.9531826\n-0.9531826\n0.0236756\n0.0236748\n\n\nX1\n0.4938831\n0.4938831\n0.0238303\n0.0238295\n\n\nX2\n-0.4719703\n-0.4719703\n0.0240294\n0.0240286\n\n\n\n\n\n\n\n\nAlright, lots of fun here. Now, let’s use the program above to estimate a logit model using the democratic peace data.\n\n\n\n\ndp &lt;- read.csv(\"/users/dave/documents/teaching/606J-mle/2022/exercises/ex1/dp.csv\", header=TRUE)\n\n# for the log_likelihood function below, define y as the variable \"dispute\" in the dp data frame, and x1, x2, and x3 as the variables \"caprat\", \"border\", and \"deml\" in the dp data frame, respectively.\n\nX &lt;- as.matrix(dp[, c(\"caprat\", \"border\", \"deml\")])\ny &lt;- as.vector(dp$dispute)\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson \n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # Add intercept term to X if not already present\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#define the gradient function -  partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- 1 / (1 + exp(-X %*% beta))\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  p &lt;- ncol(X)\n  \n  # Initialize beta\n  beta &lt;- rep(0, p)\n  \n  # Store gradients and log-likelihoods\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n  for (iter in 1:max_iter) {\n    # Compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # Compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # Compute Hessian - 2nd partial derivatives \n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # Update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # Check for convergence - end if change in beta is less than tolerance value \n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n  }\n\n#compute standard errors for this model; take the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\nlogit$coefficients\n\n               [,1]\n       -3.666760193\ncaprat -0.002687365\nborder  0.945976154\ndeml   -0.078401814\n\nlogit$st.errors\n\n                   caprat       border         deml \n0.0721818683 0.0003920263 0.0738842477 0.0067789319 \n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#html table comparing glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:3, valign = \"top\")\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-3.6667602\n-3.6667602\n0.0721816\n0.0721819\n\n\nXcaprat\n-0.0026874\n-0.0026874\n0.0003920\n0.0003920\n\n\nXborder\n0.9459761\n0.9459762\n0.0738841\n0.0738842\n\n\nXdeml\n-0.0784018\n-0.0784018\n0.0067789\n0.0067789\n\n\n\n\n\n\n\n# extract LL from glm \n# library(stats)  \n# logLik(glm_fit)\n\nWhat did we just do?\n\nWe wrote a log-likelihood function for a logistic regression model with multiple predictors.\nTo maximize the log-likelihood, we implemented a Newton-Raphson algorithm in the following steps:\n\nDefined the gradient and Hessian functions. The gradient is the vector of first partial derivatives of the log-likelihood, and the Hessian is the matrix of second partial derivatives.\nInitialized the beta coefficients and stored gradients and log-likelihoods.\nIterated the algorithm to update beta values.\nChecked for convergence based on the change in beta.\n\nOnce the model converged, we computed standard errors for the estimated coefficients by taking the square root of the main diagonal elements of the inverse negative Hessian.\n\n\n\n\n\n#plot gradient convergence\ngradients &lt;- do.call(cbind, logit$gradient_history)\ngradients &lt;- as.data.frame(gradients) %&gt;%\n  mutate(coef=c(\"Intercept\", \"caprat\", \"border\", \"deml\")) \n\ngradients &lt;- gradients %&gt;% \n  pivot_longer(-coef, names_to = \"iteration\", values_to = \"value\")\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(gradients, \"line\", hcaes(x=iteration, y=value, group=coef, color=coef)) %&gt;%\n  hc_title(text = \"Gradient Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Gradient\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\n\n\n#plot ll convergence\n\nlls &lt;- as.data.frame(logit$ll_history) %&gt;%\n  mutate(iteration=1:nrow(.))\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(lls, \"line\", hcaes(x=iteration, y=`logit$ll_history`)) %&gt;%\n  hc_title(text = \"Log-Likelihood Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_colors(bucolors)"
  },
  {
    "objectID": "llfmax.html#programming-the-llf",
    "href": "llfmax.html#programming-the-llf",
    "title": "Writing the Likelihood",
    "section": "",
    "text": "The next step is to write a function that calculates the log-likelihood for a given value of \\(\\pi\\) and the observed data \\(y\\). Do this over and over for different values of \\(\\pi\\) and find the value of \\(\\pi\\) that maximizes the log-likelihood. First, let’s do this using the maxLik library in R.\n\nlibrary(maxLik)\n# Write the binomial log-likelihood function\nlog_likelihood &lt;- function(pi, y) {\n  sum(y * log(pi) + (1 - y) * log(1 - pi))\n}\n\n#maximize the function using maxLik\n\nm1m &lt;- maxLik(y=y, log_likelihood, start = c(0.5), method = \"NR\") #Newton-Raphson method, starting value of 0.5\n\nsummary(m1m)\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 3 iterations\nReturn code 2: successive function values within tolerance limit (tol)\nLog-Likelihood: -604.816 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(&gt; t)    \n[1,]  0.29300    0.01441   20.34  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\nNotice the value of \\(\\pi\\) that maximizes the log-likelihood is the same as the sample mean of \\(y\\), which is 0.293. The function recovers the true value of \\(\\pi\\) in this case."
  },
  {
    "objectID": "llfmax.html#grid-search",
    "href": "llfmax.html#grid-search",
    "title": "Writing the Likelihood",
    "section": "Grid Search",
    "text": "Grid Search\nOne way to accomplish this is to use a grid search, where we calculate the log-likelihood for a range of values of \\(\\pi\\) and find the value that maximizes the log-likelihood. We can repeat this to the desired level of specificity, progressively narrowing the grid. In this example, we’ll just do one search out to 3 decimal places. The steps are these:\n\ngenerate the range of candidate values of our parameter \\(\\pi\\) to plug into the log-likelihood function.\nplug each value into the log-likelihood function to get the log-likelihood for that value.\nidentify which value of \\(\\pi\\) maximizes the log-likelihood.\n\n\n# generate a vector of values of pi \n\npi_trials &lt;- seq(0, 1, by = 0.001)\n\n# Calculate log-likelihood for each pi value\n\nll_values &lt;- sapply(pi_trials, log_likelihood, y = y)\n\n# Find the pi value that maximizes the log-likelihood\n\npi_hat &lt;- pi_trials[which.max(ll_values)]\nprint(pi_hat)\n\n[1] 0.293\n\n\nYou’ll note we recover the sample mean just as we did above using the maxlik function. Let’s plot the log-likelihood against the values of \\(\\pi\\) to see where the maximum is.\n\n\ncode\n# Plot log-likelihood against pi\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(data.frame(pi = pi_trials, ll = ll_values), \"line\", hcaes(x = pi, y = ll)) %&gt;%\n  hc_title(text = \"What value of Pi maximizes the log-likelihood?\") %&gt;%\n  hc_xAxis(title = list(text = \"Pi\"), \n           plotLines = list(\n    list(color =\"red\", value = pi_hat ))) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_legend(enabled = FALSE) %&gt;%\n  hc_colors(bucolors)\n\n\n\n\n\n\nSo what’s gone on here? We have 1000 observations of a binary variable; maximum likelihood is answering the question “what is the value of the parameter \\(\\pi\\) that makes the observed data most likely?” To get that answer, we have tried candidate values of \\(\\pi\\) to see which one maximizes the log-likelihood function.\nWe’ve maximized the function two ways - using a grid search process, and using an optimization algorith implement in the maxLik package; it uses the Newton-Raphson method to find the maximum of the likelihood function. The start argument is the initial guess for the parameter \\(\\pi\\). The method argument specifies the optimization method, in this case, Newton-Raphson."
  },
  {
    "objectID": "llfmax.html#review",
    "href": "llfmax.html#review",
    "title": "Writing the Likelihood",
    "section": "Review",
    "text": "Review\nLet’s recall the motivation for turning to ML. The data we have are not suitable for OLS because our observation of the data is limited. For instance, we observe a binary variable \\([0,1]\\) rather than the underlying continuous probability beneath it. If we could observe and measure that probability, we’d do so and perhaps use OLS to estimate a model. But since we only observe \\([0,1]\\), our observation is limited.\nOLS is poorly suited to model this variable especially if one of our goals is to generate predictions of that underlying probability. That is, one of our goals might be to estimate a measure of the \\(y\\) variable we wish we could measure directly.\nMLE is better suited to this task - it asks us what the data generating process is that produced the observed data, and to build a model appropriate to the limited observation of that \\(y\\) variable.\nTo do so, we need to:\n\ndescribe the observed distribution of \\(y\\)\nconsider what we wish we could measure - this points to the key quantities of interest we want to derive from the model.\ndescribe \\(y\\) according to a probability distribution\nwrite a log-likelihood function appropriate to that probability distribution.\nparameterize the log-likelihood function such that we have a link distribution to map the linear prediction, \\(x\\beta\\), onto the probability space of \\(y\\)."
  },
  {
    "objectID": "llfmax.html#outline",
    "href": "llfmax.html#outline",
    "title": "Writing the Likelihood",
    "section": "",
    "text": "These slides are aimed at walking through the process of writing a likelihood function based on the data we observe, then programming that likelihood function and optimizing it with respect to the data."
  },
  {
    "objectID": "llfmax.html#binary-data",
    "href": "llfmax.html#binary-data",
    "title": "Writing the Likelihood",
    "section": "",
    "text": "Let’s start with a y variable as follows:\n\n\ncode\n#binary y variable\nset.seed(8675309)\nn &lt;- 1000\nX &lt;- matrix(rnorm(n * 2), ncol = 2)\ntrue_beta &lt;- c(-1, 0.5, -0.5)\nz &lt;- cbind(1, X) %*% true_beta\nprob &lt;- 1 / (1 + exp(-z))\ny &lt;- rbinom(n, 1, prob)\n\n\nlibrary(kableExtra)\ntable(y)%&gt;% \n  as.data.frame() %&gt;% \n  rename( \"count\" = \"Freq\") %&gt;% \n  mutate(y = ifelse(y == 0, \"0\", \"1\")) %&gt;% \n  kable(\"html\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  collapse_rows(columns = 1, valign = \"top\")\n\n\n\n\n\n\ny\ncount\n\n\n\n\n0\n707\n\n\n1\n293\n\n\n\n\n\n\n\n\nThe variable, \\(y\\), takes on values of zero and one; let’s write this in terms of the binomial distribution parameter \\(\\pi\\), so it takes on the value of one with probability \\(\\pi\\) and zero with probability \\(1-\\pi\\).\n\\[ y_i = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{} \\pi_{i}\\\\\n         0, & \\mbox{} 1-\\pi_{i}\n         \\end{array}\n     \\right.\\]\nThe likelihood of a single observation is:\n\\[ L(\\pi_i|y) = \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\]\nThis is a statement of the likelihood any particular value of \\(\\pi\\) generated the data.\nThe log-likelihood function for a single observation is:\n\\[ \\log L(\\pi_i |y ) = y_i \\log(\\pi_i) + (1-y_i) \\log(1-\\pi_i) \\]\nThe log-likelihood function for the entire dataset is the sum of the log-likelihoods for each observation:\n\\[ \\log L(\\pi|y) = \\sum_{i=1}^{n} y_i \\log(\\pi_i) + (1-y_i) \\log(1-\\pi_i) \\]"
  },
  {
    "objectID": "binarymodels24.html#parameterize-the-model",
    "href": "binarymodels24.html#parameterize-the-model",
    "title": "Binary Response Models",
    "section": "Parameterize the model",
    "text": "Parameterize the model\nParameterize \\(\\pi_i\\) - make \\(\\pi_i\\) a function of some variables and their slope effects, \\(x\\beta\\) - this is the systematic component of the model:\n\\[\\pi_i= F(x \\beta)\\]\nThis is the binomial log-likelihood function.\n\\[\\ln \\mathcal{L} (\\pi| \\ y) = \\sum \\limits_{i=1}^{n}  \\left[ y_i \\ln (F(x_i\\widehat{\\beta})) +  (1-y_i) \\ln(1-F(x_i\\widehat{\\beta}))\\right]\\]\nBut we need to fill in \\(F\\), the link function."
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "Binomial Models\nPrediction Methods\n\n\n\n Back to top"
  },
  {
    "objectID": "binarymodels24.html#questions",
    "href": "binarymodels24.html#questions",
    "title": "Binary Response Models",
    "section": "",
    "text": "How can we model a binary \\(y\\) variable?\nDoes OLS (the linear probability model) work sufficiently well?\nHow can we build a maximum likelihood model?"
  },
  {
    "objectID": "llfmax.html#motivating-likelihood",
    "href": "llfmax.html#motivating-likelihood",
    "title": "Writing the Likelihood",
    "section": "",
    "text": "Let’s recall the motivation for turning to ML. The data we have are not suitable for OLS because our observation of the data is limited. For instance, we observe a binary variable \\([0,1]\\) rather than the underlying continuous probability beneath it. If we could observe and measure that probability, we’d do so and perhaps use OLS to estimate a model. But since we only observe \\([0,1]\\), our observation is limited.\nOLS is poorly suited to model this variable especially if one of our goals is to generate predictions of that underlying probability. That is, one of our goals might be to estimate a measure of the \\(y\\) variable we wish we could measure directly.\nMLE is better suited to this task - it asks us what the data generating process is that produced the observed data, and to build a model appropriate to the limited observation of that \\(y\\) variable.\nTo do so, we need to:\n\ndescribe the observed distribution of \\(y\\)\nconsider what we wish we could measure - this points to the key quantities of interest we want to derive from the model.\ndescribe \\(y\\) according to a probability distribution; write that distribution\nwrite a log-likelihood function appropriate to that probability distribution.\nparameterize the log-likelihood function such that we have a link distribution to map the linear prediction, \\(x\\beta\\), onto the probability space of \\(y\\)."
  },
  {
    "objectID": "llfmax.html#binary-y-variable",
    "href": "llfmax.html#binary-y-variable",
    "title": "Writing the Likelihood",
    "section": "",
    "text": "Let’s start with a \\(y\\) variable as follows:\n\n\ncode\n#binary y variable\nset.seed(8675309)\nn &lt;- 1000\nX &lt;- matrix(rnorm(n * 2), ncol = 2)\ntrue_beta &lt;- c(-1, 0.5, -0.5)\nz &lt;- cbind(1, X) %*% true_beta\nprob &lt;- 1 / (1 + exp(-z))\ny &lt;- rbinom(n, 1, prob)\n# y &lt;- as.factor(y)\n\nlibrary(kableExtra)\ntable(y)%&gt;%\n  as.data.frame() %&gt;%\n  rename( \"count\" = \"Freq\") %&gt;%\n  mutate(y = ifelse(y == 0, \"0\", \"1\")) %&gt;%\n  kable(\"html\") %&gt;%\n  kable_styling(\"striped\", full_width = F) %&gt;%\n  column_spec(1, border_right = T) %&gt;%\n  collapse_rows(columns = 1, valign = \"top\")\n\n\n\n\n\n\ny\ncount\n\n\n\n\n0\n707\n\n\n1\n293\n\n\n\n\n\n\n\n\nThe variable, \\(y\\), takes on values of zero and one - it appears binomial; let’s write this in terms of the binomial distribution parameter \\(\\pi\\), so it takes on the value of one with probability \\(\\pi\\) and zero with probability \\(1-\\pi\\).\n\\[ y_i = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{} \\pi_{i}\\\\\n         0, & \\mbox{} 1-\\pi_{i}\n         \\end{array}\n     \\right.\\]\nThe likelihood of a single observation is:\n\\[ L(\\pi_i|y) = \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\]\nThis is a statement of the likelihood that any particular value of \\(\\pi\\) generated an observation. We want the likelihood any value of \\(\\pi\\) generated the entire dataset - this is the joint likelihood of all the observations.\n\\[ L(\\pi|y) = \\prod_{i=1}^{n} \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\] recalling that a joint probability is the product of individual probabilities.\nLet’s take the log of this likelihood function - the natural log makes computation easier. Even for computers, this reduces the computational intensity especially with respect to extremely small decimals.\n\\[ \\ln L(\\pi|y) = \\sum_{i=1}^{n} y_i \\ln(\\pi_i) + (1-y_i) \\ln(1-\\pi_i) \\]\nWe can take this a step further and parameterize \\(\\pi\\) as a function of \\(X\\beta\\) where \\(X\\) is the matrix of predictors and \\(\\beta\\) is the vector of coefficients such that\n\\[ \\pi_i = F({X_i\\beta}) \\]\nSo our estimate of the binomial probability is a function of the linear predictor \\(X\\beta\\). We map \\(X\\beta\\) onto the probability space \\(\\pi\\) using a link function, \\(F\\). The most common link functions are the logistic (logit model), and the standard normal (probit model). Let’s write the logit link:\n\\[ \\pi_i = \\frac{1}{1 + e^{-(X_i\\beta)}} \\]\nand now let’s write this all in the log-likelihood function:\n\\[ \\ln L(\\beta|y) = \\sum_{i=1}^{n} y_i \\ln(\\frac{1}{1 + e^{-(X_i\\beta)}}) + (1-y_i) \\ln(1-\\frac{1}{1 + e^{-(X_i\\beta)}}) \\]"
  },
  {
    "objectID": "llfmax (1).html",
    "href": "llfmax (1).html",
    "title": "Writing the Likelihood",
    "section": "",
    "text": "The goal here is to walk through the process of writing a likelihood function based on the data we observe, then programming that likelihood function and optimizing it with respect to the data.\nLet’s start with a y variable as follows:\ncode\n#binary y variable\nset.seed(8675309)\nn &lt;- 1000\nX &lt;- matrix(rnorm(n * 2), ncol = 2)\ntrue_beta &lt;- c(-1, 0.5, -0.5)\nz &lt;- cbind(1, X) %*% true_beta\nprob &lt;- 1 / (1 + exp(-z))\ny &lt;- rbinom(n, 1, prob)\n\n\nlibrary(kableExtra)\ntable(y)%&gt;% \n  as.data.frame() %&gt;% \n  rename( \"count\" = \"Freq\") %&gt;% \n  mutate(y = ifelse(y == 0, \"0\", \"1\")) %&gt;% \n  kable(\"html\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  collapse_rows(columns = 1, valign = \"top\")\n\n\n\n\n\n\ny\ncount\n\n\n\n\n0\n707\n\n\n1\n293\nThe variable, \\(y\\), takes on values of zero and one; let’s write this in terms of the binomial distribution parameter \\(\\pi\\), so it takes on the value of one with probability \\(\\pi\\) and zero with probability \\(1-\\pi\\).\n\\[ y_i = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{} \\pi_{i}\\\\\n         0, & \\mbox{} 1-\\pi_{i}\n         \\end{array}\n     \\right.\\]\nThe likelihood of a single observation is:\n\\[ L(\\pi_i|y) = \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\]\nThis is a statement of the likelihood any particular value of \\(\\pi\\) generated the data.\nThe log-likelihood function for a single observation is:\n\\[ \\log L(\\pi_i |y ) = y_i \\log(\\pi_i) + (1-y_i) \\log(1-\\pi_i) \\]\nThe log-likelihood function for the entire dataset is the sum of the log-likelihoods for each observation:\n\\[ \\log L(\\pi|y) = \\sum_{i=1}^{n} y_i \\log(\\pi_i) + (1-y_i) \\log(1-\\pi_i) \\]"
  },
  {
    "objectID": "llfmax (1).html#programming-the-llf",
    "href": "llfmax (1).html#programming-the-llf",
    "title": "Writing the Likelihood",
    "section": "Programming the LLF",
    "text": "Programming the LLF\nThe next step is to write a function that calculates the log-likelihood for a given value of \\(\\pi\\) and the observed data \\(y\\). Do this over and over for different values of \\(\\pi\\) and find the value of \\(\\pi\\) that maximizes the log-likelihood. First, let’s do this using the maxLik library in R.\n\nlibrary(maxLik)\n# Write the binomial log-likelihood function\nlog_likelihood &lt;- function(pi, y) {\n  sum(y * log(pi) + (1 - y) * log(1 - pi))\n}\n\n#maximize the function using maxLik\n\nm1m &lt;- maxLik(y=y, log_likelihood, start = c(0.5), method = \"NR\") #Newton-Raphson method, starting value of 0.5\n\nsummary(m1m)\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 3 iterations\nReturn code 2: successive function values within tolerance limit (tol)\nLog-Likelihood: -604.816 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(&gt; t)    \n[1,]  0.29300    0.01441   20.34  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\nNotice the value of \\(\\pi\\) that maximizes the log-likelihood is the same as the sample mean of \\(y\\), which is 0.293. The function recovers the true value of \\(\\pi\\) in this case."
  },
  {
    "objectID": "llfmax (1).html#grid-search",
    "href": "llfmax (1).html#grid-search",
    "title": "Writing the Likelihood",
    "section": "Grid Search",
    "text": "Grid Search\nOne way to accomplish this is to use a grid search, where we calculate the log-likelihood for a range of values of \\(\\pi\\) and find the value that maximizes the log-likelihood. We repeat this to the desired level of specificity.\n\n# generate a vector of values of pi \n\npi_trials &lt;- seq(0, 1, by = 0.001)\n\n# Calculate log-likelihood for each pi value\n\nll_values &lt;- sapply(pi_trials, log_likelihood, y = y)\n\n# Find the pi value that maximizes the log-likelihood\n\npi_hat &lt;- pi_trials[which.max(ll_values)]\n\n# Plot log-likelihood values\n\nggplot(data.frame(pi = pi_trials, ll = ll_values), aes(x = pi, y = ll)) +\n  geom_line() +\n  geom_vline(xintercept = pi_hat, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Log-Likelihood vs. Pi\",\n       x = \"Pi\",\n       y = \"Log-Likelihood\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nSo what’s gone on here? We have 1000 observations of a binary variable; maximum likelihood is answering the question “what is the value of the parameter \\(\\pi\\) that makes the observed data most likely?” To get that answer, write the likelhood of any value of \\(\\pi\\) given the data, then maximize that function.\nHow did we maximize the function? Here, we used the maxLik package, which uses the Newton-Raphson method to find the maximum of the likelihood function. The start argument is the initial guess for the parameter \\(\\pi\\). The method argument specifies the optimization method, in this case, Newton-Raphson."
  },
  {
    "objectID": "llfmax (1).html#optimization",
    "href": "llfmax (1).html#optimization",
    "title": "Writing the Likelihood",
    "section": "Optimization",
    "text": "Optimization\nWhat is “optimization”? In this case, Newton-Raphson is a method for finding the maximum of a function. It’s an iterative process that starts with an initial guess for the parameter \\(\\pi\\) and updates it in the direction of the maximum until it converges to the maximum.\nxn = xn-1 – f(xn-1)/f’(xn-1)\nLet’s write the Newton-Raphson algorithm ourselves:\n\nNewton-Raphson\n\n# use same data generated above; same LLF as above, aiming to recover pi\n\n#define the gradient - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(pi, y) {\n  sum(y / pi - (1 - y) / (1 - pi))\n}\n\n#define the Hessian - matrix of partial second derivatives of the log-likelihood function.\n\nhessian &lt;- function(pi, y) {\n  sum(-y / pi^2 - (1 - y) / (1 - pi)^2)\n}\n\n#define initial guess for pi\n\npi &lt;- 0.5\n\n#set convergence criteria - when new pi is within 1e-6 of old pi, stop iterating\n\ntol &lt;- 1e-6\n\n#initialize iteration counter, set maximum iterations\n\niter &lt;- 0\nmax_iter &lt;- 100\n\n#iterate the Newton-Raphson algorithm\n\nfor (iter in 1:max_iter) {\n  # Compute gradient  \n  grad &lt;- gradient(pi, y)\n  \n  # Compute Hessian\n  hess &lt;- hessian(pi, y)\n  \n  # Update pi\n  pi_new &lt;- pi - (grad / hess)\n  \n  pi &lt;- pi_new\n  \n  # Check for convergence\n  if (abs(pi_new - pi) &lt; tol) {\n    break\n  }\n  \n  iter &lt;- iter + 1\n}\n\n#after convergence, print the result\n\nprint(pi)\n\n[1] 0.293\n\n\nOk, so now we have a log-likelihood function, and an algorithm for maximizing it. Let’s write this as a function that takes the observed data and returns the estimated parameter \\(\\pi\\).\n\nlogistic_regression &lt;- function(y, max_iter = 100, tol = 1e-6) {\n  # Initialize pi\n  pi &lt;- 0.5\n  \n  # Iterate the Newton-Raphson algorithm\n  for (iter in 1:max_iter) {\n    # Compute gradient\n    grad &lt;- gradient(pi, y)\n    \n    # Compute Hessian\n    hess &lt;- hessian(pi, y)\n    \n    # Update pi\n    pi_new &lt;- pi - grad / hess\n    \n    # Check for convergence\n    if (abs(pi_new - pi) &lt; tol) {\n      break\n    }\n    \n    pi &lt;- pi_new\n  }\n  \n  return(list(\n    pi = pi,\n    iterations = iter\n  ))\n}\n\nprint(logistic_regression(y))\n\n$pi\n[1] 0.293\n\n$iterations\n[1] 2\n\n\n\n\nMultiple Predictors\nLet’s make this a little more complex by adding an intercept term and multiple predictors to the model - the likelihood now has to account for those \\(X\\) variables and maximize with respect to them.\n\n# Generate binary y, and two X variables for regression; coefficients are -1, 0.5, -0.5\n\nset.seed(8675309)\nn &lt;- 10000\nX &lt;- matrix(rnorm(n * 2), ncol = 2)\ntrue_beta &lt;- c(-1, 0.5, -0.5)\nz &lt;- cbind(1, X) %*% true_beta\nprob &lt;- 1 / (1 + exp(-z))\ny &lt;- rbinom(n, 1, prob)\n\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson iterative algorithm\n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # Add intercept term to X if not already present\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#define the gradient function - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\n#p &lt;- 1 / (1 + exp(-X %*% beta))\np &lt;- plogis(X %*% beta)\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  k &lt;- ncol(X)\n  \n  # set starting values for k columns of data (betas)\n  beta &lt;- rep(0, k) \n  \n  # Store gradients and log-likelihoods for analysis\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n  # iterate NR \n  for (iter in 1:max_iter) {\n    # compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # compute Hessian\n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # check for convergence\n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n    \n    # compute standard errors for this model by taking the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  }\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#html table comparing glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  column_spec(3, border_right = T) %&gt;%\n  column_spec(4, border_right = T) %&gt;%\n  collapse_rows(columns = 1:3, valign = \"top\")\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-0.9531826\n-0.9531826\n0.0236756\n0.0236748\n\n\nX1\n0.4938831\n0.4938831\n0.0238303\n0.0238295\n\n\nX2\n-0.4719703\n-0.4719703\n0.0240294\n0.0240286\n\n\n\n\n\n\n\n\nAlright, lots of fun here. Now, let’s use the program above to estimate a logit model using the democratic peace data.\n\n\nLogit using Democratic Peace data\n\ndp &lt;- read.csv(\"/users/dave/documents/teaching/606J-mle/2022/exercises/ex1/dp.csv\", header=TRUE)\n\n# for the log_likelihood function below, define y as the variable \"dispute\" in the dp data frame, and x1, x2, and x3 as the variables \"caprat\", \"border\", and \"deml\" in the dp data frame, respectively.\n\nX &lt;- as.matrix(dp[, c(\"caprat\", \"border\", \"deml\")])\ny &lt;- as.vector(dp$dispute)\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson \n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # Add intercept term to X if not already present\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#define the gradient function -  partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- 1 / (1 + exp(-X %*% beta))\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  p &lt;- ncol(X)\n  \n  # Initialize beta\n  beta &lt;- rep(0, p)\n  \n  # Store gradients and log-likelihoods\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n  for (iter in 1:max_iter) {\n    # Compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # Compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # Compute Hessian - 2nd partial derivatives \n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # Update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # Check for convergence - end if change in beta is less than tolerance value \n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n  }\n\n#compute standard errors for this model; take the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\nlogit$coefficients\n\n               [,1]\n       -3.666760193\ncaprat -0.002687365\nborder  0.945976154\ndeml   -0.078401814\n\nlogit$st.errors\n\n                   caprat       border         deml \n0.0721818683 0.0003920263 0.0738842477 0.0067789319 \n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#html table comparing glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:3, valign = \"top\")\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-3.6667602\n-3.6667602\n0.0721816\n0.0721819\n\n\nXcaprat\n-0.0026874\n-0.0026874\n0.0003920\n0.0003920\n\n\nXborder\n0.9459761\n0.9459762\n0.0738841\n0.0738842\n\n\nXdeml\n-0.0784018\n-0.0784018\n0.0067789\n0.0067789\n\n\n\n\n\n\n\n# extract LL from glm \n# library(stats)  \n# logLik(glm_fit)\n\nWhat did we just do?\n\nWe wrote a log-likelihood function for a logistic regression model with multiple predictors.\nTo maximize the log-likelihood, we implemented a Newton-Raphson algorithm in the following steps:\n\nDefined the gradient and Hessian functions. The gradient is the vector of first partial derivatives of the log-likelihood, and the Hessian is the matrix of second partial derivatives.\nInitialized the beta coefficients and stored gradients and log-likelihoods.\nIterated the algorithm to update beta values.\nChecked for convergence based on the change in beta.\n\nOnce the model converged, we computed standard errors for the estimated coefficients by taking the square root of the main diagonal elements of the inverse negative Hessian.\n\n\n\nPlot Gradient over iterations\n\n#plot gradient convergence\ngradients &lt;- do.call(cbind, logit$gradient_history)\ngradients &lt;- as.data.frame(gradients) %&gt;%\n  mutate(coef=c(\"Intercept\", \"caprat\", \"border\", \"deml\")) \n\ngradients &lt;- gradients %&gt;% \n  pivot_longer(-coef, names_to = \"iteration\", values_to = \"value\")\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\nsewaneecolors&lt;-list(\"#582C83\",\"#FFB81C\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(gradients, \"line\", hcaes(x=iteration, y=value, group=coef, color=coef)) %&gt;%\n  hc_title(text = \"Log-Likelihood Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\nPlot Log-Likelihood over iterations\n\n#plot ll convergence\n\nlls &lt;- as.data.frame(logit$ll_history) %&gt;%\n  mutate(iteration=1:nrow(.))\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\nsewaneecolors&lt;-list(\"#582C83\",\"#FFB81C\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(lls, \"line\", hcaes(x=iteration, y=`logit$ll_history`)) %&gt;%\n  hc_title(text = \"Log-Likelihood Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_colors(bucolors)"
  },
  {
    "objectID": "llfmax.html#maximizing-the-likelihood",
    "href": "llfmax.html#maximizing-the-likelihood",
    "title": "Writing the Likelihood",
    "section": "",
    "text": "Let’s start with a simple maximization procedure called\nThe next step is to write a function that calculates the log-likelihood for a given value of \\(\\pi\\) and the observed data \\(y\\). Do this over and over for different values of \\(\\pi\\) and find the value of \\(\\pi\\) that maximizes the log-likelihood. First, let’s do this using the maxLik library in R.\n\nlibrary(maxLik)\n# Write the binomial log-likelihood function\nlog_likelihood &lt;- function(pi, y) {\n  sum(y * log(pi) + (1 - y) * log(1 - pi))\n}\n\n#maximize the function using maxLik\n\nm1m &lt;- maxLik(y=y, log_likelihood, start = c(0.5), method = \"NR\") #Newton-Raphson method, starting value of 0.5\n\nsummary(m1m)\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 3 iterations\nReturn code 2: successive function values within tolerance limit (tol)\nLog-Likelihood: -604.816 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(&gt; t)    \n[1,]  0.29300    0.01441   20.34  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\nNotice the value of \\(\\pi\\) that maximizes the log-likelihood is the same as the sample mean of \\(y\\), which is 0.293. The function recovers the true value of \\(\\pi\\) in this case."
  },
  {
    "objectID": "binarymodels24.html#binary-y-variables-1",
    "href": "binarymodels24.html#binary-y-variables-1",
    "title": "Binary Response Models",
    "section": "Binary \\(y\\) variables",
    "text": "Binary \\(y\\) variables\nGenerally, we think of a binary variable as being the observable manifestation of some latent, unobserved continuous variable.\nIf we could adequately observe (and measure) the underlying continuous variable, we’d use some form of OLS regression to analyze that variable. But because we have limited observation, we turn to maximum likelihood methods to estimate a model that allows to use \\(y\\), but generate estimates of \\(y^*\\), the variable we wish we could measure."
  },
  {
    "objectID": "binarymodels24.html#example---democratic-peace-data",
    "href": "binarymodels24.html#example---democratic-peace-data",
    "title": "Binary Response Models",
    "section": "Example - Democratic Peace data",
    "text": "Example - Democratic Peace data\nAs a running example, I’ll use the Democratic Peace data to estimate logit and probit models. These come from Oneal and Russett (1997)’s well-known study in ISQ. The units are dyad-years; the \\(y\\) variable is the presence or absence of a militarized dispute, and the \\(x\\) variables include a measure of democracy (the lowest of the two Polity scores in the dyad), and a set of controls. The principle expectation here is that as the lowest democracy score in the dyad increases, the probability of a militarized dispute decreases.\n\nPredictions out of bounds\nThis figured plots the predictions from a logit and OLS model. Unsurprisingly, the logit predictions are probabilities, so are in the \\([0,1]\\) interval. The OLS predictions are not, and are often out of bounds.\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\nlogitpreds &lt;- predict(m1, type=\"response\")\n\nm2 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"probit\"), data=dp )\nmols &lt;-lm(dispute ~ border+deml+caprat+ally, data=dp )\nolspreds &lt;- predict(mols)\n\ndf &lt;- data.frame(logitpreds, olspreds, dispute=as.factor(dp$dispute))\n\nggplot(df, aes(x=logitpreds, y=olspreds, color=dispute)) + \n  geom_point()+\n  labs(title=\"Predictions from Logit and OLS\", x=\"Logit Predictions\", y=\"OLS Predictions\")+\n  geom_hline(yintercept=0)+\n  theme_minimal() +\n  scale_color_manual(values=c(\"#005A43\", \"#6CC24A\")) +\n  annotate(\"text\", x=.05, y=-.05, label=\"2,147 Predictions out of bounds\", color=\"red\")\n\n\n\n\n\n\n\n\n\nHere’s the distribution of predictions from the OLS model - you’ll note the modal density is around .04 (which is the sample frequency of \\(y\\).), but that a substantial and long tail are negative, so out of probability bounds.\n\n\ncode\nggplot(df, aes(x=olspreds)) + \n  geom_density(alpha=.5)+\n  labs(title=\"Density of OLS Predictions\", x=\"Predictions\", y=\"Density\")+\n  theme_minimal()+\ngeom_vline(xintercept=0, linetype=\"dashed\")\n\n\n\n\n\n\n\n\n\n\n\nHeteroskedastic Residuals\nThe residuals from the OLS model appear heteroskedastic, and the distribution is not normal. In fact, the distribution appears more binomial, clustered around zero and one. This shouldn’t be surprising since the \\(y\\) variable only takes on values of zero and one, and since we compute the residuals by \\(u = y - \\hat{y}\\).\n\n\ncode\ndf &lt;- data.frame(df, mols$residuals)\n \nggplot(df, aes(x=mols.residuals, color=dispute)) + \n  geom_density()+\n  labs(title=\"Density of OLS Residuals\", x=\"Residuals\", y=\"Density\")+\n  theme_minimal()+\n    scale_color_manual(values=c(\"#005A43\", \"#6CC24A\")) +\n  geom_vline(xintercept=0, linetype=\"dashed\")"
  },
  {
    "objectID": "binarymodels24.html#when-is-the-lpm-appropriate",
    "href": "binarymodels24.html#when-is-the-lpm-appropriate",
    "title": "Binary Response Models",
    "section": "When is the LPM Appropriate?",
    "text": "When is the LPM Appropriate?\nThe best answer is never.\n\nThere seems to be a mild trend in the discipline to rehabilitate the LPM though it’s not clear why - that is, it’s hard to find statements about the advantages of doing so in any particular setting, or about the disadvantages of estimating a logit or probit model that would lead us to prefer the LPM.\n\nOLS is a rockin’ estimator, but it’s just not well suited to limited \\(y\\) variables. Efforts to rehabilitate the LPM are like putting lipstick on a pig."
  },
  {
    "objectID": "binarymodels24.html#binomial-likelihood",
    "href": "binarymodels24.html#binomial-likelihood",
    "title": "Binary Response Models",
    "section": "Binomial Likelihood",
    "text": "Binomial Likelihood\nWrite the binomial density:\n\\[ Pr(y=1| \\pi) = \\pi_i^{y_i} (1-\\pi_i)^{1-y_i} \\]\nWrite the joint probability as a likelihood:\n\\[\\mathcal{L} (\\pi |\\ y) = \\prod \\limits_{i=1}^{n} \\left[ \\pi_i^{y_i} (1-\\pi_i)^{1-y_i}\\right]\\]\nTake the log of that likelihood:\n\\[\\ln \\mathcal{L} (\\pi| \\ y) = \\sum \\limits_{i=1}^{n}  \\left[ y_i \\ln ( \\pi_i) +  (1-y_i) \\ln(1-\\pi_i)\\right]\\]"
  },
  {
    "objectID": "binarymodels24.html#predicted-probabilities",
    "href": "binarymodels24.html#predicted-probabilities",
    "title": "Binary Response Models",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\nIn the nonlinear model, the most basic quantity is\n\\[F(x\\widehat{\\beta})\\]\nwhere \\(F\\) is the link function, mapping the linear prediction onto the probability space.\nFor the logit, the predicted probability is\n\\[Pr(y=1) = \\frac{1}{1+exp(-x\\widehat{\\beta})}\\]\nFor the probit, the predicted probability is\n\\[Pr(y=1) = \\Phi(x\\widehat{\\beta})\\]\nAgain, simply using the link function to map the linear prediction onto the probability space."
  },
  {
    "objectID": "binarymodels24.html#marginal-effects",
    "href": "binarymodels24.html#marginal-effects",
    "title": "Binary Response Models",
    "section": "Marginal Effects",
    "text": "Marginal Effects\nIn the linear model, the marginal effect of \\(x\\) is \\(\\widehat{\\beta}\\). That is, the effect of a one unit change in \\(x\\) on \\(y\\) is \\(\\widehat{\\beta}\\).\n\\[\n\\frac{\\partial \\widehat{y}}{\\partial x_k}= \\frac{\\partial x \\widehat{\\beta}}{\\partial x_k} \\nonumber \\\\  \\nonumber \\\\\n= \\widehat{\\beta} \\nonumber\n\\]\nThe marginal effect is constant with respect to \\(x_k\\). Take a look:\n\n\ncode\nx &lt;- seq(0,10,.1)\ny &lt;- 2*x\ndf &lt;- data.frame(x=x, y=y)\n\nggplot(df, aes(x=x, y=y)) + \n  geom_line()+\n  labs(title=\"Marginal Effect of x on y\", x=\"x\", y=\"y\")+\n  theme_minimal() +\n  annotate(\"text\", x=5, y=15, label=\"y = 2x\", color=\"black\")+\n  geom_segment(aes(x = 5, xend = 5, y = 0, yend = 10), color = \"red\")+\n  geom_segment(aes(x = 10, xend = 10, y = 0, yend = 20), color = \"red\")\n\n\n\n\n\n\n\n\n\nThe effect of \\(x\\) on \\(y\\) is 2 - it’s the same at \\(x=5\\) and at \\(x=10\\).\nIn the nonlinear model, the marginal effect of \\(x_k\\) depends on where \\(x\\widehat{\\beta}\\) lies with respect to the probability distribution \\(F(\\cdot)\\).\n\\[\n\\frac{\\partial Pr(y=1)}{\\partial x_k}= \\frac{\\partial F(x\\widehat{\\beta})}{\\partial x_k} \\nonumber \\\\  \\nonumber \\\\\n=  \\frac{\\partial F(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} \\cdot \\frac{\\partial (x\\widehat{\\beta})}{\\partial x_k}  \\nonumber\n\\]\nBoth of these terms simplify …\nRemember that\n\\[\n\\frac{\\partial (x\\widehat{\\beta})}{\\partial x} = \\widehat{\\beta} \\nonumber\n\\]\nand \\[\n\\frac{\\partial F(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} = f(x\\widehat{\\beta}) \\nonumber\n\\]\nwhere the derivative of the CDF is the PDF.\nPutting these together gives us:\n\\[\n\\frac{\\partial F(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} = f(x\\widehat{\\beta}) \\widehat{\\beta} \\nonumber\n\\]\nThis is \\(\\widehat{\\beta}\\) weighted by or measured at the ordinate on the PDF - the ordinate is the height of the PDF associated with a value of the \\(x\\) axis.\n\n\n\n\n\n\nImportant Concept\n\n\n\nThe effect of \\(x\\) on \\(Pr(y=1)\\) is not constant; it will be large for some values of \\(x\\) and small for others. This makes sense if we think about the sigmoid functions - the slope of the curve is steepest at \\(y=.5\\), and flattens as we move away from that point toward either limit. Take another look at Figure 1\n\n\n\nLogit Marginal Effects\n\n\nRecall \\(\\Lambda\\) is the logistic CDF = \\[1/(1+exp(-x_i\\widehat{\\beta}))\\].\n\\(\\lambda\\) is the logit PDF \\[1/(1+exp(-x_i\\widehat{\\beta}))^2\\]\nAlso, remember that\n\\[\\frac{e^{x_i\\widehat{\\beta}}}{1+e^{x_i\\widehat{\\beta}}} = \\frac{1}{1+e^{-x_i\\widehat{\\beta}}}\\]\n\\[\n\\begin{align}\n\\frac{\\partial \\Lambda(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} = \\lambda(x\\widehat{\\beta}) \\widehat{\\beta} \\\\\n= \\frac{e^{x_i\\widehat{\\beta}}}{(1+e^{x_i\\widehat{\\beta}})^2} \\widehat{\\beta}  \\\\\n=\\frac{e^{x_i\\widehat{\\beta}}}{1+e^{x_i\\widehat{\\beta}}} \\frac{1}{1+e^{x_i\\widehat{\\beta}}} \\widehat{\\beta}  \\\\\n=\\Lambda(x_i\\widehat{\\beta}) \\frac{1+e^{x_i\\widehat{\\beta}}-e^{x_i\\widehat{\\beta}}}{1+e^{x_i\\widehat{\\beta}}} \\widehat{\\beta}   \\\\\n=\\Lambda(x_i\\widehat{\\beta}) 1-\\frac{e^{x_i\\widehat{\\beta}}}{1+e^{x_i\\widehat{\\beta}}} \\widehat{\\beta}  \\\\\n=\\Lambda(x_i\\widehat{\\beta}) (1-\\Lambda(x_i\\widehat{\\beta})) \\widehat{\\beta}  \n\\end{align}\n\\]\nSo this last line indicates the marginal effect of \\(x\\) is the probability of a one times the probability of a zero times \\(\\widehat{\\beta}\\).\nThis is useful because the largest value this can take on is .25 \\((Pr(y_i=1)=0.5 \\cdot Pr(y_i=0)=0.5= 0.25)\\) - therefore, the maximum marginal effect any \\(x\\) can have is \\(0.25 \\widehat{\\beta}\\).\nLooking at the democratic peace model below, the coefficient on democracy is -.071, so the largest effect democracy can have on the probability of a militarized dispute is \\(0.25 \\cdot -.071 = -.01775\\).\n\ncode\nlibrary(stargazer)\n\nstargazer(m1,m2, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\nlogistic\n\n\nprobit\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n0.587*** (0.037)\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n-0.031*** (0.003)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n-0.001*** (0.0001)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n-0.350*** (0.038)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n-1.903*** (0.032)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n7,032.985\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\nIn the probit model, the marginal effect is:\n\\[\n\\frac{\\partial \\Phi(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} = \\phi(x\\widehat{\\beta}) \\widehat{\\beta} \\nonumber\n\\]\nThe ordinate at the maximum of the standard normal PDF is 0.3989 - rounding to 0.4, we can say that the maximum marginal effect of any \\(\\widehat{\\beta}\\) in the probit model is \\(0.4\\widehat{\\beta}\\).\nThe ordinate is at the maximum where \\(z=0\\); recall this is the standard normal, so \\(x_i\\widehat{\\beta}=z\\). When \\(z=0\\),\n\\[Pr(z)=\\frac{1}{\\sqrt{2 \\pi}} \\exp \\left[\\frac{-(z)^{2}}{2}\\right] \\nonumber \\\\ \\nonumber\\\\\n=\\frac{1}{\\sqrt{2 \\pi}} \\nonumber\\\\\n\\approx .4 \\nonumber \\]\nSo the maximum marginal effect of any \\(x\\) in the probit model is \\(0.4\\widehat{\\beta}\\)."
  },
  {
    "objectID": "binarymodels24.html#at-mean-predictions-logit",
    "href": "binarymodels24.html#at-mean-predictions-logit",
    "title": "Binary Response Models",
    "section": "At-mean predictions (logit)",
    "text": "At-mean predictions (logit)\n\n\ncode\n#summary(m1)\n#confint(m1)\n\n#new data frame for MEM prediction\nmem &lt;- data.frame(deml= c(seq(-10,10,1)), \n                  border=0, caprat=median(dp$caprat), ally=0)\n\n# type=\"link\" produces the linear predictions; transform by hand below w/EPT\nmem  &lt;-data.frame(mem, predict(m1, type=\"link\", newdata=mem, se=TRUE))\n\nmem &lt;- cbind(mem,lb=plogis(mem$fit-1.96*mem$se.fit),\n             ub=plogis(mem$fit+1.96*mem$se.fit), \n             p=plogis(mem$fit))\n\nggplot(mem, aes(x=deml, y=p)) +\n  geom_line() +\n  geom_ribbon(data=mem, aes(x=deml, ymin=lb, ymax=ub),fill = \"grey70\", alpha = .4, ) +\n  labs(x=\"Polity Score\", y=\"Pr(Dispute) (95% confidence interval)\")"
  },
  {
    "objectID": "binarymodels24.html#average-effects-logit",
    "href": "binarymodels24.html#average-effects-logit",
    "title": "Binary Response Models",
    "section": "Average effects (logit)",
    "text": "Average effects (logit)\nAverage effects are often a better choice because they represent the data more completely than central tendency can (in the at-mean effects). Here are average effects (using the logit estimates) across the range of polity, and for pairs of states that share borders and those that do not.\n\n\ncode\n#avg effects\n\n#identify the estimation sample\ndp$used &lt;- TRUE\ndp$used[na.action(m1)] &lt;- FALSE\ndpesample &lt;- dp %&gt;%  filter(used==\"TRUE\")\n\npolity &lt;- 0\nmedxbd0 &lt;- 0\nubxbd0 &lt;- 0\nlbxbd0 &lt;- 0\n# medse &lt;- 0\n# medxbd1 &lt;- 0\n# ubxbd1 &lt;- 0\n# lbxbd1 &lt;- 0\n\nfor(i in seq(1,21,1)){\n  dpesample$border&lt;- 0\n  dpesample$deml &lt;- i-11\n  polity[i] &lt;- i-11\n  allpreds &lt;- data.frame(predict(m1, type= \"response\", se.fit=TRUE, newdata = dpesample))  \n  medxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)\n  ubxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)+1.96*(median(allpreds$se.fit, na.rm=TRUE))\n  lbxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)-1.96*(median(allpreds$se.fit, na.rm=TRUE))\n}\n  \nnoborder &lt;- data.frame(polity, medxbd0, ubxbd0, lbxbd0)\n  \nfor(i in seq(1,21,1)){\n  dpesample$border&lt;- 1\n  dpesample$deml &lt;- i-11\n  polity[i] &lt;- i-11\n  allpreds &lt;- data.frame(predict(m1, type= \"response\", se.fit=TRUE, newdata = dpesample))  \n  medxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)\n  ubxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)+1.96*(median(allpreds$se.fit, na.rm=TRUE))\n  lbxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)-1.96*(median(allpreds$se.fit, na.rm=TRUE))\n}\n  \nborder &lt;- data.frame(polity, medxbd0, ubxbd0, lbxbd0)\n  \nggplot() +\n  geom_ribbon(data=noborder, aes(x=polity, ymin=lbxbd0, ymax=ubxbd0),fill = \"grey70\", alpha = .4, ) +\n  geom_line(data=noborder, aes(x=polity, y=medxbd0))+\n  geom_ribbon(data=border, aes(x=polity, ymin=lbxbd0, ymax=ubxbd0),fill = \"grey70\", alpha = .4, ) +\n  geom_line(data=border, aes(x=polity, y=medxbd0))+\n  labs ( colour = NULL, x = \"Polity Score\", y =  \"Pr(Dispute)\" )+\n  theme_minimal()+\n  ggtitle(\"Average Effects\")"
  },
  {
    "objectID": "binarymodels24.html#simulation",
    "href": "binarymodels24.html#simulation",
    "title": "Binary Response Models",
    "section": "Simulation",
    "text": "Simulation\nSimulation is an especially good approach for nonlinear models:\n\nestimate the model.\n\\(m\\) times (say, 1000 times), simulate the distribution of \\(\\widehat{\\beta}\\).\ngenerate the \\(m\\) linear predictions, \\(x\\widehat{\\beta}\\).\ntransform by the appropriate link function (logistic, standard normal CDF).\nidentify the 2.5, 50, and 97.5 percentiles.\nplot against \\(x\\).\n\n\n\ncode\n#draws from multivariate normal using logit model estimates\nsimL &lt;- data.frame(MASS::mvrnorm(1000, coef(m1), vcov(m1)))\n#draws from multivariate normal using probit model estimates\nsimP &lt;- data.frame(MASS::mvrnorm(1000, coef(m2), vcov(m2)))\n\n#Logit predictions\nlogitprobs &lt;- data.frame(dem= numeric(0) , lb=numeric(0), med= numeric(0), ub=numeric(0))\nfor (i in seq(1,21,1)) {\nsimpreds &lt;- quantile(simL$X.Intercept.+ simL$border*0+simL$deml*(i-11)+simL$caprat*median(dp$caprat)+simL$ally*0, probs=c(.025, .5, .975))\nlogitprobs[i,] &lt;- data.frame(dem=i, lb=plogis(simpreds[1]), med=plogis(simpreds[2]), ub=plogis(simpreds[3]))\n}\n\n#Probit predictions\nprobitprobs &lt;- data.frame(dem= numeric(0) , lb=numeric(0), med= numeric(0), ub=numeric(0))\nfor (i in seq(1,21,1)) {\nsimpreds &lt;- quantile(simP$X.Intercept.+ simP$border*0+simP$deml*(i-11)+simP$caprat*median(dp$caprat)+simP$ally*0, probs=c(.025, .5, .975))\nprobitprobs[i,] &lt;- data.frame(dem=i, lb=pnorm(simpreds[1]), med=pnorm(simpreds[2]), ub=pnorm(simpreds[3]))\n}\n\nlogit &lt;- ggplot() +\n  geom_ribbon(data=logitprobs, aes(x=dem, ymin=lb, ymax=ub),fill = \"grey70\", alpha = .4, ) +\n  geom_line(data=logitprobs, aes(x=dem, y=med))+\n  labs ( colour = NULL, x = \"Polity Score\", y =  \"Pr(Dispute)\" )+\n  # theme_minimal()+\n  ggtitle(\"Logit Predictions\")\n\nprobit &lt;- ggplot() +\n  geom_ribbon(data=probitprobs, aes(x=dem, ymin=lb, ymax=ub),fill = \"grey70\", alpha = .4, ) +\n  geom_line(data=probitprobs, aes(x=dem, y=med))+\n  labs ( colour = NULL, x = \"Polity Score\", y =  \"Pr(Dispute)\" )+\n  theme_minimal()+\n  ggtitle(\"Probit Predictions\")\n\nlogit+probit"
  },
  {
    "objectID": "binarymodels24.html#simulating-combinations-of-binary-variables",
    "href": "binarymodels24.html#simulating-combinations-of-binary-variables",
    "title": "Binary Response Models",
    "section": "Simulating combinations of binary variables",
    "text": "Simulating combinations of binary variables\nLet’s look at the differences among the four combinations of the binary variables, border and ally.\n\n\ncode\n## simulating for binary combinations ----\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n\n#draws from multivariate normal using logit model estimates\nsimL &lt;- data.frame(MASS::mvrnorm(1000, coef(m1), vcov(m1)))\n\n\nlogitprobs &lt;- data.frame(b0a0= numeric(0) , b1a0=numeric(0), b0a1= numeric(0), b1a1=numeric(0))\n\n  b0a0 &lt;- plogis(simL$X.Intercept.+ simL$border*0+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*0)\nb1a0 &lt;- plogis(simL$X.Intercept.+ simL$border*1+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*0)\nb0a1 &lt;- plogis(simL$X.Intercept.+ simL$border*0+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*1)\nb1a1 &lt;- plogis(simL$X.Intercept.+ simL$border*1+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*1)\n\nlogitprobs &lt;- data.frame(b0a0, b1a0, b0a1, b1a1)\n\nggplot() +\n  geom_density(data=logitprobs, aes(x=b0a0), fill=\"grey70\", alpha = .4, ) +\n  geom_density(data=logitprobs, aes(x=b1a0), fill=\"grey70\", alpha = .4, ) +\n  geom_density(data=logitprobs, aes(x=b0a1), fill=\"grey70\", alpha = .4, ) +\n  geom_density(data=logitprobs, aes(x=b1a1), fill=\"grey70\", alpha = .4, ) +\n  labs ( colour = NULL, x = \"Pr(Dispute)\", y =  \"Density\" ) +\n  theme_minimal()+\n  annotate(\"text\", x = .07, y = 150, label = \"No border, not allies\", color = \"black\") +\n  annotate(\"text\", x = .13, y = 70, label = \"Border, not allies\", color = \"black\") +\n  annotate(\"text\", x = .04, y = 200, label = \"No border, allies\", color = \"black\") +\n  annotate(\"text\", x = .09, y = 50, label = \"Border, allies\", color = \"black\") +\n  ggtitle(\"Logit Predictions\")"
  },
  {
    "objectID": "prediction24.html",
    "href": "prediction24.html",
    "title": "Prediction Methods for MLE Models",
    "section": "",
    "text": "Most MLE models are nonlinear, so their coefficients are not their marginal effects. As a result, most MLE models require a transformation of the linear prediction to generate quantities of interest. The methods outlined here apply to most MLE applications; the immediate interest and examples here use binary response models. These slides will form a foundation for prediction in other types of models we encounter.\n\n\nProbit and logit coefficients are directly interpretable in the senses that\n\nWe can interpret direction.\nWe can interpret statistical difference from zero.\nWe can say the largest marginal effect of \\(x \\approx 0.4\\cdot\\widehat{\\beta}\\) for the probit model.\nWe can say the largest marginal effect of \\(x \\approx 0.25\\cdot\\widehat{\\beta}\\) for the logit model.\nWe can say that \\(exp(\\widehat{\\beta_k})-1\\) is the percentage change in the odds that \\(y=1\\), for the logit model.\n\nIt’s still the case that we often want other quantities of interest like probabilities, and that requires the straightforward transformations of the linear prediction, \\(F(x_i\\widehat{\\beta})\\).\nLet’s look briefly at the intuition of the “maximum marginal effect” in the logit model.\n\n\ncode\nz &lt;- seq(-5,5,.1)\nncdf &lt;- pnorm(z)\nnpdf &lt;- dnorm(z)\nlcdf &lt;- plogis(z)\nlpdf &lt;- dlogis(z)\n\ndf &lt;- data.frame(ncdf=ncdf, npdf=npdf, lcdf=lcdf, lpdf=lpdf, z=z)\n\nggplot(df, aes(x=z, y=ncdf), color=\"black\") +\n  geom_line() +\n  geom_line(aes(x=z, y=lcdf), color=\"green\") +\n  geom_line(aes(x=z, y=npdf), color=\"black\") +\n  geom_line(aes(x=z, y=lpdf), color=\"green\") +\n  geom_hline(yintercept = .3989, linetype=\"dashed\") +\n  geom_hline(yintercept = .25, linetype=\"dashed\") +\n  labs(x=\"z\", y=\"Pr(y=1)\") +\n  theme_minimal() +\n  annotate(\"text\", x = 2.5, y = .36, label = \"Normal\", color = \"black\") +\n  annotate(\"text\", x = 2.5, y = .22, label = \"Logistic\", color = \"black\") \n\n\n\n\n\n\n\n\n\nThe highest points on the PDFs indicate the maximum marginal effect of \\(x\\) on \\(Pr(y=1)\\) in the logit and probit models."
  },
  {
    "objectID": "prediction24.html#at-means-predictions",
    "href": "prediction24.html#at-means-predictions",
    "title": "Prediction Methods for MLE Models",
    "section": "At-Means Predictions",
    "text": "At-Means Predictions\nAt-means predictions are what they sound like - effects with independent variables set at central tendencies. These are sometimes called “adjusted predictions.”\n\nestimate model.\ncreate out of sample data.\nvary \\(x\\) of interest; set all other \\(x\\) variables to appropriate central tendencies - hence the “at Means.”\ngenerate QIs in out of sample data."
  },
  {
    "objectID": "prediction24.html#average-effects",
    "href": "prediction24.html#average-effects",
    "title": "Prediction Methods for MLE Models",
    "section": "Average Effects",
    "text": "Average Effects\nAverage Marginal Effects are in-sample but create a counterfactual for a variable of interest, assuming the entire sample looks like that case.\nFor instance, suppose a model of wages with covariates for education and gender. We might ask the question what would the predictions look like if the entire sample were male, but otherwise looked as it does? Alternatively, what would the predictions look like if the entire sample were female, but all other variables the same as they appear in the estimation data?\nTo answer these, we’d change the gender variable to male, generate \\(x{\\widehat{\\beta}}\\) for the entire sample, and take the average, then repeat with the gender variable set to female.\nTo generate Average Effects,\n\nestimate model.\nin estimation data, set variable of interest to a particular value for the entire estimation sample.\ngenerate QIs (expected values, standard errors).\ntake average of QIs, and save.\nrepeat for all values of variable of interest, and plot."
  },
  {
    "objectID": "prediction24.html#at-means-predictions-logit",
    "href": "prediction24.html#at-means-predictions-logit",
    "title": "Prediction Methods for MLE Models",
    "section": "At-means predictions (logit)",
    "text": "At-means predictions (logit)\nHere’s an example of at-means predictions for a logit model of the democratic peace. FIrst, let’s look at the model estimates:\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n\nstargazer(m1, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\nAs with any nonlinear model, we need to compute a linear prediction, \\(x\\widehat{\\beta}\\), and then transform that to a probability. For at-means predictions, we’ll vary democracy across its range, holding the remaining variables at appropriate central tendency (e.g, mode for dummy variables, median for categorical or skewed variables, etc.) Take a look at the code:\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\nlogitpreds &lt;- predict(m1, type=\"response\")\n\nm2 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"probit\"), data=dp )\n\n#new data frame for MEM prediction\nmem &lt;- data.frame(deml= c(seq(-10,10,1)), \n                  border=0, caprat=median(dp$caprat), ally=0)\n\n# type=\"link\" produces the linear predictions; transform by hand below w/EPT\nmem  &lt;-data.frame(mem, predict(m1, type=\"link\", newdata=mem, se=TRUE))\n\nmem &lt;- cbind(mem,lb=plogis(mem$fit-1.96*mem$se.fit),\n             ub=plogis(mem$fit+1.96*mem$se.fit), \n             p=plogis(mem$fit))\n\nggplot(mem, aes(x=deml, y=p)) +\n  geom_line() +\n  geom_ribbon(data=mem, aes(x=deml, ymin=lb, ymax=ub),fill = \"grey30\", alpha = .4, ) +\n  labs(x=\"Polity Score\", y=\"Pr(Dispute) (95% confidence interval)\")"
  },
  {
    "objectID": "prediction24.html#average-effects-logit",
    "href": "prediction24.html#average-effects-logit",
    "title": "Prediction Methods for MLE Models",
    "section": "Average effects (logit)",
    "text": "Average effects (logit)\nAverage effects are often a better choice because they represent the data more completely than central tendency can (in the at-mean effects). Here are average effects (using the logit estimates) across the range of polity, and for pairs of states that share borders and those that do not.\n\n\ncode\n#avg effects\n\n#identify the estimation sample\ndp$used &lt;- TRUE\ndp$used[na.action(m1)] &lt;- FALSE\ndpesample &lt;- dp %&gt;%  filter(used==\"TRUE\")\n\npolity &lt;- 0\nmedxbd0 &lt;- 0\nubxbd0 &lt;- 0\nlbxbd0 &lt;- 0\n# medse &lt;- 0\n# medxbd1 &lt;- 0\n# ubxbd1 &lt;- 0\n# lbxbd1 &lt;- 0\n\nfor(i in seq(1,21,1)){\n  dpesample$border&lt;- 0\n  dpesample$deml &lt;- i-11\n  polity[i] &lt;- i-11\n  allpreds &lt;- data.frame(predict(m1, type= \"response\", se.fit=TRUE, newdata = dpesample))  \n  medxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)\n  ubxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)+1.96*(median(allpreds$se.fit, na.rm=TRUE))\n  lbxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)-1.96*(median(allpreds$se.fit, na.rm=TRUE))\n}\n  \nnoborder &lt;- data.frame(polity, medxbd0, ubxbd0, lbxbd0)\n  \nfor(i in seq(1,21,1)){\n  dpesample$border&lt;- 1\n  dpesample$deml &lt;- i-11\n  polity[i] &lt;- i-11\n  allpreds &lt;- data.frame(predict(m1, type= \"response\", se.fit=TRUE, newdata = dpesample))  \n  medxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)\n  ubxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)+1.96*(median(allpreds$se.fit, na.rm=TRUE))\n  lbxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)-1.96*(median(allpreds$se.fit, na.rm=TRUE))\n}\n  \nborder &lt;- data.frame(polity, medxbd0, ubxbd0, lbxbd0)\n  \n\n\nggplot() +\n  geom_ribbon(data=noborder, aes(x=polity, ymin=lbxbd0, ymax=ubxbd0),fill = \"grey30\", alpha = .4, ) +\n  geom_line(data=noborder, aes(x=polity, y=medxbd0))+\n  geom_ribbon(data=border, aes(x=polity, ymin=lbxbd0, ymax=ubxbd0),fill = \"grey30\", alpha = .4, ) +\n  geom_line(data=border, aes(x=polity, y=medxbd0))+\n  labs ( colour = NULL, x = \"Polity Score\", y =  \"Pr(Dispute)\" )+\n  theme_minimal()+\n  ggtitle(\"Average Effects\")"
  },
  {
    "objectID": "prediction24.html#simulation",
    "href": "prediction24.html#simulation",
    "title": "Prediction Methods for MLE Models",
    "section": "Simulation",
    "text": "Simulation\nSimulation is an especially good approach for nonlinear models:\n\nestimate the model.\n\\(m\\) times (say, 1000 times), simulate the distribution of \\(\\widehat{\\beta}\\).\ngenerate the \\(m\\) linear predictions, \\(x\\widehat{\\beta}\\).\ntransform by the appropriate link function (logistic, standard normal CDF).\nidentify the 2.5, 50, and 97.5 percentiles.\nplot against \\(x\\).\n\n\n\ncode\n#draws from multivariate normal using logit model estimates\nsimL &lt;- data.frame(MASS::mvrnorm(1000, coef(m1), vcov(m1)))\n#draws from multivariate normal using probit model estimates\nsimP &lt;- data.frame(MASS::mvrnorm(1000, coef(m2), vcov(m2)))\n\n#Logit predictions\nlogitprobs &lt;- data.frame(dem= numeric(0) , lb=numeric(0), med= numeric(0), ub=numeric(0))\nfor (i in seq(1,21,1)) {\nsimpreds &lt;- quantile(simL$X.Intercept.+ simL$border*0+simL$deml*(i-11)+simL$caprat*median(dp$caprat)+simL$ally*0, probs=c(.025, .5, .975))\nlogitprobs[i,] &lt;- data.frame(dem=i, lb=plogis(simpreds[1]), med=plogis(simpreds[2]), ub=plogis(simpreds[3]))\n}\n\n#Probit predictions\nprobitprobs &lt;- data.frame(dem= numeric(0) , lb=numeric(0), med= numeric(0), ub=numeric(0))\nfor (i in seq(1,21,1)) {\nsimpreds &lt;- quantile(simP$X.Intercept.+ simP$border*0+simP$deml*(i-11)+simP$caprat*median(dp$caprat)+simP$ally*0, probs=c(.025, .5, .975))\nprobitprobs[i,] &lt;- data.frame(dem=i, lb=pnorm(simpreds[1]), med=pnorm(simpreds[2]), ub=pnorm(simpreds[3]))\n}\n\nlogit &lt;- ggplot() +\n  geom_ribbon(data=logitprobs, aes(x=dem, ymin=lb, ymax=ub),fill = \"grey30\", alpha = .4, ) +\n  geom_line(data=logitprobs, aes(x=dem, y=med))+\n  labs ( colour = NULL, x = \"Polity Score\", y =  \"Pr(Dispute)\" )+\n  theme_minimal()+\n  ggtitle(\"Logit Predictions\")\n\nprobit &lt;- ggplot() +\n  geom_ribbon(data=probitprobs, aes(x=dem, ymin=lb, ymax=ub),fill = \"grey30\", alpha = .4, ) +\n  geom_line(data=probitprobs, aes(x=dem, y=med))+\n  labs ( colour = NULL, x = \"Polity Score\", y =  \"Pr(Dispute)\" )+\n  theme_minimal()+\n  ggtitle(\"Probit Predictions\")\n\nlogit+probit"
  },
  {
    "objectID": "prediction24.html#simulating-combinations-of-binary-variables",
    "href": "prediction24.html#simulating-combinations-of-binary-variables",
    "title": "Prediction Methods for MLE Models",
    "section": "Simulating combinations of binary variables",
    "text": "Simulating combinations of binary variables\nLet’s look at the differences among the four combinations of the binary variables, border and ally.\n\n\ncode\n## simulating for binary combinations ----\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n\n#draws from multivariate normal using logit model estimates\nsimL &lt;- data.frame(MASS::mvrnorm(1000, coef(m1), vcov(m1)))\n\n\nlogitprobs &lt;- data.frame(b0a0= numeric(0) , b1a0=numeric(0), b0a1= numeric(0), b1a1=numeric(0))\n\n  b0a0 &lt;- plogis(simL$X.Intercept.+ simL$border*0+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*0)\nb1a0 &lt;- plogis(simL$X.Intercept.+ simL$border*1+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*0)\nb0a1 &lt;- plogis(simL$X.Intercept.+ simL$border*0+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*1)\nb1a1 &lt;- plogis(simL$X.Intercept.+ simL$border*1+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*1)\n\nlogitprobs &lt;- data.frame(b0a0, b1a0, b0a1, b1a1)\n\nggplot() +\n  geom_density(data=logitprobs, aes(x=b0a0), fill=\"grey30\", alpha = .4, ) +\n  geom_density(data=logitprobs, aes(x=b1a0), fill=\"grey30\", alpha = .4, ) +\n  geom_density(data=logitprobs, aes(x=b0a1), fill=\"grey30\", alpha = .4, ) +\n  geom_density(data=logitprobs, aes(x=b1a1), fill=\"grey30\", alpha = .4, ) +\n  labs ( colour = NULL, x = \"Pr(Dispute)\", y =  \"Density\" ) +\n  theme_minimal()+\n  annotate(\"text\", x = .07, y = 150, label = \"No border, not allies\", color = \"black\") +\n  annotate(\"text\", x = .13, y = 70, label = \"Border, not allies\", color = \"black\") +\n  annotate(\"text\", x = .04, y = 200, label = \"No border, allies\", color = \"black\") +\n  annotate(\"text\", x = .09, y = 50, label = \"Border, allies\", color = \"black\") +\n  ggtitle(\"Logit Predictions\")"
  },
  {
    "objectID": "prediction24.html#uncertainty-standard-errors-of-linear-predictions",
    "href": "prediction24.html#uncertainty-standard-errors-of-linear-predictions",
    "title": "Prediction Methods for MLE Models",
    "section": "Uncertainty: Standard Errors of Linear Predictions",
    "text": "Uncertainty: Standard Errors of Linear Predictions\nConsider the linear prediction\n\\[X \\widehat{\\beta} \\]\nunder maximum likelihood theory:\n\\[var(X \\widehat{\\beta}) = \\mathbf{X V X'} \\]\nan \\(N x N\\) matrix, where \\(V\\) is the var-cov matrix of \\({\\widehat{\\beta}}\\). The main diagonal contains the variances of the \\(N\\) predictions. The standard errors are:\n\\[se(X \\widehat{\\beta}) = \\sqrt{diag(\\mathbf{X V X'})} \\]\nwhich is an \\(N x 1\\) vector."
  },
  {
    "objectID": "prediction24.html#uncertainty-delta-method",
    "href": "prediction24.html#uncertainty-delta-method",
    "title": "Prediction Methods for MLE Models",
    "section": "Uncertainty: Delta Method",
    "text": "Uncertainty: Delta Method\nThe ML method is appropriate for monotonic functions of \\(X \\widehat{\\beta}\\), e.g. logit, probit. In other models (e.g., multinomial logit), the function is not monotonic in \\(X \\widehat{\\beta}\\) so we use the Delta Method - this creates a linear approximation of the function. Greene (2012) (693ff) gives this as a general derivation of the variance:\n\\[Var[F(X \\widehat{\\beta})] = f(\\mathbf{x'\\widehat{\\beta}})^2 \\mathbf{x' V x} \\]\nWhere this would generate variances for whatever \\(F(X \\widehat{\\beta})\\) is, perhaps a predicted probability."
  },
  {
    "objectID": "prediction24.html#uncertainty-standard-errors-of-p-in-logit",
    "href": "prediction24.html#uncertainty-standard-errors-of-p-in-logit",
    "title": "Prediction Methods for MLE Models",
    "section": "Uncertainty: Standard Errors of \\(p\\) in Logit",
    "text": "Uncertainty: Standard Errors of \\(p\\) in Logit\nBy delta transformation is given by:\n\\[F(X \\widehat{\\beta}) * (1-F(X \\widehat{\\beta}) * \\mathbf(X V X')\\]\n\\[ = f(X \\widehat{\\beta})  * \\mathbf(X V X')\\]\nor\n\\[ p * (1-p) * stdp\\]"
  },
  {
    "objectID": "prediction24.html#standard-errors-for-predicted-probabilities",
    "href": "prediction24.html#standard-errors-for-predicted-probabilities",
    "title": "Prediction Methods for MLE Models",
    "section": "Standard Errors for Predicted Probabilities",
    "text": "Standard Errors for Predicted Probabilities\n\\[\n\\operatorname { Var } \\left[ \\operatorname { Pr } \\left( Y _ { i } = 1 \\right) \\right) ] = \\left[ \\frac { \\partial F \\left( \\mathbf { X } _ { i } \\hat { \\boldsymbol { \\beta } } \\right) } { \\partial \\hat { \\boldsymbol { \\beta } } } \\right] ^ { \\prime } \\hat { \\mathbf { v } } \\left[ \\frac { \\partial F \\left( \\mathbf { X } _ { i } \\hat { \\boldsymbol { \\beta } } \\right) } { \\partial \\hat { \\boldsymbol { \\beta } } } \\right]\n\\]\n\\[\n= \\left[ f \\left( \\mathbf { X } _ { i } \\hat { \\boldsymbol { \\beta } } \\right) \\right] ^ { 2 } \\mathbf { X } _ { i } ^ { \\prime } \\hat { \\mathbf { V } } \\mathbf { X } _ { i }\n\\]\n\\[s.e. (Pr(y=1)) = \\sqrt{\\left[ f \\left( \\mathbf { X } _ { i } \\hat { \\boldsymbol { \\beta } } \\right) \\right] ^ { 2 } \\mathbf { X } _ { i } ^ { \\prime } \\hat { \\mathbf { V } } \\mathbf { X } _ { i }}\\]"
  },
  {
    "objectID": "prediction24.html#uncertainty-ses-of-predictions-for-linear-combinations",
    "href": "prediction24.html#uncertainty-ses-of-predictions-for-linear-combinations",
    "title": "Prediction Methods for MLE Models",
    "section": "Uncertainty: SEs of Predictions for linear combinations",
    "text": "Uncertainty: SEs of Predictions for linear combinations\nA common circumstance that requires joint hypothesis tests is the case of polynomials (which are themselves interactions):\n\\[y = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_1 + \\widehat{\\beta}_2 x_{1}^2  + \\varepsilon \\]\nThe question is whether \\(\\widehat{\\beta}_1 = \\widehat{\\beta}_2  = 0\\) - the marginal effect is:\n\\[ \\widehat{\\beta}_1 + 2 \\widehat{\\beta}_2x_1\\]\nand requires the standard error for \\(\\widehat{\\beta}_1+\\widehat{\\beta}_2\\), which is:\n\\[ \\sqrt{var(\\widehat{\\beta}_1) + 4x_{1}^{2}var(\\widehat{\\beta}_2) +  4x_1 cov(\\widehat{\\beta}_1, \\widehat{\\beta}_2)  }\\]"
  },
  {
    "objectID": "prediction24.html#uncertainty-cis---end-point-transformation",
    "href": "prediction24.html#uncertainty-cis---end-point-transformation",
    "title": "Prediction Methods for MLE Models",
    "section": "Uncertainty: CIs - End Point Transformation",
    "text": "Uncertainty: CIs - End Point Transformation\nGenerate upper and lower bounds using either ML or Delta standard errors, such that\n\\[F(X \\widehat{\\beta} - c*s.e.) \\leq F(X \\widehat{\\beta}) \\leq F(X \\widehat{\\beta} + c* s.e.)\\]\n\nestimate the model, generate the linear prediction, and the standard error of the linear prediction using the either ML or Delta.\ngenerate linear boundary predictions, \\(x{\\widehat{\\beta}} \\pm c * \\text{st. err.}\\) where \\(c\\) is a critical value on the normal, eg. \\(z=1.96\\).\ntransform the linear prediction and the upper and lower boundary predictions by \\(F(\\cdot)\\).\nWith ML standard errors, EPT boundaries will obey distributional boundaries (ie, won’t fall outside 0-1 interval for probabilities); the linear end point predictions are symmetric, though they will not be symmetric in nonlinear models.\nWith delta standard errors, bounds may not obey distributional boundaries."
  },
  {
    "objectID": "prediction24.html#uncertainty-simulating-confidence-intervals-i",
    "href": "prediction24.html#uncertainty-simulating-confidence-intervals-i",
    "title": "Prediction Methods for MLE Models",
    "section": "Uncertainty: Simulating confidence intervals, I",
    "text": "Uncertainty: Simulating confidence intervals, I\n\ndraw a sample with replacement of size \\(\\tilde{N}\\) from the estimation sample.\nestimate the model parameters in that bootstrap sample.\nusing the bootstrap estimates, generate quantities of interest (e.g. \\(x\\widehat{\\beta}\\)) repeat \\(j\\) times.\ncollect all these bootstrap QIs and use either percentiles or standard deviations to measure uncertainty."
  },
  {
    "objectID": "prediction24.html#uncertainty-simulating-confidence-intervals-ii",
    "href": "prediction24.html#uncertainty-simulating-confidence-intervals-ii",
    "title": "Prediction Methods for MLE Models",
    "section": "Uncertainty: Simulating confidence intervals, II",
    "text": "Uncertainty: Simulating confidence intervals, II\n\nestimate the model.\ngenerate a large sample distribution of parameters (e.g. using drawnorm).\ngenerate quantities of interest for the distribution of parameters.\nuse either percentiles or standard deviations of the QI to measure uncertainty."
  },
  {
    "objectID": "prediction24.html#binary-response-models",
    "href": "prediction24.html#binary-response-models",
    "title": "Prediction Methods for MLE Models",
    "section": "",
    "text": "Probit and logit coefficients are directly interpretable in the senses that\n\nWe can interpret direction.\nWe can interpret statistical difference from zero.\nWe can say the largest marginal effect of \\(x \\approx 0.4\\cdot\\widehat{\\beta}\\) for the probit model.\nWe can say the largest marginal effect of \\(x \\approx 0.25\\cdot\\widehat{\\beta}\\) for the logit model.\nWe can say that \\(exp(\\widehat{\\beta_k})-1\\) is the percentage change in the odds that \\(y=1\\), for the logit model.\n\nIt’s still the case that we often want other quantities of interest like probabilities, and that requires the straightforward transformations of the linear prediction, \\(F(x_i\\widehat{\\beta})\\).\nLet’s look briefly at the intuition of the “maximum marginal effect” in the logit model.\n\n\ncode\nz &lt;- seq(-5,5,.1)\nncdf &lt;- pnorm(z)\nnpdf &lt;- dnorm(z)\nlcdf &lt;- plogis(z)\nlpdf &lt;- dlogis(z)\n\ndf &lt;- data.frame(ncdf=ncdf, npdf=npdf, lcdf=lcdf, lpdf=lpdf, z=z)\n\nggplot(df, aes(x=z, y=ncdf), color=\"black\") +\n  geom_line() +\n  geom_line(aes(x=z, y=lcdf), color=\"green\") +\n  geom_line(aes(x=z, y=npdf), color=\"black\") +\n  geom_line(aes(x=z, y=lpdf), color=\"green\") +\n  geom_hline(yintercept = .3989, linetype=\"dashed\") +\n  geom_hline(yintercept = .25, linetype=\"dashed\") +\n  labs(x=\"z\", y=\"Pr(y=1)\") +\n  theme_minimal() +\n  annotate(\"text\", x = 2.5, y = .36, label = \"Normal\", color = \"black\") +\n  annotate(\"text\", x = 2.5, y = .22, label = \"Logistic\", color = \"black\") \n\n\n\n\n\n\n\n\n\nThe highest points on the PDFs indicate the maximum marginal effect of \\(x\\) on \\(Pr(y=1)\\) in the logit and probit models."
  },
  {
    "objectID": "prediction24.html#ml-standard-errors-of-linear-predictions",
    "href": "prediction24.html#ml-standard-errors-of-linear-predictions",
    "title": "Prediction Methods for MLE Models",
    "section": "ML Standard Errors of Linear Predictions",
    "text": "ML Standard Errors of Linear Predictions\nOne commonly used measure of uncertainty is the standard error of the linear prediction, \\(X\\widehat{\\beta}\\).\nConsider the linear prediction\n\\[X \\widehat{\\beta} \\]\nunder maximum likelihood theory:\n\\[var(X \\widehat{\\beta}) = \\mathbf{X V X'} \\]\nan \\(N x N\\) matrix, where \\(V\\) is the var-cov matrix of \\({\\widehat{\\beta}}\\). The main diagonal contains the variances of the \\(N\\) predictions. The standard errors are:\n\\[se(X \\widehat{\\beta}) = \\sqrt{diag(\\mathbf{X V X'})} \\]\nwhich is an \\(N x 1\\) vector. So now we have a column vector of standard errors for the linear prediction, \\(X\\widehat{\\beta}\\). Like the linear predictions, these are not transformed into probabilities, so when we compute confidence intervals, we need to map the upper and lower bounds onto the probability space.\n\\[F(X \\widehat{\\beta} - c*s.e.) \\leq F(X \\widehat{\\beta}) \\leq F(X \\widehat{\\beta} + c* s.e.)\\]"
  },
  {
    "objectID": "prediction24.html#delta-method-standard-errors",
    "href": "prediction24.html#delta-method-standard-errors",
    "title": "Prediction Methods for MLE Models",
    "section": "Delta Method standard errors",
    "text": "Delta Method standard errors\nThe maximum likelihood method is appropriate for monotonic functions of \\(X \\widehat{\\beta}\\), e.g. logit, probit. In other models (e.g., multinomial logit), the function is not monotonic in \\(X \\widehat{\\beta}\\) so we use the Delta Method - this creates a linear approximation of the function. Greene (2012) (693ff) gives this as a general derivation of the variance:\n\\[Var[F(X \\widehat{\\beta})] = f(\\mathbf{x'\\widehat{\\beta}})^2 \\mathbf{x' V x} \\]\nwhere this would generate variances for whatever \\(F(X \\widehat{\\beta})\\) is, perhaps a predicted probability.\n\nDelta method standard errors for Logit\nFor the logit, the delta standard errors are given by:\n\\[F(X \\widehat{\\beta}) * (1-F(X \\widehat{\\beta}) * \\mathbf(X V X')\\]\n\\[ = f(X \\widehat{\\beta})  *  \\mathbf{\\sqrt{X V X'}}\\]\nor\n\\[ p * (1-p) * stdp\\]\nwhere \\(stdp\\) is the standard error of the linear prediction."
  },
  {
    "objectID": "prediction24.html#ses-of-predictions-for-linear-combinations",
    "href": "prediction24.html#ses-of-predictions-for-linear-combinations",
    "title": "Prediction Methods for MLE Models",
    "section": "SEs of Predictions for linear combinations",
    "text": "SEs of Predictions for linear combinations\nA common circumstance that requires joint hypothesis tests is the case of polynomials (which are themselves interactions):\n\\[y = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_1 + \\widehat{\\beta}_2 x_{1}^2  + \\varepsilon \\]\nThe question is whether \\(\\widehat{\\beta}_1 = \\widehat{\\beta}_2  = 0\\) - the marginal effect is:\n\\[ \\widehat{\\beta}_1 + 2 \\widehat{\\beta}_2x_1\\]\nand requires the standard error for \\(\\widehat{\\beta}_1+\\widehat{\\beta}_2\\), which is:\n\\[ \\sqrt{var(\\widehat{\\beta}_1) + 4x_{1}^{2}var(\\widehat{\\beta}_2) +  4x_1 cov(\\widehat{\\beta}_1, \\widehat{\\beta}_2)  }\\]"
  },
  {
    "objectID": "prediction24.html#cis---end-point-transformation",
    "href": "prediction24.html#cis---end-point-transformation",
    "title": "Prediction Methods for MLE Models",
    "section": "CIs - End Point Transformation",
    "text": "CIs - End Point Transformation\nGenerate upper and lower bounds using either ML or Delta standard errors, such that\n\\[F(X \\widehat{\\beta} - c*s.e.) \\leq F(X \\widehat{\\beta}) \\leq F(X \\widehat{\\beta} + c* s.e.)\\]\n\nestimate the model, generate the linear prediction, and the standard error of the linear prediction using the either ML or Delta.\ngenerate linear boundary predictions, \\(x{\\widehat{\\beta}} \\pm c * \\text{st. err.}\\) where \\(c\\) is a critical value on the normal, eg. \\(z=1.96\\).\ntransform the linear prediction and the upper and lower boundary predictions by \\(F(\\cdot)\\).\nWith ML standard errors, EPT boundaries will obey distributional boundaries (ie, won’t fall outside 0-1 interval for probabilities); the linear end point predictions are symmetric, though they will not be symmetric in nonlinear models.\nWith delta standard errors, bounds may not obey distributional boundaries."
  },
  {
    "objectID": "prediction24.html#simulating-confidence-intervals-i",
    "href": "prediction24.html#simulating-confidence-intervals-i",
    "title": "Prediction Methods for MLE Models",
    "section": "Simulating confidence intervals, I",
    "text": "Simulating confidence intervals, I\n\ndraw a sample with replacement of size \\(\\tilde{N}\\) from the estimation sample.\nestimate the model parameters in that bootstrap sample.\nusing the bootstrap estimates, generate quantities of interest (e.g. \\(x\\widehat{\\beta}\\)) repeat \\(j\\) times.\ncollect all these bootstrap QIs and use either percentiles or standard deviations to measure uncertainty."
  }
]