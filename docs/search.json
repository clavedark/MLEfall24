[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site:\nCreated with Quarto.\nAbout me:\nProfessor of political science, PhD Florida State 1999. I’ve taught this class for a long time, but never the same way twice. I study models of political violence, collect data on protests and repression.\n\n\n\n Back to top"
  },
  {
    "objectID": "likelihood24.html#how-would-you-characterize-the-variable-measuring-deaths-by-mule-kick",
    "href": "likelihood24.html#how-would-you-characterize-the-variable-measuring-deaths-by-mule-kick",
    "title": "Likelihood",
    "section": "How would you characterize the variable measuring deaths by mule kick?",
    "text": "How would you characterize the variable measuring deaths by mule kick?\n\nVariable measures events.\nEvents are discrete, not continuous.\nAre events correlated or independent?\nVariable is bounded; cannot be below zero.\n\nSo we need to think about two different things here - the distribution of \\(y\\) based on its observed distribution, and the link between the \\(X\\) variables and \\(\\widetilde{y}\\), the latent quantity of interest.\n\nWhat distribution might describe the frequency of mule kick deaths?\nNeeds to be discrete.\nNeeds to characterize rare events - at most we see about four per period, so relatively rare.\nNeeds to have a lower bound at zero (since we can’t observe negative numbers of deaths), and upper bound at \\(+\\infty\\)\n\n\n\\[Pr(Y=y_{i})=\\frac{e^{-\\lambda}\\lambda^{y_{i}}}{y_{i}!}\\]"
  },
  {
    "objectID": "likelihood24.html#the-poisson-distribution",
    "href": "likelihood24.html#the-poisson-distribution",
    "title": "Likelihood",
    "section": "The Poisson Distribution",
    "text": "The Poisson Distribution\nThe Poisson distribution is a discrete distribution that characterizes the number of events that occur in a fixed interval of time (or sometimes, space). Below are 3 Poisson distributions with means of .5, 1.5, and 2.5.\n\n\ncode\n#using a large sample, n=1000, simulate and plot the poisson distribution for mean values of .5, 1.5, and 2.5\n\n# Load required libraries\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\n\n# Set seed for reproducibility\nset.seed(8675309)\n\n# Simulate data\nn &lt;- 10000  # n for each distribution\nlambdas &lt;- c(0.5, 1.5, 2.5)  # Means \n\ndf &lt;- data.frame(\n  lambda_0.5 = rpois(n, lambda = 0.5),\n  lambda_1.5 = rpois(n, lambda = 1.5),\n  lambda_2.5 = rpois(n, lambda = 2.5)\n)\n\n# Reshape the data for ggplot\ndf_long &lt;- df %&gt;%\n  pivot_longer(cols = everything(), \n               names_to = \"distribution\", \n               values_to = \"value\") %&gt;%\n  mutate(distribution = factor(distribution, \n                               levels = c(\"lambda_0.5\", \"lambda_1.5\", \"lambda_2.5\"),\n                               labels = c(\"λ = 0.5\", \"λ = 1.5\", \"λ = 2.5\")))\n\n# plot \nggplot(df_long, aes(x = value, fill = distribution)) +\n  geom_density(position = \"identity\", alpha = 0.5, adjust=4) +\n  facet_wrap(~ distribution, ncol = 1, scales = \"free_y\") +\n  scale_fill_manual(values = c(\"#000000\", \"#6CC24A\", \"#005A43\")) +\n  labs(title = \"Simulated Poisson Distributions\",\n       x = \"Value\",\n       y = \"Count\",\n       fill = \"Distribution\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nThinking in terms of the data, let’s write a likelihood function, the joint probability for all \\(i\\) observations in the sample, \\(n\\):\n\\[\n\\mathcal{L}(\\lambda)= \\prod_{i=1}^{n} \\left[\\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!} \\right]\n\\]\nTake the natural log of the likelihood function:\n\\[\n\\ln \\mathcal{L}(\\lambda)= \\ln \\left\\{\\prod_{i=1}^{n} \\left[\\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!} \\right] \\right\\}\n\\]\n\\[\\ln \\mathcal{L}(\\lambda)= \\sum_{i=1}^{n} \\left[-\\lambda + y_i \\ln(\\lambda) - \\ln(y_i!) \\right]\n\\]\nWhat about the \\(X\\) variables? Parameterize the model with respect to those variables such that they influence the mean, \\(\\lambda\\). So let’s make \\(\\lambda\\) a function of the \\(X\\) variables and their effects, \\(\\beta\\), using the exponential distribution as the link function:\n\\[\nE[Y|X]=\\lambda =exp(X\\beta)\n\\]\nThe exponential ensures we won’t have negative predictions.\nPutting all this together we have:\n\\[\n\\ln \\mathcal{L}(\\lambda)= \\sum_{i=1}^{n} \\left[-e^{X\\beta} + y_i \\ln(X\\beta) - \\ln(y_i!) \\right]\n\\]"
  },
  {
    "objectID": "likelihood24.html#estimation-technology-ols",
    "href": "likelihood24.html#estimation-technology-ols",
    "title": "Likelihood",
    "section": "Estimation Technology: OLS",
    "text": "Estimation Technology: OLS\nRecall that the technology of OLS is to assume a normally distributed error term, minimize the sum of those squared errors analytically using calculus."
  },
  {
    "objectID": "likelihood24.html#estimation-technology-mle",
    "href": "likelihood24.html#estimation-technology-mle",
    "title": "Likelihood",
    "section": "Estimation Technology: MLE",
    "text": "Estimation Technology: MLE\nThe technology of ML is to maximize the LLF with respect to \\(\\beta\\). We can do this in a couple of different ways:\n\nanalytic methods - solve calculus. Some/many models do not have analytical or closed form solutions.\nnumerical methods - use an algorithm to estimate starting values for \\(\\theta\\), then hill climb until the first derivative is zero, and the second derivative is negative. This is iterative, trying values, looking at the derivatives. This is what nearly all ML estimation uses - there are different algorithms for doing this. The most commonly used is the Newton Raphson method - it’s illustrated in detail in the maximization slides\n\n\nAnalytic Methods\nWith some functions, we can solve for the unknowns directly. Let’s return to the Poisson log-likelihood function and consider data on the number of civil wars in Africa over a ten year period, and the data are as follows:\n\\(y\\) = {5 0 1 1 0 3 2 3 4 1}\nThe log-likelihood function is:\n\\[\\begin{aligned}\n\\ln \\mathcal{L}(\\lambda|Y) =\\ln \\left[ \\prod\\limits_{i=1}^{N} \\frac{e^{-\\lambda}\\lambda^{y_{i}}}{y_{i}!}\\right]\\nonumber \\\\ \\nonumber \\\\\n=-N \\lambda+ \\sum(y_{i}) \\ln(\\lambda) - \\sum(\\ln(y_{i}!)) \\nonumber\n\\end{aligned}\\]\n\\(\\lambda\\) is the unknown we want to solve for. Taking the derivative with respect to \\(\\lambda\\) and setting equal to zero:\n\\[\\begin{aligned}\n\\frac{\\partial \\ln \\mathcal{L}}{\\partial \\lambda}=-N \\lambda+ \\sum(y_{i}) \\ln(\\lambda) - \\sum(\\ln(y_{i}!)) \\nonumber \\\\ \\nonumber  \\\\\n0=-N + \\frac{\\sum y_{i}}{\\lambda} \\nonumber\\\\ \\nonumber \\\\\n\\color{red}{\\widehat{\\lambda}= \\frac{\\sum y_{i}}{N}} \\nonumber\n\\end{aligned}\\]\nThis is just the sample mean of course - let’s plug in our data and solve for \\(\\lambda\\):\n\\[\\widehat{\\lambda}= \\frac{20}{10} \\] So the value of \\(\\lambda\\) the maximizes the function is 2. This is a trivial example in the sense that applications with \\(x\\) variables are sufficiently complicated that analytical methods are not usually possible, so we turn to numerical methods."
  },
  {
    "objectID": "likelihood24.html#normal-linear-llf",
    "href": "likelihood24.html#normal-linear-llf",
    "title": "Likelihood",
    "section": "Normal (linear) LLF:",
    "text": "Normal (linear) LLF:\n\\[\n= -\\frac{N}{2}(\\ln(2\\pi)) -\\frac{N}{2}(\\ln(\\sigma^{2})) -\\frac{1}{2\\sigma^{2}}\\left[\\sum\\limits_{i=1}^{n}(y_{i}-\\mu)^{2}\\right] \\nonumber\n\\]\nNotice \\(N\\) in the numerator; recall the rule of summation that \\(\\sum\\limits_{i=1}^{n}a= n\\cdot a\\).\nNow, take the derivative of the log-likelihood with respect to each of the parameters in turn (ignoring constant terms and terms that pertain exclusively to the other parameter).\n\\[\n\\frac{\\partial \\ln L}{\\partial \\mu}= \\frac{1}{\\sigma^{2}}\\sum(y_{i}-\\mu)=0 \\nonumber\\\\\n=\\sum(y_{i}-\\mu) = \\sum y_{i}- \\sum \\mu  \\nonumber\\\\\n=\\sum y_{i}- N \\mu = 0 \\nonumber \\\\\n\\mu=\\frac{\\sum y_{i}}{N} = \\widehat{y}\\nonumber\n\\]\nwe can also solve for \\(\\sigma^{2}\\), getting\n\\[\n\\frac{\\partial \\ln L}{\\partial \\sigma^{2}}= -\\frac{N}{2 \\sigma^{2}}+\\frac{1}{2 \\sigma^{4}} +\\sum(y_{i}-\\mu)=0 \\nonumber\\\\ \\nonumber\\\\\n=-\\frac{N}{2}\\sigma^{2}+\\frac{1}{2}\\sum(y_{i}-\\bar{y})^{2}= 0 \\nonumber\\\\ \\nonumber\\\\\n\\ldots\n\\widehat{\\sigma^{2}}=\\frac{\\sum(y_{i}-\\bar{y})^{2}}{N} \\nonumber\n\\]\nThis is a biased estimator of \\(\\sigma^{2}\\); \\(\\sigma^{2}\\) is underestimated because the denominator should be \\(N-1\\).\nThe same thing in matrix notation:\n\\[ln\\mathcal{L}(y | X, \\beta, \\sigma^2) = -\\frac{N}{2} ln(2\\pi) - \\frac{N}{2} ln(\\sigma^2) -\\frac{1}{2} \\left[ \\frac{(y-X\\beta)'(y-X\\beta)}{\\sigma^2} \\right] \\]\nrewriting to isolate the parameters:\n\\[\nln\\mathcal{L}(y | X, \\beta, \\sigma^2) = -\\frac{N}{2} ln(2\\pi) - \\frac{N}{2} ln(\\sigma^2) -\\frac{1}{2\\sigma^2} \\left[ yy'- 2y' X\\beta +\\beta' X' X\\beta) \\right]\n\\]\nTake derivatives of \\(\\ln \\mathcal{L}\\) w.r.t. \\(\\beta\\) and \\(\\sigma^2\\) (and skipping a lot here):\n\\[\\frac{\\partial ln \\mathcal{L}}{\\partial \\beta} = \\frac{1}{\\sigma^2} (X'y - X'X \\beta)\\]\nsetting equal to zero …\n\\[ \\frac{1}{\\sigma^2} (X'y - X'X \\beta) = 0\\] \\[X'X \\beta = X'y\\] \\[\\widehat{\\beta} = (X'X)^{-1} X' y \\]\n…going through the same thing for \\(\\sigma^2\\) gives us:\n\\[\\widehat{\\sigma^2} = \\frac{e'e}{N}\\]\nSo aside from seeing how analytic methods work, we have also seen that the BLUE OLS estimator is the ML estimator for \\(\\beta\\), and that the variance estimate in ML is biased downward (the denominator is always too large by \\(k-1\\)). This difference disappears in large samples.\nWhy do we leave OLS if these are the same? Because this is the rare case defined by normal data which both satisfies the OLS requirement for a normal disturbance, and permits MLE estimation with a normal LLF. With non-normal data, OLS and ML estimators diverge quite a lot."
  },
  {
    "objectID": "likelihood24.html#numerical-methods",
    "href": "likelihood24.html#numerical-methods",
    "title": "Likelihood",
    "section": "Numerical Methods",
    "text": "Numerical Methods\nNumerical methods are computationally intensive ways to plug in possible parameter values, generate a log likelihood, and then use calculus to evaluate whether the that value is a maximum. We use numerical methods when no analytic or “closed form” solution exists which is essentially all the time.\nDo this by evaluating:\n\nthe first derivative of the LLF - by finding the point on the function where a tangent line has a slope equal to zero, we know we’ve found an inflection point.\nthe second derivative of the LLF - if the rate of change in the function at the very next point is increasing, it’s a minimum; decreasing, it’s a maximum.\n\nthe Hessian matrix - the matrix of second derivatives - tells us the curvature of the LLF, or the rate of change.\n\nSuppose that we have the event count data reported above representing civil wars in Africa, and that we want to compute the likelihood of \\(\\lambda|Y\\). We can compute the likelihood using numerical methods; one specific technique is a grid search procedure. Just as we might try different values for \\(x\\) when graphing a function, \\(f(x)\\) in algebra, we will insert possible values for \\(\\lambda\\) into the log-likelihood function in such a way that we can identify an apparent maximum (a value for \\(\\lambda\\) for which the log-likelihood is at its largest compared to contiguous values of \\(\\lambda\\)). Put another way, we take a guess at the value of \\(\\lambda\\), compute the log-likelihood, and take another guess at \\(\\lambda\\), compute the log-likelihood and compare the two estimates of the likelihood; we repeat this process until a pattern emerges such that we can discern a maximum value.\nThe log-likelihood function for the poisson distributed data on civil wars is\n\\[\n\\ln \\mathcal{L}(\\lambda|Y)= \\ln \\left[\\frac{e^{-10\\lambda}\\lambda^{20}}{207360}\\right] \\nonumber  \\\\ \\nonumber \\\\\n= -10 \\lambda+ 20 \\ln(\\lambda) - \\ln(207360) \\nonumber\n\\]\nSuppose we make some guesses regarding the value of \\(\\lambda\\), plug them into the function and compare the resulting values of the log-likelihood - take a look at the code chunk below:\n\n\ncode\n#iterate over lambda, create data frame of lambda and log-likelihood\nlambda &lt;- seq(0.1, 3.5, by=0.1)\nllf &lt;- NULL\nfor (i in 1:length(lambda)){\n  L &lt;- -10*lambda[i] + 20*log(lambda[i]) - log(207360)\n  llf &lt;- data.frame(rbind(llf, c(lambda=lambda[i], ll=L)))\n}\n\n#highchart with reference line at maximum value of the log-likelihood\nhighchart() %&gt;% \n  hc_add_series(llf, \"line\", hcaes(x=lambda, y=ll)) %&gt;% \n  hc_title(text=\"Log-Likelihood Estimates\") %&gt;% \n  hc_subtitle(text=\"Civil Wars in Africa\") %&gt;% \n hc_xAxis(title = list(text = \"Lambda\"), plotLines = list(list(value = 2, color=\"red\"))) %&gt;%\n  hc_yAxis(title = list(text = \"log-likelihood\"), plotLines = list(list(value = max(llf$ll), color=\"red\"))) %&gt;%\n  hc_tooltip(pointFormat = \"Lambda: {point.x}&lt;br&gt;Log-Likelihood: {point.y}\") %&gt;% \n  hc_colors(\"#005A43\") \n\n\n\n\n\n\nWe can see that the largest value of the likelihood is where \\(\\lambda\\) = 2 - that’s the value that maximizes the likelihood function. And not surprisingly, notice that we have arrived at the same solution we produced in the analytic example above. This is another trivial example insofar as grid search methods are usually not sufficient for solving multivariate problems (nor for computing the variance-covariance matrix)."
  },
  {
    "objectID": "likelihood24.html#how-numerical-methods-work",
    "href": "likelihood24.html#how-numerical-methods-work",
    "title": "Likelihood",
    "section": "How numerical methods work",
    "text": "How numerical methods work\n\nChoose starting values of \\(\\beta\\) (sometimes from OLS) to estimate the log-likelihood.\nTake the derivative of the log-likelihood with respect to the parameters to find the gradient}. The gradient (or the gradient matrix, a \\(kxk\\) matrix) tells us the direction of the slope of a line tangent to the curve at the point of the log-likelihood estimate.\nIf the gradient is positive (if the matrix is positive definite), then \\(ln \\mathcal{L}\\) is increasing in \\(\\beta\\) - the slope is increasing, so increase our estimate of \\(\\beta\\) and try again.\nIf the gradient is negative (if the matrix is negative definite), the \\(ln \\mathcal{L}\\) is decreasing in \\(\\beta\\) - the slope is decreasing, so we’ve passed the maximum; choose a smaller value for \\(\\beta\\) and try again.\nAs the log-likelihood approaches the maximum, the gradient approaches zero - the slope of the line tangent to the curve at the point of the log-likelihood estimate is approaching zero, indicating we’re reaching the maximum of the function. Stop the search and evaluate the estimates of \\(\\beta\\) that produced the zero gradient.\nThroughout this process, we need to evaluate the second derivatives in order to figure out the rate at which the slope is changing; this helps us tell how close or far we are from the maximum. The second derivative describes the curvature of the LLF, or the rate of change.\nThe matrix of second derivatives (the Hessian matrix) or its approximation also provide the source of our estimates of the variance, and thus the standard errors.\n\nThe first derivative tells us the direction in which the function is changing. This is obviously important since we’re trying to find the maximum.\nThink of this as trying to figure out when you’re exactly at the top of a hill. The slope (the grade, the gradient) is positive while you’re climbing to the top, it’s zero at the top, and it’s negative on the way down the other side.\nBut is the hill flat or steep? If it’s flat, then the change in the slope between point A and point B is likely to be very small - this, of course, can make it difficult to know exactly when we’re at the top (the maximum). On the other hand, if the hill is very steep, the change in the slope between two points is pretty substantial. Put another way, the rate of change in the slope is larger (faster) the steeper the slope; it’s smaller (slower) the flatter the slope.\nThis matters to maximization because the second derivatives tell us how big (or small) a step we should take up the hill as we try to find the top. Suppose that the function is very flat; as indicated above, the change in the slope between two points would be small, so we can take larger steps in order to try to find the maximum. The second derivatives would tell us that the rate of change is very small, so we should take larger steps.\nThe software performing the estimation will choose the next value of \\(\\beta\\) a bit further away from the last value it tried. On the other hand, if the second derivatives are large so the rate of change is fast, we want to take relatively small steps so we don’t step right over the maximum. In any case, that’s the intuition for why we need to know the matrix of second derivatives."
  },
  {
    "objectID": "likelihood24.html#variance-covariance-matrix",
    "href": "likelihood24.html#variance-covariance-matrix",
    "title": "Likelihood",
    "section": "Variance-Covariance matrix",
    "text": "Variance-Covariance matrix\nEstimating the second derivatives can be a real nightmare in estimation, but it’s important not only for finding the maximum of the function (and therefore in estimating the \\(\\beta\\)s), but for computing the variance-covariance matrix as well. Here are our options:\n\nFind the Hessian. The Hessian is a \\(kxk\\) matrix of the second derivatives of the log-likelihood function with respect to \\(\\beta\\), where the second derivatives are on the main diagonal. Commonly estimated using the Newton-Raphson algorithm.\nFind the information matrix. This is the negative of the expected value of the Hessian matrix, computed using the method of scoring.\n\nOuter product approximation, where we sum the squares of the first derivatives (thus avoiding the second derivatives all together). This is computed using the Berndt, Hall, Hall, and Hausman} or BHHH algorithm."
  },
  {
    "objectID": "likelihood24.html#grid-search",
    "href": "likelihood24.html#grid-search",
    "title": "Likelihood",
    "section": "Grid Search",
    "text": "Grid Search\nGrid search is another method for maximization. The process is to iteratively try values for the parameters of interest, refining those values as the log-likelihood gets larger and larger. In general, we plug in values for the parameters, compute the likelihood, then identify the largest LL value - the parameters that produce that value are our answer.\nThis method is instructive for how numerical methods work, but not practical in most applications with more than a couple of unknowns."
  },
  {
    "objectID": "likelihood24.html#latent-variable-motivation",
    "href": "likelihood24.html#latent-variable-motivation",
    "title": "Likelihood",
    "section": "Latent Variable Motivation",
    "text": "Latent Variable Motivation\nThere are a couple of (related) ways to motivate the model. Let’s assume a latent quantity we’re interested, denoted \\(y^*\\), but our observations of \\(y\\) are limited to successes (\\(y_i=1\\)) and failures (\\(y_i=0\\)).\n\\[\ny^{*}=\\mathbf{x_i \\beta}+\\epsilon_i \\nonumber\n\\]\nfor \\(y^{*}\\), the latent variable,\n\\[\ny_{i} = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{if $y^{*}_{1}&gt;\\kappa$} \\\\\n         0, & \\mbox{if $y^{*}_{1} \\leq \\kappa$}\n         \\end{array}\n     \\right.\n\\]\nwhere \\(\\kappa\\) is an unobserved threshold.\nMake probabilities statements,\n\\[\nPr(y_i=1) = Pr(y^{*}_{1}&gt;\\kappa) \\nonumber \\\\ \\nonumber \\\\\n=Pr(\\mathbf{x_i \\beta}+\\epsilon_i&gt;\\kappa) \\nonumber \\\\ \\nonumber \\\\\n=Pr(\\epsilon_i&gt;\\kappa-\\mathbf{x_i \\beta}) \\nonumber\n\\]\nNormalizing \\(\\kappa=0\\),\n\\[\nPr(y_i=1)=Pr(\\epsilon_i&gt;-\\mathbf{x_i \\beta}) \\nonumber\n\\]\n\\[\nPr(y_i=1)=Pr(\\epsilon_i&gt;-\\mathbf{x_i \\beta}) \\nonumber \\\\ \\nonumber \\\\\n=1-F(-\\mathbf{x_i \\beta}) \\nonumber\n\\]\nassuming \\(F\\) is symmetric, with unit variance,\n\\[\n\\pi_i= Pr(y_i=1)=1-F(-\\mathbf{x_i \\beta}) = F(\\mathbf{x_i \\beta}) \\\\   \\nonumber \\\\\n1-\\pi_i=Pr(y_i=0)= 1-F(\\mathbf{x_i \\beta})\n\\]"
  },
  {
    "objectID": "likelihood24.html#binomial-likelihood-function",
    "href": "likelihood24.html#binomial-likelihood-function",
    "title": "Likelihood",
    "section": "Binomial Likelihood Function",
    "text": "Binomial Likelihood Function\nThe observed data are binary, assumed binomial (\\(\\pi\\)), \\(y_i=0,1\\). The likelihood function must have two parts, one for cases where \\(y_i=0\\), the other for cases where \\(y_i=1\\). Recalling that \\(\\pi=F(\\mathbf{x_i \\beta})\\), and \\(1-\\pi= 1-F(\\mathbf{x_i \\beta})\\),\n\\[\nPr(y_1,y_2,y_3 \\ldots y_n) =  \\prod_{y=1}F(\\mathbf{x_i \\beta}) \\prod_{y=0}[1-F(\\mathbf{x_i \\beta})]\\nonumber\n\\]\nThis is the joint probability we observe all the data, \\(Y\\), simultaneously. We can rewrite this as the likelihood of observing the data given \\(\\beta\\),\n\\[\n\\mathcal{L} (Y|\\beta) =  \\prod_{i=1}^{N} [F(\\mathbf{x_i \\beta})]^{y_i} [1-F(\\mathbf{x_i \\beta})]^{1-y_i}\\nonumber\n\\]\nAnd take the natural log\n\\[\n\\ln(\\mathcal{L} (Y|\\beta)) = \\ln( \\prod_{i=1}^{N} [F(\\mathbf{x_i \\beta})]^{y_i} [1-F(\\mathbf{x_i \\beta})]^{1-y_i})\\nonumber \\\\ \\nonumber \\\\\n= \\sum_{i=1}^{N} y_i \\ln F(\\mathbf{x_i \\beta})+ (1-y_i) \\ln[1-F(\\mathbf{x_i \\beta})] \\nonumber\n\\]\nNote the two parts of the LLF corresponding to the limited observations in the data, 0,1."
  },
  {
    "objectID": "likelihood24.html#choosing-a-link",
    "href": "likelihood24.html#choosing-a-link",
    "title": "Likelihood",
    "section": "Choosing a Link",
    "text": "Choosing a Link\nLet’s make this a probit model by assuming the link to the latent variable is standard normal, so \\(F(\\cdot)\\sim N_{i.i.d.}(0,1)\\):\n\\[\n\\ln \\mathcal{L} (Y|\\beta) = \\sum_{i=1}^{N} y_i \\ln \\Phi(\\mathbf{x_i \\beta})+ (1-y_i) \\ln[1-\\Phi(\\mathbf{x_i \\beta})] \\nonumber\n\\]\nLogit would look like this:\n\\[\n\\ln \\mathcal{L} (Y|\\beta) = \\sum_{i=1}^{N} \\left\\{ y_i \\ln \\left(\\frac{1}{1+e^{-\\mathbf{x_i \\beta}}}\\right)+ (1-y_i) \\ln \\left[1-\\left(\\frac{1}{1+e^{-\\mathbf{x_i \\beta}}}\\right)\\right] \\right\\}\\nonumber\n\\]"
  },
  {
    "objectID": "likelihood24.html#estimation",
    "href": "likelihood24.html#estimation",
    "title": "Likelihood",
    "section": "Estimation",
    "text": "Estimation\nIdeally, we’d like just to estimate by finding the values of \\(\\beta\\) that maximize the log-likelihood function, and do so analytically (i.e., using calculus). This is what we do in OLS, though with respect to minimizing the sum of the squared residuals. But because the solution is non-linear in \\(\\beta\\), there is no closed form or simple analytic solution.\nAs a result, ML models produce estimates of \\(\\beta\\) by using numerical optimization methods. These are generally iterative attempts to narrow down the range in which the maximum lies by plugging in different values of \\(\\beta\\) until the range is so small, we can safely say we’ve maximized the function using those values of \\(\\beta\\)."
  },
  {
    "objectID": "likelihood24.html#maximization",
    "href": "likelihood24.html#maximization",
    "title": "Likelihood",
    "section": "Maximization",
    "text": "Maximization\nSuppose we have binary data that look like this:\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\nOur question is what is the distribution parameter most likely responsible for having generated these observed data.\nWe need to plug a hypothetical value for the distribution parameter into the log-likelihood function, compute log-likelihoods for each observation, then do the same thing with other hypothetical values. Whichever value produces the biggest log-likelihoods is the value most likely responsible for producing the data we have.\nWhat do we mean by distribution parameter? Well, in the LLF below, we’ve referred to our unknown as \\(F(\\mathbf{x_i \\beta})\\), but we really mean we need an estimate of the parameter \\(\\Theta\\) which represents the effects of the \\(X\\)s via the functional form we’ve imposed by assuming a distribution of \\(\\epsilon\\). In this particular case (for simplicity) we don’t have any \\(X\\) variables.\n\\[\n\\ln(\\mathcal{L} (Y|\\Theta)) = \\sum_{i=1}^{N} y_i \\ln F(\\mathbf{\\Theta})+ (1-y_i) \\ln[1-F(\\mathbf{\\Theta})] \\nonumber\n\\]\nHere’s what we’ll do:\n\nchoose some hypothetical values of \\(\\Theta\\); since this is binary, and our latent variable a probability, let’s choose values from .2 to .8.\ncompute \\(N\\) log-likelihoods for each value of \\(\\Theta\\); N=20, and we have 7 values of \\(\\Theta\\).\nsum the \\(N\\) log-likelihoods for each value of \\(\\Theta\\); so we’ll end up with 7 summed log-likelihoods.\nevaluate the summed log-likelihoods, and see which is largest.\ndeclare the value of \\(\\Theta\\) that produced that largest summed log-likelihood as the parameter most likely to have generated the data.\n\nLet \\(\\Theta=.2\\):\n\\[\n\\ln(\\mathcal{L}_{i=1} (Y|\\Theta)) = 0 *\\ln (.2)+ (1-0) \\ln[1-.2] =-0.2231\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=2} (Y|\\Theta)) = 0 *\\ln (.2)+ (1-0) \\ln[1-.2]=-0.2231 \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=3} (Y|\\Theta)) = 0 *\\ln (.2)+ (1-0) \\ln[1-.2]=-0.2231 \\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=11} (Y|\\Theta)) = 1 *\\ln (.2)+ (1-1) \\ln[1-.2] =-1.609\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=12} (Y|\\Theta)) = 1 *\\ln (.2)+ (1-1) \\ln[1-.2]=-1.609 \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=13} (Y|\\Theta)) = 1 *\\ln (.2)+ (1-1) \\ln[1-.2] =-1.609\\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\sum_{i=1}^{20}=-18.32100 \\nonumber\n\\]\nLet \\(\\Theta=.3\\):\n\\[\n\\ln(\\mathcal{L}_{i=1} (Y|\\Theta)) = 0 *\\ln (.3)+ (1-0) \\ln[1-.3] =-0.3567\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=2} (Y|\\Theta)) = 0 *\\ln (.3)+ (1-0) \\ln[1-.3]=-0.3567 \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=3} (Y|\\Theta)) = 0 *\\ln (.3)+ (1-0) \\ln[1-.3]=-0.3567 \\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=11} (Y|\\Theta)) = 1 *\\ln (.3)+ (1-1) \\ln[1-.3] =-1.204\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=12} (Y|\\Theta)) = 1 *\\ln (.3)+ (1-1) \\ln[1-.3]=-1.204\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=13} (Y|\\Theta)) = 1 *\\ln (.3)+ (1-1) \\ln[1-.3] =-1.204\\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\sum_{i=1}^{20}=-15.60700 \\nonumber\n\\]\nLet \\(\\Theta=.4\\):\n\\[\n\\ln(\\mathcal{L}_{i=1} (Y|\\Theta)) = 0 *\\ln (.4)+ (1-0) \\ln[1-.5] =-0.5108\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=2} (Y|\\Theta)) = 0 *\\ln (.4)+ (1-0) \\ln[1-.5]=-0.5108 \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=3} (Y|\\Theta)) = 0 *\\ln (.4)+ (1-0) \\ln[1-.5]=-0.5108 \\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=11} (Y|\\Theta)) = 1 *\\ln (.4)+ (1-1) \\ln[1-.4] =-0.9163\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=12} (Y|\\Theta)) = 1 *\\ln (.4)+ (1-1) \\ln[1-.4]=-0.9163\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=13} (Y|\\Theta)) = 1 *\\ln (.4)+ (1-1) \\ln[1-.4] =-0.9163\\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\sum_{i=1}^{20}=-14.2711636 \\nonumber\n\\]\nLet \\(\\Theta=.5\\):\n\\[\n\\ln(\\mathcal{L}_{i=1} (Y|\\Theta)) = 0 *\\ln (.5)+ (1-0) \\ln[1-.5] =-0.6931\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=2} (Y|\\Theta)) = 0 *\\ln (.5)+ (1-0) \\ln[1-.5]=-0.6931 \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=3} (Y|\\Theta)) = 0 *\\ln (.5)+ (1-0) \\ln[1-.5]=-0.6931 \\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=11} (Y|\\Theta)) = 1 *\\ln (.5)+ (1-1) \\ln[1-.5] =-0.6931\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=12} (Y|\\Theta)) = 1 *\\ln (.5)+ (1-1) \\ln[1-.5]=-0.6931\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=13} (Y|\\Theta)) = 1 *\\ln (.5)+ (1-1) \\ln[1-.5] =-0.6931\\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\sum_{i=1}^{20}=-13.8629436 \\nonumber\n\\]\nLet’s compare what we have so far:\n\\[llf_{.2}&lt;llf_{.3}&lt;llf_{.4}&lt;llf_{.5}\\]\nYou can probably see some symmetry here due to the fact that half the data are ones, half zeros, so completing:\\~\\\n\\(llf_{.2}&lt;llf_{.3}&lt;llf_{.4}&lt;llf_{.5}&gt;llf_{.6}&gt;llf_{.7}&gt;llf_{.8}\\) \\~\\\nSo \\(\\Theta=.5\\) produces the largest log-likelihood (-13.86) and thus is the parameter most likely to have produced the observed data."
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "Binomial Models - video\nPrediction Methods - spring 2024 prediction video\nLikelihood Theory and Applications\nMaximization Methods - video\nBinary Model Extensions I\nDiscrete Hazards\nBinary Model Extensions II\nMechanism Paper Assignment\nInteractions in Nonlinear Models\n\n\n\n Back to top"
  },
  {
    "objectID": "binarymodels24.html",
    "href": "binarymodels24.html",
    "title": "Binary Response Models",
    "section": "",
    "text": "How can we model a binary \\(y\\) variable?\nDoes OLS (the linear probability model) work sufficiently well?\nHow can we build a maximum likelihood model?"
  },
  {
    "objectID": "binarymodels24.html#questions",
    "href": "binarymodels24.html#questions",
    "title": "Binary Response Models",
    "section": "",
    "text": "How can we model a binary \\(y\\) variable?\nDoes OLS (the linear probability model) work sufficiently well?\nHow can we build a maximum likelihood model?"
  },
  {
    "objectID": "binarymodels24.html#example---democratic-peace-data",
    "href": "binarymodels24.html#example---democratic-peace-data",
    "title": "Binary Response Models",
    "section": "Example - Democratic Peace data",
    "text": "Example - Democratic Peace data\nAs a running example, I’ll use the Democratic Peace data to estimate logit and probit models. These come from Oneal and Russett (1997)’s well-known study in ISQ. The units are dyad-years; the \\(y\\) variable is the presence or absence of a militarized dispute, and the \\(x\\) variables include a measure of democracy (the lowest of the two Polity scores in the dyad), and a set of controls. The principle expectation here is that as the lowest democracy score in the dyad increases, the probability of a militarized dispute decreases.\n\nPredictions out of bounds\nThis figured plots the predictions from a logit and OLS model. Unsurprisingly, the logit predictions are probabilities, so are in the \\([0,1]\\) interval. The OLS predictions are not, and are often out of bounds.\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\nlogitpreds &lt;- predict(m1, type=\"response\")\n\nm2 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"probit\"), data=dp )\nmols &lt;-lm(dispute ~ border+deml+caprat+ally, data=dp )\nolspreds &lt;- predict(mols)\n\ndf &lt;- data.frame(logitpreds, olspreds, dispute=as.factor(dp$dispute))\n\nggplot(df, aes(x=logitpreds, y=olspreds, color=dispute)) + \n  geom_point()+\n  labs(title=\"Predictions from Logit and OLS\", x=\"Logit Predictions\", y=\"OLS Predictions\")+\n  geom_hline(yintercept=0)+\n  theme_minimal() +\n  scale_color_manual(values=c(\"#005A43\", \"#6CC24A\")) +\n  annotate(\"text\", x=.05, y=-.05, label=\"2,147 Predictions out of bounds\", color=\"red\")\n\n\n\n\n\n\n\n\n\nHere’s the distribution of predictions from the OLS model - you’ll note the modal density is around .04 (which is the sample frequency of \\(y\\).), but that a substantial and long tail are negative, so out of probability bounds.\n\n\ncode\nggplot(df, aes(x=olspreds)) + \n  geom_density(alpha=.5)+\n  labs(title=\"Density of OLS Predictions\", x=\"Predictions\", y=\"Density\")+\n  theme_minimal()+\ngeom_vline(xintercept=0, linetype=\"dashed\")\n\n\n\n\n\n\n\n\n\n\n\nHeteroskedastic Residuals\nThe residuals from the OLS model appear heteroskedastic, and the distribution is not normal. In fact, the distribution appears more binomial, clustered around zero and one. This shouldn’t be surprising since the \\(y\\) variable only takes on values of zero and one, and since we compute the residuals by \\(u = y - \\hat{y}\\).\n\n\ncode\ndf &lt;- data.frame(df, mols$residuals)\n \nggplot(df, aes(x=mols.residuals, color=dispute)) + \n  geom_density()+\n  labs(title=\"Density of OLS Residuals\", x=\"Residuals\", y=\"Density\")+\n  theme_minimal()+\n    scale_color_manual(values=c(\"#005A43\", \"#6CC24A\")) +\n  geom_vline(xintercept=0, linetype=\"dashed\")"
  },
  {
    "objectID": "binarymodels24.html#when-is-the-lpm-appropriate",
    "href": "binarymodels24.html#when-is-the-lpm-appropriate",
    "title": "Binary Response Models",
    "section": "When is the LPM Appropriate?",
    "text": "When is the LPM Appropriate?\nThe best answer is never.\n\nThere seems to be a mild trend in the discipline to rehabilitate the LPM though it’s not clear why - that is, it’s hard to find statements about the advantages of doing so in any particular setting, or about the disadvantages of estimating a logit or probit model that would lead us to prefer the LPM.\n\nOLS is a rockin’ estimator, but it’s just not well suited to limited \\(y\\) variables. Efforts to rehabilitate the LPM are like putting lipstick on a pig."
  },
  {
    "objectID": "binarymodels24.html#examples-of-limited-dvs",
    "href": "binarymodels24.html#examples-of-limited-dvs",
    "title": "Binary Response Models",
    "section": "Examples of Limited DVs",
    "text": "Examples of Limited DVs\n\nbinary variables: 0=peace, 1=war; 0=vote, 1=don’t vote.\nunordered or nominal categorical variables: type of car you prefer: Honda, Toyota, Ford, Buick; policy choices; party or candidate choices.\nordered variables that take on few values: some survey responses.\ndiscrete count variables; number of episodes of scarring torture in a country-year, 0, 1, 2, 3, …, \\(\\infty\\); the number of flawed computer chips produced in a factory in a shift; the number of times a person has been arrested; the number of self-reported extramarital affairs; number of visits to your primary care doctor.\ntime to failure; how long a civil war lasts; how long a patient survives disease; how long a leader survives in office."
  },
  {
    "objectID": "binarymodels24.html#binary-y-variables-1",
    "href": "binarymodels24.html#binary-y-variables-1",
    "title": "Binary Response Models",
    "section": "Binary \\(y\\) variables",
    "text": "Binary \\(y\\) variables\nGenerally, we think of a binary variable as being the observable manifestation of some latent, unobserved continuous variable.\nIf we could adequately observe (and measure) the underlying continuous variable, we’d use some form of OLS regression to analyze that variable. But because we have limited observation, we turn to maximum likelihood methods to estimate a model that allows to use \\(y\\), but generate estimates of \\(y^*\\), the variable we wish we could measure."
  },
  {
    "objectID": "binarymodels24.html#a-nonlinear-model-for-binary-data",
    "href": "binarymodels24.html#a-nonlinear-model-for-binary-data",
    "title": "Binary Response Models",
    "section": "A nonlinear model for binary data",
    "text": "A nonlinear model for binary data\nSo \\(y\\) is binary, and we’ve established the linear model is not appropriate. The observed variable, \\(y\\), appears to be binomial (iid):\n\\[ y \\sim f_{binomial}(\\pi_i)\\]\n\\[ y = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{} \\pi_{i}\\\\\n         0, & \\mbox{} 1-\\pi_{i}\n         \\end{array}\n     \\right. \\]\n\\[ \\pi_i = F(x_i\\widehat{\\beta}) \\] \\[1- \\pi_i=1-F(x_i\\widehat{\\beta})\\]"
  },
  {
    "objectID": "binarymodels24.html#binomial-likelihood",
    "href": "binarymodels24.html#binomial-likelihood",
    "title": "Binary Response Models",
    "section": "Binomial Likelihood",
    "text": "Binomial Likelihood\nWrite the binomial density:\n\\[ Pr(y=1| \\pi) = \\pi_i^{y_i} (1-\\pi_i)^{1-y_i} \\]\nWrite the joint probability as a likelihood:\n\\[\\mathcal{L} (\\pi |\\ y) = \\prod \\limits_{i=1}^{n} \\left[ \\pi_i^{y_i} (1-\\pi_i)^{1-y_i}\\right]\\]\nTake the log of that likelihood:\n\\[\\ln \\mathcal{L} (\\pi| \\ y) = \\sum \\limits_{i=1}^{n}  \\left[ y_i \\ln ( \\pi_i) +  (1-y_i) \\ln(1-\\pi_i)\\right]\\]"
  },
  {
    "objectID": "binarymodels24.html#parameterize-the-model",
    "href": "binarymodels24.html#parameterize-the-model",
    "title": "Binary Response Models",
    "section": "Parameterize the model",
    "text": "Parameterize the model\nParameterize \\(\\pi_i\\) - make \\(\\pi_i\\) a function of some variables and their slope effects, \\(x\\beta\\) - this is the systematic component of the model:\n\\[\\pi_i= F(x \\beta)\\]\nThis is the binomial log-likelihood function.\n\\[\\ln \\mathcal{L} (\\pi| \\ y) = \\sum \\limits_{i=1}^{n}  \\left[ y_i \\ln (F(x_i\\widehat{\\beta})) +  (1-y_i) \\ln(1-F(x_i\\widehat{\\beta}))\\right]\\]\nBut we need to fill in \\(F\\), the link function."
  },
  {
    "objectID": "binarymodels24.html#link-function",
    "href": "binarymodels24.html#link-function",
    "title": "Binary Response Models",
    "section": "Link Function",
    "text": "Link Function\nWe parameterized \\(\\pi_i\\):\n\\[\\pi_i= F(x \\beta)\\]\nand now need to choose an appropriate link function for \\(F\\) such that:\n\nour prediction of \\(\\widehat{\\pi_i}\\) is bounded [0,1].\n\\(x_i \\widehat{\\beta}\\) can range over the interval \\([-\\infty, +\\infty]\\) and map onto the [0,1] interval.\n\nThere’s a large number of sigmoid shaped probability functions that will satisfy these needs.\n\n\n\n\n\n\nImportant Concept\n\n\n\nThe link function maps or transforms the linear prediction on the sigmoid probability space, and obeys the bounds of 0,1.\n\n\nThe most commonly used link functions are the standard normal (probit)}\n\\[Pr(y_i=1 | X) = \\Phi(x_i\\widehat{\\beta}) \\]\nand the logistic (logit) CDFs.\n\\[Pr(y_i=1 | X) = \\frac{1}{1+exp^{-(x_i\\widehat{\\beta})}} \\]\nHere are the logistic and Normal CDFs:\n\n\ncode\nz &lt;- seq(-5,5,.1)\nl &lt;- seq(0,1,.01)\ns1 &lt;- 1/(1+exp(-z))\ns2 &lt;- pnorm(z)\n\ndf &lt;- data.frame(z=z, l=l, s1=s1, s2=s2)\n\nggplot(df, aes(x=z, y=l)) + \n  geom_line(aes(x=z, y=l), color=\"black\", linetype=\"dashed\" )+\n  geom_line(aes(x=z, y=s1), color=\"#005A43\")+\n  geom_line(aes(x=z, y=s2), color=\"#6CC24A\")+\n  labs(title=\"Logistic and Normal CDFs\", x=expression(x*beta), y=\"Pr(y=1)\")+\n  theme_minimal() +\n  annotate(\"text\", x=1.3, y=.7, label=\"logistic\", color=\"black\")+\n  annotate(\"text\", x=-.2, y=.15, label=\"normal\", color=\"black\")\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nNote the sigmoid functions approach the limits at decreasing rates; the fastest rate of change is at \\(y=.5\\), a point around which the curves are symmetric. The point \\(y=.5\\) is the transition point below which we’d predict a zero, above which we’d predict a one if we were interested in classifying cases into zeros and ones. Classification is a common use for models like these, say distinguishing spam from non-spam emails, or predicting the presence or absence of a disease. More on this later."
  },
  {
    "objectID": "binarymodels24.html#probit-and-logit-llfs",
    "href": "binarymodels24.html#probit-and-logit-llfs",
    "title": "Binary Response Models",
    "section": "Probit and Logit LLFs",
    "text": "Probit and Logit LLFs\nProbit - link between \\(x\\hat{\\beta}\\) and \\(Pr(y=1)\\) is standard normal CDF: \\[\n\\ln \\mathcal{L} (Y|\\beta) = \\sum_{i=1}^{N} y_i \\ln \\Phi(\\mathbf{x_i \\beta})+ (1-y_i) \\ln[1-\\Phi(\\mathbf{x_i \\beta})] \\nonumber\n\\]\nLogit (logistic CDF):\n\\[\n\\ln \\mathcal{L} (Y|\\beta) = \\sum_{i=1}^{N} \\left\\{ y_i \\ln \\left(\\frac{1}{1+e^{-\\mathbf{x_i \\beta}}}\\right)+ (1-y_i) \\ln \\left[1-\\left(\\frac{1}{1+e^{-\\mathbf{x_i \\beta}}}\\right)\\right] \\right\\}\\nonumber\n\\]"
  },
  {
    "objectID": "binarymodels24.html#predicted-probabilities",
    "href": "binarymodels24.html#predicted-probabilities",
    "title": "Binary Response Models",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\nIn the nonlinear model, the most basic quantity is\n\\[F(x\\widehat{\\beta})\\]\nwhere \\(F\\) is the link function, mapping the linear prediction onto the probability space.\nFor the logit, the predicted probability is\n\\[Pr(y=1) = \\frac{1}{1+exp(-x\\widehat{\\beta})}\\]\nFor the probit, the predicted probability is\n\\[Pr(y=1) = \\Phi(x\\widehat{\\beta})\\]\nAgain, simply using the link function to map the linear prediction onto the probability space."
  },
  {
    "objectID": "binarymodels24.html#marginal-effects",
    "href": "binarymodels24.html#marginal-effects",
    "title": "Binary Response Models",
    "section": "Marginal Effects",
    "text": "Marginal Effects\nIn the linear model, the marginal effect of \\(x\\) is \\(\\widehat{\\beta}\\). That is, the effect of a one unit change in \\(x\\) on \\(y\\) is \\(\\widehat{\\beta}\\).\n\\[\n\\frac{\\partial \\widehat{y}}{\\partial x_k}= \\frac{\\partial x \\widehat{\\beta}}{\\partial x_k} \\nonumber \\\\  \\nonumber \\\\\n= \\widehat{\\beta} \\nonumber\n\\]\nThe marginal effect is constant with respect to \\(x_k\\). Take a look:\n\n\ncode\nx &lt;- seq(0,10,.1)\ny &lt;- 2*x\ndf &lt;- data.frame(x=x, y=y)\n\nggplot(df, aes(x=x, y=y)) + \n  geom_line()+\n  labs(title=\"Marginal Effect of x on y\", x=\"x\", y=\"y\")+\n  theme_minimal() +\n  annotate(\"text\", x=5, y=15, label=\"y = 2x\", color=\"black\")+\n  geom_segment(aes(x = 5, xend = 5, y = 0, yend = 10), color = \"red\")+\n  geom_segment(aes(x = 10, xend = 10, y = 0, yend = 20), color = \"red\")\n\n\n\n\n\n\n\n\n\nThe effect of \\(x\\) on \\(y\\) is 2 - it’s the same at \\(x=5\\) and at \\(x=10\\).\nIn the nonlinear model, the marginal effect of \\(x_k\\) depends on where \\(x\\widehat{\\beta}\\) lies with respect to the probability distribution \\(F(\\cdot)\\).\n\\[\n\\frac{\\partial Pr(y=1)}{\\partial x_k}= \\frac{\\partial F(x\\widehat{\\beta})}{\\partial x_k} \\nonumber \\\\  \\nonumber \\\\\n=  \\frac{\\partial F(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} \\cdot \\frac{\\partial (x\\widehat{\\beta})}{\\partial x_k}  \\nonumber\n\\]\nBoth of these terms simplify …\nRemember that\n\\[\n\\frac{\\partial (x\\widehat{\\beta})}{\\partial x} = \\widehat{\\beta} \\nonumber\n\\]\nand \\[\n\\frac{\\partial F(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} = f(x\\widehat{\\beta}) \\nonumber\n\\]\nwhere the derivative of the CDF is the PDF.\nPutting these together gives us:\n\\[\n\\frac{\\partial F(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} = f(x\\widehat{\\beta}) \\widehat{\\beta} \\nonumber\n\\]\nThis is \\(\\widehat{\\beta}\\) weighted by or measured at the ordinate on the PDF - the ordinate is the height of the PDF associated with a value of the \\(x\\) axis.\n\n\n\n\n\n\nImportant Concept\n\n\n\nThe effect of \\(x\\) on \\(Pr(y=1)\\) is not constant; it will be large for some values of \\(x\\) and small for others. This makes sense if we think about the sigmoid functions - the slope of the curve is steepest at \\(y=.5\\), and flattens as we move away from that point toward either limit. Take another look at Figure 1\n\n\n\nLogit Marginal Effects\n\n\nRecall \\(\\Lambda\\) is the logistic CDF = \\[1/(1+exp(-x_i\\widehat{\\beta}))\\].\n\\(\\lambda\\) is the logit PDF \\[1/(1+exp(-x_i\\widehat{\\beta}))^2\\]\nAlso, remember that\n\\[\\frac{e^{x_i\\widehat{\\beta}}}{1+e^{x_i\\widehat{\\beta}}} = \\frac{1}{1+e^{-x_i\\widehat{\\beta}}}\\]\n\\[\n\\begin{align}\n\\frac{\\partial \\Lambda(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} = \\lambda(x\\widehat{\\beta}) \\widehat{\\beta} \\\\\n= \\frac{e^{x_i\\widehat{\\beta}}}{(1+e^{x_i\\widehat{\\beta}})^2} \\widehat{\\beta}  \\\\\n=\\frac{e^{x_i\\widehat{\\beta}}}{1+e^{x_i\\widehat{\\beta}}} \\frac{1}{1+e^{x_i\\widehat{\\beta}}} \\widehat{\\beta}  \\\\\n=\\Lambda(x_i\\widehat{\\beta}) \\frac{1+e^{x_i\\widehat{\\beta}}-e^{x_i\\widehat{\\beta}}}{1+e^{x_i\\widehat{\\beta}}} \\widehat{\\beta}   \\\\\n=\\Lambda(x_i\\widehat{\\beta}) 1-\\frac{e^{x_i\\widehat{\\beta}}}{1+e^{x_i\\widehat{\\beta}}} \\widehat{\\beta}  \\\\\n=\\Lambda(x_i\\widehat{\\beta}) (1-\\Lambda(x_i\\widehat{\\beta})) \\widehat{\\beta}  \n\\end{align}\n\\]\nSo this last line indicates the marginal effect of \\(x\\) is the probability of a one times the probability of a zero times \\(\\widehat{\\beta}\\).\nThis is useful because the largest value this can take on is .25 \\((Pr(y_i=1)=0.5 \\cdot Pr(y_i=0)=0.5= 0.25)\\) - therefore, the maximum marginal effect any \\(x\\) can have is \\(0.25 \\widehat{\\beta}\\).\nLooking at the democratic peace model below, the coefficient on democracy is -.071, so the largest effect democracy can have on the probability of a militarized dispute is \\(0.25 \\cdot -.071 = -.01775\\).\n\ncode\nlibrary(stargazer)\n\nstargazer(m1,m2, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\nlogistic\n\n\nprobit\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n0.587*** (0.037)\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n-0.031*** (0.003)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n-0.001*** (0.0001)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n-0.350*** (0.038)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n-1.903*** (0.032)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n7,032.985\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\nIn the probit model, the marginal effect is:\n\\[\n\\frac{\\partial \\Phi(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} = \\phi(x\\widehat{\\beta}) \\widehat{\\beta} \\nonumber\n\\]\nThe ordinate at the maximum of the standard normal PDF is 0.3989 - rounding to 0.4, we can say that the maximum marginal effect of any \\(\\widehat{\\beta}\\) in the probit model is \\(0.4\\widehat{\\beta}\\).\nThe ordinate is at the maximum where \\(z=0\\); recall this is the standard normal, so \\(x_i\\widehat{\\beta}=z\\). When \\(z=0\\),\n\\[Pr(z)=\\frac{1}{\\sqrt{2 \\pi}} \\exp \\left[\\frac{-(z)^{2}}{2}\\right] \\nonumber \\\\ \\nonumber\\\\\n=\\frac{1}{\\sqrt{2 \\pi}} \\nonumber\\\\\n\\approx .4 \\nonumber \\]\nSo the maximum marginal effect of any \\(x\\) in the probit model is \\(0.4\\widehat{\\beta}\\)."
  },
  {
    "objectID": "binarymodels24.html#logit-odds-interpretation",
    "href": "binarymodels24.html#logit-odds-interpretation",
    "title": "Binary Response Models",
    "section": "Logit Odds Interpretation",
    "text": "Logit Odds Interpretation\nThe odds are given by the probability an event occurs divided by the probability it does not:\n\\[\n\\Omega(X) = \\frac{Pr(y=1)}{1-Pr(y=1)} \\nonumber\n= \\frac{\\Lambda(X\\widehat{\\beta})}{(1-\\Lambda(X\\widehat{\\beta}))} \\nonumber\n\\]"
  },
  {
    "objectID": "binarymodels24.html#logit-log-odds",
    "href": "binarymodels24.html#logit-log-odds",
    "title": "Binary Response Models",
    "section": "Logit Log-odds",
    "text": "Logit Log-odds\nLogging …\n\\[\\ln \\Omega(X) = \\ln \\left(\\frac{\\Lambda(X\\widehat{\\beta})}{(1-\\Lambda(X\\widehat{\\beta}))}\\right) =X\\widehat{\\beta} \\]\n\\[\n\\frac{\\partial \\ln \\Omega}{\\partial X} = \\widehat{\\beta} \\nonumber\n\\]\nWhich shows the change in the log-odds given a change in \\(X\\) is constant (and therefore linear). This quantity is sometimes called “the logit.”"
  },
  {
    "objectID": "binarymodels24.html#logit-odds-ratios",
    "href": "binarymodels24.html#logit-odds-ratios",
    "title": "Binary Response Models",
    "section": "Logit Odds Ratios",
    "text": "Logit Odds Ratios\nOdds ratios are very useful:\n\\[\n\\frac{ \\Omega x_k + 1}{\\Omega x_k} =exp(\\widehat{\\beta_k}) \\nonumber\n\\]\ncomparing the difference in odds between two values of \\(x_k\\); note the change in value does not have to be 1.\n\\[\n\\frac{ \\Omega x_k + \\iota}{\\Omega x_k} =exp(\\widehat{\\beta_k}* \\iota) \\nonumber\n\\]\nNot only is it simple to exponentiate \\(\\widehat{\\beta_k}\\), but the interpretation is that \\(x\\) increases/decreases \\(Pr(y=1)\\) by that factor, \\(exp(\\widehat{\\beta_k})\\), and more usefully, that:\n\\[\n100*(exp(\\widehat{\\beta_k})-1) \\nonumber\n\\]\nis the percentage change in the odds given a one unit change in \\(x_k\\).\nSo a logit coefficient of .226\n\\[\n100*(exp(.226)-1) =25.36 \\nonumber\n\\]\nProduces a 25.36% increase in the odds of \\(y\\) occurring."
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code",
    "section": "",
    "text": "exercise #1\nexercise #2\n\n\n\n Back to top"
  },
  {
    "objectID": "mlesyllabus24.html#seminar-description",
    "href": "mlesyllabus24.html#seminar-description",
    "title": "MLE Syllabus",
    "section": "Seminar Description",
    "text": "Seminar Description\nThis 4 credit hour seminar is a survey of maximum likelihood methods and their applications to empirical political questions. It presumes students have a detailed and intuitive knowledge of least squares, probability theory, basic skills in scalar and matrix algebra, and a basic understanding of calculus. The course will deal mainly in understanding the principles of maximum likelihood estimation, under what conditions we move away from least squares, and what particular models are appropriate given observed data. The seminar will focus on application and interpretation of ML models and linking theory to statistical models. The course emphasizes coding and data viz in R and Stata.\nThe class meets one time per week for three hours. My office hours are designed to be homework help hours where I’ll work in the grad lab with any of you who are working on the exercises. The most productive pathway for this class is for you to get in the habit of working together, and those office hours are a good time for this."
  },
  {
    "objectID": "mlesyllabus24.html#course-purpose",
    "href": "mlesyllabus24.html#course-purpose",
    "title": "MLE Syllabus",
    "section": "Course Purpose",
    "text": "Course Purpose\nThis seminar fulfills the advanced quantitative methods requirement in the Ph.D. curriculum. The method of maximum likelihood underlies a majority of quantitative models in Political Science; this class teaches students to be astute consumers of such models, and how to implement and interpret ML models. These are crucial skills for dissertations in Political Science, and for producing publishable quantitative research."
  },
  {
    "objectID": "mlesyllabus24.html#learning-objectives",
    "href": "mlesyllabus24.html#learning-objectives",
    "title": "MLE Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nStudents will encounter an array of maximum likelihood models in this course. By the end of the course, students will have mastered the theory of maximum likelihood sufficient to write and program likelihood functions in ; they will be able to choose, estimate, and interpret appropriate models, model specifications, and model evaluation tools given their data; and they will be able to produce sophisticated quantities of interest (e.g. predicted probabilities, expected values, confidence intervals) via a variety of techniques including simulation and end point transformation. Students will also be able to present model findings verbally and graphically."
  },
  {
    "objectID": "mlesyllabus24.html#class-meetings-office-hours-assignments",
    "href": "mlesyllabus24.html#class-meetings-office-hours-assignments",
    "title": "MLE Syllabus",
    "section": "Class Meetings, Office Hours, Assignments",
    "text": "Class Meetings, Office Hours, Assignments\nThe course will meet this fall entirely in-person in the Social Science Experiment Lab on Wednesdays 9:40am-12:40pm.\nOffice hours are Mondays 1:30pm-3:30pm. I’ll likely hold these in the grad work room to help with your assignments. For an appointment, email me and we’ll sort out a time.\nAll assignments should be turned in on Brightspace - please submit ::\n\nPDFs generated from LaTeX or R Markdown (Quarto).\nannotated R scripts.\nwhere necessary, data.\n\nAssignments should be instantly replicable - running the code file should produce all models, tables, plots, etc."
  },
  {
    "objectID": "mlesyllabus24.html#reading",
    "href": "mlesyllabus24.html#reading",
    "title": "MLE Syllabus",
    "section": "Reading",
    "text": "Reading\nThe reading material for the course is important because it often demonstrates application of various MLE models; seeing how folks apply these and how they motivate their applications is really informative, something you cannot miss. We often won’t directly discuss the readings, but don’t let that imply they’re not important. If I get the sense we’re not keeping up with reading, expect the syllabus to change to incorporate quizzes or other accountability measures.\nReading for the course will consist of several books and articles (listed by week below). The books listed below also have Amazon links - you’ll find most of these cheaper used online.\n\nRequired\n\nBox-Steffensmeier, Janet and Jones, Brad. 2004. Event History Modeling. Cambridge. ISBN 0521546737\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Sage Publications Inc. ISBN 0803973748\nWard, Michael D. and John S. Ahlquist. 2018 Maximum Likelihood for Social Science. Cambridge. ISBN 978-1316636824.\n\n\n\nRecommended\nUseful, but not required (though some required reading in the first one):\n\nGary King. 1998. Unifying Political Methodology. University of Michigan Press. ISBN 0472085549\nJ. Scott Long. 2014. Regression Models for Categorical Dependent Variables Using Stata. 3rd Ed. Stata Press. ISBN 1597181110 (this book is good for practical/applied examples even if R is your primary language)\n\nGary King’s book is regarded as seminal in developing ML applications in political science. Scott Long’s is a similarlyaccessible treatment of a host of ML models and applications (and the Stata book is a great applied companion). Together, these two books are probably the most important on the syllabus as they are both accessible, but comprehensive and technical enough to be useful. Ward and Ahlquist’s book is a new overview of applied ML in a political science setting. Box-Steffensmeier and Jones is a thorough and accessible treatment of hazard models in a variety of empirical settings.\n\n\nAdditional Resources\nOther useful books include:\n\nCameron, A. Colin and Trivedi, Pravin K. 1998. Regression Analysis of Count Data. Cambridge. ISBN 0521635675\nMaddala, Gregory. 1983. Limited Dependent and Qualitative Variables in Econometrics. Cambridge. ISBN 0521338255\nPaul D Allison - Event History Analysis : Regression for Longitudinal Event Data. Sage Publications Inc. ISBN 0803920555\nTim Futing Liao - Interpreting Probability Models : Logit, Probit, and Other Generalized Linear Models. Sage Publications Inc. ISBN 0803949995\nJohn H Aldrich and Forrest D Nelson - Linear Probability, Logit, and Probit Models. Sage Publications Inc. ISBN 0803921330\nFred C Pampel - Logistic Regression : A Primer. Sage Publications Inc. ISBN 0761920102\nVani Kant Borooah - Logit and Probit : Ordered and Multinomial Models. Sage Publications Inc. ISBN 0761922423\nScott R Eliason - Maximum Likelihood Estimation : Logic and Practice. Sage Publications Inc. ISBN 0803941072\nRichard Breen - Regression Models : Censored, Sample Selected, or Truncated Data. Sage Publications Inc. ISBN 0803957106\nKrishnan Namboodiri - Matrix Algebra : An Introduction. ISBN 0803920520"
  },
  {
    "objectID": "mlesyllabus24.html#course-requirements-and-grades",
    "href": "mlesyllabus24.html#course-requirements-and-grades",
    "title": "MLE Syllabus",
    "section": "Course Requirements and Grades",
    "text": "Course Requirements and Grades\nThe seminar requires the following:\n\nProblem sets - 60% total\nMechanism papers - 40%\n\nPlease note that all written assignments must be submitted as PDFs either compiled in LaTeX or in R markdown (Quarto).\nYou’ll complete a series of problem sets, mostly applied. How many will depend on how things move along during the term. Regarding the problem sets - the work you turn in for the problem sets should clearly be your own, but I urge you to work together - doing so is a great way to learn and to overcome problems.\nThe mechanism papers are a series of three short papers you’ll write during the semester aimed at learning to identify and describe causal mechanisms, then at producing a causal mechanism. More on these early in the term.\nA word about completeness - attempt everything. To receive a passing grade in the course, you must finish all elements of the course, so all problem sets, all exams, papers, etc. To complete an element, you must at least attempt all parts of the element - so if a problem set has 10 problems, you must attempt all 10 or the assignment is incomplete, you’ve not completed every element of the course, and you cannot pass. I realize there may be problems you have trouble with and even get wrong, but you must try - the bottom line is don’t turn in incomplete work.\nGrades will be assigned on the following scale:\n\n\n\n\n\n\n\n\n\n\n\nGrade\nRange\nGrade\nRange\n\n\n\n\nA\n94-100%\nC+\n77-79%\n\n\nA-\n90–93%\nC\n73-76%\n\n\nB+\n87–89%\nC-\n70-72%\n\n\nB\n83-86%\nD\n60-69%\n\n\nB-\n80-82%\nF\n&lt;60%\n\n\n\n\n\n\nCourse Policies\n\nAttendance\nAttendance is expected, and is essential if you’re to succeed in this class.\n\n\nAcademic Integrity\nIdeas are the currency in academic exchange, so acknowledging where ideas come from is important. Acknowledging the sources of ideas also helps us identify an idea’s lineage which can be important for understanding how that line of thought has developed, and toward promoting future growth. As graduate students, you should have a good understanding of academic honesty and best practices. Here are details of Binghamton’s honesty policy.\n\n\n\nCourse Schedule\nWeek 1, Aug 21 – Binary \\(y\\) Variables I - probit/logit, QI\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 1, 2, 4\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 3.    \n\nWeek 2, Aug 28 – Likelihood Theory and ML Estimation\n\nGary King. 1998. Unifying Political Methodology. Chapter 1-4\nJ. Scott Long. 1997. *Regression Models for Categorical and Limited Dependent Variables}. Chapters 1-2.\n\nWeek 3, Sept 4 – Binary \\(y\\) Variables II - symmetry, fit, diagnostics, prediction\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 3, 5, 6, 7\n\n\n\nNagler (1994)\nKing and Zeng (2001)\nFranklin and Kosaki (1989)\nC. Zorn (2005)\n\nWeek 4, Sept 11 – Binary \\(y\\) Variables III (discrete hazards)\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 11 \nBeck, Katz, and Tucker (1998)\nCarter and Signorino (2010)\n\nWeek 5, Sept 18 – Binary \\(y\\) Variables IV - variance, order\n\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 5.\nFranklin (1991)\nAlvarez and Brehm (1995)\nClark and Nordstrom (2005)\n\nWeek 6, Sept 25 – Assumptions and Specification - interactions, functional form, measurement of \\(y\\)\n\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 5.\nClark, Nordstrom, and Reed (2008)\nClarke and Stone (2008)\nBerry, Golder, and Milton (2012)\nBrambor, Clark, and Golder (2006)\n\nWeek 7, Oct 2 – No class, Yom Kippur\nWeek 8, Oct 9 – Choice Models I (Unordered \\(y\\) Variables) - MNL, MNP, CL (IIA)\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 9\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 6.\nAlvarez and Nagler (1998)\nLacy and Burden (1999)\nC. J. W. Zorn (1996)\n\nWeek 9, Oct 16– Choice Models II (Unordered Dependent Variables continued, and systems of eqs, ordered)\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 8\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 5.\nFranklin and Kosaki (1989)\n\nWeek 10, Oct 23 – Event Count Models I - poisson, dispersion\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 10\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 8.1, 8.2.\nGowa (1998)\nFordham (1998)\n\nWeek 11, Oct 30 – Event Count Models II - negative binomial, zero-altered\n\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 8.3-8.7.\nC. J. W. Zorn (1998)\nClark (2003)\n\nWeek 12, Nov 6 – Continuous Time Hazard Models I - parametric, semi-parametric models\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 11\nJanet Box-Steffensmeier and Brad Jones. 2004. Event History Modeling. Chs. 1-4\nJ. M. Box-Steffensmeier, Arnold, and Zorn (1997)\n\nWeek 13, Nov 13 – Continuous Time Hazard Models II - parametric models, special topics\n\nJanet Box-Steffensmeier and Brad Jones. 2004. Event History Modeling. Chs. 5-11\nC. J. W. Zorn (2000)\nBennett and Stam (1996)\nJ. Box-Steffensmeier, Reiter, and Zorn (2003)\n\nWeek 14, Nov 20 – Censored/Truncated Variables, Samples - selection models - J. Scott Long. 1997. *Regression Models for Categorical and Limited Dependent Variables}. Chapter 7\n\nReed (2000)\nSignorino (1999)\nTimpone (1998)\n\nWeek 15, Nov 27 – no class, Thanksgiving\nWeek 16, Dec 4 – Review\n\n\nReferences"
  },
  {
    "objectID": "mlesyllabus24.html#course-policies",
    "href": "mlesyllabus24.html#course-policies",
    "title": "MLE Syllabus",
    "section": "Course Policies",
    "text": "Course Policies\n\nAttendance\nAttendance is expected, and is essential if you’re to succeed in this class.\n\n\nAcademic Integrity\nIdeas are the currency in academic exchange, so acknowledging where ideas come from is important. Acknowledging the sources of ideas also helps us identify an idea’s lineage which can be important for understanding how that line of thought has developed, and toward promoting future growth. As graduate students, you should have a good understanding of academic honesty and best practices. Here are details of Binghamton’s honesty policy."
  },
  {
    "objectID": "mlesyllabus24.html#course-schedule",
    "href": "mlesyllabus24.html#course-schedule",
    "title": "MLE Syllabus",
    "section": "Course Schedule",
    "text": "Course Schedule\nWeek 1, Aug 21 – Binary \\(y\\) Variables I - probit/logit, QI\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 1, 2, 4\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 3.    \n\nWeek 2, Aug 28 – Likelihood Theory and ML Estimation\n\nGary King. 1998. Unifying Political Methodology. Chapter 1-4\nJ. Scott Long. 1997. *Regression Models for Categorical and Limited Dependent Variables}. Chapters 1-2.\n\nWeek 3, Sept 4 – Binary \\(y\\) Variables II - symmetry, fit, diagnostics, prediction\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 3, 5, 6, 7\n\n\n\nNagler (1994)\nKing and Zeng (2001)\nFranklin and Kosaki (1989)\nC. Zorn (2005)\n\nWeek 4, Sept 11 – Binary \\(y\\) Variables III (discrete hazards)\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 11 \nBeck, Katz, and Tucker (1998)\nCarter and Signorino (2010)\n\nWeek 5, Sept 18 – Binary \\(y\\) Variables IV - variance, order\n\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 5.\nFranklin (1991)\nAlvarez and Brehm (1995)\nClark and Nordstrom (2005)\n\nWeek 6, Sept 25 – Assumptions and Specification - interactions, functional form, measurement of \\(y\\)\n\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 5.\nClark, Nordstrom, and Reed (2008)\nClarke and Stone (2008)\nBerry, Golder, and Milton (2012)\nBrambor, Clark, and Golder (2006)\n\nWeek 7, Oct 2 – No class, Yom Kippur\nWeek 8, Oct 9 – Choice Models I (Unordered \\(y\\) Variables) - MNL, MNP, CL (IIA)\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 9\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 6.\nAlvarez and Nagler (1998)\nLacy and Burden (1999)\nC. J. W. Zorn (1996)\n\nWeek 9, Oct 16– Choice Models II (Unordered Dependent Variables continued, and systems of eqs, ordered)\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 8\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 5.\nFranklin and Kosaki (1989)\n\nWeek 10, Oct 23 – Event Count Models I - poisson, dispersion\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 10\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 8.1, 8.2.\nGowa (1998)\nFordham (1998)\n\nWeek 11, Oct 30 – Event Count Models II - negative binomial, zero-altered\n\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 8.3-8.7.\nC. J. W. Zorn (1998)\nClark (2003)\n\nWeek 12, Nov 6 – Continuous Time Hazard Models I - parametric, semi-parametric models\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 11\nJanet Box-Steffensmeier and Brad Jones. 2004. Event History Modeling. Chs. 1-4\nJ. M. Box-Steffensmeier, Arnold, and Zorn (1997)\n\nWeek 13, Nov 13 – Continuous Time Hazard Models II - parametric models, special topics\n\nJanet Box-Steffensmeier and Brad Jones. 2004. Event History Modeling. Chs. 5-11\nC. J. W. Zorn (2000)\nBennett and Stam (1996)\nJ. Box-Steffensmeier, Reiter, and Zorn (2003)\n\nWeek 14, Nov 20 – Censored/Truncated Variables, Samples - selection models - J. Scott Long. 1997. *Regression Models for Categorical and Limited Dependent Variables}. Chapter 7\n\nReed (2000)\nSignorino (1999)\nTimpone (1998)\n\nWeek 15, Nov 27 – no class, Thanksgiving\nWeek 16, Dec 4 – Review"
  },
  {
    "objectID": "prediction24.html",
    "href": "prediction24.html",
    "title": "Prediction Methods for MLE Models",
    "section": "",
    "text": "Most MLE models are nonlinear, so their coefficients are not their marginal effects. As a result, most MLE models require a transformation of the linear prediction to generate quantities of interest. The methods outlined here apply to most MLE applications; the immediate interest and examples here use binary response models. These slides will form a foundation for prediction in other types of models we encounter.\n\n\nProbit and logit coefficients are directly interpretable in the senses that\n\nWe can interpret direction.\nWe can interpret statistical difference from zero.\nWe can say the largest marginal effect of \\(x \\approx 0.4\\cdot\\widehat{\\beta}\\) for the probit model.\nWe can say the largest marginal effect of \\(x \\approx 0.25\\cdot\\widehat{\\beta}\\) for the logit model.\nWe can say that \\(exp(\\widehat{\\beta_k})-1\\) is the percentage change in the odds that \\(y=1\\), for the logit model.\n\nIt’s still the case that we often want other quantities of interest like probabilities, and that requires the straightforward transformations of the linear prediction, \\(F(x_i\\widehat{\\beta})\\).\nLet’s look briefly at the intuition of the “maximum marginal effect” in the logit model.\n\n\ncode\nz &lt;- seq(-5,5,.1)\nncdf &lt;- pnorm(z)\nnpdf &lt;- dnorm(z)\nlcdf &lt;- plogis(z)\nlpdf &lt;- dlogis(z)\n\ndf &lt;- data.frame(ncdf=ncdf, npdf=npdf, lcdf=lcdf, lpdf=lpdf, z=z)\n\nggplot(df, aes(x=z, y=ncdf), color=\"black\") +\n  geom_line() +\n  geom_line(aes(x=z, y=lcdf), color=\"green\") +\n  geom_line(aes(x=z, y=npdf), color=\"black\") +\n  geom_line(aes(x=z, y=lpdf), color=\"green\") +\n  geom_hline(yintercept = .3989, linetype=\"dashed\") +\n  geom_hline(yintercept = .25, linetype=\"dashed\") +\n  labs(x=\"z\", y=\"Pr(y=1)\") +\n  theme_minimal() +\n  annotate(\"text\", x = 2.5, y = .36, label = \"Normal\", color = \"black\") +\n  annotate(\"text\", x = 2.5, y = .22, label = \"Logistic\", color = \"black\") \n\n\n\n\n\n\n\n\n\nThe highest points on the PDFs indicate the maximum marginal effect of \\(x\\) on \\(Pr(y=1)\\) in the logit and probit models."
  },
  {
    "objectID": "prediction24.html#binary-response-models",
    "href": "prediction24.html#binary-response-models",
    "title": "Prediction Methods for MLE Models",
    "section": "",
    "text": "Probit and logit coefficients are directly interpretable in the senses that\n\nWe can interpret direction.\nWe can interpret statistical difference from zero.\nWe can say the largest marginal effect of \\(x \\approx 0.4\\cdot\\widehat{\\beta}\\) for the probit model.\nWe can say the largest marginal effect of \\(x \\approx 0.25\\cdot\\widehat{\\beta}\\) for the logit model.\nWe can say that \\(exp(\\widehat{\\beta_k})-1\\) is the percentage change in the odds that \\(y=1\\), for the logit model.\n\nIt’s still the case that we often want other quantities of interest like probabilities, and that requires the straightforward transformations of the linear prediction, \\(F(x_i\\widehat{\\beta})\\).\nLet’s look briefly at the intuition of the “maximum marginal effect” in the logit model.\n\n\ncode\nz &lt;- seq(-5,5,.1)\nncdf &lt;- pnorm(z)\nnpdf &lt;- dnorm(z)\nlcdf &lt;- plogis(z)\nlpdf &lt;- dlogis(z)\n\ndf &lt;- data.frame(ncdf=ncdf, npdf=npdf, lcdf=lcdf, lpdf=lpdf, z=z)\n\nggplot(df, aes(x=z, y=ncdf), color=\"black\") +\n  geom_line() +\n  geom_line(aes(x=z, y=lcdf), color=\"green\") +\n  geom_line(aes(x=z, y=npdf), color=\"black\") +\n  geom_line(aes(x=z, y=lpdf), color=\"green\") +\n  geom_hline(yintercept = .3989, linetype=\"dashed\") +\n  geom_hline(yintercept = .25, linetype=\"dashed\") +\n  labs(x=\"z\", y=\"Pr(y=1)\") +\n  theme_minimal() +\n  annotate(\"text\", x = 2.5, y = .36, label = \"Normal\", color = \"black\") +\n  annotate(\"text\", x = 2.5, y = .22, label = \"Logistic\", color = \"black\") \n\n\n\n\n\n\n\n\n\nThe highest points on the PDFs indicate the maximum marginal effect of \\(x\\) on \\(Pr(y=1)\\) in the logit and probit models."
  },
  {
    "objectID": "prediction24.html#at-means-predictions",
    "href": "prediction24.html#at-means-predictions",
    "title": "Prediction Methods for MLE Models",
    "section": "At-Means Predictions",
    "text": "At-Means Predictions\nAt-means predictions are what they sound like - effects with independent variables set at central tendencies. These are sometimes called “adjusted predictions.”\n\nestimate model.\ncreate out of sample data.\nvary \\(x\\) of interest; set all other \\(x\\) variables to appropriate central tendencies - hence the “at Means.”\ngenerate QIs in out of sample data."
  },
  {
    "objectID": "prediction24.html#average-effects",
    "href": "prediction24.html#average-effects",
    "title": "Prediction Methods for MLE Models",
    "section": "Average Effects",
    "text": "Average Effects\nAverage Marginal Effects are in-sample but create a counterfactual for a variable of interest, assuming the entire sample looks like that case.\nFor instance, suppose a model of wages with covariates for education and gender. We might ask the question what would the predictions look like if the entire sample were male, but otherwise looked as it does? Alternatively, what would the predictions look like if the entire sample were female, but all other variables the same as they appear in the estimation data?\nTo answer these, we’d change the gender variable to male, generate \\(x{\\widehat{\\beta}}\\) for the entire sample, and take the average, then repeat with the gender variable set to female.\nTo generate Average Effects,\n\nestimate model.\nin estimation data, set variable of interest to a particular value for the entire estimation sample.\ngenerate QIs (expected values, standard errors).\ntake average of QIs, and save.\nrepeat for all values of variable of interest, and plot."
  },
  {
    "objectID": "prediction24.html#at-means-predictions-logit",
    "href": "prediction24.html#at-means-predictions-logit",
    "title": "Prediction Methods for MLE Models",
    "section": "At-means predictions (logit)",
    "text": "At-means predictions (logit)\nHere’s an example of at-means predictions for a logit model of the democratic peace. FIrst, let’s look at the model estimates:\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n\nstargazer(m1, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\nAs with any nonlinear model, we need to compute a linear prediction, \\(x\\widehat{\\beta}\\), and then transform that to a probability. For at-means predictions, we’ll vary democracy across its range, holding the remaining variables at appropriate central tendency (e.g, mode for dummy variables, median for categorical or skewed variables, etc.) Take a look at the code:\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\nlogitpreds &lt;- predict(m1, type=\"response\")\n\nm2 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"probit\"), data=dp )\n\n#new data frame for MEM prediction\nmem &lt;- data.frame(deml= c(seq(-10,10,1)), \n                  border=0, caprat=median(dp$caprat), ally=0)\n\n# type=\"link\" produces the linear predictions; transform by hand below w/EPT\nmem  &lt;-data.frame(mem, predict(m1, type=\"link\", newdata=mem, se=TRUE))\n\nmem &lt;- cbind(mem,lb=plogis(mem$fit-1.96*mem$se.fit),\n             ub=plogis(mem$fit+1.96*mem$se.fit), \n             p=plogis(mem$fit))\n\nggplot(mem, aes(x=deml, y=p)) +\n  geom_line() +\n  geom_ribbon(data=mem, aes(x=deml, ymin=lb, ymax=ub),fill = \"grey30\", alpha = .4, ) +\n  labs(x=\"Polity Score\", y=\"Pr(Dispute) (95% confidence interval)\")"
  },
  {
    "objectID": "prediction24.html#average-effects-logit",
    "href": "prediction24.html#average-effects-logit",
    "title": "Prediction Methods for MLE Models",
    "section": "Average effects (logit)",
    "text": "Average effects (logit)\nAverage effects are often a better choice because they represent the data more completely than central tendency can (in the at-mean effects). Here are average effects (using the logit estimates) across the range of polity, and for pairs of states that share borders and those that do not.\n\n\ncode\n#avg effects\n\n#identify the estimation sample\ndp$used &lt;- TRUE\ndp$used[na.action(m1)] &lt;- FALSE\ndpesample &lt;- dp %&gt;%  filter(used==\"TRUE\")\n\npolity &lt;- 0\nmedxbd0 &lt;- 0\nubxbd0 &lt;- 0\nlbxbd0 &lt;- 0\n# medse &lt;- 0\n# medxbd1 &lt;- 0\n# ubxbd1 &lt;- 0\n# lbxbd1 &lt;- 0\n\nfor(i in seq(1,21,1)){\n  dpesample$border&lt;- 0\n  dpesample$deml &lt;- i-11\n  polity[i] &lt;- i-11\n  allpreds &lt;- data.frame(predict(m1, type= \"response\", se.fit=TRUE, newdata = dpesample))  \n  medxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)\n  ubxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)+1.96*(median(allpreds$se.fit, na.rm=TRUE))\n  lbxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)-1.96*(median(allpreds$se.fit, na.rm=TRUE))\n}\n  \nnoborder &lt;- data.frame(polity, medxbd0, ubxbd0, lbxbd0)\n  \nfor(i in seq(1,21,1)){\n  dpesample$border&lt;- 1\n  dpesample$deml &lt;- i-11\n  polity[i] &lt;- i-11\n  allpreds &lt;- data.frame(predict(m1, type= \"response\", se.fit=TRUE, newdata = dpesample))  \n  medxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)\n  ubxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)+1.96*(median(allpreds$se.fit, na.rm=TRUE))\n  lbxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)-1.96*(median(allpreds$se.fit, na.rm=TRUE))\n}\n  \nborder &lt;- data.frame(polity, medxbd0, ubxbd0, lbxbd0)\n  \n\n\nggplot() +\n  geom_ribbon(data=noborder, aes(x=polity, ymin=lbxbd0, ymax=ubxbd0),fill = \"grey30\", alpha = .4, ) +\n  geom_line(data=noborder, aes(x=polity, y=medxbd0))+\n  geom_ribbon(data=border, aes(x=polity, ymin=lbxbd0, ymax=ubxbd0),fill = \"grey30\", alpha = .4, ) +\n  geom_line(data=border, aes(x=polity, y=medxbd0))+\n  labs ( colour = NULL, x = \"Polity Score\", y =  \"Pr(Dispute)\" )+\n  theme_minimal()+\n  ggtitle(\"Average Effects\")"
  },
  {
    "objectID": "prediction24.html#simulation",
    "href": "prediction24.html#simulation",
    "title": "Prediction Methods for MLE Models",
    "section": "Simulation",
    "text": "Simulation\nSimulation is an especially good approach for nonlinear models:\n\nestimate the model.\n\\(m\\) times (say, 1000 times), simulate the distribution of \\(\\widehat{\\beta}\\).\ngenerate the \\(m\\) linear predictions, \\(x\\widehat{\\beta}\\).\ntransform by the appropriate link function (logistic, standard normal CDF).\nidentify the 2.5, 50, and 97.5 percentiles.\nplot against \\(x\\).\n\n\n\ncode\n#draws from multivariate normal using logit model estimates\nsimL &lt;- data.frame(MASS::mvrnorm(1000, coef(m1), vcov(m1)))\n#draws from multivariate normal using probit model estimates\nsimP &lt;- data.frame(MASS::mvrnorm(1000, coef(m2), vcov(m2)))\n\n#Logit predictions\nlogitprobs &lt;- data.frame(dem= numeric(0) , lb=numeric(0), med= numeric(0), ub=numeric(0))\nfor (i in seq(1,21,1)) {\nsimpreds &lt;- quantile(simL$X.Intercept.+ simL$border*0+simL$deml*(i-11)+simL$caprat*median(dp$caprat)+simL$ally*0, probs=c(.025, .5, .975))\nlogitprobs[i,] &lt;- data.frame(dem=i, lb=plogis(simpreds[1]), med=plogis(simpreds[2]), ub=plogis(simpreds[3]))\n}\n\n#Probit predictions\nprobitprobs &lt;- data.frame(dem= numeric(0) , lb=numeric(0), med= numeric(0), ub=numeric(0))\nfor (i in seq(1,21,1)) {\nsimpreds &lt;- quantile(simP$X.Intercept.+ simP$border*0+simP$deml*(i-11)+simP$caprat*median(dp$caprat)+simP$ally*0, probs=c(.025, .5, .975))\nprobitprobs[i,] &lt;- data.frame(dem=i, lb=pnorm(simpreds[1]), med=pnorm(simpreds[2]), ub=pnorm(simpreds[3]))\n}\n\nlogit &lt;- ggplot() +\n  geom_ribbon(data=logitprobs, aes(x=dem, ymin=lb, ymax=ub),fill = \"grey30\", alpha = .4, ) +\n  geom_line(data=logitprobs, aes(x=dem, y=med))+\n  labs ( colour = NULL, x = \"Polity Score\", y =  \"Pr(Dispute)\" )+\n  theme_minimal()+\n  ggtitle(\"Logit Predictions\")\n\nprobit &lt;- ggplot() +\n  geom_ribbon(data=probitprobs, aes(x=dem, ymin=lb, ymax=ub),fill = \"grey30\", alpha = .4, ) +\n  geom_line(data=probitprobs, aes(x=dem, y=med))+\n  labs ( colour = NULL, x = \"Polity Score\", y =  \"Pr(Dispute)\" )+\n  theme_minimal()+\n  ggtitle(\"Probit Predictions\")\n\nlogit+probit"
  },
  {
    "objectID": "prediction24.html#simulating-combinations-of-binary-variables",
    "href": "prediction24.html#simulating-combinations-of-binary-variables",
    "title": "Prediction Methods for MLE Models",
    "section": "Simulating combinations of binary variables",
    "text": "Simulating combinations of binary variables\nLet’s look at the differences among the four combinations of the binary variables, border and ally.\n\n\ncode\n## simulating for binary combinations ----\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n\n#draws from multivariate normal using logit model estimates\nsimL &lt;- data.frame(MASS::mvrnorm(1000, coef(m1), vcov(m1)))\n\n\nlogitprobs &lt;- data.frame(b0a0= numeric(0) , b1a0=numeric(0), b0a1= numeric(0), b1a1=numeric(0))\n\n  b0a0 &lt;- plogis(simL$X.Intercept.+ simL$border*0+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*0)\nb1a0 &lt;- plogis(simL$X.Intercept.+ simL$border*1+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*0)\nb0a1 &lt;- plogis(simL$X.Intercept.+ simL$border*0+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*1)\nb1a1 &lt;- plogis(simL$X.Intercept.+ simL$border*1+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*1)\n\nlogitprobs &lt;- data.frame(b0a0, b1a0, b0a1, b1a1)\n\nggplot() +\n  geom_density(data=logitprobs, aes(x=b0a0), fill=\"grey30\", alpha = .4, ) +\n  geom_density(data=logitprobs, aes(x=b1a0), fill=\"grey30\", alpha = .4, ) +\n  geom_density(data=logitprobs, aes(x=b0a1), fill=\"grey30\", alpha = .4, ) +\n  geom_density(data=logitprobs, aes(x=b1a1), fill=\"grey30\", alpha = .4, ) +\n  labs ( colour = NULL, x = \"Pr(Dispute)\", y =  \"Density\" ) +\n  theme_minimal()+\n  annotate(\"text\", x = .07, y = 150, label = \"No border, not allies\", color = \"black\") +\n  annotate(\"text\", x = .13, y = 70, label = \"Border, not allies\", color = \"black\") +\n  annotate(\"text\", x = .04, y = 200, label = \"No border, allies\", color = \"black\") +\n  annotate(\"text\", x = .09, y = 50, label = \"Border, allies\", color = \"black\") +\n  ggtitle(\"Logit Predictions\")"
  },
  {
    "objectID": "prediction24.html#ml-standard-errors-of-linear-predictions",
    "href": "prediction24.html#ml-standard-errors-of-linear-predictions",
    "title": "Prediction Methods for MLE Models",
    "section": "ML Standard Errors of Linear Predictions",
    "text": "ML Standard Errors of Linear Predictions\nOne commonly used measure of uncertainty is the standard error of the linear prediction, \\(X\\widehat{\\beta}\\).\nConsider the linear prediction\n\\[X \\widehat{\\beta} \\]\nunder maximum likelihood theory:\n\\[var(X \\widehat{\\beta}) = \\mathbf{X V X'} \\]\nan \\(N x N\\) matrix, where \\(V\\) is the var-cov matrix of \\({\\widehat{\\beta}}\\). The main diagonal contains the variances of the \\(N\\) predictions. The standard errors are:\n\\[se(X \\widehat{\\beta}) = \\sqrt{diag(\\mathbf{X V X'})} \\]\nwhich is an \\(N x 1\\) vector. So now we have a column vector of standard errors for the linear prediction, \\(X\\widehat{\\beta}\\). Like the linear predictions, these are not transformed into probabilities, so when we compute confidence intervals, we need to map the upper and lower bounds onto the probability space.\n\\[F(X \\widehat{\\beta} - c*s.e.) \\leq F(X \\widehat{\\beta}) \\leq F(X \\widehat{\\beta} + c* s.e.)\\]"
  },
  {
    "objectID": "prediction24.html#delta-method-standard-errors",
    "href": "prediction24.html#delta-method-standard-errors",
    "title": "Prediction Methods for MLE Models",
    "section": "Delta Method standard errors",
    "text": "Delta Method standard errors\nThe maximum likelihood method is appropriate for monotonic functions of \\(X \\widehat{\\beta}\\), e.g. logit, probit. In other models (e.g., multinomial logit), the function is not monotonic in \\(X \\widehat{\\beta}\\) so we use the Delta Method - this creates a linear approximation of the function. Greene (2012) (693ff) gives this as a general derivation of the variance:\n\\[Var[F(X \\widehat{\\beta})] = f(\\mathbf{x'\\widehat{\\beta}})^2 \\mathbf{x' V x} \\]\nwhere this would generate variances for whatever \\(F(X \\widehat{\\beta})\\) is, perhaps a predicted probability.\n\nDelta method standard errors for Logit\nFor the logit, the delta standard errors are given by:\n\\[F(X \\widehat{\\beta}) * (1-F(X \\widehat{\\beta}) * \\mathbf(X V X')\\]\n\\[ = f(X \\widehat{\\beta})  *  \\mathbf{\\sqrt{X V X'}}\\]\nor\n\\[ p * (1-p) * stdp\\]\nwhere \\(stdp\\) is the standard error of the linear prediction."
  },
  {
    "objectID": "prediction24.html#ses-of-predictions-for-linear-combinations",
    "href": "prediction24.html#ses-of-predictions-for-linear-combinations",
    "title": "Prediction Methods for MLE Models",
    "section": "SEs of Predictions for linear combinations",
    "text": "SEs of Predictions for linear combinations\nA common circumstance that requires joint hypothesis tests is the case of polynomials (which are themselves interactions):\n\\[y = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_1 + \\widehat{\\beta}_2 x_{1}^2  + \\varepsilon \\]\nThe question is whether \\(\\widehat{\\beta}_1 = \\widehat{\\beta}_2  = 0\\) - the marginal effect is:\n\\[ \\widehat{\\beta}_1 + 2 \\widehat{\\beta}_2x_1\\]\nand requires the standard error for \\(\\widehat{\\beta}_1+\\widehat{\\beta}_2\\), which is:\n\\[ \\sqrt{var(\\widehat{\\beta}_1) + 4x_{1}^{2}var(\\widehat{\\beta}_2) +  4x_1 cov(\\widehat{\\beta}_1, \\widehat{\\beta}_2)  }\\]"
  },
  {
    "objectID": "prediction24.html#cis---end-point-transformation",
    "href": "prediction24.html#cis---end-point-transformation",
    "title": "Prediction Methods for MLE Models",
    "section": "CIs - End Point Transformation",
    "text": "CIs - End Point Transformation\nGenerate upper and lower bounds using either ML or Delta standard errors, such that\n\\[F(X \\widehat{\\beta} - c*s.e.) \\leq F(X \\widehat{\\beta}) \\leq F(X \\widehat{\\beta} + c* s.e.)\\]\n\nestimate the model, generate the linear prediction, and the standard error of the linear prediction using the either ML or Delta.\ngenerate linear boundary predictions, \\(x{\\widehat{\\beta}} \\pm c * \\text{st. err.}\\) where \\(c\\) is a critical value on the normal, eg. \\(z=1.96\\).\ntransform the linear prediction and the upper and lower boundary predictions by \\(F(\\cdot)\\).\nWith ML standard errors, EPT boundaries will obey distributional boundaries (ie, won’t fall outside 0-1 interval for probabilities); the linear end point predictions are symmetric, though they will not be symmetric in nonlinear models.\nWith delta standard errors, bounds may not obey distributional boundaries."
  },
  {
    "objectID": "prediction24.html#simulating-confidence-intervals-i",
    "href": "prediction24.html#simulating-confidence-intervals-i",
    "title": "Prediction Methods for MLE Models",
    "section": "Simulating confidence intervals, I",
    "text": "Simulating confidence intervals, I\n\ndraw a sample with replacement of size \\(\\tilde{N}\\) from the estimation sample.\nestimate the model parameters in that bootstrap sample.\nusing the bootstrap estimates, generate quantities of interest (e.g. \\(x\\widehat{\\beta}\\)) repeat \\(j\\) times.\ncollect all these bootstrap QIs and use either percentiles or standard deviations to measure uncertainty."
  },
  {
    "objectID": "prediction24.html#uncertainty-simulating-confidence-intervals-ii",
    "href": "prediction24.html#uncertainty-simulating-confidence-intervals-ii",
    "title": "Prediction Methods for MLE Models",
    "section": "Uncertainty: Simulating confidence intervals, II",
    "text": "Uncertainty: Simulating confidence intervals, II\n\nestimate the model.\ngenerate a large sample distribution of parameters (e.g. using drawnorm).\ngenerate quantities of interest for the distribution of parameters.\nuse either percentiles or standard deviations of the QI to measure uncertainty."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Maximum Likelihood, Fall 2024",
    "section": "",
    "text": "This is the course website for PLSC 606J, Maximum Likelihood Estimation, Fall 2024. The course is an advanced course in data science focusing on maximum likelihood methods and their applications in political science.\n\nSyllabus\nSlides\nCode\n\n\n\n\n Back to top"
  },
  {
    "objectID": "mlesyllabus24.html#resources",
    "href": "mlesyllabus24.html#resources",
    "title": "MLE Syllabus",
    "section": " Resources",
    "text": "Resources\nThere are lots of good, free  resources online. Here are a few:\n\nModern Statistics with R\nR for Data Science\nR Markdown: The Definitive Guide\nR Graphics Cookbook\nAdvanced R\nR Markdown Cookbook\nData Science:A First Introduction\nThe Big Book of R"
  },
  {
    "objectID": "llfmax.html",
    "href": "llfmax.html",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "The goal here is to walk through the process of writing a likelihood function based on the data we observe, then programming that likelihood function and optimizing it with respect to the data.\n\n\nLet’s recall the motivation for turning to ML. The data we have are not suitable for OLS because our observation of the data is limited. For instance, we observe a binary variable \\([0,1]\\) rather than the underlying continuous probability beneath it. If we could observe and measure that probability, we’d do so and perhaps use OLS to estimate a model. But since we only observe \\([0,1]\\), our observation is limited.\nOLS is poorly suited to model this variable especially if one of our goals is to generate predictions of that underlying probability. That is, one of our goals might be to estimate a measure of the \\(y\\) variable we wish we could measure directly.\nMLE is better suited to this task - it asks us what the data generating process is that produced the observed data, and to build a model appropriate to the limited observation of that \\(y\\) variable.\nTo do so, we need to:\n\ndescribe the observed distribution of \\(y\\)\nconsider what we wish we could measure - this points to the key quantities of interest we want to derive from the model.\ndescribe \\(y\\) according to a probability distribution; write that distribution\nwrite a log-likelihood function appropriate to that probability distribution.\nparameterize the log-likelihood function such that we have a link distribution to map the linear prediction, \\(x\\beta\\), onto the probability space of \\(y\\).\n\n\n\n\nLet’s start with a \\(y\\) variable as follows:\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                y\n                Freq\n              \n        \n        \n        \n                \n                  0\n                  707\n                \n                \n                  1\n                  293\n                \n        \n      \n    \n\n\n\nThe variable, \\(y\\), takes on values of zero and one - it appears binomial; let’s write this in terms of the binomial distribution parameter \\(\\pi\\), so it takes on the value of one with probability \\(\\pi\\) and zero with probability \\(1-\\pi\\).\n\\[ y_i = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{} \\pi_{i}\\\\\n         0, & \\mbox{} 1-\\pi_{i}\n         \\end{array}\n     \\right.\\]\nThe likelihood of a single observation is:\n\\[ \\mathcal{L}(\\pi_i|y) = \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\]\nThis is a statement of the likelihood that any particular value of \\(\\pi\\) generated an observation. We want the likelihood any value of \\(\\pi\\) generated the entire dataset - that is, we want the joint likelihood of all the observations.\n\\[ \\mathcal{L}(\\pi|y) = \\prod_{i=1}^{n} \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\] recalling that a joint probability is the product of individual probabilities.\nLet’s take the log of this likelihood function - the natural log makes computation easier. Even for computers, this reduces the computational intensity especially with respect to extremely small decimals.\n\\[ \\ln \\mathcal{L}(\\pi|y) = \\sum_{i=1}^{n} y_i \\ln(\\pi_i) + (1-y_i) \\ln(1-\\pi_i) \\]\nWe can take this a step further and parameterize \\(\\pi\\) as a function of \\(X\\beta\\) where \\(X\\) is the matrix of predictors and \\(\\beta\\) is the vector of coefficients such that\n\\[ \\pi_i = F({X_i\\beta}) \\]\nSo our estimate of the binomial probability is a function of the linear predictor \\(X\\beta\\). We map \\(X\\beta\\) onto the probability space \\(\\pi\\) using a link function, \\(F\\). The most common link functions are the logistic (logit model), and the standard normal (probit model). Let’s write the logit link:\n\\[ \\pi_i = \\frac{1}{1 + e^{-(X_i\\beta)}} \\]\nand now let’s write this all in the log-likelihood function:\n\\[ \\ln \\mathcal{L}(\\beta|y) = \\sum_{i=1}^{n} y_i \\ln(\\frac{1}{1 + e^{-(X_i\\beta)}}) + (1-y_i) \\ln(1-\\frac{1}{1 + e^{-(X_i\\beta)}}) \\]\nThis is the logit log-likelihood function for a binary \\(y\\) variable.\n\n\n\nHow do we solve this function for the data \\(y\\)? Put differently, what is the value of \\(\\pi\\) that most likely generated the data, \\(y\\)? There are a few ways to do this - one instructive method is to use a grid search, where we calculate the log-likelihood for a range of values of \\(\\pi\\) and find the value that maximizes the log-likelihood. We can repeat this to the desired level of specificity, progressively narrowing the grid. In this example, we’ll just do one search out to 3 decimal places. The steps are these:\n\ngenerate the range of candidate values of our parameter \\(\\pi\\) to plug into the log-likelihood function.\nplug each value into the log-likelihood function to compute the log-likelihood for that value.\nidentify which value of \\(\\pi\\) maximizes the log-likelihood.\n\nHere’s code to do this:\n\n\ncode\n# generate a vector of values of pi \n\npi_trials &lt;- seq(0, 1, by = 0.001)\n\n# write the log-likelihood function\n\nlog_likelihood &lt;- function(pi_trials, y) {\n  sum(y * log(pi_trials) + (1 - y) * log(1 - pi_trials))\n}\n\n# Calculate log-likelihood for each pi value\n\nll_values &lt;- sapply(pi_trials, log_likelihood, y = y)\n\n# Find the pi value that maximizes the log-likelihood\n\npi_hat &lt;- pi_trials[which.max(ll_values)]\nprint(pi_hat)\n\n\n[1] 0.293\n\n\nYou’ll note we recover the sample mean of \\(y\\). Let’s plot the log-likelihood against the values of \\(\\pi\\) to visualize the maximum.\n\n\ncode\n# Plot log-likelihood against pi\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(data.frame(pi = pi_trials, ll = ll_values), \"line\", hcaes(x = pi, y = ll)) %&gt;%\n  hc_title(text = \"Grid search - maximizing the log-likelihood\") %&gt;%\n  hc_xAxis(title = list(text = \"Pi\"), \n           plotLines = list(\n    list(color =\"red\", value = pi_hat ))) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_legend(enabled = FALSE) %&gt;%\n  hc_colors(bucolors)\n\n\n\n\n\n\n\n\n\nGrid searches are not sufficient for multivariate models, especially where we also need measures of uncertainty. Here, we turn to numerical optimization.\nNumerical optimization is a field aimed at finding “best” outcomes or answers depending on some set of criteria - often, the “best” is the maximum or minimum of some function - that’s the case in maximum likelihood.\n\n\nThere are lots of ways to optimize a function - in statistical software, most of them derive from Newton’s iterative method. A very common application of this is the Newton-Raphson method. Newton-Raphson is an iterative process that starts with an initial guess for the parameter \\(\\pi\\) (or the vector of unknowns, \\(\\beta\\)) and updates it in the direction of the maximum until it converges to the maximum. At each step, it computes the first and second derivatives of the likelihood function, then uses these to update the parameter estimate(s). The update rule is:\n\\[ \\pi_{new} = \\pi_{old} - H(\\pi)^{-1}*g(\\pi) \\]\nSo the new estimate of the parameter is the old estimate minus the first derivative (gradient) of the log-likelihood function pre-multiplied by the Hessian (second derivative) of the log-likelihood function. Here, \\(g(\\pi)\\) is the gradient of the log-likelihood function and \\(H(\\pi)\\) is the Hessian of the log-likelihood function. The algorithm iterates this process until the change in \\(\\pi\\) is sufficiently small, below a specified threshold. Since \\(g\\) and \\(H\\) are matrices, you should see how this straightforwardly applied to a multivariate regression.\nLet’s write the Newton-Raphson algorithm ourselves - take a look at the code chunk below. You’ll see it produces the same result as the grid search; the sample frequency of \\(y\\).\n\n\n\n\n\ncode\n# use same data generated above; same LLF as above, aiming to recover pi\n\n#write the gradient - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(pi, y) {\n  sum(y / pi - (1 - y) / (1 - pi))\n}\n\n#write the Hessian - matrix of partial second derivatives of the log-likelihood function.\n\nhessian &lt;- function(pi, y) {\n  sum(-y / pi^2 - (1 - y) / (1 - pi)^2)\n}\n\n#declare a starting value for pi\n\npi &lt;- 0.5\n\n#set convergence criteria - when new pi is within 1e-6 of old pi, stop iterating\n\ntol &lt;- 1e-6\n\n#initialize iteration counter, set maximum iterations; how many times will we do this before we stop if we don't converge prior? \n\niter &lt;- 0\nmax_iter &lt;- 100\n\n#iterate the Newton-Raphson algorithm\n\nfor (iter in 1:max_iter) {\n  # Compute gradient  \n  grad &lt;- gradient(pi, y)\n  \n  # Compute Hessian\n  hess &lt;- hessian(pi, y)\n  \n  # Update pi\n  pi_new &lt;- pi - (grad / hess)\n  \n  pi &lt;- pi_new\n  \n  # Check for convergence\n  if (abs(pi_new - pi) &lt; tol) {\n    break\n  }\n  \n  iter &lt;- iter + 1\n}\n\n#after convergence, print the result\n\nprint(pi)\n\n\n[1] 0.293\n\n\nSo what’s gone on here? We have 1000 observations of a binary variable; maximum likelihood is answering the question “what is the value of the parameter \\(\\pi\\) that makes the observed data most likely?” To get that answer, we have tried candidate values of \\(\\pi\\) to see which one maximizes the log-likelihood function.\nWe’ve maximized the function two ways - using a grid search process, and using the Newton-Raphson algorithm. The latter is more efficient and is the basis for most optimization algorithms in statistical software, and easy to adapt to estimate multiple parameters.\nHere’s one more approach also using the Newton-Raphson method. It calls the maxLik package to do the optimization instead of our having to write the algorithm ourselves.\n\n\ncode\nlibrary(maxLik)\n# Write the binomial log-likelihood function\nlog_likelihood &lt;- function(pi, y) {\n  sum(y * log(pi) + (1 - y) * log(1 - pi))\n}\n\n#maximize the function using maxLik\n\nm1m &lt;- maxLik(y=y, log_likelihood, start = c(0.5), method = \"NR\") #Newton-Raphson method, starting value of 0.5\n\nsummary(m1m)\n\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 3 iterations\nReturn code 2: successive function values within tolerance limit (tol)\nLog-Likelihood: -604.816 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(&gt; t)    \n[1,]  0.29300    0.01441   20.34  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\nAgain, the value of \\(\\pi\\) that maximizes the log-likelihood is the same as the sample frequency of \\(y\\), which is 0.293.\n\n\n\nLet’s make this more realistic in terms of resembling the models we want to run by adding an intercept term and multiple predictors to the model - the likelihood now has to account for those \\(X\\) variables and maximize with respect to them.\n\ngenerate some data on \\(y\\), and \\(X\\)\nwrite the likelihood\nwrite the Newton-Raphson algorithm\napply to the data\n\n\n\ncode\n# Generate binary y, and two X variables for regression; coefficients are -1, 0.5, -0.5\n\nset.seed(8675309)\nn &lt;- 10000\nX &lt;- matrix(rnorm(n * 2), ncol = 2)\ntrue_beta &lt;- c(-1, 0.5, -0.5)\nz &lt;- cbind(1, X) %*% true_beta\nprob &lt;- 1 / (1 + exp(-z))\ny &lt;- rbinom(n, 1, prob)\n\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson iterative algorithm\n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # Add intercept term to X if not already present\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#write the gradient function - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- plogis(X %*% beta)\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  k &lt;- ncol(X)\n  \n# set starting values for k columns of data (betas)\n  beta &lt;- rep(0, k) \n  \n# Store gradients and log-likelihoods for analysis\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n# iterate NR \n  for (iter in 1:max_iter) {\n    # compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # compute Hessian\n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # check for convergence\n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n    \n    # compute standard errors for this model by taking the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  }\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#html table comparing glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\", caption=\"Comparing GLM and Dave Estimates\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  column_spec(3, border_right = T) %&gt;%\n  column_spec(4, border_right = T) %&gt;%\n  collapse_rows(columns = 1:3, valign = \"top\") \n\n\n\n\nTable 1: Comparing GLM and Dave Estimates\n\n\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-0.9531826\n-0.9531826\n0.0236756\n0.0236748\n\n\nX1\n0.4938831\n0.4938831\n0.0238303\n0.0238295\n\n\nX2\n-0.4719703\n-0.4719703\n0.0240294\n0.0240286\n\n\n\n\n\n\n\n\n\n\n\nTable 1 compares the coefficients and standard errors from the Newton-Raphson algorithm to the coefficients and standard errors from the glm function. The estimates are the same, the standard errors very close.\nAlright, lots of fun here. Now, let’s use the program above to estimate a logit model using the democratic peace data.\n\n\n\nTable 2 compares the coefficients and standard errors from the program we wrote to those produced by the glm function. The regression here is a logit model predicting the onset of a militarized dispute as a function of the logged capabilities ratio, an indicator of whether the pair of states share a border, and the lowest democracy (polity) score in the pair.\n\n\ncode\ndp &lt;- read.csv(\"/users/dave/documents/teaching/606J-mle/2022/exercises/ex1/dp.csv\", header=TRUE)\n\n# for the log_likelihood function below, define y as the variable \"dispute\" in the dp data frame, and x1, x2, and x3 as the variables \"lncaprat\", \"border\", and \"deml\" in the dp data frame, respectively.\ndp$lncaprat &lt;- log(dp$caprat)\nX &lt;- as.matrix(dp[, c(\"lncaprat\", \"border\", \"deml\")])\ny &lt;- as.vector(dp$dispute)\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson \n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # check for constant; add one if necessary\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#write the gradient function -  partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- 1 / (1 + exp(-X %*% beta))\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  p &lt;- ncol(X)\n  \n  # set starting values for beta\n  beta &lt;- rep(0, p)\n  \n  # Store gradients and log-likelihoods\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n  for (iter in 1:max_iter) {\n    # Compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # Compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # Compute Hessian - 2nd partial derivatives \n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # Update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # Check for convergence - end if change in beta is less than tolerance value \n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n  }\n\n#compute standard errors for this model; take the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n# logit$coefficients\n# logit$st.errors\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#compare glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\", caption=\"Comparing GLM and Dave Estimates, Democratic Peace Model\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:3, valign = \"top\")\n\n\n\n\nTable 2: Comparing GLM and Dave Estimates, Democratic Peace Model\n\n\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-3.2407604\n-3.2407605\n0.1034544\n0.1034630\n\n\nXlncaprat\n-0.2028768\n-0.2028768\n0.0237847\n0.0237860\n\n\nXborder\n0.7827135\n0.7827135\n0.0852298\n0.0852337\n\n\nXdeml\n-0.0783467\n-0.0783467\n0.0068065\n0.0068078\n\n\n\n\n\n\n\n\n\n\n\nRecapping what we’ve done:\n\nWe wrote a log-likelihood function for a logistic regression model with multiple predictors.\nTo maximize the log-likelihood, we implemented a Newton-Raphson algorithm in the following steps:\n\nDefined the gradient and Hessian functions. The gradient is the vector of first partial derivatives of the log-likelihood, and the Hessian is the matrix of second partial derivatives.\nInitialized the beta coefficients and stored gradients and log-likelihoods.\nIterated the algorithm to update beta values.\nChecked for convergence based on the change in beta.\n\nOnce the model converged, we computed standard errors for the estimated coefficients by taking the square root of the main diagonal elements of the inverse negative Hessian.\n\n\n\n\nHere, you can see how the gradients change over each iteration.\n\n\ncode\n#plot gradient convergence\ngradients &lt;- do.call(cbind, logit$gradient_history)\ngradients &lt;- as.data.frame(gradients) %&gt;%\n  mutate(coef=c(\"Intercept\", \"lncaprat\", \"border\", \"deml\")) \n\ngradients &lt;- gradients %&gt;% \n  pivot_longer(-coef, names_to = \"iteration\", values_to = \"value\")\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(gradients, \"line\", hcaes(x=iteration, y=value, group=coef, color=coef)) %&gt;%\n  hc_title(text = \"Gradient Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Gradient\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\n\n\nAnd here, we can see how the log-likelihood changes over iterations:\n\n\ncode\n#plot ll convergence\n\nlls &lt;- as.data.frame(logit$ll_history) %&gt;%\n  mutate(iteration=1:nrow(.))\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(lls, \"line\", hcaes(x=iteration, y=`logit$ll_history`)) %&gt;%\n  hc_title(text = \"Log-Likelihood Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\n\n\nHere are two ways to recover the final log-likelihood from the glm model. The first uses the logLik function from the stats package. The second calculates the log-likelihood by hand - generating the predictions and plugging those into the log-likelihood function, then summing.\n\\[ LL = \\sum_{i=1}^{n} y_i \\ln(p) + (1-y_i) \\ln(1-p) \\]\n\n\ncode\n# use the logLik function from the stats package  \n\nllest &lt;- logLik(glm_fit)\n\n# or calculate it by hand - generate the predictions and plug those into the log-likelihood function, then sum.\n\npred &lt;- predict(glm_fit, type = \"response\")\nllestbyhand &lt;- sum(dp$dispute * log(pred) + (1 - dp$dispute) * log(1 - pred))\n\n\ndata.frame(llest, llestbyhand) %&gt;% \n  kable(\"html\", caption=\"Recovering the Log-Likelihood\" ) %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:2, valign = \"top\")\n\n\n\n\nRecovering the Log-Likelihood\n\n\nllest\nllestbyhand\n\n\n\n\n-3565.219\n-3565.219"
  },
  {
    "objectID": "llfmax.html#motivating-likelihood",
    "href": "llfmax.html#motivating-likelihood",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "Let’s recall the motivation for turning to ML. The data we have are not suitable for OLS because our observation of the data is limited. For instance, we observe a binary variable \\([0,1]\\) rather than the underlying continuous probability beneath it. If we could observe and measure that probability, we’d do so and perhaps use OLS to estimate a model. But since we only observe \\([0,1]\\), our observation is limited.\nOLS is poorly suited to model this variable especially if one of our goals is to generate predictions of that underlying probability. That is, one of our goals might be to estimate a measure of the \\(y\\) variable we wish we could measure directly.\nMLE is better suited to this task - it asks us what the data generating process is that produced the observed data, and to build a model appropriate to the limited observation of that \\(y\\) variable.\nTo do so, we need to:\n\ndescribe the observed distribution of \\(y\\)\nconsider what we wish we could measure - this points to the key quantities of interest we want to derive from the model.\ndescribe \\(y\\) according to a probability distribution; write that distribution\nwrite a log-likelihood function appropriate to that probability distribution.\nparameterize the log-likelihood function such that we have a link distribution to map the linear prediction, \\(x\\beta\\), onto the probability space of \\(y\\)."
  },
  {
    "objectID": "llfmax.html#binary-y-variable",
    "href": "llfmax.html#binary-y-variable",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "Let’s start with a \\(y\\) variable as follows:\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                y\n                Freq\n              \n        \n        \n        \n                \n                  0\n                  707\n                \n                \n                  1\n                  293\n                \n        \n      \n    \n\n\n\nThe variable, \\(y\\), takes on values of zero and one - it appears binomial; let’s write this in terms of the binomial distribution parameter \\(\\pi\\), so it takes on the value of one with probability \\(\\pi\\) and zero with probability \\(1-\\pi\\).\n\\[ y_i = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{} \\pi_{i}\\\\\n         0, & \\mbox{} 1-\\pi_{i}\n         \\end{array}\n     \\right.\\]\nThe likelihood of a single observation is:\n\\[ \\mathcal{L}(\\pi_i|y) = \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\]\nThis is a statement of the likelihood that any particular value of \\(\\pi\\) generated an observation. We want the likelihood any value of \\(\\pi\\) generated the entire dataset - that is, we want the joint likelihood of all the observations.\n\\[ \\mathcal{L}(\\pi|y) = \\prod_{i=1}^{n} \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\] recalling that a joint probability is the product of individual probabilities.\nLet’s take the log of this likelihood function - the natural log makes computation easier. Even for computers, this reduces the computational intensity especially with respect to extremely small decimals.\n\\[ \\ln \\mathcal{L}(\\pi|y) = \\sum_{i=1}^{n} y_i \\ln(\\pi_i) + (1-y_i) \\ln(1-\\pi_i) \\]\nWe can take this a step further and parameterize \\(\\pi\\) as a function of \\(X\\beta\\) where \\(X\\) is the matrix of predictors and \\(\\beta\\) is the vector of coefficients such that\n\\[ \\pi_i = F({X_i\\beta}) \\]\nSo our estimate of the binomial probability is a function of the linear predictor \\(X\\beta\\). We map \\(X\\beta\\) onto the probability space \\(\\pi\\) using a link function, \\(F\\). The most common link functions are the logistic (logit model), and the standard normal (probit model). Let’s write the logit link:\n\\[ \\pi_i = \\frac{1}{1 + e^{-(X_i\\beta)}} \\]\nand now let’s write this all in the log-likelihood function:\n\\[ \\ln \\mathcal{L}(\\beta|y) = \\sum_{i=1}^{n} y_i \\ln(\\frac{1}{1 + e^{-(X_i\\beta)}}) + (1-y_i) \\ln(1-\\frac{1}{1 + e^{-(X_i\\beta)}}) \\]\nThis is the logit log-likelihood function for a binary \\(y\\) variable."
  },
  {
    "objectID": "llfmax.html#maximizing-the-likelihood",
    "href": "llfmax.html#maximizing-the-likelihood",
    "title": "Writing the Likelihood",
    "section": "",
    "text": "How do we solve this function for the data \\(y\\)? Put differently, what is the value of \\(\\pi\\) that most likely generated the data, \\(y\\)? There are a few ways to do this - one instructive method is to use a grid search, where we calculate the log-likelihood for a range of values of \\(\\pi\\) and find the value that maximizes the log-likelihood. We can repeat this to the desired level of specificity, progressively narrowing the grid. In this example, we’ll just do one search out to 3 decimal places. The steps are these:\n\ngenerate the range of candidate values of our parameter \\(\\pi\\) to plug into the log-likelihood function.\nplug each value into the log-likelihood function to compute the log-likelihood for that value.\nidentify which value of \\(\\pi\\) maximizes the log-likelihood.\n\nHere’s code to do this:\n\n\ncode\n# generate a vector of values of pi \n\npi_trials &lt;- seq(0, 1, by = 0.001)\n\n# write the log-likelihood function\n\nlog_likelihood &lt;- function(pi_trials, y) {\n  sum(y * log(pi_trials) + (1 - y) * log(1 - pi_trials))\n}\n\n# Calculate log-likelihood for each pi value\n\nll_values &lt;- sapply(pi_trials, log_likelihood, y = y)\n\n# Find the pi value that maximizes the log-likelihood\n\npi_hat &lt;- pi_trials[which.max(ll_values)]\nprint(pi_hat)\n\n\n[1] 0.293\n\n\nYou’ll note we recover the sample mean of \\(y\\). Let’s plot the log-likelihood against the values of \\(\\pi\\) to visualize the maximum.\n\n\ncode\n# Plot log-likelihood against pi\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(data.frame(pi = pi_trials, ll = ll_values), \"line\", hcaes(x = pi, y = ll)) %&gt;%\n  hc_title(text = \"What value of Pi maximizes the log-likelihood?\") %&gt;%\n  hc_xAxis(title = list(text = \"Pi\"), \n           plotLines = list(\n    list(color =\"red\", value = pi_hat ))) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_legend(enabled = FALSE) %&gt;%\n  hc_colors(bucolors)"
  },
  {
    "objectID": "llfmax.html#optimization",
    "href": "llfmax.html#optimization",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "Grid searches are not sufficient for multivariate models, especially where we also need measures of uncertainty. Here, we turn to numerical optimization.\nNumerical optimization is a field aimed at finding “best” outcomes or answers depending on some set of criteria - often, the “best” is the maximum or minimum of some function - that’s the case in maximum likelihood.\n\n\nThere are lots of ways to optimize a function - in statistical software, most of them derive from Newton’s iterative method. A very common application of this is the Newton-Raphson method. Newton-Raphson is an iterative process that starts with an initial guess for the parameter \\(\\pi\\) (or the vector of unknowns, \\(\\beta\\)) and updates it in the direction of the maximum until it converges to the maximum. At each step, it computes the first and second derivatives of the likelihood function, then uses these to update the parameter estimate(s). The update rule is:\n\\[ \\pi_{new} = \\pi_{old} - H(\\pi)^{-1}*g(\\pi) \\]\nSo the new estimate of the parameter is the old estimate minus the first derivative (gradient) of the log-likelihood function pre-multiplied by the Hessian (second derivative) of the log-likelihood function. Here, \\(g(\\pi)\\) is the gradient of the log-likelihood function and \\(H(\\pi)\\) is the Hessian of the log-likelihood function. The algorithm iterates this process until the change in \\(\\pi\\) is sufficiently small, below a specified threshold. Since \\(g\\) and \\(H\\) are matrices, you should see how this straightforwardly applied to a multivariate regression.\nLet’s write the Newton-Raphson algorithm ourselves - take a look at the code chunk below. You’ll see it produces the same result as the grid search; the sample frequency of \\(y\\).\n\n\n\n\n\ncode\n# use same data generated above; same LLF as above, aiming to recover pi\n\n#write the gradient - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(pi, y) {\n  sum(y / pi - (1 - y) / (1 - pi))\n}\n\n#write the Hessian - matrix of partial second derivatives of the log-likelihood function.\n\nhessian &lt;- function(pi, y) {\n  sum(-y / pi^2 - (1 - y) / (1 - pi)^2)\n}\n\n#declare a starting value for pi\n\npi &lt;- 0.5\n\n#set convergence criteria - when new pi is within 1e-6 of old pi, stop iterating\n\ntol &lt;- 1e-6\n\n#initialize iteration counter, set maximum iterations; how many times will we do this before we stop if we don't converge prior? \n\niter &lt;- 0\nmax_iter &lt;- 100\n\n#iterate the Newton-Raphson algorithm\n\nfor (iter in 1:max_iter) {\n  # Compute gradient  \n  grad &lt;- gradient(pi, y)\n  \n  # Compute Hessian\n  hess &lt;- hessian(pi, y)\n  \n  # Update pi\n  pi_new &lt;- pi - (grad / hess)\n  \n  pi &lt;- pi_new\n  \n  # Check for convergence\n  if (abs(pi_new - pi) &lt; tol) {\n    break\n  }\n  \n  iter &lt;- iter + 1\n}\n\n#after convergence, print the result\n\nprint(pi)\n\n\n[1] 0.293\n\n\nSo what’s gone on here? We have 1000 observations of a binary variable; maximum likelihood is answering the question “what is the value of the parameter \\(\\pi\\) that makes the observed data most likely?” To get that answer, we have tried candidate values of \\(\\pi\\) to see which one maximizes the log-likelihood function.\nWe’ve maximized the function two ways - using a grid search process, and using the Newton-Raphson algorithm. The latter is more efficient and is the basis for most optimization algorithms in statistical software, and easy to adapt to estimate multiple parameters.\nHere’s one more approach also using the Newton-Raphson method. It calls the maxLik package to do the optimization instead of our having to write the algorithm ourselves.\n\n\ncode\nlibrary(maxLik)\n# Write the binomial log-likelihood function\nlog_likelihood &lt;- function(pi, y) {\n  sum(y * log(pi) + (1 - y) * log(1 - pi))\n}\n\n#maximize the function using maxLik\n\nm1m &lt;- maxLik(y=y, log_likelihood, start = c(0.5), method = \"NR\") #Newton-Raphson method, starting value of 0.5\n\nsummary(m1m)\n\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 3 iterations\nReturn code 2: successive function values within tolerance limit (tol)\nLog-Likelihood: -604.816 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(&gt; t)    \n[1,]  0.29300    0.01441   20.34  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\nAgain, the value of \\(\\pi\\) that maximizes the log-likelihood is the same as the sample frequency of \\(y\\), which is 0.293.\n\n\n\nLet’s make this more realistic in terms of resembling the models we want to run by adding an intercept term and multiple predictors to the model - the likelihood now has to account for those \\(X\\) variables and maximize with respect to them.\n\ngenerate some data on \\(y\\), and \\(X\\)\nwrite the likelihood\nwrite the Newton-Raphson algorithm\napply to the data\n\n\n\ncode\n# Generate binary y, and two X variables for regression; coefficients are -1, 0.5, -0.5\n\nset.seed(8675309)\nn &lt;- 10000\nX &lt;- matrix(rnorm(n * 2), ncol = 2)\ntrue_beta &lt;- c(-1, 0.5, -0.5)\nz &lt;- cbind(1, X) %*% true_beta\nprob &lt;- 1 / (1 + exp(-z))\ny &lt;- rbinom(n, 1, prob)\n\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson iterative algorithm\n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # Add intercept term to X if not already present\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#write the gradient function - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- plogis(X %*% beta)\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  k &lt;- ncol(X)\n  \n# set starting values for k columns of data (betas)\n  beta &lt;- rep(0, k) \n  \n# Store gradients and log-likelihoods for analysis\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n# iterate NR \n  for (iter in 1:max_iter) {\n    # compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # compute Hessian\n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # check for convergence\n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n    \n    # compute standard errors for this model by taking the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  }\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#html table comparing glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\", caption=\"Comparing GLM and Dave Estimates\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  column_spec(3, border_right = T) %&gt;%\n  column_spec(4, border_right = T) %&gt;%\n  collapse_rows(columns = 1:3, valign = \"top\") \n\n\n\n\nTable 1: Comparing GLM and Dave Estimates\n\n\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-0.9531826\n-0.9531826\n0.0236756\n0.0236748\n\n\nX1\n0.4938831\n0.4938831\n0.0238303\n0.0238295\n\n\nX2\n-0.4719703\n-0.4719703\n0.0240294\n0.0240286\n\n\n\n\n\n\n\n\n\n\n\nTable 1 compares the coefficients and standard errors from the Newton-Raphson algorithm to the coefficients and standard errors from the glm function. The estimates are the same, the standard errors very close.\nAlright, lots of fun here. Now, let’s use the program above to estimate a logit model using the democratic peace data.\n\n\n\nTable 2 compares the coefficients and standard errors from the program we wrote to those produced by the glm function. The regression here is a logit model predicting the onset of a militarized dispute as a function of the logged capabilities ratio, an indicator of whether the pair of states share a border, and the lowest democracy (polity) score in the pair.\n\n\ncode\ndp &lt;- read.csv(\"/users/dave/documents/teaching/606J-mle/2022/exercises/ex1/dp.csv\", header=TRUE)\n\n# for the log_likelihood function below, define y as the variable \"dispute\" in the dp data frame, and x1, x2, and x3 as the variables \"lncaprat\", \"border\", and \"deml\" in the dp data frame, respectively.\ndp$lncaprat &lt;- log(dp$caprat)\nX &lt;- as.matrix(dp[, c(\"lncaprat\", \"border\", \"deml\")])\ny &lt;- as.vector(dp$dispute)\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson \n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # check for constant; add one if necessary\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#write the gradient function -  partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- 1 / (1 + exp(-X %*% beta))\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  p &lt;- ncol(X)\n  \n  # set starting values for beta\n  beta &lt;- rep(0, p)\n  \n  # Store gradients and log-likelihoods\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n  for (iter in 1:max_iter) {\n    # Compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # Compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # Compute Hessian - 2nd partial derivatives \n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # Update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # Check for convergence - end if change in beta is less than tolerance value \n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n  }\n\n#compute standard errors for this model; take the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n# logit$coefficients\n# logit$st.errors\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#compare glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\", caption=\"Comparing GLM and Dave Estimates, Democratic Peace Model\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:3, valign = \"top\")\n\n\n\n\nTable 2: Comparing GLM and Dave Estimates, Democratic Peace Model\n\n\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-3.2407604\n-3.2407605\n0.1034544\n0.1034630\n\n\nXlncaprat\n-0.2028768\n-0.2028768\n0.0237847\n0.0237860\n\n\nXborder\n0.7827135\n0.7827135\n0.0852298\n0.0852337\n\n\nXdeml\n-0.0783467\n-0.0783467\n0.0068065\n0.0068078\n\n\n\n\n\n\n\n\n\n\n\nRecapping what we’ve done:\n\nWe wrote a log-likelihood function for a logistic regression model with multiple predictors.\nTo maximize the log-likelihood, we implemented a Newton-Raphson algorithm in the following steps:\n\nDefined the gradient and Hessian functions. The gradient is the vector of first partial derivatives of the log-likelihood, and the Hessian is the matrix of second partial derivatives.\nInitialized the beta coefficients and stored gradients and log-likelihoods.\nIterated the algorithm to update beta values.\nChecked for convergence based on the change in beta.\n\nOnce the model converged, we computed standard errors for the estimated coefficients by taking the square root of the main diagonal elements of the inverse negative Hessian.\n\n\n\n\nHere, you can see how the gradients change over each iteration.\n\n\ncode\n#plot gradient convergence\ngradients &lt;- do.call(cbind, logit$gradient_history)\ngradients &lt;- as.data.frame(gradients) %&gt;%\n  mutate(coef=c(\"Intercept\", \"lncaprat\", \"border\", \"deml\")) \n\ngradients &lt;- gradients %&gt;% \n  pivot_longer(-coef, names_to = \"iteration\", values_to = \"value\")\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(gradients, \"line\", hcaes(x=iteration, y=value, group=coef, color=coef)) %&gt;%\n  hc_title(text = \"Gradient Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Gradient\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\n\n\nAnd here, we can see how the log-likelihood changes over iterations:\n\n\ncode\n#plot ll convergence\n\nlls &lt;- as.data.frame(logit$ll_history) %&gt;%\n  mutate(iteration=1:nrow(.))\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(lls, \"line\", hcaes(x=iteration, y=`logit$ll_history`)) %&gt;%\n  hc_title(text = \"Log-Likelihood Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\n\n\nHere are two ways to recover the final log-likelihood from the glm model. The first uses the logLik function from the stats package. The second calculates the log-likelihood by hand - generating the predictions and plugging those into the log-likelihood function, then summing.\n\\[ LL = \\sum_{i=1}^{n} y_i \\ln(p) + (1-y_i) \\ln(1-p) \\]\n\n\ncode\n# use the logLik function from the stats package  \n\nllest &lt;- logLik(glm_fit)\n\n# or calculate it by hand - generate the predictions and plug those into the log-likelihood function, then sum.\n\npred &lt;- predict(glm_fit, type = \"response\")\nllestbyhand &lt;- sum(dp$dispute * log(pred) + (1 - dp$dispute) * log(1 - pred))\n\n\ndata.frame(llest, llestbyhand) %&gt;% \n  kable(\"html\", caption=\"Recovering the Log-Likelihood\" ) %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:2, valign = \"top\")\n\n\n\n\nRecovering the Log-Likelihood\n\n\nllest\nllestbyhand\n\n\n\n\n-3565.219\n-3565.219"
  },
  {
    "objectID": "llfmax.html#maximizing-the-likelihood---grid-search",
    "href": "llfmax.html#maximizing-the-likelihood---grid-search",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "How do we solve this function for the data \\(y\\)? Put differently, what is the value of \\(\\pi\\) that most likely generated the data, \\(y\\)? There are a few ways to do this - one instructive method is to use a grid search, where we calculate the log-likelihood for a range of values of \\(\\pi\\) and find the value that maximizes the log-likelihood. We can repeat this to the desired level of specificity, progressively narrowing the grid. In this example, we’ll just do one search out to 3 decimal places. The steps are these:\n\ngenerate the range of candidate values of our parameter \\(\\pi\\) to plug into the log-likelihood function.\nplug each value into the log-likelihood function to compute the log-likelihood for that value.\nidentify which value of \\(\\pi\\) maximizes the log-likelihood.\n\nHere’s code to do this:\n\n\ncode\n# generate a vector of values of pi \n\npi_trials &lt;- seq(0, 1, by = 0.001)\n\n# write the log-likelihood function\n\nlog_likelihood &lt;- function(pi_trials, y) {\n  sum(y * log(pi_trials) + (1 - y) * log(1 - pi_trials))\n}\n\n# Calculate log-likelihood for each pi value\n\nll_values &lt;- sapply(pi_trials, log_likelihood, y = y)\n\n# Find the pi value that maximizes the log-likelihood\n\npi_hat &lt;- pi_trials[which.max(ll_values)]\nprint(pi_hat)\n\n\n[1] 0.293\n\n\nYou’ll note we recover the sample mean of \\(y\\). Let’s plot the log-likelihood against the values of \\(\\pi\\) to visualize the maximum.\n\n\ncode\n# Plot log-likelihood against pi\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(data.frame(pi = pi_trials, ll = ll_values), \"line\", hcaes(x = pi, y = ll)) %&gt;%\n  hc_title(text = \"Grid search - maximizing the log-likelihood\") %&gt;%\n  hc_xAxis(title = list(text = \"Pi\"), \n           plotLines = list(\n    list(color =\"red\", value = pi_hat ))) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_legend(enabled = FALSE) %&gt;%\n  hc_colors(bucolors)"
  },
  {
    "objectID": "likelihood24.html",
    "href": "likelihood24.html",
    "title": "Likelihood",
    "section": "",
    "text": "Why do we turn to maximum likelihood instead of OLS, especially given the simplicity and robustness of OLS?\nOur data can’t meet the OLS assumptions; \\(y\\) is often limited* such that we observe \\(y\\) but really want to measure \\(y^*\\).\n\\(y^*\\) is often a continuous (unlimited) variable we wish we could measure - if we could, we’d use OLS to estimate its correlates.\nOLS asks us to make the data satisfy the model. MLE asks us to build a model based on the data. Since our data often don’t satisfy the OLS assumptions, MLE offers a flexible alternative.\n\n\n\n\n\nML asks us to think of the data as given and to imagine the model that might best represent the Data Generating Process.\n\nIn OLS, the model is fixed - the assumptions about \\(\\epsilon\\) are given.\nIn ML, the data are fixed - we construct a model that reflects the nature of the data.\nWhat we assume about the unobservables is informed by what we know about the observables, the \\(y\\) variable.\nA useful way to describe \\(y\\) is to characterize its observed or empirical distribution.\nOnce we know the distribution of \\(y\\), we can begin building a model based on that distribution.\n\n\n\n\n\nWe are limited in what we can observe and/or measure in \\(y\\). E.g., we observe an individual voting or not; we cannot observe the chances an individual votes.\n\nThe \\(y\\) variable has both observed and latent qualities; label the latent variable \\(\\widetilde{y}\\).\nOften, we are more interested in this latent variable. We are more interested in the latent chance of voting than the observed behavior.\n\\(\\widetilde{y}\\) is the variable we wish we could measure.\nThe latent and observed variables are often distributed differently. Observed voting \\(y=(0,1)\\); latent variable \\(Pr(y=1)\\).\nLinking \\(X\\) variables to the latent \\(\\widetilde{y}\\) requires rescaling; this is often because changes in \\(\\widetilde{y}\\) given \\(X\\) are nonlinear and in different units. In the example above, \\(y\\) is a binary realizing of voting, so is binomial. \\(\\widetilde{y}\\) is the probability of voting, so is continuous, bounded between 0 and 1.\nBesides, \\(\\widetilde{y}\\) is unobserved - so we have to generate it from the model.\nTo link \\(X\\) with \\(\\widetilde{y}\\) or \\(y\\), we assume their relationship follows some distribution; we call this the link distribution.\n\n\n\n\n\nOur data are often limited, and not suitable for OLS. OLS asks us to transform data to make it meet the model assumptions (e.g., Generalized Least Squares); MLE builds the model based on the data."
  },
  {
    "objectID": "likelihood24.html#ml-ols",
    "href": "likelihood24.html#ml-ols",
    "title": "Likelihood",
    "section": "",
    "text": "Why do we turn to maximum likelihood instead of OLS, especially given the simplicity and robustness of OLS?\nOur data can’t meet the OLS assumptions; \\(y\\) is often limited* such that we observe \\(y\\) but really want to measure \\(y^*\\).\n\\(y^*\\) is often a continuous (unlimited) variable we wish we could measure - if we could, we’d use OLS to estimate its correlates.\nOLS asks us to make the data satisfy the model. MLE asks us to build a model based on the data. Since our data often don’t satisfy the OLS assumptions, MLE offers a flexible alternative."
  },
  {
    "objectID": "likelihood24.html#ml",
    "href": "likelihood24.html#ml",
    "title": "Likelihood",
    "section": "",
    "text": "ML asks us to think of the data as given and to imagine the model that might best represent the Data Generating Process.\n\nIn OLS, the model is fixed - the assumptions about \\(\\epsilon\\) are given.\nIn ML, the data are fixed - we construct a model that reflects the nature of the data.\nWhat we assume about the unobservables is informed by what we know about the observables, the \\(y\\) variable.\nA useful way to describe \\(y\\) is to characterize its observed or empirical distribution.\nOnce we know the distribution of \\(y\\), we can begin building a model based on that distribution."
  },
  {
    "objectID": "likelihood24.html#limited-dependent-variables",
    "href": "likelihood24.html#limited-dependent-variables",
    "title": "Likelihood",
    "section": "",
    "text": "We are limited in what we can observe and/or measure in \\(y\\). E.g., we observe an individual voting or not; we cannot observe the chances an individual votes.\n\nThe \\(y\\) variable has both observed and latent qualities; label the latent variable \\(\\widetilde{y}\\).\nOften, we are more interested in this latent variable. We are more interested in the latent chance of voting than the observed behavior.\n\\(\\widetilde{y}\\) is the variable we wish we could measure.\nThe latent and observed variables are often distributed differently. Observed voting \\(y=(0,1)\\); latent variable \\(Pr(y=1)\\).\nLinking \\(X\\) variables to the latent \\(\\widetilde{y}\\) requires rescaling; this is often because changes in \\(\\widetilde{y}\\) given \\(X\\) are nonlinear and in different units. In the example above, \\(y\\) is a binary realizing of voting, so is binomial. \\(\\widetilde{y}\\) is the probability of voting, so is continuous, bounded between 0 and 1.\nBesides, \\(\\widetilde{y}\\) is unobserved - so we have to generate it from the model.\nTo link \\(X\\) with \\(\\widetilde{y}\\) or \\(y\\), we assume their relationship follows some distribution; we call this the link distribution."
  },
  {
    "objectID": "likelihood24.html#the-big-point",
    "href": "likelihood24.html#the-big-point",
    "title": "Likelihood",
    "section": "",
    "text": "Our data are often limited, and not suitable for OLS. OLS asks us to transform data to make it meet the model assumptions (e.g., Generalized Least Squares); MLE builds the model based on the data."
  },
  {
    "objectID": "likelihood24.html#for-example",
    "href": "likelihood24.html#for-example",
    "title": "Likelihood",
    "section": "For example …",
    "text": "For example …\n\n\\(Y = (0,1)\\), does an individual vote or not. This is binary, discrete, let’s say binomial.\n\\(\\tilde{y}\\) is the latent probability an individual votes. Wish we could measure this, use OLS.\nWrite a function based on \\(Y\\): \\(\\pi^y (1-\\pi)^{(1-y)}\\) , the binomial PDF (without the n-tuple).\n\\(\\pi\\) is given by some \\(X\\) variables we think affect voting - so \\(\\pi = x\\beta\\).\nSo substitute for \\(\\pi\\) …\\((x\\beta)^y (1-x\\beta)^{(1-y)}\\)\nNow, link the linear prediction to \\(\\tilde{Y}\\) - map \\(x\\beta\\) onto the Pr(vote). How about using the Standard Normal CDF \\(\\Phi\\)?\n\n\\(\\Phi(x\\beta)^y  \\Phi(1-x\\beta)^{(1-y)}\\) - this is now the core of the Probit."
  },
  {
    "objectID": "likelihood24.html#likelihood",
    "href": "likelihood24.html#likelihood",
    "title": "Likelihood",
    "section": "Likelihood",
    "text": "Likelihood\nKing (pp. 9, 14) puts it this way:\n\\[Y\\sim f(y|\\theta,\\alpha)\\]\nand\n\\[\\theta = G(X,\\beta)\\]\nso our data, \\(Y\\) has a probability distribution given by parameters \\(\\theta, \\alpha\\), and \\(\\theta\\) is a function of some variables, \\(X\\) and their parameters, \\(\\beta\\).\n\nAll of this comprises the model in King’s lingo, so a basic probability statement appears as:\n\\[Pr(y|\\mathcal{M}) \\equiv Pr(\\mathrm{data|model})\\]\nThis is a conditional probability resting on two conditions:\n\nThe data are random and unknown.\nThe model is known.\n\nUh oh. The model is known? The data aren’t? This works in many probability settings. What is the probability of rolling 3 threes in 6 rolls of a six-sided die? What is the probability you draw an ace from a standard deck of cards? In cases like these, the model is known even before we observe events (data).\n\nIn cases like these, the model’s parameters are known and fixed. In our applications, the model and its parameters are the unknowns, but the events or data are known, fixed, and given.\nSuppose I flip a coin to decide whether I roll a 20-sided die, or a 6-sided die. You do not observe any of this - I only tell you I rolled a 4 - which die did I roll? This is the problem we try to deal with in ML - what is the data generating process that most likely produced the observed data. Unlike the simple die roll, the model isn’t known. Instead, there are many possible models that could have produced the data."
  },
  {
    "objectID": "likelihood24.html#inverse-probability",
    "href": "likelihood24.html#inverse-probability",
    "title": "Likelihood",
    "section": "Inverse Probability",
    "text": "Inverse Probability\nGiven these conditions, the more sensible model would be:\n\\[Pr(\\mathcal{M}|y)\\]\nThis is the inverse probability model - but it requires knowledge (or strong assumptions) regarding an important element of the (unknown) model, \\(\\theta\\). Even Bayes can’t really do this."
  },
  {
    "objectID": "likelihood24.html#likelihood-1",
    "href": "likelihood24.html#likelihood-1",
    "title": "Likelihood",
    "section": "Likelihood",
    "text": "Likelihood\nLikelihood estimates the model, \\(\\mathcal{M}\\) given the data, but assumes that \\(\\theta\\) can take on different values, representing different (competing) hypothetical models.\nThe set of \\(\\theta\\)’s are the competing models or data generating processes that could have produced the observed data set.\n\nKeeping with King’s notation, the likelihood axiom is:\n\\[\\mathcal{L}(\\tilde{\\theta}|y,\\mathcal{M}*)\\equiv L(\\tilde{\\theta}|y) \\] \\[=k(y)Pr(y|\\tilde{\\theta})\\] \\[\\propto Pr(y|\\tilde{\\theta})\\]\nwhere \\(\\tilde{\\theta}\\) represents the hypothetical value of \\(\\theta\\) (rather than its true value).\nThe term \\(k(y)\\) (known as the “constant of proportionality”) is a constant across all the hypothetical values of \\(\\tilde{\\theta}\\) (and thus drops out) but is the key to the likelihood axiom; it represents the functional form through which the data (\\(y\\)) shape \\(\\tilde{\\theta}\\) and thus allow us to estimate the likelihood as a measure of relative (instead of absolute) uncertainty; our uncertainty in this case is relative to the other possible functions of \\(y\\) and the hypothetical values of \\(\\tilde{\\theta}\\).\nAs King (p. 61) puts it , \\(k(y)\\) “measures the relative likelihood of a specified hypothetical model \\(\\tilde{\\beta}\\) producing the data we observed.”\nIt turns out that the likelihood of observing the data is proportional to the probability of observing the data."
  },
  {
    "objectID": "likelihood24.html#take-away-points",
    "href": "likelihood24.html#take-away-points",
    "title": "Likelihood",
    "section": "Take Away Points",
    "text": "Take Away Points\n\nWe want to know the probability (the model) of observing some data; if we find the model parameters with the highest likelihood of generating the observed data, we also know the probability because the two are proportional.\nLikelihoods are always negative; they do not have a scale or specific meaning; they do not transform into probabilities or anything familiar.\nWe find the parameter values that produce the largest likelihood values; those parameters that maximize the likelihood then can be translated into sensible quantities - they can be mapped onto \\(\\tilde{y}\\), giving us a measure of the variable we wish we had.\nIn OLS, we compute parameter estimates that minimize the sum (squared) distance of all the observed points to the predicted points - we minimize the errors in this fashion.\nThe technology of MLE is trial and error - choose some values for \\(\\tilde{\\theta}\\), compute the likelihood, then repeat. Compare all the likelihoods - the values of \\(\\tilde{\\theta}\\) that produced the highest likelihood value are the ones that most likely generated the observed data."
  },
  {
    "objectID": "likelihood24.html#writing-the-likelihood",
    "href": "likelihood24.html#writing-the-likelihood",
    "title": "Likelihood",
    "section": "Writing the Likelihood",
    "text": "Writing the Likelihood\nSince we’ve determined \\(y\\) is normal, write the Normal PDF.\n\\[Pr(Y=y_{i})=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{\\left[\\frac{-(y_{i}-\\mu_{i})^{2}}{2\\sigma^{2}}\\right]}\\]\nWe’re interested in the joint probability of the observations, the probability the data result from a particular Data Generating Process. Assuming the observations in the data are independent of one another, the joint density is equal to the product of the marginal probabilities:\n\\[Pr(A~ \\mathrm{and}~ B)=Pr(A)\\cdot Pr(B)\\]\nso the joint probability of \\(y\\), written in terms of the likelihood, is given by\n\\[\\mathcal{L} (\\mu, \\sigma^{2}| y )= \\prod\\limits_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{\\left[\\frac{-(y_{i}-\\mu_{i})^{2}}{2\\sigma^{2}}\\right]}\\]\nAdding is easier than multiplying; since we can transform the likelihood function by any monotonic form, we can take its natural log to replace the products with summations:\n\\[\\ln \\mathcal{L} (\\mu, \\sigma^{2}|y) = \\ln \\prod\\limits_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{\\left[\\frac{-(y_{i}-\\mu_{i})^{2}}{2\\sigma^{2}}\\right]}\\]\n\\[= \\sum \\ln \\left[\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{\\left[\\frac{-(y_{i}-\\mu_{i})^{2}}{2\\sigma^{2}}\\right]} \\right]\\]\n\\[=\\sum\\left( -\\frac{1}{2}(\\ln(2\\pi))-\\frac{1}{2}(\\ln(\\sigma^{2}))-\\frac{1}{2\\sigma^{2}}\\left[\\sum\\limits_{i=1}^{n}(y_{i}-\\mu)^{2}\\right] \\right)\\]"
  },
  {
    "objectID": "likelihood24.html#the-linear-model",
    "href": "likelihood24.html#the-linear-model",
    "title": "Likelihood",
    "section": "The Linear model",
    "text": "The Linear model\nIt should be pretty evident this is the linear model.\n\nwe started with data that looked normal; continuous, unbounded, infinitely differentiable.\nassuming normality, we wrote a LLF in terms of the distribution parameters \\(\\mu, \\sigma^2\\).\nbut we want \\(\\mu\\) itself to vary with some \\(X\\) variables; \\(y\\) isn’t just characterized by a grand mean, but by a set of of conditional means given by \\(X\\).\nso we need to parameterize the model - write the distribution parameter as a function of some variables.\ngenerally, this is to declare \\(\\theta = F(x\\beta)\\).\nin the linear/normal case, to declare \\(\\mu=F(x\\beta)\\). Of course, \\(F\\) is linear, we just write \\(\\mu=x\\beta\\).\nin the LLF, substitute \\(x\\beta\\) for \\(\\mu\\), and now we’re taking the difference between \\(y\\) and \\(x\\beta\\), squaring those differences, and weighting them by the variance.\n\n\nTo put it all together \\(\\ldots\\)\n\\[\\ln \\mathcal{L}(\\mu, \\sigma^{2}|y) = \\ln \\prod\\limits_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} exp \\left[\\frac{-(y-x\\beta)^{2}}{2\\sigma^{2}}\\right] \\] \\[= \\sum \\ln \\left\\{\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} exp \\left[\\frac{-(y-x\\beta)^{2}}{2\\sigma^{2}}\\right]\\right\\}\\]\n\\[= \\sum\\left(-\\frac{1}{2}(\\ln(2\\pi)) -\\frac{1}{2}(\\ln(\\sigma^{2})) -\\frac{1}{2\\sigma^{2}}\\left[\\sum\\limits_{i=1}^{n}(y-x\\beta)^{2}\\right] \\right)\\]\nDoes this look familiar? A lot like deriving OLS, eh?"
  },
  {
    "objectID": "likelihood24.html#linear-regression-in-ml",
    "href": "likelihood24.html#linear-regression-in-ml",
    "title": "Likelihood",
    "section": "Linear regression in ML",
    "text": "Linear regression in ML\nThis is the Normal (linear) log-likelihood function. It presumes some data, \\(\\mathbf{y}\\) and some unknowns \\(\\mathbf{\\beta, \\sigma^2}\\). You should note the kernel of the function is the sum of the squared differences of \\(y\\) and \\(x\\beta\\).\n\\[\\ln\\mathcal{L} =\\sum\\left(-\\frac{1}{2}(\\ln(2\\pi)) -\\frac{1}{2}(\\ln(\\sigma^{2})) -\\frac{1}{2\\sigma^{2}}\\color{red}{\\left[\\sum\\limits_{i=1}^{n}(y-x\\beta)^{2}\\right]} \\right)\\]\nIt turns out if we take the derivative of this function with respect to \\(\\beta\\) and \\(\\sigma^2\\), the result is the ML estimator, and the OLS estimator. They’re the same."
  },
  {
    "objectID": "likelihood24.html#linearity",
    "href": "likelihood24.html#linearity",
    "title": "Likelihood",
    "section": "Linearity",
    "text": "Linearity\nThis model is linear - our estimates, \\(x\\beta\\) map directly onto \\(y\\) - there is no latent variable, \\(\\tilde{y}\\) to map onto - so \\(x\\beta = \\widehat{y}\\).\nNote that this is not because \\(y\\) is normal. Rather, the fact that \\(y\\) is continuous and contains a lot of information and is not limited makes it more likely \\(y\\) is normal.\nThough ML is most often used in cases where \\(y\\) is limited (so OLS is inappropriate), it’s important to see that we can estimate the linear model using either technology (OLS, ML) and get the same estimates."
  },
  {
    "objectID": "likelihood24.html#footnotes",
    "href": "likelihood24.html#footnotes",
    "title": "Likelihood",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee von Bortkiewicz, Ladislaus (1898). Das Gesetz der Kleinen Zahlen. Leipzig: Teubner.↩︎"
  },
  {
    "objectID": "likelihood24.html#numerical-method-intuition",
    "href": "likelihood24.html#numerical-method-intuition",
    "title": "Likelihood",
    "section": "Numerical method intuition",
    "text": "Numerical method intuition\n\nChoose starting values of \\(\\beta\\) (sometimes from OLS) to estimate the log-likelihood.\nTake the derivative of the log-likelihood with respect to the parameters to find the gradient}. The gradient (or the gradient matrix, a \\(kxk\\) matrix) tells us the direction of the slope of a line tangent to the curve at the point of the log-likelihood estimate.\nIf the gradient is positive (if the matrix is positive definite), then \\(ln \\mathcal{L}\\) is increasing in \\(\\beta\\) - the slope is increasing, so increase our estimate of \\(\\beta\\) and try again.\nIf the gradient is negative (if the matrix is negative definite), the \\(ln \\mathcal{L}\\) is decreasing in \\(\\beta\\) - the slope is decreasing, so we’ve passed the maximum; choose a smaller value for \\(\\beta\\) and try again.\nAs the log-likelihood approaches the maximum, the gradient approaches zero - the slope of the line tangent to the curve at the point of the log-likelihood estimate is approaching zero, indicating we’re reaching the maximum of the function. Stop the search and evaluate the estimates of \\(\\beta\\) that produced the zero gradient.\nThroughout this process, we need to evaluate the second derivatives in order to figure out the rate at which the slope is changing; this helps us tell how close or far we are from the maximum. The second derivative describes the curvature of the LLF, or the rate of change.\nThe matrix of second derivatives (the Hessian matrix) or its approximation also provide the source of our estimates of the variance, and thus the standard errors.\n\n\nThe first derivative tells us the direction in which the function is changing. This is obviously important since we’re trying to find the maximum.\nThink of this as trying to figure out when you’re exactly at the top of a hill. The slope (the grade, the gradient) is positive while you’re climbing to the top, it’s zero at the top, and it’s negative on the way down the other side.\nBut is the hill flat or steep? If it’s flat, then the change in the slope between point A and point B is likely to be very small - this, of course, can make it difficult to know exactly when we’re at the top (the maximum). On the other hand, if the hill is very steep, the change in the slope between two points is pretty substantial. Put another way, the rate of change in the slope is larger (faster) the steeper the slope; it’s smaller (slower) the flatter the slope.\nThis matters to maximization because the second derivatives tell us how big (or small) a step we should take up the hill as we try to find the top. Suppose that the function is very flat; as indicated above, the change in the slope between two points would be small, so we can take larger steps in order to try to find the maximum. The second derivatives would tell us that the rate of change is very small, so we should take larger steps.\nThe software performing the estimation will choose the next value of \\(\\beta\\) a bit further away from the last value it tried. On the other hand, if the second derivatives are large so the rate of change is fast, we want to take relatively small steps so we don’t step right over the maximum. In any case, that’s the intuition for why we need to know the matrix of second derivatives."
  },
  {
    "objectID": "llfmax24.html",
    "href": "llfmax24.html",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "The goal here is to walk through the process of writing a likelihood function based on the data we observe, then programming that likelihood function and optimizing it with respect to the data.\n\n\nLet’s recall the motivation for turning to ML. The data we have are not suitable for OLS because our observation of the data is limited. For instance, we observe a binary variable \\([0,1]\\) rather than the underlying continuous probability beneath it. If we could observe and measure that probability, we’d do so and perhaps use OLS to estimate a model. But since we only observe \\([0,1]\\), our observation is limited.\nOLS is poorly suited to model this variable especially if one of our goals is to generate predictions of that underlying probability. That is, one of our goals might be to estimate a measure of the \\(y\\) variable we wish we could measure directly.\nMLE is better suited to this task - it asks us what the data generating process is that produced the observed data, and to build a model appropriate to the limited observation of that \\(y\\) variable.\nTo do so, we need to:\n\ndescribe the observed distribution of \\(y\\)\nconsider what we wish we could measure - this points to the key quantities of interest we want to derive from the model.\ndescribe \\(y\\) according to a probability distribution; write that distribution\nwrite a log-likelihood function appropriate to that probability distribution.\nparameterize the log-likelihood function such that we have a link distribution to map the linear prediction, \\(x\\beta\\), onto the probability space of \\(y\\).\n\n\n\n\nLet’s start with a \\(y\\) variable as follows:\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                y\n                Freq\n              \n        \n        \n        \n                \n                  0\n                  707\n                \n                \n                  1\n                  293\n                \n        \n      \n    \n\n\n\nThe variable, \\(y\\), takes on values of zero and one - it appears binomial; let’s write this in terms of the binomial distribution parameter \\(\\pi\\), so it takes on the value of one with probability \\(\\pi\\) and zero with probability \\(1-\\pi\\).\n\\[ y_i = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{} \\pi_{i}\\\\\n         0, & \\mbox{} 1-\\pi_{i}\n         \\end{array}\n     \\right.\\]\nThe likelihood of a single observation is:\n\\[ \\mathcal{L}(\\pi_i|y) = \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\]\nThis is a statement of the likelihood that any particular value of \\(\\pi\\) generated an observation. We want the likelihood any value of \\(\\pi\\) generated the entire dataset - that is, we want the joint likelihood of all the observations.\n\\[ \\mathcal{L}(\\pi|y) = \\prod_{i=1}^{n} \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\] recalling that a joint probability is the product of individual probabilities.\nLet’s take the log of this likelihood function - the natural log makes computation easier. Even for computers, this reduces the computational intensity especially with respect to extremely small decimals.\n\\[ \\ln \\mathcal{L}(\\pi|y) = \\sum_{i=1}^{n} y_i \\ln(\\pi_i) + (1-y_i) \\ln(1-\\pi_i) \\]\nWe can take this a step further and parameterize \\(\\pi\\) as a function of \\(X\\beta\\) where \\(X\\) is the matrix of predictors and \\(\\beta\\) is the vector of coefficients such that\n\\[ \\pi_i = F({X_i\\beta}) \\]\nSo our estimate of the binomial probability is a function of the linear predictor \\(X\\beta\\). We map \\(X\\beta\\) onto the probability space \\(\\pi\\) using a link function, \\(F\\). The most common link functions are the logistic (logit model), and the standard normal (probit model). Let’s write the logit link:\n\\[ \\pi_i = \\frac{1}{1 + e^{-(X_i\\beta)}} \\]\nand now let’s write this all in the log-likelihood function:\n\\[ \\ln \\mathcal{L}(\\beta|y) = \\sum_{i=1}^{n} y_i \\ln(\\frac{1}{1 + e^{-(X_i\\beta)}}) + (1-y_i) \\ln(1-\\frac{1}{1 + e^{-(X_i\\beta)}}) \\]\nThis is the logit log-likelihood function for a binary \\(y\\) variable.\n\n\n\nHow do we solve this function for the data \\(y\\)? Put differently, what is the value of \\(\\pi\\) that most likely generated the data, \\(y\\)? There are a few ways to do this - one instructive method is to use a grid search, where we calculate the log-likelihood for a range of values of \\(\\pi\\) and find the value that maximizes the log-likelihood. We can repeat this to the desired level of specificity, progressively narrowing the grid. In this example, we’ll just do one search out to 3 decimal places. The steps are these:\n\ngenerate the range of candidate values of our parameter \\(\\pi\\) to plug into the log-likelihood function.\nplug each value into the log-likelihood function to compute the log-likelihood for that value.\nidentify which value of \\(\\pi\\) maximizes the log-likelihood.\n\nHere’s code to do this:\n\n\ncode\n# generate a vector of values of pi \n\npi_trials &lt;- seq(0, 1, by = 0.001)\n\n# write the log-likelihood function\n\nlog_likelihood &lt;- function(pi_trials, y) {\n  sum(y * log(pi_trials) + (1 - y) * log(1 - pi_trials))\n}\n\n# Calculate log-likelihood for each pi value\n\nll_values &lt;- sapply(pi_trials, log_likelihood, y = y)\n\n# Find the pi value that maximizes the log-likelihood\n\npi_hat &lt;- pi_trials[which.max(ll_values)]\nprint(pi_hat)\n\n\n[1] 0.293\n\n\nYou’ll note we recover the sample mean of \\(y\\). Let’s plot the log-likelihood against the values of \\(\\pi\\) to visualize the maximum.\n\n\ncode\n# Plot log-likelihood against pi\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(data.frame(pi = pi_trials, ll = ll_values), \"line\", hcaes(x = pi, y = ll)) %&gt;%\n  hc_title(text = \"Grid search - maximizing the log-likelihood\") %&gt;%\n  hc_xAxis(title = list(text = \"Pi\"), \n           plotLines = list(\n    list(color =\"red\", value = pi_hat ))) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_legend(enabled = FALSE) %&gt;%\n  hc_colors(bucolors)\n\n\n\n\n\n\n\n\n\nGrid searches are not sufficient for multivariate models, especially where we also need measures of uncertainty. Here, we turn to numerical optimization.\nNumerical optimization is a field aimed at finding “best” outcomes or answers depending on some set of criteria - often, the “best” is the maximum or minimum of some function - that’s the case in maximum likelihood.\n\n\nThere are lots of ways to optimize a function - in statistical software, most of them derive from Newton’s iterative method. A very common application of this is the Newton-Raphson method. Newton-Raphson is an iterative process that starts with an initial guess for the parameter \\(\\pi\\) (or the vector of unknowns, \\(\\beta\\)) and updates it in the direction of the maximum until it converges to the maximum. At each step, it computes the first and second derivatives of the likelihood function, then uses these to update the parameter estimate(s). The update rule is:\n\\[ \\pi_{new} = \\pi_{old} - H(\\pi)^{-1}*g(\\pi) \\]\nSo the new estimate of the parameter is the old estimate minus the first derivative (gradient) of the log-likelihood function pre-multiplied by the Hessian (second derivative) of the log-likelihood function. Here, \\(g(\\pi)\\) is the gradient of the log-likelihood function and \\(H(\\pi)\\) is the Hessian of the log-likelihood function. The algorithm iterates this process until the change in \\(\\pi\\) is sufficiently small, below a specified threshold. Since \\(g\\) and \\(H\\) are matrices, you should see how this straightforwardly applied to a multivariate regression.\nLet’s write the Newton-Raphson algorithm ourselves - take a look at the code chunk below. You’ll see it produces the same result as the grid search; the sample frequency of \\(y\\).\n\n\n\n\n\ncode\n# use same data generated above; same LLF as above, aiming to recover pi\n\n#write the gradient - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(pi, y) {\n  sum(y / pi - (1 - y) / (1 - pi))\n}\n\n#write the Hessian - matrix of partial second derivatives of the log-likelihood function.\n\nhessian &lt;- function(pi, y) {\n  sum(-y / pi^2 - (1 - y) / (1 - pi)^2)\n}\n\n#declare a starting value for pi\n\npi &lt;- 0.5\n\n#set convergence criteria - when new pi is within 1e-6 of old pi, stop iterating\n\ntol &lt;- 1e-6\n\n#initialize iteration counter, set maximum iterations; how many times will we do this before we stop if we don't converge prior? \n\niter &lt;- 0\nmax_iter &lt;- 100\n\n#iterate the Newton-Raphson algorithm\n\nfor (iter in 1:max_iter) {\n  # Compute gradient  \n  grad &lt;- gradient(pi, y)\n  \n  # Compute Hessian\n  hess &lt;- hessian(pi, y)\n  \n  # Update pi\n  pi_new &lt;- pi - (grad / hess)\n  \n  pi &lt;- pi_new\n  \n  # Check for convergence\n  if (abs(pi_new - pi) &lt; tol) {\n    break\n  }\n  \n  iter &lt;- iter + 1\n}\n\n#after convergence, print the result\n\nprint(pi)\n\n\n[1] 0.293\n\n\nSo what’s gone on here? We have 1000 observations of a binary variable; maximum likelihood is answering the question “what is the value of the parameter \\(\\pi\\) that makes the observed data most likely?” To get that answer, we have tried candidate values of \\(\\pi\\) to see which one maximizes the log-likelihood function.\nWe’ve maximized the function two ways - using a grid search process, and using the Newton-Raphson algorithm. The latter is more efficient and is the basis for most optimization algorithms in statistical software, and easy to adapt to estimate multiple parameters.\nHere’s one more approach also using the Newton-Raphson method. It calls the maxLik package to do the optimization instead of our having to write the algorithm ourselves.\n\n\ncode\nlibrary(maxLik)\n# Write the binomial log-likelihood function\nlog_likelihood &lt;- function(pi, y) {\n  sum(y * log(pi) + (1 - y) * log(1 - pi))\n}\n\n#maximize the function using maxLik\n\nm1m &lt;- maxLik(y=y, log_likelihood, start = c(0.5), method = \"NR\") #Newton-Raphson method, starting value of 0.5\n\nsummary(m1m)\n\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 3 iterations\nReturn code 2: successive function values within tolerance limit (tol)\nLog-Likelihood: -604.816 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(&gt; t)    \n[1,]  0.29300    0.01441   20.34  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\nAgain, the value of \\(\\pi\\) that maximizes the log-likelihood is the same as the sample frequency of \\(y\\), which is 0.293.\n\n\n\nLet’s make this more realistic in terms of resembling the models we want to run by adding an intercept term and multiple predictors to the model - the likelihood now has to account for those \\(X\\) variables and maximize with respect to them.\n\ngenerate some data on \\(y\\), and \\(X\\)\nwrite the likelihood\nwrite the Newton-Raphson algorithm\napply to the data\n\n\n\ncode\n# Generate binary y, and two X variables for regression; coefficients are -1, 0.5, -0.5\n\nset.seed(8675309)\nn &lt;- 10000\nX &lt;- matrix(rnorm(n * 2), ncol = 2)\ntrue_beta &lt;- c(-1, 0.5, -0.5)\nz &lt;- cbind(1, X) %*% true_beta\nprob &lt;- 1 / (1 + exp(-z))\ny &lt;- rbinom(n, 1, prob)\n\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson iterative algorithm\n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # Add intercept term to X if not already present\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#write the gradient function - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- plogis(X %*% beta)\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  k &lt;- ncol(X)\n  \n# set starting values for k columns of data (betas)\n  beta &lt;- rep(0, k) \n  \n# Store gradients and log-likelihoods for analysis\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n# iterate NR \n  for (iter in 1:max_iter) {\n    # compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # compute Hessian\n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # check for convergence\n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n    \n    # compute standard errors for this model by taking the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  }\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#html table comparing glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\", caption=\"Comparing GLM and Dave Estimates\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  column_spec(3, border_right = T) %&gt;%\n  column_spec(4, border_right = T) %&gt;%\n  collapse_rows(columns = 1:3, valign = \"top\") \n\n\n\n\nTable 1: Comparing GLM and Dave Estimates\n\n\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-0.9531826\n-0.9531826\n0.0236756\n0.0236748\n\n\nX1\n0.4938831\n0.4938831\n0.0238303\n0.0238295\n\n\nX2\n-0.4719703\n-0.4719703\n0.0240294\n0.0240286\n\n\n\n\n\n\n\n\n\n\n\nTable 1 compares the coefficients and standard errors from the Newton-Raphson algorithm to the coefficients and standard errors from the glm function. The estimates are the same, the standard errors very close.\nAlright, lots of fun here. Now, let’s use the program above to estimate a logit model using the democratic peace data.\n\n\n\nTable 2 compares the coefficients and standard errors from the program we wrote to those produced by the glm function. The regression here is a logit model predicting the onset of a militarized dispute as a function of the logged capabilities ratio, an indicator of whether the pair of states share a border, and the lowest democracy (polity) score in the pair.\n\n\ncode\ndp &lt;- read.csv(\"/users/dave/documents/teaching/606J-mle/2022/exercises/ex1/dp.csv\", header=TRUE)\n\n# for the log_likelihood function below, define y as the variable \"dispute\" in the dp data frame, and x1, x2, and x3 as the variables \"lncaprat\", \"border\", and \"deml\" in the dp data frame, respectively.\ndp$lncaprat &lt;- log(dp$caprat)\nX &lt;- as.matrix(dp[, c(\"lncaprat\", \"border\", \"deml\")])\ny &lt;- as.vector(dp$dispute)\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson \n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # check for constant; add one if necessary\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#write the gradient function -  partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- 1 / (1 + exp(-X %*% beta))\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  p &lt;- ncol(X)\n  \n  # set starting values for beta\n  beta &lt;- rep(0, p)\n  \n  # Store gradients and log-likelihoods\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n  for (iter in 1:max_iter) {\n    # Compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # Compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # Compute Hessian - 2nd partial derivatives \n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # Update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # Check for convergence - end if change in beta is less than tolerance value \n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n  }\n\n#compute standard errors for this model; take the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n# logit$coefficients\n# logit$st.errors\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#compare glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\", caption=\"Comparing GLM and Dave Estimates, Democratic Peace Model\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:3, valign = \"top\")\n\n\n\n\nTable 2: Comparing GLM and Dave Estimates, Democratic Peace Model\n\n\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-3.2407604\n-3.2407605\n0.1034544\n0.1034630\n\n\nXlncaprat\n-0.2028768\n-0.2028768\n0.0237847\n0.0237860\n\n\nXborder\n0.7827135\n0.7827135\n0.0852298\n0.0852337\n\n\nXdeml\n-0.0783467\n-0.0783467\n0.0068065\n0.0068078\n\n\n\n\n\n\n\n\n\n\n\nRecapping what we’ve done:\n\nWe wrote a log-likelihood function for a logistic regression model with multiple predictors.\nTo maximize the log-likelihood, we implemented a Newton-Raphson algorithm in the following steps:\n\nDefined the gradient and Hessian functions. The gradient is the vector of first partial derivatives of the log-likelihood, and the Hessian is the matrix of second partial derivatives.\nInitialized the beta coefficients and stored gradients and log-likelihoods.\nIterated the algorithm to update beta values.\nChecked for convergence based on the change in beta.\n\nOnce the model converged, we computed standard errors for the estimated coefficients by taking the square root of the main diagonal elements of the inverse negative Hessian.\n\n\n\n\nHere, you can see how the gradients change over each iteration.\n\n\ncode\n#plot gradient convergence\ngradients &lt;- do.call(cbind, logit$gradient_history)\ngradients &lt;- as.data.frame(gradients) %&gt;%\n  mutate(coef=c(\"Intercept\", \"lncaprat\", \"border\", \"deml\")) \n\ngradients &lt;- gradients %&gt;% \n  pivot_longer(-coef, names_to = \"iteration\", values_to = \"value\")\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(gradients, \"line\", hcaes(x=iteration, y=value, group=coef, color=coef)) %&gt;%\n  hc_title(text = \"Gradient Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Gradient\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\n\n\nAnd here, we can see how the log-likelihood changes over iterations:\n\n\ncode\n#plot ll convergence\n\nlls &lt;- as.data.frame(logit$ll_history) %&gt;%\n  mutate(iteration=1:nrow(.))\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(lls, \"line\", hcaes(x=iteration, y=`logit$ll_history`)) %&gt;%\n  hc_title(text = \"Log-Likelihood Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\n\n\nHere are two ways to recover the final log-likelihood from the glm model. The first uses the logLik function from the stats package. The second calculates the log-likelihood by hand - generating the predictions and plugging those into the log-likelihood function, then summing.\n\\[ LL = \\sum_{i=1}^{n} y_i \\ln(p) + (1-y_i) \\ln(1-p) \\]\n\n\ncode\n# use the logLik function from the stats package  \n\nllest &lt;- logLik(glm_fit)\n\n# or calculate it by hand - generate the predictions and plug those into the log-likelihood function, then sum.\n\npred &lt;- predict(glm_fit, type = \"response\")\nllestbyhand &lt;- sum(dp$dispute * log(pred) + (1 - dp$dispute) * log(1 - pred))\n\n\ndata.frame(llest, llestbyhand) %&gt;% \n  kable(\"html\", caption=\"Recovering the Log-Likelihood\" ) %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:2, valign = \"top\")\n\n\n\n\nRecovering the Log-Likelihood\n\n\nllest\nllestbyhand\n\n\n\n\n-3565.219\n-3565.219"
  },
  {
    "objectID": "llfmax24.html#motivating-likelihood",
    "href": "llfmax24.html#motivating-likelihood",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "Let’s recall the motivation for turning to ML. The data we have are not suitable for OLS because our observation of the data is limited. For instance, we observe a binary variable \\([0,1]\\) rather than the underlying continuous probability beneath it. If we could observe and measure that probability, we’d do so and perhaps use OLS to estimate a model. But since we only observe \\([0,1]\\), our observation is limited.\nOLS is poorly suited to model this variable especially if one of our goals is to generate predictions of that underlying probability. That is, one of our goals might be to estimate a measure of the \\(y\\) variable we wish we could measure directly.\nMLE is better suited to this task - it asks us what the data generating process is that produced the observed data, and to build a model appropriate to the limited observation of that \\(y\\) variable.\nTo do so, we need to:\n\ndescribe the observed distribution of \\(y\\)\nconsider what we wish we could measure - this points to the key quantities of interest we want to derive from the model.\ndescribe \\(y\\) according to a probability distribution; write that distribution\nwrite a log-likelihood function appropriate to that probability distribution.\nparameterize the log-likelihood function such that we have a link distribution to map the linear prediction, \\(x\\beta\\), onto the probability space of \\(y\\)."
  },
  {
    "objectID": "llfmax24.html#binary-y-variable",
    "href": "llfmax24.html#binary-y-variable",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "Let’s start with a \\(y\\) variable as follows:\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                y\n                Freq\n              \n        \n        \n        \n                \n                  0\n                  707\n                \n                \n                  1\n                  293\n                \n        \n      \n    \n\n\n\nThe variable, \\(y\\), takes on values of zero and one - it appears binomial; let’s write this in terms of the binomial distribution parameter \\(\\pi\\), so it takes on the value of one with probability \\(\\pi\\) and zero with probability \\(1-\\pi\\).\n\\[ y_i = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{} \\pi_{i}\\\\\n         0, & \\mbox{} 1-\\pi_{i}\n         \\end{array}\n     \\right.\\]\nThe likelihood of a single observation is:\n\\[ \\mathcal{L}(\\pi_i|y) = \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\]\nThis is a statement of the likelihood that any particular value of \\(\\pi\\) generated an observation. We want the likelihood any value of \\(\\pi\\) generated the entire dataset - that is, we want the joint likelihood of all the observations.\n\\[ \\mathcal{L}(\\pi|y) = \\prod_{i=1}^{n} \\pi_{i}^{y_i} (1-\\pi_{i})^{1-y_i} \\] recalling that a joint probability is the product of individual probabilities.\nLet’s take the log of this likelihood function - the natural log makes computation easier. Even for computers, this reduces the computational intensity especially with respect to extremely small decimals.\n\\[ \\ln \\mathcal{L}(\\pi|y) = \\sum_{i=1}^{n} y_i \\ln(\\pi_i) + (1-y_i) \\ln(1-\\pi_i) \\]\nWe can take this a step further and parameterize \\(\\pi\\) as a function of \\(X\\beta\\) where \\(X\\) is the matrix of predictors and \\(\\beta\\) is the vector of coefficients such that\n\\[ \\pi_i = F({X_i\\beta}) \\]\nSo our estimate of the binomial probability is a function of the linear predictor \\(X\\beta\\). We map \\(X\\beta\\) onto the probability space \\(\\pi\\) using a link function, \\(F\\). The most common link functions are the logistic (logit model), and the standard normal (probit model). Let’s write the logit link:\n\\[ \\pi_i = \\frac{1}{1 + e^{-(X_i\\beta)}} \\]\nand now let’s write this all in the log-likelihood function:\n\\[ \\ln \\mathcal{L}(\\beta|y) = \\sum_{i=1}^{n} y_i \\ln(\\frac{1}{1 + e^{-(X_i\\beta)}}) + (1-y_i) \\ln(1-\\frac{1}{1 + e^{-(X_i\\beta)}}) \\]\nThis is the logit log-likelihood function for a binary \\(y\\) variable."
  },
  {
    "objectID": "llfmax24.html#maximizing-the-likelihood---grid-search",
    "href": "llfmax24.html#maximizing-the-likelihood---grid-search",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "How do we solve this function for the data \\(y\\)? Put differently, what is the value of \\(\\pi\\) that most likely generated the data, \\(y\\)? There are a few ways to do this - one instructive method is to use a grid search, where we calculate the log-likelihood for a range of values of \\(\\pi\\) and find the value that maximizes the log-likelihood. We can repeat this to the desired level of specificity, progressively narrowing the grid. In this example, we’ll just do one search out to 3 decimal places. The steps are these:\n\ngenerate the range of candidate values of our parameter \\(\\pi\\) to plug into the log-likelihood function.\nplug each value into the log-likelihood function to compute the log-likelihood for that value.\nidentify which value of \\(\\pi\\) maximizes the log-likelihood.\n\nHere’s code to do this:\n\n\ncode\n# generate a vector of values of pi \n\npi_trials &lt;- seq(0, 1, by = 0.001)\n\n# write the log-likelihood function\n\nlog_likelihood &lt;- function(pi_trials, y) {\n  sum(y * log(pi_trials) + (1 - y) * log(1 - pi_trials))\n}\n\n# Calculate log-likelihood for each pi value\n\nll_values &lt;- sapply(pi_trials, log_likelihood, y = y)\n\n# Find the pi value that maximizes the log-likelihood\n\npi_hat &lt;- pi_trials[which.max(ll_values)]\nprint(pi_hat)\n\n\n[1] 0.293\n\n\nYou’ll note we recover the sample mean of \\(y\\). Let’s plot the log-likelihood against the values of \\(\\pi\\) to visualize the maximum.\n\n\ncode\n# Plot log-likelihood against pi\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(data.frame(pi = pi_trials, ll = ll_values), \"line\", hcaes(x = pi, y = ll)) %&gt;%\n  hc_title(text = \"Grid search - maximizing the log-likelihood\") %&gt;%\n  hc_xAxis(title = list(text = \"Pi\"), \n           plotLines = list(\n    list(color =\"red\", value = pi_hat ))) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_legend(enabled = FALSE) %&gt;%\n  hc_colors(bucolors)"
  },
  {
    "objectID": "llfmax24.html#optimization",
    "href": "llfmax24.html#optimization",
    "title": "Maximizing the Log-Likelihood Function",
    "section": "",
    "text": "Grid searches are not sufficient for multivariate models, especially where we also need measures of uncertainty. Here, we turn to numerical optimization.\nNumerical optimization is a field aimed at finding “best” outcomes or answers depending on some set of criteria - often, the “best” is the maximum or minimum of some function - that’s the case in maximum likelihood.\n\n\nThere are lots of ways to optimize a function - in statistical software, most of them derive from Newton’s iterative method. A very common application of this is the Newton-Raphson method. Newton-Raphson is an iterative process that starts with an initial guess for the parameter \\(\\pi\\) (or the vector of unknowns, \\(\\beta\\)) and updates it in the direction of the maximum until it converges to the maximum. At each step, it computes the first and second derivatives of the likelihood function, then uses these to update the parameter estimate(s). The update rule is:\n\\[ \\pi_{new} = \\pi_{old} - H(\\pi)^{-1}*g(\\pi) \\]\nSo the new estimate of the parameter is the old estimate minus the first derivative (gradient) of the log-likelihood function pre-multiplied by the Hessian (second derivative) of the log-likelihood function. Here, \\(g(\\pi)\\) is the gradient of the log-likelihood function and \\(H(\\pi)\\) is the Hessian of the log-likelihood function. The algorithm iterates this process until the change in \\(\\pi\\) is sufficiently small, below a specified threshold. Since \\(g\\) and \\(H\\) are matrices, you should see how this straightforwardly applied to a multivariate regression.\nLet’s write the Newton-Raphson algorithm ourselves - take a look at the code chunk below. You’ll see it produces the same result as the grid search; the sample frequency of \\(y\\).\n\n\n\n\n\ncode\n# use same data generated above; same LLF as above, aiming to recover pi\n\n#write the gradient - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(pi, y) {\n  sum(y / pi - (1 - y) / (1 - pi))\n}\n\n#write the Hessian - matrix of partial second derivatives of the log-likelihood function.\n\nhessian &lt;- function(pi, y) {\n  sum(-y / pi^2 - (1 - y) / (1 - pi)^2)\n}\n\n#declare a starting value for pi\n\npi &lt;- 0.5\n\n#set convergence criteria - when new pi is within 1e-6 of old pi, stop iterating\n\ntol &lt;- 1e-6\n\n#initialize iteration counter, set maximum iterations; how many times will we do this before we stop if we don't converge prior? \n\niter &lt;- 0\nmax_iter &lt;- 100\n\n#iterate the Newton-Raphson algorithm\n\nfor (iter in 1:max_iter) {\n  # Compute gradient  \n  grad &lt;- gradient(pi, y)\n  \n  # Compute Hessian\n  hess &lt;- hessian(pi, y)\n  \n  # Update pi\n  pi_new &lt;- pi - (grad / hess)\n  \n  pi &lt;- pi_new\n  \n  # Check for convergence\n  if (abs(pi_new - pi) &lt; tol) {\n    break\n  }\n  \n  iter &lt;- iter + 1\n}\n\n#after convergence, print the result\n\nprint(pi)\n\n\n[1] 0.293\n\n\nSo what’s gone on here? We have 1000 observations of a binary variable; maximum likelihood is answering the question “what is the value of the parameter \\(\\pi\\) that makes the observed data most likely?” To get that answer, we have tried candidate values of \\(\\pi\\) to see which one maximizes the log-likelihood function.\nWe’ve maximized the function two ways - using a grid search process, and using the Newton-Raphson algorithm. The latter is more efficient and is the basis for most optimization algorithms in statistical software, and easy to adapt to estimate multiple parameters.\nHere’s one more approach also using the Newton-Raphson method. It calls the maxLik package to do the optimization instead of our having to write the algorithm ourselves.\n\n\ncode\nlibrary(maxLik)\n# Write the binomial log-likelihood function\nlog_likelihood &lt;- function(pi, y) {\n  sum(y * log(pi) + (1 - y) * log(1 - pi))\n}\n\n#maximize the function using maxLik\n\nm1m &lt;- maxLik(y=y, log_likelihood, start = c(0.5), method = \"NR\") #Newton-Raphson method, starting value of 0.5\n\nsummary(m1m)\n\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 3 iterations\nReturn code 2: successive function values within tolerance limit (tol)\nLog-Likelihood: -604.816 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(&gt; t)    \n[1,]  0.29300    0.01441   20.34  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\nAgain, the value of \\(\\pi\\) that maximizes the log-likelihood is the same as the sample frequency of \\(y\\), which is 0.293.\n\n\n\nLet’s make this more realistic in terms of resembling the models we want to run by adding an intercept term and multiple predictors to the model - the likelihood now has to account for those \\(X\\) variables and maximize with respect to them.\n\ngenerate some data on \\(y\\), and \\(X\\)\nwrite the likelihood\nwrite the Newton-Raphson algorithm\napply to the data\n\n\n\ncode\n# Generate binary y, and two X variables for regression; coefficients are -1, 0.5, -0.5\n\nset.seed(8675309)\nn &lt;- 10000\nX &lt;- matrix(rnorm(n * 2), ncol = 2)\ntrue_beta &lt;- c(-1, 0.5, -0.5)\nz &lt;- cbind(1, X) %*% true_beta\nprob &lt;- 1 / (1 + exp(-z))\ny &lt;- rbinom(n, 1, prob)\n\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson iterative algorithm\n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # Add intercept term to X if not already present\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#write the gradient function - matrix of partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- plogis(X %*% beta)\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  k &lt;- ncol(X)\n  \n# set starting values for k columns of data (betas)\n  beta &lt;- rep(0, k) \n  \n# Store gradients and log-likelihoods for analysis\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n# iterate NR \n  for (iter in 1:max_iter) {\n    # compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # compute Hessian\n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # check for convergence\n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n    \n    # compute standard errors for this model by taking the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  }\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#html table comparing glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\", caption=\"Comparing GLM and Dave Estimates\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  column_spec(3, border_right = T) %&gt;%\n  column_spec(4, border_right = T) %&gt;%\n  collapse_rows(columns = 1:3, valign = \"top\") \n\n\n\n\nTable 1: Comparing GLM and Dave Estimates\n\n\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-0.9531826\n-0.9531826\n0.0236756\n0.0236748\n\n\nX1\n0.4938831\n0.4938831\n0.0238303\n0.0238295\n\n\nX2\n-0.4719703\n-0.4719703\n0.0240294\n0.0240286\n\n\n\n\n\n\n\n\n\n\n\nTable 1 compares the coefficients and standard errors from the Newton-Raphson algorithm to the coefficients and standard errors from the glm function. The estimates are the same, the standard errors very close.\nAlright, lots of fun here. Now, let’s use the program above to estimate a logit model using the democratic peace data.\n\n\n\nTable 2 compares the coefficients and standard errors from the program we wrote to those produced by the glm function. The regression here is a logit model predicting the onset of a militarized dispute as a function of the logged capabilities ratio, an indicator of whether the pair of states share a border, and the lowest democracy (polity) score in the pair.\n\n\ncode\ndp &lt;- read.csv(\"/users/dave/documents/teaching/606J-mle/2022/exercises/ex1/dp.csv\", header=TRUE)\n\n# for the log_likelihood function below, define y as the variable \"dispute\" in the dp data frame, and x1, x2, and x3 as the variables \"lncaprat\", \"border\", and \"deml\" in the dp data frame, respectively.\ndp$lncaprat &lt;- log(dp$caprat)\nX &lt;- as.matrix(dp[, c(\"lncaprat\", \"border\", \"deml\")])\ny &lt;- as.vector(dp$dispute)\n\n# Define the log-likelihood function\nlog_likelihood &lt;- function(beta, X, y) {\n  z &lt;- X %*% beta\n  probs &lt;- plogis(z)\n  sum(y * log(probs) + (1 - y) * log(1 - probs))\n}\n\n# maximize using newton-raphson \n\nlogistic_regression &lt;- function(X, y, max_iter = 100, tol = 1e-6) {\n  # check for constant; add one if necessary\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n#write the gradient function -  partial first derivatives of the log-likelihood function.\n\ngradient &lt;- function(beta, X, y) {\np &lt;- 1 / (1 + exp(-X %*% beta))\n  t(X) %*% (y - p)\n}\n\n  n &lt;- nrow(X)\n  p &lt;- ncol(X)\n  \n  # set starting values for beta\n  beta &lt;- rep(0, p)\n  \n  # Store gradients and log-likelihoods\n  gradient_history &lt;- list()\n  ll_history &lt;- numeric()\n  \n  for (iter in 1:max_iter) {\n    # Compute gradient\n    grad &lt;- gradient(beta, X, y)\n    gradient_history[[iter]] &lt;- grad\n    \n    # Compute log-likelihood\n    ll &lt;- log_likelihood(beta, X, y)\n    ll_history[iter] &lt;- ll\n    \n    # Compute Hessian - 2nd partial derivatives \n    z &lt;- X %*% beta\n    probs &lt;- plogis(z)\n    W &lt;- diag(as.vector(probs * (1 - probs)))\n    hessian &lt;- -t(X) %*% W %*% X\n    \n    # Update beta\n    delta &lt;- solve(hessian, grad)\n    beta_new &lt;- beta - delta\n    \n    # Check for convergence - end if change in beta is less than tolerance value \n    if (max(abs(beta_new - beta)) &lt; tol) {\n      break\n    }\n    \n    beta &lt;- beta_new\n  }\n\n#compute standard errors for this model; take the square root of the main diagonal elements of the inverse negative Hessian\n  se &lt;- sqrt(diag(solve(-hessian)))\n\n  \n  return(list(\n    coefficients = beta,\n    st.errors = se,\n    log_likelihood = ll,\n    iterations = iter,\n    gradient_history = gradient_history,\n    ll_history = ll_history\n  ))\n}\n\n#estimate the model\nlogit &lt;- logistic_regression(X, y)\n# logit$coefficients\n# logit$st.errors\n\n#compare to glm estimates \nglm_fit &lt;- glm(y ~ X, family = binomial)\n\n# compare the results \nglmcoefs &lt;- coef(glm_fit)\nglmse &lt;- sqrt(diag(vcov(glm_fit)))\n\n#compare glmcoefs to logit$coefficients\n\nlibrary(kableExtra)\ndata.frame(glmcoefs, logit$coefficients, glmse, logit$st.errors) %&gt;% \n  kable(\"html\", caption=\"Comparing GLM and Dave Estimates, Democratic Peace Model\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:3, valign = \"top\")\n\n\n\n\nTable 2: Comparing GLM and Dave Estimates, Democratic Peace Model\n\n\n\n\n\n\n\n\nglmcoefs\nlogit.coefficients\nglmse\nlogit.st.errors\n\n\n\n\n(Intercept)\n-3.2407604\n-3.2407605\n0.1034544\n0.1034630\n\n\nXlncaprat\n-0.2028768\n-0.2028768\n0.0237847\n0.0237860\n\n\nXborder\n0.7827135\n0.7827135\n0.0852298\n0.0852337\n\n\nXdeml\n-0.0783467\n-0.0783467\n0.0068065\n0.0068078\n\n\n\n\n\n\n\n\n\n\n\nRecapping what we’ve done:\n\nWe wrote a log-likelihood function for a logistic regression model with multiple predictors.\nTo maximize the log-likelihood, we implemented a Newton-Raphson algorithm in the following steps:\n\nDefined the gradient and Hessian functions. The gradient is the vector of first partial derivatives of the log-likelihood, and the Hessian is the matrix of second partial derivatives.\nInitialized the beta coefficients and stored gradients and log-likelihoods.\nIterated the algorithm to update beta values.\nChecked for convergence based on the change in beta.\n\nOnce the model converged, we computed standard errors for the estimated coefficients by taking the square root of the main diagonal elements of the inverse negative Hessian.\n\n\n\n\nHere, you can see how the gradients change over each iteration.\n\n\ncode\n#plot gradient convergence\ngradients &lt;- do.call(cbind, logit$gradient_history)\ngradients &lt;- as.data.frame(gradients) %&gt;%\n  mutate(coef=c(\"Intercept\", \"lncaprat\", \"border\", \"deml\")) \n\ngradients &lt;- gradients %&gt;% \n  pivot_longer(-coef, names_to = \"iteration\", values_to = \"value\")\n\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(gradients, \"line\", hcaes(x=iteration, y=value, group=coef, color=coef)) %&gt;%\n  hc_title(text = \"Gradient Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Gradient\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\n\n\nAnd here, we can see how the log-likelihood changes over iterations:\n\n\ncode\n#plot ll convergence\n\nlls &lt;- as.data.frame(logit$ll_history) %&gt;%\n  mutate(iteration=1:nrow(.))\nbucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nhighcharter::highchart() %&gt;%\n  hc_add_series(lls, \"line\", hcaes(x=iteration, y=`logit$ll_history`)) %&gt;%\n  hc_title(text = \"Log-Likelihood Convergence\") %&gt;%\n  hc_xAxis(title = list(text = \"Iteration\")) %&gt;%\n  hc_yAxis(title = list(text = \"Log-Likelihood\")) %&gt;%\n  hc_colors(bucolors) \n\n\n\n\n\n\n\n\n\nHere are two ways to recover the final log-likelihood from the glm model. The first uses the logLik function from the stats package. The second calculates the log-likelihood by hand - generating the predictions and plugging those into the log-likelihood function, then summing.\n\\[ LL = \\sum_{i=1}^{n} y_i \\ln(p) + (1-y_i) \\ln(1-p) \\]\n\n\ncode\n# use the logLik function from the stats package  \n\nllest &lt;- logLik(glm_fit)\n\n# or calculate it by hand - generate the predictions and plug those into the log-likelihood function, then sum.\n\npred &lt;- predict(glm_fit, type = \"response\")\nllestbyhand &lt;- sum(dp$dispute * log(pred) + (1 - dp$dispute) * log(1 - pred))\n\n\ndata.frame(llest, llestbyhand) %&gt;% \n  kable(\"html\", caption=\"Recovering the Log-Likelihood\" ) %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) %&gt;% \n  collapse_rows(columns = 1:2, valign = \"top\")\n\n\n\n\nRecovering the Log-Likelihood\n\n\nllest\nllestbyhand\n\n\n\n\n-3565.219\n-3565.219"
  },
  {
    "objectID": "binaryextensions124.html",
    "href": "binaryextensions124.html",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "",
    "text": "symmetry - what are the implications of using symmetric links, especially given data on \\(y\\)?\nclassification\nmodel evaluation and fit\nrareness - what happens when there are few events?\n\nWe’re going to start with symmetry, then thinking about prediction and classification and fit."
  },
  {
    "objectID": "binaryextensions124.html#symmetry-and-asymmetry",
    "href": "binaryextensions124.html#symmetry-and-asymmetry",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Symmetry and Asymmetry",
    "text": "Symmetry and Asymmetry\nCompare three CDFs: the logistic, the clog-log, and a skewed logit function. The logistic is symmetric, the clog-log and skewed logit are not. You should notice the probability associated with x=0 for each function - the logistic is .5, the clog-log is about .64, and the skewed logit is about than .71.\nFor skewed binary \\(y\\) variables, it could be that one of these CDFs is a more appropriate link function than the symmetric logistic or normal. Implementing these merely requires substituting the appropriate CDF into the log-likelihood function.\n\n\ncode\n# Load required libraries\nlibrary(highcharter)\nlibrary(dplyr)\n\n# Binghamton University colors\nbinghamton_green &lt;- \"#005A43\"\nbinghamton_gray &lt;- \"#707070\"\nbinghamton_yellow &lt;- \"#FFC726\"\n\n# Generate data\nx &lt;- seq(-5, 5, length.out = 1000)\nlogistic_cdf &lt;- plogis(x)\ncloglog_cdf &lt;- 1 - exp(-exp(x))\n\n# Skewed logit function (shape parameter = 0.5)\n\nskewed_logit_cdf &lt;- 1 / (1 + exp(-x)) ^ 0.5 \n\n# Create data frame\ndf &lt;- data.frame(x = x, logistic = logistic_cdf, cloglog = cloglog_cdf, skewed_logit = skewed_logit_cdf)\n\n# Create the highchart\nhc &lt;- highchart() %&gt;%\n  hc_chart(type = \"line\") %&gt;%\n  hc_title(text = \"Comparison of CDFs: Logistic, Clog-log, and Skewed Logit\") %&gt;%\n  hc_xAxis(\n    title = list(text = \"x\"),\n    plotLines = list(\n      list(\n        color = \"#999\",\n        width = 2,\n        value = 0,\n        zIndex = 3,\n        label = list(text = \"x = 0\")\n      )\n    )\n  ) %&gt;%\n  hc_yAxis(\n    title = list(text = \"Cumulative Probability\"),\n    plotLines = list(\n      list(\n        color = \"#999\",\n        width = 2,\n        value = 0.5,\n        zIndex = 3,\n        label = list(text = \"y = 0.5\")\n      )\n    )\n  ) %&gt;%\n  hc_tooltip(\n    shared = TRUE,\n    formatter = JS(\"function() {\n      return 'x: ' + this.x.toFixed(4) + '&lt;br&gt;' +\n             'Logistic: ' + this.points[0].y.toFixed(4) + '&lt;br&gt;' +\n             'Clog-log: ' + this.points[1].y.toFixed(4) + '&lt;br&gt;' +\n             'Skewed Logit: ' + this.points[2].y.toFixed(4);\n    }\")\n  ) %&gt;%\n  hc_plotOptions(series = list(marker = list(enabled = FALSE))) %&gt;%\n  \n  # Add logistic CDF\n  hc_add_series(\n    data = df,\n    type = \"line\",\n    name = \"Logistic\",\n    color = binghamton_green,\n    hcaes(x = x, y = logistic)\n  ) %&gt;%\n  \n  # Add clog-log CDF\n  hc_add_series(\n    data = df,\n    type = \"line\",\n    name = \"Clog-log\",\n    color = binghamton_gray,\n    hcaes(x = x, y = cloglog)\n  ) %&gt;%\n  \n  # Add skewed logit CDF\n  hc_add_series(\n    data = df,\n    type = \"line\",\n    name = \"Skewed Logit\",\n    color = binghamton_yellow,\n    hcaes(x = x, y = skewed_logit)\n  )\n\n# Display the chart\nhc"
  },
  {
    "objectID": "binaryextensions124.html#skewed-logit",
    "href": "binaryextensions124.html#skewed-logit",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Skewed Logit",
    "text": "Skewed Logit\nNagler (1994) proposes the skewed logit (scobit) model - it’s a binary response model, the usual LLF, with a different link (the Burr-10):\n\\[ Pr(y=1) = \\frac{1}{(1+e^{-x\\beta})^\\alpha}\\]\nNote that if \\(\\alpha=1\\) this is the logistic CDF. If it is less than 1, the fastest rate of change is at \\(Pr(y =1 &lt; .5)\\); when greater than 1, the fastest rate of change, is at \\(Pr(y=1 &gt; .5)\\)\nNagler’s logic is that symmetric links require the assumption that individuals in the model are most sensitive to the effects of the \\(X\\) variables at or around \\(Pr(y=1) = .5\\). Looking at the (symmetric) logit curve above, you can see that’s where the derivative with respect to changes in \\(x\\) is greatest. If \\(y\\) is about half ones, half zeros, this may make sense - but often, we have \\(y\\) variables that are not symmetrically distributed like this. It makes sense in such cases not to assume the fastest rate of change, and the transition point from zero to one, is at \\(Pr(y=1) = .5\\).\nThe scobit model allows us to estimate the \\(\\alpha\\) parameter, which tells us where the fastest rate of change is in the CDF - that is, the transition point is an empirical question, not an assumption.\nThe model appears rarely in the political science literature; a Google Scholar search indicates most of its use is transportation analysis. A cursory survey also indicates the scobit estimates are often not that different from logit estimates. Estimation sometimes is funky insofar as we cannot always tell if changes in the likelihood are due to changes in \\(\\beta\\) or in \\(\\alpha\\)."
  },
  {
    "objectID": "binaryextensions124.html#skewed-logit-cdfs",
    "href": "binaryextensions124.html#skewed-logit-cdfs",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Skewed Logit CDFs",
    "text": "Skewed Logit CDFs\n\\includegraphics&lt;1&gt;[scale=.80]{/Users/dave/Documents/teaching/606J-mle/2022/slides/L3_binaryextensions/scobit22.pdf}"
  },
  {
    "objectID": "binaryextensions124.html#symmetry-1",
    "href": "binaryextensions124.html#symmetry-1",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Symmetry",
    "text": "Symmetry\nThe big point here is not that we should or should not use the scobit, but that we need to be very aware that the assumption in models with symmetric links is that the biggest effect of an \\(x\\) variable is at \\(Pr(y=1) = 0.5\\) which is where \\(x\\beta=0\\)."
  },
  {
    "objectID": "binaryextensions124.html#why-does-symmetry-matter",
    "href": "binaryextensions124.html#why-does-symmetry-matter",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Why does symmetry matter?",
    "text": "Why does symmetry matter?\n\nsymmetry determines where the greatest effect of \\(x\\) is.\nsymmetry ensures rates of change above and below \\(x=0\\) are the same as they approach the limits.\nsymmetry implies the theoretical threshold, \\(\\tau\\), separating observed zeros and ones is \\(\\tau=0.5\\).\nif we want to use the model to generate predicted values of \\(y\\) (rather than \\(y^*\\)), we need some threshold for classification.\n\nSome (very few) questions naturally link to a clear threshold like 0.5 …election outcomes? But …are we measuring the correct outcome variable?"
  },
  {
    "objectID": "binaryextensions124.html#confusion-matrix",
    "href": "binaryextensions124.html#confusion-matrix",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nThe “confusion matrix” (I didn’t make this up) illustrates that intersection and identifies where our classification is “confused.”\n\n\n\n\ncode\nlibrary(kableExtra)\n# opts &lt;- options(knitr.kable.NA = \"\")\ndf &lt;- data.frame(\"Predicted Positive\" = c(\"True Positive\", \"False Positive\"), \"Predicted Negative\" = c(\"False Positive\", \"True Negative\"), \"Rate\" = c(\"TPR=TP/P\", \"FPR=FP/N\"))\nrow.names(df) &lt;- c(\"Observed Positive\", \"Observed Negative\")\n\ntbl &lt;- kbl(df, align = rep('c', 4)) |&gt; \n  column_spec(2:4, border_left = T) |&gt; \n  column_spec(1,  bold=T) |&gt;\n  column_spec(1:4, width = \"2cm\", color = 'white', background = '#005A43', include_thead = TRUE) |&gt;\n  column_spec(2:4, background = \"inherit\", color=\"inherit\") |&gt;\n  row_spec(1, extra_css = \"border-bottom: 1px solid\") |&gt;\n  kable_paper(\"hover\", bootstrap_options = c(\"condensed\", \"responsive\"), full_width = F, font_size = 20)\ntbl\n\n\n\n\n\n\nPredicted.Positive\nPredicted.Negative\nRate\n\n\n\n\nObserved Positive\nTrue Positive\nFalse Positive\nTPR=TP/P\n\n\nObserved Negative\nFalse Positive\nTrue Negative\nFPR=FP/N\n\n\n\n\n\n\n\n\n\n\n\nTrue Positive Rate: correctly classify positive outcomes. This is often called “sensitivity.”\nFalse Positive Rate: we incorrectly classify negative outcomes (\\(y=0\\)) as positive (\\(y=1\\)). This is often called “1-specificity.” Specificity is the True Negative Rate, or the probability of correctly classifying a negative outcome (\\(y=0\\))."
  },
  {
    "objectID": "binaryextensions124.html#using-the-confusion-matrix-to-measure-model-fit",
    "href": "binaryextensions124.html#using-the-confusion-matrix-to-measure-model-fit",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Using the Confusion Matrix to Measure Model Fit",
    "text": "Using the Confusion Matrix to Measure Model Fit\nSo here’s the deal:\n\nestimate the model.\ngenerate the predicted probability \\(y=1\\) for each observation.\nassume a threshold separating zeros and ones; usually \\(\\tau=0.5\\).\nif \\(Pr(y=1 \\geq 0.5)\\), predict a positive outcome (predict \\(y=1\\)).\nif \\(Pr(y=0 &lt; 0.5)\\), predict a negative outcome (predict \\(y=0\\)).\nusing the observed and predicted outcomes, generate a confusion table, and compute measures of fit like “percent correctly predicted” (PCP) and “proportional reduction of error” (PRE).\n\n\nPercent Correctly Predicted (PCP)\nOne thing we can do is sum the main diagonal and divide by the estimation sample: \\((TP+TN)/N\\). This gives us the Percent Correctly Predicted (PCP).\n\n\nProportional Reduction of Error (PRE)}\nA second thing is to compute the Proportional Reduction of Error (PRE) - the difference between the correct predictions of the model versus a null model usually the unconditional frequency of \\(y\\) - in other words, we use the Percent in the Modal Category (PCM) of the \\(y\\) variable.\n\\[\\frac{correct_{\\text{informed}} - correct_{\\text{null}}}{N - correct_{\\text{null}}} \\]\nor\n\\[\\frac{PCP- PMC}{1-PMC} \\]\n\n\nExample: NAFTA vote, 1993\nSo here’s a model predicting members of the US House votes for or against the NAFTA treaty in 1993: 1 is a “yes” vote; the \\(x\\) variables are party (Democrat) and an ideology score.\n\n\ncode\nnafta &lt;- read.csv(\"NAFTA.csv\")\n\nnaftamodel &lt;- glm(vote ~ democrat + cope93, data=nafta, family=binomial(link=\"logit\"))\n\nstargazer(naftamodel, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: NAFTA vote\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Party (Democrat=1)\", \"Ideology (COPE score)\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: NAFTA vote\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParty (Democrat=1)\n\n\n1.990*** (0.504)\n\n\n\n\nIdeology (COPE score)\n\n\n-0.054*** (0.007)\n\n\n\n\nConstant\n\n\n2.380*** (0.272)\n\n\n\n\n\n\n\n\nObservations\n\n\n434\n\n\n\n\nAkaike Inf. Crit.\n\n\n465.904\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\nAnd here’s the confusion matrix from that model assuming a threshold of \\(\\tau=.5\\) - it compares the observed vote frequency against how we classify our predictions based on a predicted probability greater than or less than 0.5. This is generated using the caret package in R.\n\n\ncode\n# Load required libraries\nlibrary(caret)\n\n# Assuming you have a fitted GLM object called 'glm_model'\n# and test data 'test_data' with actual outcomes in 'test_data$actual'\n\ntest_data &lt;- nafta %&gt;% dplyr::select(vote, democrat, cope93) %&gt;% mutate(actual = vote)\n# Make predictions on the test data\npredictions &lt;- predict(naftamodel, newdata = test_data, type = \"response\")\n\n# Create confusion matrix\n# You may need to adjust the threshold (default is 0.5)\npredicted_classes &lt;- ifelse(predictions &gt; 0.5, 1, 0)\nconf_matrix &lt;- confusionMatrix(factor(predicted_classes), factor(test_data$actual))\n\n# Print confusion matrix\nprint(conf_matrix)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 160  77\n         1  40 157\n                                         \n               Accuracy : 0.7304         \n                 95% CI : (0.686, 0.7716)\n    No Information Rate : 0.5392         \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      \n                                         \n                  Kappa : 0.4647         \n                                         \n Mcnemar's Test P-Value : 0.0008741      \n                                         \n            Sensitivity : 0.8000         \n            Specificity : 0.6709         \n         Pos Pred Value : 0.6751         \n         Neg Pred Value : 0.7970         \n             Prevalence : 0.4608         \n         Detection Rate : 0.3687         \n   Detection Prevalence : 0.5461         \n      Balanced Accuracy : 0.7355         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\nYou should see the main diagonal presents the number of correct predictions - the off-diagonal elements are the incorrect predictions. If we sum the main diagonal and divide by \\(N\\), we get the Percent Correctly Predicted (PCP). In this case, the PCP is 0.735 - 73.5% of the votes are correctly predicted.\nThis all depends on the threshold (.5) - in the case of a Congressional vote, especially a relatively close vote like this one, the threshold might not be crazy. But it might be in other cases, and arbitrarily choosing a value for \\(\\tau\\) is problematic. So another approach is to compute the ROC curve.\nWhat makes this work relatively well in the NAFTA context? As you’ll see below, it works less well in the democratic peace models.\n\n\nExample: Democratic Peace\nHere’s a basic democratic peace model:\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\ndpm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n\nstargazer(dpm1, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\nLet’s generate the confusion matrix for the democratic peace model, threshold set at \\(\\tau =0.5\\):\n\n\ncode\nlibrary(kableExtra)\n# opts &lt;- options(knitr.kable.NA = \"\")\ndf &lt;- data.frame(\"Predicted Positive\" = c(\"0\", \"0\"), \"Predicted Negative\" = c(\"897\", \"19,245\"), \"Rate\" = c(\"TPR=0/897=0\", \"FPR=0/19,245=0\"))\nrow.names(df) &lt;- c(\"Observed Positive\", \"Observed Negative\")\n\ntbl &lt;- kbl(df, align = rep('c', 4)) |&gt; \n  column_spec(2:4, border_left = T) |&gt; \n  column_spec(1,  bold=T) |&gt;\n  column_spec(1:4, width = \"2cm\", color = 'white', background = '#005A43', include_thead = TRUE) |&gt;\n  column_spec(2:4, background = \"inherit\", color=\"inherit\") |&gt;\n  row_spec(1, extra_css = \"border-bottom: 1px solid\") |&gt;\n  kable_paper(\"hover\", bootstrap_options = c(\"condensed\", \"responsive\"), full_width = F, font_size = 20)\ntbl\n\n\n\n\n\n\nPredicted.Positive\nPredicted.Negative\nRate\n\n\n\n\nObserved Positive\n0\n897\nTPR=0/897=0\n\n\nObserved Negative\n0\n19,245\nFPR=0/19,245=0\n\n\n\n\n\n\n\nThe problem is \\(\\tau\\); at \\(\\tau=.5\\) we get none of the ones correct. Here’s the democratic peace at \\(\\tau = 0.1\\):\n\n\ncode\nlibrary(kableExtra)\n# opts &lt;- options(knitr.kable.NA = \"\")\ndf &lt;- data.frame(\"Predicted Positive\" = c(\"303\", \"1,764\"), \"Predicted Negative\" = c(\"594\", \"17,481\"), \"Rate\" = c(\"TPR=303/897=.34\", \"FPR=1,764/19,245=0.092\"))\nrow.names(df) &lt;- c(\"Observed Positive\", \"Observed Negative\")\n\ntbl &lt;- kbl(df, align = rep('c', 4)) |&gt; \n  column_spec(2:4, border_left = T) |&gt; \n  column_spec(1,  bold=T) |&gt;\n  column_spec(1:4, width = \"2cm\", color = 'white', background = '#005A43', include_thead = TRUE) |&gt;\n  column_spec(2:4, background = \"inherit\", color=\"inherit\") |&gt;\n  row_spec(1, extra_css = \"border-bottom: 1px solid\") |&gt;\n  kable_paper(\"hover\", bootstrap_options = c(\"condensed\", \"responsive\"), full_width = F, font_size = 20)\ntbl\n\n\n\n\n\n\nPredicted.Positive\nPredicted.Negative\nRate\n\n\n\n\nObserved Positive\n303\n594\nTPR=303/897=.34\n\n\nObserved Negative\n1,764\n17,481\nFPR=1,764/19,245=0.092\n\n\n\n\n\n\n\nAt \\(\\tau = 0.1\\), we get a TPR = 0.34 and FPR = 0.092. The PCP = (303+17481)/20142 = 0.88 …which is lower, but we get about 1/3 of the ones correct."
  },
  {
    "objectID": "binaryextensions124.html#percent-correctly-predicted-pcp",
    "href": "binaryextensions124.html#percent-correctly-predicted-pcp",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Percent Correctly Predicted (PCP)",
    "text": "Percent Correctly Predicted (PCP)\nOne thing we can do is sum the main diagonal and divide by the estimation sample: \\((TP+TN)/N\\). This gives us the Percent Correctly Predicted (PCP)."
  },
  {
    "objectID": "binaryextensions124.html#proportional-reduction-of-error-pre",
    "href": "binaryextensions124.html#proportional-reduction-of-error-pre",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Proportional Reduction of Error (PRE)}",
    "text": "Proportional Reduction of Error (PRE)}\nA second thing is to compute the Proportional Reduction of Error (PRE) - the difference between the correct predictions of the model versus a null model usually the unconditional frequency of \\(y\\) - in other words, we use the Percent in the Modal Category (PCM).\n\\[\\frac{correct_{\\text{informed}} - correct_{\\text{null}}}{N - correct_{\\text{null}}} \\]\nor\n\\[\\frac{PCP- PMC}{1-PMC} \\]"
  },
  {
    "objectID": "binaryextensions124.html#example-democratic-peace",
    "href": "binaryextensions124.html#example-democratic-peace",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Example: Democratic Peace",
    "text": "Example: Democratic Peace\nHere’s a basic democratic peace model:\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\ndpm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n\nstargazer(dpm1, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\nLet’s generate the confusion matrix for the democratic peace model, threshold set at \\(\\tau =0.5\\):\n\n\ncode\nlibrary(kableExtra)\n# opts &lt;- options(knitr.kable.NA = \"\")\ndf &lt;- data.frame(\"Predicted Positive\" = c(\"0\", \"0\"), \"Predicted Negative\" = c(\"897\", \"19,245\"), \"Rate\" = c(\"TPR=0/897=0\", \"FPR=0/19,245=0\"))\nrow.names(df) &lt;- c(\"Observed Positive\", \"Observed Negative\")\n\ntbl &lt;- kbl(df, align = rep('c', 4)) |&gt; \n  column_spec(2:4, border_left = T) |&gt; \n  column_spec(1,  bold=T) |&gt;\n  column_spec(1:4, width = \"2cm\", color = 'white', background = '#005A43', include_thead = TRUE) |&gt;\n  column_spec(2:4, background = \"inherit\", color=\"inherit\") |&gt;\n  row_spec(1, extra_css = \"border-bottom: 1px solid\") |&gt;\n  kable_paper(\"hover\", bootstrap_options = c(\"condensed\", \"responsive\"), full_width = F, font_size = 20)\ntbl\n\n\n\n\n\n\nPredicted.Positive\nPredicted.Negative\nRate\n\n\n\n\nObserved Positive\n0\n897\nTPR=0/897=0\n\n\nObserved Negative\n0\n19,245\nFPR=0/19,245=0\n\n\n\n\n\n\n\nThe problem is \\(\\tau\\); at \\(\\tau=.5\\) we get none of the ones correct. Here’s the democratic peace at \\(\\tau = 0.1\\):\n\n\ncode\nlibrary(kableExtra)\n# opts &lt;- options(knitr.kable.NA = \"\")\ndf &lt;- data.frame(\"Predicted Positive\" = c(\"303\", \"1,764\"), \"Predicted Negative\" = c(\"594\", \"17,481\"), \"Rate\" = c(\"TPR=303/897=.34\", \"FPR=1,764/19,245=0.092\"))\nrow.names(df) &lt;- c(\"Observed Positive\", \"Observed Negative\")\n\ntbl &lt;- kbl(df, align = rep('c', 4)) |&gt; \n  column_spec(2:4, border_left = T) |&gt; \n  column_spec(1,  bold=T) |&gt;\n  column_spec(1:4, width = \"2cm\", color = 'white', background = '#005A43', include_thead = TRUE) |&gt;\n  column_spec(2:4, background = \"inherit\", color=\"inherit\") |&gt;\n  row_spec(1, extra_css = \"border-bottom: 1px solid\") |&gt;\n  kable_paper(\"hover\", bootstrap_options = c(\"condensed\", \"responsive\"), full_width = F, font_size = 20)\ntbl\n\n\n\n\n\n\nPredicted.Positive\nPredicted.Negative\nRate\n\n\n\n\nObserved Positive\n303\n594\nTPR=303/897=.34\n\n\nObserved Negative\n1,764\n17,481\nFPR=1,764/19,245=0.092\n\n\n\n\n\n\n\nAt \\(\\tau = 0.1\\), we get a TPR = 0.34 and FPR = 0.092. The PCP = (303+17481)/20142 = 0.88 …which is lower, but we get about 1/3 of the ones correct."
  },
  {
    "objectID": "binaryextensions124.html#receiver-operator-characteristic-roc-curves",
    "href": "binaryextensions124.html#receiver-operator-characteristic-roc-curves",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Receiver-Operator Characteristic (ROC) Curves",
    "text": "Receiver-Operator Characteristic (ROC) Curves\nThe problem is choosing the threshold - imagine that we compute a confusion matrix for all possible thresholds, \\(\\tau=.01, .02, .03 \\ldots 1\\), then compute TPR and FPR, and plot them against one another. This is an ROC curve.\nROCs originate in efforts to distinguish signal from noise in radar returns - the British built a radar system before WWII to detect German air attacks; they had the problem of distinguishing planes (signal) from flocks of geese (noise). As the turned up the sensitivity of the radar, they more often correctly detected planes, but they also lacked specificity and detected more geese too. So there was a tradeoff between sensitivity (correctly identifying positive signals as positive) and specificity (incorrectly identify negative signals as positive).\nROCs measure these two dimensions and graph them against one another:\n\nsensitivity - true positive rate at every possible latent threshold between zero and one.\n1- specificity - false positive rate at every possible latent threshold between zero and one. This is 1 minus the True Negative Rate\n\nHere’s the ROC for the NAFTA model - we’ll use the pROC package in R to compute the ROC and plot it:\n\n\ncode\nlibrary(pROC)\n\n# NAFTA\n\nnaftaroc &lt;- roc(test_data$actual, predictions, plot=TRUE, grid=TRUE, partial.auc.correct=TRUE,\n         print.auc=TRUE)"
  },
  {
    "objectID": "binaryextensions124.html#roc-intepretation",
    "href": "binaryextensions124.html#roc-intepretation",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "ROC Intepretation",
    "text": "ROC Intepretation\n\nthe diagonal is a model guessing randomly zero or one - no better than a coin toss.\nabove that line, the model is improving our classification over random guesses.\nthe top left corner would indicate a model that classifies perfectly - 100% sensitivity (TPR), and 0% FPR.\nbelow the diagonal line, the model is classifying worse than a coin toss would.\nwe can compute the Area Under the Curve (AUC) as a percentage - AUC is often reported to indicate model fit. In the NAFTA model, the AUC is .843. AUC closer to one indicates better fit; closer to .5 indicates worse fit, similar to random guessing.\nwe can plot ROCs from different models on the same space and compare their fits.\nThe x-axis is 1-specificity, or the False Positive Rate."
  },
  {
    "objectID": "binaryextensions124.html#correctly-predicted",
    "href": "binaryextensions124.html#correctly-predicted",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Correctly Predicted",
    "text": "Correctly Predicted"
  },
  {
    "objectID": "binaryextensions124.html#roc-democratic-peace",
    "href": "binaryextensions124.html#roc-democratic-peace",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "ROC Democratic Peace",
    "text": "ROC Democratic Peace\nHere’s we compare fit for two models, one including “borders,” the other excluding it. Here are the two models :\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\ndpm1 &lt;-glm(dispute ~ border+deml+caprat+ally+border, family=binomial(link=\"logit\"), data=dp )\ndpm2 &lt;-glm(dispute ~ deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n  \nstargazer(list(dpm1,dpm2), type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n-0.078*** (0.007)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n-0.005*** (0.0005)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n-0.374*** (0.076)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n-2.979*** (0.064)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n7,262.635\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\nAnd compute the ROC for each model - here Claude.ai and I have written a function to compute the ROC and AUC for each model, and then plot them on the same space.\n\n\ncode\n# part written by Claude.ai\n# compute ROC curve\ncompute_roc &lt;- function(actual, predictions) {\n  # Sort actual and predictions in descending order of predicted probabilities\n  sorted_data &lt;- data.frame(actual, predictions)\n  sorted_data &lt;- sorted_data[order(sorted_data$predictions, decreasing = TRUE), ]\n\n  # Initialize variables\n  n_positive &lt;- sum(actual)\n  n_negative &lt;- length(actual) - n_positive\n  tp &lt;- 0\n  fp &lt;- 0\n  tpr &lt;- c()\n  fpr &lt;- c()\n\n  # Iterate over sorted data\n  for (i in 1:nrow(sorted_data)) {\n    if (sorted_data$actual[i] == 1) {\n      tp &lt;- tp + 1\n    } else {\n      fp &lt;- fp + 1\n    }\n\n    # Calculate true positive rate (TPR) and false positive rate (FPR)\n    tpr &lt;- c(tpr, tp / n_positive)\n    fpr &lt;- c(fpr, fp / n_negative)\n  }\n\n  # Create ROC curve\n  roc_curve &lt;- data.frame(fpr, tpr)\n  return(roc_curve)\n}\n\n# Function to compute AUC\ncompute_auc &lt;- function(fpr, tpr) {\n  # Sort FPR and TPR\n  ord &lt;- order(fpr)\n  fpr &lt;- fpr[ord]\n  tpr &lt;- tpr[ord]\n  \n  # Compute AUC using trapezoidal rule\n  auc &lt;- sum((fpr[-1] - fpr[-length(fpr)]) * (tpr[-1] + tpr[-length(tpr)]) / 2)\n  return(auc)\n}\n\n# Democratic Peace models\n\ntest_dataB &lt;- dp %&gt;% dplyr::select(border,deml,caprat,ally) %&gt;% \n  mutate(actual = dp$dispute)\n\ntest_dataNB &lt;- dp %&gt;% dplyr::select(deml,caprat,ally) %&gt;% \n  mutate(actual = dp$dispute)\n\n# Make predictions on the test data\npredictionsB &lt;- predict(dpm1, newdata = test_dataB, type = \"response\")\npredictionsNB &lt;- predict(dpm2, newdata = test_dataNB, type = \"response\")\n\n\nroc_curveB &lt;- compute_roc(test_dataB$actual, predictionsB)\n#AUC\nauc_border &lt;- compute_auc(roc_curveB$fpr, roc_curveB$tpr)\nroc_curveNB &lt;- compute_roc(test_dataNB$actual, predictionsNB)\nauc_noborder &lt;- compute_auc(roc_curveNB$fpr, roc_curveNB$tpr)\n\n# ROC Plot, pasting auc_border and auc_noborder on plot\n\nggplot() +\n  geom_line(data = roc_curveB, aes(x = fpr, y = tpr, color = \"Borders\")) +\n  geom_line(data = roc_curveNB, aes(x = fpr, y = tpr, color = \"No Borders\")) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"solid\", color = \"red\") +\n  labs(title = \"ROC Curve: Democratic Peace\", x = \"False Positive Rate\", y = \"True Positive Rate\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"Borders\" = \"#005A43\", \"No Borders\" = \"#6CC24A\"))+\n  annotate(\"text\", x = 0.75, y = 0.5, label = paste(\"AUC Model 1: \", round(auc_border, 2)), color = \"#005A43\") +\n  annotate(\"text\", x = 0.75, y = 0.4, label = paste(\"AUC Model 2: \", round(auc_noborder, 2)), color = \"#6CC24A\")\n\n\n\n\n\n\n\n\n\ncode\n#bucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nAlso, note the measure Area Under the Curve (AUC) for each model - the AUC is often reported to indicate model fit. The AUC for the model including borders is 0.75, while the AUC for the model excluding borders is 0.72. A model with an AUC of 0.5 is no better than a coin toss, while a model with an AUC of 1 is perfect."
  },
  {
    "objectID": "binaryextensions124.html#single-coefficient-estimates",
    "href": "binaryextensions124.html#single-coefficient-estimates",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Single Coefficient Estimates",
    "text": "Single Coefficient Estimates\nOne property of MLEs is they are asymptotically multivariate normal; inference is straightforward because the variances are also normal so the ratio of \\(\\beta /se\\) is a z-score."
  },
  {
    "objectID": "binaryextensions124.html#model-evaluation",
    "href": "binaryextensions124.html#model-evaluation",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nMost commonly, we evaluate model fit using one of the “trinity” of tests:\n\nlog-likelihood ratio tests (LLR)\nWald tests\nLagrange Multiplier tests (LM)\n\nThe first two are the most common, and it’s not clear one is better than the other.\n\nLog-Likelihood Ratio Test\nThe LLR test requires estimating two models - a null or constrained model, (\\(M_0\\)), and informed (unconstrained) model (\\(M_1\\)) - it compares the heights of the log-likelihood functions of the two models:\n\\[ \\chi^2 = -2 (ln\\mathcal{L}(M_0) - ln\\mathcal{L}(M_1))  \\]\nThe log-likelihoods here are literally the values of the \\(ln\\mathcal{L}\\) at the estimated maxima of the functions. Their difference is distributed \\(\\chi^2\\) with \\(k_1-k_0\\) degrees of freedom.\nThe LLR is simple to compute (you can do it in your head), but requires estimating two nested models. Recall, two models are nested iff the regressors in the constrained model are a strict subset of those in the unconstrained model, and the samples are identical.\n\n\nWald \\(\\chi^2\\)\nThe Wald test is similar, but only requires the unconstrained or informed model. During maximization, it examines the distance between \\(M_0\\) and \\(M_1\\), and weights that distance by the rate of change between the two (second derivative). If the distance is large and the rate of change is fast, the informed model improves a good bit on the uninformed one. You can imagine other combinations of distance and curvature. Long (p. 88) has a great illustration of this.\n\n\nIn Practice\nThese two tests are asymptotically equivalent. In practice, it makes little difference in most cases which you choose, provided the models are nested.\nStata reports LLR for most models, but reports Wald if you request robust standard errors.\n\n\nLimits\nThe limits of these tests is they apply only to nested models - models where the regressors in the constrained model are a strict subset of those in the unconstrained model and where the samples are identical.\n\n\nInformation Criterion Tests\nAlternatively, we can use information criterion tests - the two most common are the Akaike and Bayesian Information Criteria tests (AIC, BIC). These are both formulated to penalize likelihoods for the number of parameters estimated; this in effect rewards better specification (good variables, but few) and penalizes “garbage can” approaches (including lots of poor predictors).\nIC tests are useful for either nested or nonnested models. This is a significant though under-appreciated virtue of such tests.\n\n\nAkaike and Bayesian Information Criterion tests\n\\[AIC =  -2 ln(\\mathcal{L}) + 2k \\]\n\\[BIC =  -2 ln(\\mathcal{L}) + ln(N) k \\]\nwhere \\(k\\) is the number of parameters.\n\nProcess\nAIC: Estimate model 1; generate the AIC. Estimate model 2; generate the AIC. The model with the smaller AIC is the preferred model (see Long 1997: 110).\nBIC: Estimate model 1; generate the BIC. Estimate model 2; generate the BIC. Compute \\(BIC_1 - BIC_2\\) - the smaller BIC value is the preferable model. The strength of the test statistic is given by Rafferty (1996): absolute value of this difference 0-2 = weak; 2-6= Positive; 6-10= Strong; greater than 10 = Very Strong (see Long 1997: 112)."
  },
  {
    "objectID": "binaryextensions124.html#rare-events-logit",
    "href": "binaryextensions124.html#rare-events-logit",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Rare Events Logit",
    "text": "Rare Events Logit\nWhat they propose is:\n\nSelect all the cases with events (failures).\nRandomly choose a sample of the non-event (censored) cases (they say 2-5 times the size of the failure group).\nThis smaller sample makes data collection possible (compared to the gigantic number of zeros in some event data).\nEstimate a logit on the new, smaller sample, and adjust the estimates for the sample.\n\nThe Rare Events Logit doesn’t appear in the literature much, though it’s not uncommon for reviewers to ask for it. In my experience inferences from this model don’t vary much from the usual logit. The model does present a major opportunity for data collection efforts."
  },
  {
    "objectID": "binaryextensions124.html#example-nafta-vote-1993",
    "href": "binaryextensions124.html#example-nafta-vote-1993",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Example: NAFTA vote, 1993",
    "text": "Example: NAFTA vote, 1993\nSo here’s a model predicting members of the US House votes for or against the NAFTA treaty in 1993: 1 is a “yes” vote; the \\(x\\) variables are party (Democrat) and an ideology score.\n\n\ncode\nnafta &lt;- read.csv(\"NAFTA.csv\")\n\nnaftamodel &lt;- glm(vote ~ democrat + cope93, data=nafta, family=binomial(link=\"logit\"))\n\nstargazer(naftamodel, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: NAFTA vote\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Party (Democrat=1)\", \"Ideology (COPE score)\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: NAFTA vote\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParty (Democrat=1)\n\n\n1.990*** (0.504)\n\n\n\n\nIdeology (COPE score)\n\n\n-0.054*** (0.007)\n\n\n\n\nConstant\n\n\n2.380*** (0.272)\n\n\n\n\n\n\n\n\nObservations\n\n\n434\n\n\n\n\nAkaike Inf. Crit.\n\n\n465.904\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\nAnd here’s the confusion matrix from that model - it compares the observed vote frequency against how we classify our predictions based on a predicted probability greater than or less than 0.5.\n\n\ncode\n# Load required libraries\nlibrary(pROC)\nlibrary(caret)\n\n# Assuming you have a fitted GLM object called 'glm_model'\n# and test data 'test_data' with actual outcomes in 'test_data$actual'\n\ntest_data &lt;- nafta %&gt;% dplyr::select(vote, democrat, cope93) %&gt;% mutate(actual = vote)\n# Make predictions on the test data\npredictions &lt;- predict(naftamodel, newdata = test_data, type = \"response\")\n\n# Create confusion matrix\n# You may need to adjust the threshold (default is 0.5)\npredicted_classes &lt;- ifelse(predictions &gt; 0.5, 1, 0)\nconf_matrix &lt;- confusionMatrix(factor(predicted_classes), factor(test_data$actual))\n\n# Print confusion matrix\nprint(conf_matrix)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 160  77\n         1  40 157\n                                         \n               Accuracy : 0.7304         \n                 95% CI : (0.686, 0.7716)\n    No Information Rate : 0.5392         \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      \n                                         \n                  Kappa : 0.4647         \n                                         \n Mcnemar's Test P-Value : 0.0008741      \n                                         \n            Sensitivity : 0.8000         \n            Specificity : 0.6709         \n         Pos Pred Value : 0.6751         \n         Neg Pred Value : 0.7970         \n             Prevalence : 0.4608         \n         Detection Rate : 0.3687         \n   Detection Prevalence : 0.5461         \n      Balanced Accuracy : 0.7355         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\ncode\n# # Create ROC curve\n# roc_obj &lt;- roc(test_data$actual, predictions)\n# \n# # Plot ROC curve\n# plot(roc_obj, main = \"ROC Curve\", col = \"blue\", lwd = 2)\n# abline(a = 0, b = 1, lty = 2, col = \"red\")\n# \n# # Compute AUC\n# auc_value &lt;- auc(roc_obj)\n# print(paste(\"AUC:\", round(auc_value, 4)))\n\n\n\n\ncode\n# Function to compute ROC curve\ncompute_roc &lt;- function(actual, predictions) {\n  # Sort actual and predictions in descending order of predicted probabilities\n  sorted_data &lt;- data.frame(actual, predictions)\n  sorted_data &lt;- sorted_data[order(sorted_data$predictions, decreasing = TRUE), ]\n\n  # Initialize variables\n  n_positive &lt;- sum(actual)\n  n_negative &lt;- length(actual) - n_positive\n  tp &lt;- 0\n  fp &lt;- 0\n  tpr &lt;- c()\n  fpr &lt;- c()\n\n  # Iterate over sorted data\n  for (i in 1:nrow(sorted_data)) {\n    if (sorted_data$actual[i] == 1) {\n      tp &lt;- tp + 1\n    } else {\n      fp &lt;- fp + 1\n    }\n\n    # Calculate true positive rate (TPR) and false positive rate (FPR)\n    tpr &lt;- c(tpr, tp / n_positive)\n    fpr &lt;- c(fpr, fp / n_negative)\n  }\n\n  # Create ROC curve\n  roc_curve &lt;- data.frame(fpr, tpr)\n  return(roc_curve)\n}\n\n# Example usage\n\ntest_data &lt;- nafta %&gt;% dplyr::select(vote, democrat, cope93) %&gt;% mutate(actual = vote)\n# Make predictions on the test data\npredictions &lt;- predict(naftamodel, newdata = test_data, type = \"response\")\n\n\nroc_curve &lt;- compute_roc(test_data$actual, predictions)\n\n# Plot ROC curve\nplot(roc_curve$fpr, roc_curve$tpr, type = \"l\", xlab = \"False Positive Rate\", ylab = \"True Positive Rate\")\n\n# Add diagonal line\nabline(a = 0, b = 1, lty = 2, col = \"red\")\n\n# Add AUC to plot\n\nauc_value &lt;- auc(roc_curve$fpr, roc_curve$tpr)\ntext(0.5, 0.5, paste(\"AUC =\", round(auc_value, 2)), pos = 4)\n\n\n\n\n\n\n\n\n\ncode\n#use ggplot to plot the ROC curve\n\nroc_curve &lt;- data.frame(fpr = roc_curve$fpr, tpr = roc_curve$tpr)\n\nggplot(roc_curve, aes(x = fpr, y = tpr)) +\n  geom_line() +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"ROC Curve\", x = \"False Positive Rate\", y = \"True Positive Rate\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nYou’ll note these examples using data on the US House vote on the NAFTA treaty make some sense - these measures of goodness of fit tell us how much our covariates improve classification.\nWhat makes this work in the NAFTA context? Because the next example using the democratic peace data, does not work so well."
  },
  {
    "objectID": "binaryextensions124.html#footnotes",
    "href": "binaryextensions124.html#footnotes",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nsee what I did there?↩︎"
  },
  {
    "objectID": "discretehazards24.html",
    "href": "discretehazards24.html",
    "title": "Discrete Time Hazard Models",
    "section": "",
    "text": "consider what “memory” might look like in a binary time series setting.\nintroduce concepts underlying hazard models.\nunderstand the discrete time hazard model."
  },
  {
    "objectID": "discretehazards24.html#memoryless",
    "href": "discretehazards24.html#memoryless",
    "title": "Discrete Time Hazard Models",
    "section": "Memoryless",
    "text": "Memoryless\nBy construction, this model lacks any memory. \\(Pr(Y_{i,t}=1)\\) is a function of \\(X_{i,t}\\), but is independent of anything that happened prior to \\(t\\).\nThis means dyads that have been at peace for 2 years and for 22 years are treated as the same, varying only on the \\(X\\) variables.\nNote this is by choice - we’re assuming \\(y_t\\) has no bearing on \\(y_{t+1}\\), so there is no persistence or memory from period to period. We’ll encounter a range of models that are explicitly aimed at understanding this question: How does having survived up until now affect the chances of failing now?"
  },
  {
    "objectID": "discretehazards24.html#time",
    "href": "discretehazards24.html#time",
    "title": "Discrete Time Hazard Models",
    "section": "Time",
    "text": "Time\nWhat would data for mortality (or any sort of spells) look like? There are two basic types:\nSurvival:\n\\[Age = 1, 2, 3, 4, 5, \\ldots 87, 88, 89\\]\nor\nFailure:\n\\[Death = 0, 0, 0, 0, 0, \\ldots 0, 0, 1\\]\nNote that these convey the two parts of the hazard - how many periods the individual survives, and at what period the individual fails.\nThese also represent two ways to think about time: continuously:\n\\[Age = 1, 2, 3, 4, 5, \\ldots 87, 88, 89\\]\nor discretely:\n\\[Death = 0, 0, 0, 0, 0, \\ldots 0, 0, 1\\]\nIn truth, we almost always measure time discretely in the sense that failure can only happen in certain intervals (days, weeks, years, etc.), even though time is continuous and failure can happen in much smaller increments than these (e.g. minutes, seconds). Still, most treatments consider two types of hazard models:\n\nContinuous time models using data like \\(Age = 1, 2, 3, 4, 5, \\ldots 87, 88, 89\\) as the \\(y\\) variable.\nDiscrete time models using data like \\(Death = 0, 0, 0, 0, 0, \\ldots 0, 0, 1\\) as the \\(y\\) variable. These are usually estimated using a binomial LLF (e.g. the logit model)."
  },
  {
    "objectID": "discretehazards24.html#hazard-models",
    "href": "discretehazards24.html#hazard-models",
    "title": "Discrete Time Hazard Models",
    "section": "Hazard models",
    "text": "Hazard models\nA key feature of any hazard model is that the model accounts for both time until failure, and the realization of failure. In continuous or discrete time models, both of these are part of the estimation.\n\n\n\n\n\n\nNote\n\n\n\nAn aside on naming - models like these are interchangeably called “hazard models,” “survival models,” “duration models,” or “event history models.” They all refer to the same basic idea - modeling the time until an event occurs. They can sometimes indicate whether the quantity of interest is the hazard or survival - note that these are opposites in the sense that the hazard is the probability of failure at a particular point in time, while survival is the probability of surviving up to that point in time. “Event history” often refers to discrete time models."
  },
  {
    "objectID": "discretehazards24.html#binary-time-series-cross-section-data",
    "href": "discretehazards24.html#binary-time-series-cross-section-data",
    "title": "Discrete Time Hazard Models",
    "section": "Binary Time Series Cross Section data",
    "text": "Binary Time Series Cross Section data\nIt’s common to have binary \\(y\\) variables observed for cross sections over time - these are Binary Time Series Cross Section (BTSCS) data. This is the form the democratic peace data takes, and is a common form of data in the social sciences. BTSCS data are grouped duration data, and failure is measured in discrete time.\nHere’s an example of BTSCS data thinking of disputes in dyads over time.\n\n\n\n\nBTSCS data\n\n\n\n\n\n\n\n\n\nState\nYear\nDispute\nCensored\n\n\n\nUS-Cuba\n1960\n0\n0\n\n\n\nUS-Cuba\n1961\n1\n0\n\n\n\nUS-Cuba\n1962\n0\n0\n\n\n\nUS-Cuba\n1963\n0\n0\n\n\n\nUS-Cuba\n1964\n0\n0\n\n\n\nUS-Cuba\n1965\n0\n0\n\n\n\nUS-Cuba\n1966\n0\n0\n\n\n\nUS-Cuba\n1967\n1\n0\n\n\n\nUS-Cuba\n1968\n0\n0\n\n\n\nUS-Cuba\n1969\n0\n0\n\n\n\nUS-Cuba\n1970\n0\n1\n\n\n\nUS-UK\n1960\n0\n0\n\n\n\nUS-UK\n1961\n0\n0\n\n\n\nUS-UK\n1962\n0\n0\n\n\n\nUS-UK\n1963\n0\n0\n\n\n\nUS-UK\n1964\n0\n0\n\n\n\nUS-UK\n1965\n0\n0\n\n\n\nUS-UK\n1966\n0\n0\n\n\n\nUS-UK\n1967\n0\n0"
  },
  {
    "objectID": "discretehazards24.html#terminology",
    "href": "discretehazards24.html#terminology",
    "title": "Discrete Time Hazard Models",
    "section": "Terminology",
    "text": "Terminology\nLet’s begin thinking about terminology:\n\nwe observe at each point \\(t\\) whether a unit fails or not. Failure means experiencing the event of interest. In mortality studies, this is is death; in the democratic peace data, the event is a militarized dispute.\neach unit is at risk until it exits the data either because the period of observation ends, or because it fails and can only fail once. In mortality studies, an individual can only fail once; in the democratic peace data, a dyad can fail multiple times.\na unit survives some spell up to the point at which it fails. We can count these time periods to measure survival time.\nthe period a unit survives is called a spell; spells end at failure.\nwe have no idea what happened to these units prior to 1960; the units are left-censored.\nwe have no idea what happens to these units after 1970; the units are right censored. Any unit that does not experience the failure event during the period of study is right-censored."
  },
  {
    "objectID": "discretehazards24.html#spells",
    "href": "discretehazards24.html#spells",
    "title": "Discrete Time Hazard Models",
    "section": "Spells",
    "text": "Spells\nHere are different spells:\n\n\ncode\nlibrary(tidyverse)\nlibrary(highcharter)\n\n# Binghamton University colors\nbinghamton_colors &lt;- c(\"#005A43\", \"#8C2132\", \"#FFD100\", \"#000000\", \"#636466\")\n\n# Create dataframes for each case with updated labels\ncases &lt;- list(\n  list(x = c(3, 6), y = c(1, 1), name = \"uncensored\"),\n  list(x = c(-0.5, 2.5), y = c(2, 2), name = \"left censored\"),\n  list(x = c(2.8, 8), y = c(3, 3), name = \"fails at last period\"),\n  list(x = c(3.5, 10), y = c(4, 4), name = \"right censored\"),\n  list(x = c(5.5, 7), y = c(5, 5), name = \"uncensored\")\n)\n\n# Create the plot\nhc &lt;- highchart() %&gt;%\n  hc_chart(type = \"line\") %&gt;%\n  hc_xAxis(\n    title = list(text = \"time\"),\n    plotLines = list(\n      list(value = 2, width = 2, color = \"black\"),\n      list(value = 8, width = 2, color = \"black\")\n    ),\n    min = 0,\n    max = 10\n  ) %&gt;%\n  hc_yAxis(\n    title = list(text = \"case\"),\n    min = 0,\n    max = 5,\n    tickInterval = 1\n  ) %&gt;%\n  hc_plotOptions(\n    series = list(\n      lineWidth = 3,\n      marker = list(enabled = FALSE)\n    )\n  ) %&gt;%\n  hc_legend(enabled = FALSE)\n\n# Add each case as a separate series with Binghamton colors\nfor (i in seq_along(cases)) {\n  hc &lt;- hc %&gt;% hc_add_series(\n    data = list_parse2(data.frame(x = cases[[i]]$x, y = cases[[i]]$y)),\n    name = cases[[i]]$name,\n    color = binghamton_colors[i]\n  )\n}\n\n# Display the plot\nhc\n\n\n\n\n\n\n\nsome units survive through the end of the study; these units are right-censored. That is, they do not fail during the period of observation.\nfailure is only observed per year; so failure is grouped by year; these are grouped duration data. We could, for instance, graph the density of failures at each point in time, effectively grouping them by failure time.\nthe chances of failing at \\(t\\), given survival til \\(t\\) is the hazard of failure; at any point in time, this is called the hazard rate, denoted \\(h(t)\\).\nin the model above, \\(h(t)\\) does not depend on what happened at \\(t-1\\), so \\(h(t)\\) is constant over time or is time invariant, or is duration independent."
  },
  {
    "objectID": "discretehazards24.html#survival-spells",
    "href": "discretehazards24.html#survival-spells",
    "title": "Discrete Time Hazard Models",
    "section": "Survival Spells",
    "text": "Survival Spells\nWe can measure survival spells; time elapsed until failure or censoring. These are the same data as above, just re-formed so the units are different. Note the summed survival time is equal to the total time at risk. So for the US-Cuba dyad, the total time at risk is 11 years. Also, notice that the US-Cuba dyad is censored in 1970. It survives 3 years since its last dispute, but the end of that spell is our observation period, not another dispute.\n\n\n\n\nSpell data\n\n\n\n\n\n\n\n\n\n\nState\nYear\nDispute\nfail\ncensored\nsurvival\n\n\nUS-Cuba\n1961\n1\n1\n0\n2\n\n\nUS-Cuba\n1967\n1\n1\n0\n6\n\n\nUS-Cuba\n1970\n0\n0\n1\n3\n\n\nUS-UK\n1970\n0\n0\n1\n11"
  },
  {
    "objectID": "discretehazards24.html#why-not-use-ols",
    "href": "discretehazards24.html#why-not-use-ols",
    "title": "Discrete Time Hazard Models",
    "section": "Why not use OLS?",
    "text": "Why not use OLS?\nIt looks almost as if we could estimate a linear regression of the survival variable. Time is continuous, after all. Problems with doing so include:\n\nwe can’t have negative survival time.\nfailing at \\(t = 8\\) is conditional on having survived until \\(t=8\\); can’t include this in a linear model.\nsome observations never fail during the period of observation (are censored).\nthe outcome variable of interest is latent - it’s the hazard rate."
  },
  {
    "objectID": "discretehazards24.html#discrete-time-hazards",
    "href": "discretehazards24.html#discrete-time-hazards",
    "title": "Discrete Time Hazard Models",
    "section": "Discrete Time Hazards",
    "text": "Discrete Time Hazards\nUntil the late 1990s, studies using BTSCS data ignored memory. Put differently, the conventional way to model these data was using a binomial model like the logit. Beck, Katz, and Tucker’s (1998) paper pointed out the problems with doing this, and suggested an easy fix.\n\nThe problem\nThe standard logit/probit model in these data assumes the errors are i.i.d. - that the disturbances are uncorrelated. A somewhat more interesting observation is that the model assumes no relationship between the outcome at \\(t\\) and the outcome at \\(t-1, t-2 \\ldots t-k\\). So the observations on \\(y\\) arise independently of one another …almost as if each observation is an independent Bernoulli trial. The model is misspecified, and likely the parameter estimates are biased.\n\n\nThe solution\nInclude “survival time” as a right hand side variable. Doing so explicitly models the effect of surviving up til \\(t\\) on the probability of failing at \\(t\\).\nBKT suggest including a function of survival time so the effect of time isn’t constrained to be monotonic. They suggested using cubic splines of survival time; Carter and Signorino (2010) later show polynomials for survival time are just as good and easier to compute/understand.\n\n\nThe result\nThis fundamentally changed models on BTSCS data - the state of the art is to include survival time, thereby measuring “memory” in the \\(y\\) series. While most BTSCS models since Beck, Katz, and Tucker (1998) include survival time, relatively few interpret it; that’s okay insofar as the effect of survival might not be of theoretical interest. Most incorrectly interpret the predictions as probabilities - they are hazards."
  },
  {
    "objectID": "discretehazards24.html#survival-time",
    "href": "discretehazards24.html#survival-time",
    "title": "Discrete Time Hazard Models",
    "section": "Survival time",
    "text": "Survival time\nSurvival time: the time up to failure, in the interval \\(t_0, t_{\\infty}\\) such that \\(t \\in \\{1,2,3 \\ldots t_{\\infty} \\}\\)"
  },
  {
    "objectID": "discretehazards24.html#failure",
    "href": "discretehazards24.html#failure",
    "title": "Discrete Time Hazard Models",
    "section": "Failure",
    "text": "Failure\nThe probability of the failure event:\n\\[\\begin{aligned}\nf(t) = Pr(t_i=t) \\nonumber\n\\end{aligned}\\]\nThis is the density."
  },
  {
    "objectID": "discretehazards24.html#cumulative-function",
    "href": "discretehazards24.html#cumulative-function",
    "title": "Discrete Time Hazard Models",
    "section": "Cumulative Function",
    "text": "Cumulative Function\nWrite the cumulative probability of failure up to \\(t_i\\).\n\\[\\begin{aligned}\nF(t) = \\sum_{i=1}^{\\infty} f(t_i) \\nonumber\n\\end{aligned}\\]\nNow, consider the probability of surviving up until \\(t\\) - this is equal to 1 minus the CDF, so\n\\[S(t) = 1-F(t) = P(t_{i} \\geq t)\\]\nMost importantly, the conditional probability given by the probability of failing at \\(t_i\\) given survival up until \\(t_i\\):\n\\[\\begin{aligned}\nh(t)=Pr(t=t_i | t \\geq t_i) \\nonumber \\\\\n= \\frac{f(t)}{S(t)} \\nonumber\n\\end{aligned}\\]\nThis is the hazard rate."
  },
  {
    "objectID": "discretehazards24.html#hazard-rate",
    "href": "discretehazards24.html#hazard-rate",
    "title": "Discrete Time Hazard Models",
    "section": "Hazard Rate",
    "text": "Hazard Rate\n\\[\\begin{aligned}\nh(t)=Pr(t=t_i | t \\geq t_i) \\nonumber \\\\\n= \\frac{f(t)}{S(t)} \\nonumber\n\\end{aligned}\\]\nThe hazard rate is conceptually important because it explicitly relates the past to the present, thereby incorporating memory into the statistical model. The hazard is different from \\(Pr(y_t=1)\\) because it conditions on what has happened prior to \\(t\\)."
  },
  {
    "objectID": "discretehazards24.html#discrete-time-ht",
    "href": "discretehazards24.html#discrete-time-ht",
    "title": "Discrete Time Hazard Models",
    "section": "Discrete time h(t)",
    "text": "Discrete time h(t)\nWhat we have derived is the discrete time hazard function - time is measured in discrete units (e.g. years, not parts of years like months or days); some processes only make sense in discrete terms - e.g. a member of the US House can only be turned out by voters every two years, not before."
  },
  {
    "objectID": "discretehazards24.html#density",
    "href": "discretehazards24.html#density",
    "title": "Discrete Time Hazard Models",
    "section": "Density",
    "text": "Density\nSince the probability of survival at some value of \\(t\\) is the probability of survival at \\(t\\) given survival up to \\(t\\), the conditional probability of survival is 1 minus the hazard rate:\n\\[Pr(t_j&gt;t | t_j\\geq t) = 1 - h(t)\\]\nWe can rewrite the survivor function as a product of the probabilities of surviving up to \\(t\\):\n\\[S(t) = \\prod_{j=0}^{t} \\{1-h(t-j)\\}\\]\nWe can rewrite the density \\(f(t)\\):\n\\[f(t) = h(t)S(t)\\]"
  },
  {
    "objectID": "discretehazards24.html#estimation",
    "href": "discretehazards24.html#estimation",
    "title": "Discrete Time Hazard Models",
    "section": "Estimation",
    "text": "Estimation\nLet’s build a likelihood - as you might have guessed, it needs to involve \\(f(t)\\) and \\(S(t)\\) (failure and survival times) so we can estimate \\(h(t)\\).\n\\[ \\mathcal{L} = \\prod_{t_i\\leq t} f(t_i) \\prod_{t_i\\geq t} S(t_i) \\]\nthen, think of censoring where \\(y_{i,t}\\) indicates when, and whether a subject ever fails; if zero, censored, if one, uncensored (fails during our period of observation).\n\\[ \\mathcal{L} = \\prod_{t_i\\leq t} f(t_i)^{y_{i,t}} \\prod_{t_i\\geq t} S(t_i)^{1- y_{i,t}} \\]\nThis should be looking familiar.\nNow, substituting:\n\\[ \\mathcal{L} = \\prod_{i=1}^{N} \\Bigg\\{ h(t) \\prod_{j=1}^{t-1}   [1-h(t-i)] \\Bigg\\} ^{y_{i,t}} \\Bigg\\{ \\prod_{j=1}^{t}   [1-h(t-i)] \\Bigg\\}^{1- y_{i,t}}\\]\nAnd substitute an appropriate link density for \\(f(t)\\) and \\(S(t)\\), e.g., exponential,\n\\[f(t) = \\lambda(t) exp^{\\lambda(t)}\\] \\[S(t) = exp^{-\\lambda(t)}\\] \\[h(t) = \\lambda\\]\nWeibull: \\[f(t) = \\lambda p (\\lambda(t))^{p-1} exp^{-(\\lambda t)^p}\\] \\[S(t) = exp^{-(\\lambda t)^p}\\] \\[h(t) = \\lambda p (\\lambda t)^{p-1}\\]\netc …"
  },
  {
    "objectID": "discretehazards24.html#constant-ht---no-memory",
    "href": "discretehazards24.html#constant-ht---no-memory",
    "title": "Discrete Time Hazard Models",
    "section": "Constant \\(h(t)\\) - no memory",
    "text": "Constant \\(h(t)\\) - no memory\nRevisiting …\nThis is the case where\n\\[h_0(t) = \\frac{1}{1+ exp(\\beta_0)}\\]\nthe baseline hazard is the constant. Even with \\(x\\) variables, there is still no accounting for time - the \\(x\\) effects are only shifts around this baseline hazard.\n\\[h_0(t) = \\frac{1}{1+ exp(\\beta_0+ x'\\beta)}\\]\nthis is still a constant baseline hazard with the effects of \\(x\\) deviating around it."
  },
  {
    "objectID": "discretehazards24.html#non-constant-ht",
    "href": "discretehazards24.html#non-constant-ht",
    "title": "Discrete Time Hazard Models",
    "section": "Non-constant \\(h(t)\\)",
    "text": "Non-constant \\(h(t)\\)\nSo how to deal with this, incorporating memory: thinking in terms of hazards rather than probabilities (i.e., conditional rather than unconditional probabilities), what if we measure survival time?\n\nThe binary \\(y\\) variable is an indicator of failure at \\(t\\); the model estimates \\(f(t)\\), which we’ve said is not especially informative since subjects might fail before \\(t\\).\nThink of the number of periods up to failure as the cumulative survival time, \\(S(t)\\).\n\nSee how we’re starting to construct the hazard rate by its parts."
  },
  {
    "objectID": "discretehazards24.html#measuring-survival-time",
    "href": "discretehazards24.html#measuring-survival-time",
    "title": "Discrete Time Hazard Models",
    "section": "Measuring survival time",
    "text": "Measuring survival time\n\ncount periods of survival up to failure. This is a counter of survival time. generate a binary variable for each survival period.\nEither include those survival dummies in the logit, or include the survival counter itself with polynomials, e.g. \\(t^2, t^3, \\ldots\\).\ninterpret those coefficients as baseline hazards for groups that survive to \\(t_i\\).\nwith all \\(x\\) variables set to zero, the probability of failure is now given by the constant and the appropriate dummy or counter coefficient.\nNote the quantity of interest is not constant across time: it’s a conditional probability; the probability of failing at \\(t\\) given the estimated probability of survival through \\(t-1\\) - \\(h(t)|S(t)\\)."
  },
  {
    "objectID": "discretehazards24.html#understanding-substantive-variables-in-the-hazard-context",
    "href": "discretehazards24.html#understanding-substantive-variables-in-the-hazard-context",
    "title": "Discrete Time Hazard Models",
    "section": "Understanding substantive variables in the hazard context",
    "text": "Understanding substantive variables in the hazard context\nThe survival variables now permit the baseline hazard to vary. The effects of \\(x\\) variables can be thought of as deviations from those baseline hazards. For example, think about the models presented above, but with democracy. The estimates on democracy will shift the baseline hazard up or down.\n\n\ncode\nsurvivaldp &lt;- survivaldp %&gt;% mutate(stime2=stime^2, stime3=stime^3)\n\ndpm4 &lt;-glm(dispute ~ stime+stime2+stime3+deml, family=binomial(link=\"logit\"), data=survivaldp )\n\nstargazer(dpm4, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Survival Time\", \"Survival Time2\", \"Survival Time3\", \"Democracy\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Time\n\n\n-0.823*** (0.035)\n\n\n\n\nSurvival Time2\n\n\n0.051*** (0.004)\n\n\n\n\nSurvival Time3\n\n\n-0.001*** (0.0001)\n\n\n\n\nDemocracy\n\n\n-0.065*** (0.007)\n\n\n\n\nConstant\n\n\n-1.298*** (0.065)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n5,650.709\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\n\ncode\ndpm4 &lt;-glm(dispute ~ stime+stime2+stime3+deml+caprat, family=binomial(link=\"logit\"), data=survivaldp )\n\n# copy estimation data for avg effects\ndppred &lt;- survivaldp\n\n# df for output\ndf &lt;- data.frame(time=seq(1,34,1))\nfor (d in seq(-10,10,10)) {\n    df[paste0(\"p\", d+10)] &lt;- NA\n  df[paste0(\"Se\", d+10)] &lt;- NA\n}\n\n# predictions\nfor (d in seq(-10,10,10)) {\n  dppred$deml &lt;- d\n for (t in seq(1,34,1)) {  \n  dppred$stime &lt;- t\n  dppred$stime2 &lt;- t^2 \n  dppred$stime3 &lt;- t^3\n  pred &lt;- predict(dpm4, newdata=dppred, type=\"response\", se=TRUE)\n  df[t, paste0(\"p\", d+10)] &lt;- mean(pred$fit, na.rm=TRUE)\n  df[t, paste0(\"Se\", d+10)] &lt;- mean(pred$se.fit, na.rm=TRUE)\n  df$time[t] &lt;- t\n}\n}\n\n# plot\nggplot(df, aes(x=time, y=p0)) +\n  geom_line(color=\"#005A43\") +\n  geom_ribbon(aes(ymin=p0-1.96*Se0, ymax=p0+1.96*Se0), fill=\"#6CC24A\", alpha=0.4) +\n  geom_line(aes(y=p10), color=\"#005A43\") +\n  geom_ribbon(aes(ymin=p10-1.96*Se10, ymax=p10+1.96*Se10), fill=\"#A7DA92\", alpha=0.4) +\n  geom_line(aes(y=p20), color=\"#005A43\") +\n  geom_ribbon(aes(ymin=p20-1.96*Se20, ymax=p20+1.96*Se20), fill=\"#005A43\", alpha=0.4) +\n  labs(x=\"Time since last dispute\", y=\"Pr(Dispute)\") +\n  theme_minimal()"
  },
  {
    "objectID": "discretehazards24.html#summary",
    "href": "discretehazards24.html#summary",
    "title": "Discrete Time Hazard Models",
    "section": "Summary",
    "text": "Summary\n\nWe relaxed the assumption of temporal independence.\nDid so by conceiving of the QI as a hazard rather than a probability.\nBuilt a model that estimates \\(h(t)\\) such that we don’t have to assume \\(h(t)\\) is constant.\nHave permitted memory in the model such that the past can shape the present."
  },
  {
    "objectID": "discretehazards24.html#temporal-independence",
    "href": "discretehazards24.html#temporal-independence",
    "title": "Discrete Time Hazard Models",
    "section": "Temporal Independence",
    "text": "Temporal Independence\nIn this model, \\(Pr(y_i=1) = F(x_i\\beta + \\beta_0)\\), \\(x_i \\beta\\) induces deviation from the constant or baseline level; but there is no temporal variation, temporal persistence, or memory. What happened last year has no bearing on what we observe this year. Repeating, it is as if these are Bernoulli trials.\n\nAn alternative - transitions\nWhat happens if we lag \\(y\\) as we might with a continuous variable (say, in OLS), such that the estimation model is\n\\[y_t = \\beta_0 + \\beta_1(x_1) + \\ldots + \\gamma(y_{t-1}) + \\varepsilon\\]\nWith binary time series data, lagging \\(y\\) would measure changes of state - these are a class known as transition models (there are a variety of these).\nIn general, these are interactive models where\n\\[Pr(y_i=1) = F(x_{i,t}\\beta + y_{i,t-1}*x_{i,t} \\gamma)\\]\nand \\(\\gamma\\) measures the difference in effect when \\(y_{i,t-1} = 0\\) (this is just \\(\\beta\\)), and when \\(y_{i,t-1}  = 1\\); denote this \\(\\alpha\\). So \\(\\gamma = \\beta - \\alpha\\). That difference indicates the conditional probability of state transitions from the state where \\(y\\) takes on one value, to the state where it takes on the other value.\nTransition models are useful, but measure something fundamentally different from the latent hazard rate, or the chances of failure given a history of survival. Put differently, lagging \\(y\\) in a binary variable model does not measure memory or persistence; it does not measure the extent to which the observed value today depends on the value yesterday; it does not measure how the latent probability of failure today depends on surviving through yesterday."
  },
  {
    "objectID": "discretehazards24.html#data",
    "href": "discretehazards24.html#data",
    "title": "Discrete Time Hazard Models",
    "section": "Data",
    "text": "Data\nThe democratic peace data is panel data - composed of cross sections observed over time. The \\(y\\) variable is binary, and measures a rare event - conflict. So the \\(y\\) variable for any particular panel is usually a string of zeros, occasionally punctuated by a one.\nWhat we’d really like to know is if/how strings of zeros affect the chances of a one at any given point in time. This is a question of hazards."
  },
  {
    "objectID": "discretehazards24.html#discrete-time---binary-time-series-cross-section-data",
    "href": "discretehazards24.html#discrete-time---binary-time-series-cross-section-data",
    "title": "Discrete Time Hazard Models",
    "section": "Discrete Time - Binary Time Series Cross Section data",
    "text": "Discrete Time - Binary Time Series Cross Section data\nTime is continuous insofar as time units are infinitely divisible, but in practice, we measure time in discrete units like days, months, years, etc. In the democratic peace data above, we will have two dyads “fail” (have disputes) at year 3; but it’s nearly certain one of those dyads started its dispute before the other one. We group the data by these time intervals (years, in this case). So we are measuring time discretely (i.e, in years), but the underlying process is continuous. Moreover, the fact these can be grouped by failure time makes them grouped duration data.\nIt’s common to have binary \\(y\\) variables observed for cross sections over time - these are Binary Time Series Cross Section (BTSCS) data. This is the form the democratic peace data takes, and is a common form of data in the social sciences. BTSCS data are grouped duration data, and failure is measured in discrete time.\nHere’s an example of BTSCS data thinking of disputes in dyads over time.\n\n\n\n\nBTSCS data\n\n\n\n\n\n\n\n\n\nState\nYear\nDispute\nCensored\n\n\n\nUS-Cuba\n1960\n0\n0\n\n\n\nUS-Cuba\n1961\n1\n0\n\n\n\nUS-Cuba\n1962\n0\n0\n\n\n\nUS-Cuba\n1963\n0\n0\n\n\n\nUS-Cuba\n1964\n0\n0\n\n\n\nUS-Cuba\n1965\n0\n0\n\n\n\nUS-Cuba\n1966\n0\n0\n\n\n\nUS-Cuba\n1967\n1\n0\n\n\n\nUS-Cuba\n1968\n0\n0\n\n\n\nUS-Cuba\n1969\n0\n0\n\n\n\nUS-Cuba\n1970\n0\n1\n\n\n\nUS-UK\n1960\n0\n0\n\n\n\nUS-UK\n1961\n0\n0\n\n\n\nUS-UK\n1962\n0\n0\n\n\n\nUS-UK\n1963\n0\n0\n\n\n\nUS-UK\n1964\n0\n0\n\n\n\nUS-UK\n1965\n0\n0\n\n\n\nUS-UK\n1966\n0\n0\n\n\n\nUS-UK\n1967\n0\n0"
  },
  {
    "objectID": "discretehazards24.html#illustration",
    "href": "discretehazards24.html#illustration",
    "title": "Discrete Time Hazard Models",
    "section": "Illustration",
    "text": "Illustration\nHere are different spells:\n\n\ncode\nlibrary(tidyverse)\nlibrary(highcharter)\n\n# Binghamton University colors\nbinghamton_colors &lt;- c(\"#005A43\", \"#8C2132\", \"#FFD100\", \"#000000\", \"#636466\")\n\n# Create dataframes for each case with updated labels\ncases &lt;- list(\n  list(x = c(3, 6), y = c(1, 1), name = \"uncensored\"),\n  list(x = c(-0.5, 2.5), y = c(2, 2), name = \"left censored\"),\n  list(x = c(2.8, 8), y = c(3, 3), name = \"fails at last period\"),\n  list(x = c(3.5, 10), y = c(4, 4), name = \"right censored\"),\n  list(x = c(5.5, 7), y = c(5, 5), name = \"uncensored\")\n)\n\n# Create the plot\nhc &lt;- highchart() %&gt;%\n  hc_chart(type = \"line\") %&gt;%\n  hc_xAxis(\n    title = list(text = \"time\"),\n    plotLines = list(\n      list(value = 2, width = 2, color = \"black\"),\n      list(value = 8, width = 2, color = \"black\")\n    ),\n    min = 0,\n    max = 10\n  ) %&gt;%\n  hc_yAxis(\n    title = list(text = \"case\"),\n    min = 0,\n    max = 5,\n    tickInterval = 1\n  ) %&gt;%\n  hc_plotOptions(\n    series = list(\n      lineWidth = 3,\n      marker = list(enabled = FALSE)\n    )\n  ) %&gt;%\n  hc_legend(enabled = FALSE)\n\n# Add each case as a separate series with Binghamton colors\nfor (i in seq_along(cases)) {\n  hc &lt;- hc %&gt;% hc_add_series(\n    data = list_parse2(data.frame(x = cases[[i]]$x, y = cases[[i]]$y)),\n    name = cases[[i]]$name,\n    color = binghamton_colors[i]\n  )\n}\n\n# Display the plot\nhc\n\n\n\n\n\n\n\nsome units survive through the end of the study; these units are right censored. That is, they do not fail during the period of observation.\nfailure is only observed per year; so failure is grouped by year; these are grouped duration data. We could, for instance, graph the density of failures at each point in time, effectively grouping them by failure time.\nthe probability of failing at \\(t\\), given survival til \\(t\\) is the hazard of failure; at any point in time, this is called the hazard rate, denoted \\(h(t)\\).\nin the democratic peace model above, \\(h(t)\\) does not depend on what happened at \\(t-1\\), so \\(h(t)\\) is constant over time or is time invariant, or is duration independent."
  },
  {
    "objectID": "discretehazards24.html#the-problem",
    "href": "discretehazards24.html#the-problem",
    "title": "Discrete Time Hazard Models",
    "section": "The problem",
    "text": "The problem\nThe standard logit/probit model in these data assumes the errors are i.i.d. - that the disturbances are uncorrelated. A somewhat more interesting observation is that the model assumes no relationship between the outcome at \\(t\\) and the outcome at \\(t-1, t-2 \\ldots t-k\\). So the observations on \\(y\\) arise independently of one another …almost as if each observation is an independent Bernoulli trial. If this isn’t true, the model is misspecified, and likely the parameter estimates are biased.\nIn the context of the democratic peace data, this means the probability of a dispute at any point in time is unrelated to how long that particular dyad has been at peace. Whether it’s been at peace for 1 year or 10 years has no bearing on the chances of conflict now. On its face, this is a heroic assumption."
  },
  {
    "objectID": "discretehazards24.html#the-solution",
    "href": "discretehazards24.html#the-solution",
    "title": "Discrete Time Hazard Models",
    "section": "The solution",
    "text": "The solution\nAt its root, this is a model specification issue - we think time since last dispute is probably related to the chances of a dispute today, but no such measure is in the model. BKT suggest including “survival time” as a right hand side variable. Doing so explicitly models the effect of surviving up til \\(t\\) on the probability of failing at \\(t\\).\nBKT suggest including nonlinear functions of survival time so the effect of time isn’t constrained to be monotonic. They suggested using cubic splines of survival time; Carter and Signorino (2010) later show polynomials for survival time are just as good and easier to compute/understand."
  },
  {
    "objectID": "discretehazards24.html#the-result",
    "href": "discretehazards24.html#the-result",
    "title": "Discrete Time Hazard Models",
    "section": "The result",
    "text": "The result\nThis fundamentally changed models on BTSCS data - the state of the art since then is to include survival time, thereby measuring “memory” in the \\(y\\) series. While most BTSCS models since Beck, Katz, and Tucker (1998) include survival time, relatively few interpret it; that’s okay insofar as the effect of survival might not be of theoretical interest. Most incorrectly interpret the predictions as probabilities - they are now conditional probabilities, $pr(fail | survival), so hazards.\n\nConstant \\(h(t)\\) - no memory\nRevisiting …in this model, \\(Pr(y_i=1) = F(x_i\\beta + \\beta_0)\\), \\(x_i \\beta\\) induces deviation from the constant or baseline level; but there is no temporal variation, temporal persistence, or memory. What happened last year has no bearing on what we observe this year. Repeating, it is as if these are Bernoulli trials.\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\ndpm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n\nstargazer(dpm1, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\n\ncode\n# at mean data over 30 observations\n\natmean &lt;- data.frame(stime=seq(1,34,1), deml=median(dp$deml), border=0, caprat=median(dp$caprat), ally=0)\n\npredictions &lt;- data.frame(atmean, predict(dpm1, newdata=atmean, type=\"response\", se=TRUE)) %&gt;% mutate(fit=round(fit, 2) )\n\n\nggplot(predictions, aes(x=stime, y=fit)) +\n  geom_line(color=\"#005A43\", size=1) +\n  labs(x=\"Time since last dispute\", y=\"Pr(Dispute)\") +\n  geom_ribbon(aes(ymin=fit-1.96*se.fit, ymax=fit+1.96*se.fit), fill=\"grey90\", alpha=0.4) +\n  theme_minimal() +\n  annotate(\"text\", x = 15, y = 0.041, label = \"Effect of Democracy\", color = \"red\", size = 3) +\n  theme(panel.grid.major = element_line(color = \"black\",\n                                        size = 0.25,\n                                        linetype = 1)) +\n  theme(panel.grid.minor = element_line(color = \"black\",\n                                        size = 0.15,\n                                        linetype = 1))\n\n\n\n\n\n\n\n\n\nThis is the case where\n\\[h_0(t) = \\frac{1}{1+ exp(\\beta_0)}\\]\nthe baseline hazard is the constant. Even with \\(x\\) variables, there is still no accounting for time - the \\(x\\) effects are only shifts around this baseline hazard.\n\\[h_0(t) = \\frac{1}{1+ exp(\\beta_0+ x'\\beta)}\\]\nthis is still a constant baseline hazard with the effects of \\(x\\) deviating around it.\n\n\nMeasuring survival time\n\ncount periods of survival up to failure. This is a counter of survival time. generate a binary variable for each survival period.\nEither include those survival dummies in the logit, or include the survival counter itself with polynomials, e.g. \\(t^2, t^3, \\ldots\\).\ninterpret those coefficients as baseline hazards for groups that survive to \\(t_i\\).\nwith all \\(x\\) variables set to zero, the probability of failure is now given by the constant and the appropriate dummy or counter coefficient.\nNote the quantity of interest is not constant across time: it’s a conditional probability; the probability of failing at \\(t\\) given the estimated probability of survival through \\(t-1\\) - \\(h(t)|S(t)\\).\n\n\nHere’s what the survival time counter looks like in the democratic peace data:\n\n\ncode\nsdpshort &lt;- survivaldp %&gt;% head(160) %&gt;% dplyr::select(dyad, year, dispute, stime)\n\nlibrary(kableExtra)\ntibble(sdpshort)%&gt;% \n    kable(\"html\", caption=\"Survival Time, Democratic Peace Data\") %&gt;% \n  kable_styling(\"striped\", full_width = F) %&gt;% \n  column_spec(1, border_right = T) %&gt;% \n  column_spec(2, border_right = T) \n\n\n\nSurvival Time, Democratic Peace Data\n\n\ndyad\nyear\ndispute\nstime\n\n\n\n\n2020\n1951\n0\n0\n\n\n2020\n1952\n0\n1\n\n\n2020\n1953\n0\n2\n\n\n2020\n1954\n0\n3\n\n\n2020\n1955\n0\n4\n\n\n2020\n1956\n0\n5\n\n\n2020\n1957\n0\n6\n\n\n2020\n1958\n0\n7\n\n\n2020\n1959\n0\n8\n\n\n2020\n1960\n0\n9\n\n\n2020\n1961\n0\n10\n\n\n2020\n1962\n0\n11\n\n\n2020\n1963\n0\n12\n\n\n2020\n1964\n0\n13\n\n\n2020\n1965\n0\n14\n\n\n2020\n1966\n0\n15\n\n\n2020\n1967\n0\n16\n\n\n2020\n1968\n0\n17\n\n\n2020\n1969\n0\n18\n\n\n2020\n1970\n0\n19\n\n\n2020\n1971\n0\n20\n\n\n2020\n1972\n0\n21\n\n\n2020\n1973\n0\n22\n\n\n2020\n1974\n1\n23\n\n\n2020\n1975\n1\n0\n\n\n2020\n1976\n0\n0\n\n\n2020\n1977\n0\n1\n\n\n2020\n1978\n0\n2\n\n\n2020\n1979\n1\n3\n\n\n2020\n1980\n0\n0\n\n\n2020\n1981\n0\n1\n\n\n2020\n1982\n0\n2\n\n\n2020\n1983\n0\n3\n\n\n2020\n1984\n0\n4\n\n\n2020\n1985\n0\n5\n\n\n2041\n1961\n0\n0\n\n\n2041\n1962\n0\n1\n\n\n2041\n1963\n1\n2\n\n\n2041\n1964\n0\n0\n\n\n2041\n1965\n0\n1\n\n\n2041\n1966\n0\n2\n\n\n2041\n1967\n0\n3\n\n\n2041\n1968\n0\n4\n\n\n2041\n1969\n0\n5\n\n\n2041\n1970\n0\n6\n\n\n2041\n1971\n0\n7\n\n\n2041\n1972\n0\n8\n\n\n2041\n1973\n0\n9\n\n\n2041\n1974\n0\n10\n\n\n2041\n1975\n0\n11\n\n\n2041\n1976\n0\n12\n\n\n2041\n1977\n0\n13\n\n\n2041\n1978\n0\n14\n\n\n2041\n1979\n0\n15\n\n\n2041\n1980\n0\n16\n\n\n2041\n1981\n0\n17\n\n\n2041\n1982\n0\n18\n\n\n2041\n1983\n0\n19\n\n\n2041\n1984\n0\n20\n\n\n2041\n1985\n0\n21\n\n\n2042\n1951\n0\n0\n\n\n2042\n1952\n0\n1\n\n\n2042\n1953\n0\n2\n\n\n2042\n1954\n0\n3\n\n\n2042\n1955\n0\n4\n\n\n2042\n1956\n0\n5\n\n\n2042\n1957\n0\n6\n\n\n2042\n1958\n0\n7\n\n\n2042\n1959\n1\n8\n\n\n2042\n1960\n0\n0\n\n\n2042\n1962\n0\n1\n\n\n2042\n1964\n0\n2\n\n\n2042\n1965\n0\n3\n\n\n2042\n1966\n0\n4\n\n\n2042\n1967\n0\n5\n\n\n2042\n1968\n0\n6\n\n\n2042\n1969\n0\n7\n\n\n2042\n1970\n0\n8\n\n\n2042\n1971\n0\n9\n\n\n2042\n1972\n0\n10\n\n\n2042\n1973\n0\n11\n\n\n2042\n1974\n0\n12\n\n\n2042\n1975\n0\n13\n\n\n2042\n1976\n0\n14\n\n\n2042\n1977\n0\n15\n\n\n2042\n1978\n0\n16\n\n\n2042\n1979\n0\n17\n\n\n2042\n1980\n0\n18\n\n\n2042\n1981\n0\n19\n\n\n2042\n1982\n0\n20\n\n\n2042\n1983\n0\n21\n\n\n2042\n1984\n0\n22\n\n\n2042\n1985\n0\n23\n\n\n2051\n1962\n0\n0\n\n\n2051\n1963\n0\n1\n\n\n2051\n1964\n0\n2\n\n\n2051\n1965\n0\n3\n\n\n2051\n1966\n0\n4\n\n\n2051\n1967\n0\n5\n\n\n2051\n1968\n0\n6\n\n\n2051\n1969\n0\n7\n\n\n2051\n1970\n0\n8\n\n\n2051\n1971\n0\n9\n\n\n2051\n1972\n0\n10\n\n\n2051\n1973\n0\n11\n\n\n2051\n1974\n0\n12\n\n\n2051\n1975\n0\n13\n\n\n2051\n1976\n0\n14\n\n\n2051\n1977\n0\n15\n\n\n2051\n1978\n0\n16\n\n\n2051\n1979\n0\n17\n\n\n2051\n1980\n0\n18\n\n\n2051\n1981\n0\n19\n\n\n2051\n1982\n0\n20\n\n\n2051\n1983\n0\n21\n\n\n2051\n1984\n0\n22\n\n\n2051\n1985\n0\n23\n\n\n2052\n1962\n0\n0\n\n\n2052\n1963\n0\n1\n\n\n2052\n1964\n0\n2\n\n\n2052\n1965\n0\n3\n\n\n2052\n1966\n0\n4\n\n\n2052\n1967\n0\n5\n\n\n2052\n1968\n0\n6\n\n\n2052\n1969\n0\n7\n\n\n2052\n1970\n0\n8\n\n\n2052\n1971\n0\n9\n\n\n2052\n1972\n0\n10\n\n\n2052\n1973\n0\n11\n\n\n2052\n1974\n0\n12\n\n\n2052\n1975\n0\n13\n\n\n2052\n1976\n0\n14\n\n\n2052\n1977\n0\n15\n\n\n2052\n1978\n0\n16\n\n\n2052\n1979\n0\n17\n\n\n2052\n1980\n0\n18\n\n\n2052\n1981\n0\n19\n\n\n2052\n1982\n0\n20\n\n\n2052\n1983\n0\n21\n\n\n2052\n1984\n0\n22\n\n\n2052\n1985\n0\n23\n\n\n2070\n1951\n0\n0\n\n\n2070\n1952\n0\n1\n\n\n2070\n1953\n0\n2\n\n\n2070\n1954\n0\n3\n\n\n2070\n1955\n0\n4\n\n\n2070\n1956\n1\n5\n\n\n2070\n1957\n0\n0\n\n\n2070\n1958\n0\n1\n\n\n2070\n1959\n0\n2\n\n\n2070\n1960\n0\n3\n\n\n2070\n1961\n0\n4\n\n\n2070\n1962\n0\n5\n\n\n2070\n1963\n0\n6\n\n\n2070\n1964\n0\n7\n\n\n2070\n1965\n0\n8\n\n\n2070\n1966\n0\n9\n\n\n2070\n1967\n0\n10\n\n\n2070\n1968\n0\n11\n\n\n2070\n1969\n0\n12\n\n\n\n\n\n\n\n\n\nMonotonic hazard\nSo how to deal with this, incorporating memory: thinking in terms of hazards rather than probabilities (i.e., conditional rather than unconditional probabilities), what if we measure survival time?\n\nThe binary \\(y\\) variable is an indicator of failure at \\(t\\); the model estimates \\(f(t)\\), which we’ve said is not especially informative since subjects might fail before \\(t\\).\nThink of the number of periods up to failure as the cumulative survival time, \\(S(t)\\).\n\nSee how we’re starting to construct the hazard rate by its parts.\nHere’s an example in the democratic peace data:\n\\(dispute = \\beta_0+ \\beta_1(survival)\\)\n\n\ncode\ndpm2 &lt;-glm(dispute ~ stime, family=binomial(link=\"logit\"), data=survivaldp )\n\nstargazer(dpm2, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Survival Time\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Time\n\n\n-0.265*** (0.010)\n\n\n\n\nConstant\n\n\n-1.513*** (0.045)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n6,208.082\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\nAnd here are predicted probabilities from the model.\n\n\ncode\natmean &lt;- data.frame(stime=seq(1,34,1))\n\npredictions &lt;- data.frame(atmean, predict(dpm2, newdata=atmean, type=\"response\", se=TRUE)) \n\nggplot(predictions, aes(x=stime, y=fit)) +\n  geom_line() +\n  labs(x=\"Time since last dispute\", y=\"Pr(Dispute)\") +\n  geom_ribbon(aes(ymin=fit-1.96*se.fit, ymax=fit+1.96*se.fit), fill=\"#005A43\", alpha=0.4) +\n  theme_minimal()+\n  theme(panel.grid.major = element_line(color = \"black\",\n                                        size = 0.25,\n                                        linetype = 1)) +\n  theme(panel.grid.minor = element_line(color = \"black\",\n                                        size = 0.15,\n                                        linetype = 1))\n\n\n\n\n\n\n\n\n\nWhere the baseline hazard is no longer constant - it can increase or decrease monotonically:\n\\[h_0(t) = \\frac{1}{1+ exp(\\beta_0 + \\gamma_0(t))}\\]\nthe baseline hazard is the constant plus the effect of survival time - the \\(x\\) effects are shifts around this baseline hazard.\n\\[h_0(t) = \\frac{1}{1+ exp(\\beta_0 + \\gamma_0(t)) + x'\\beta)}\\]\nThe baseline hazard is no longer constrained to be constant, though it can be if \\(\\gamma_0=0\\).\n\nThis model accounts for “memory” - the QI is now the hazard.\nThe hazard is not constrained to be constant, but is constrained to be monotonic.\nTo relax this, we can\n\ninclude dummy variables - these are discrete time indicators based on the counter.\ninclude cubic splines or lowess estimates - these are smoothed time functions based on the counter.\ninclude polynomials of the time counter.\n\n\n\n\nNon-monotonic hazard\n\n\ncode\nsurvivaldp &lt;- survivaldp %&gt;% mutate(stime2=stime^2, stime3=stime^3)\n\ndpm3 &lt;-glm(dispute ~ stime+stime2+stime3, family=binomial(link=\"logit\"), data=survivaldp )\n\nstargazer(dpm3, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Survival Time\", \"Survival Time2\", \"Survival Time3\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Time\n\n\n-0.830*** (0.035)\n\n\n\n\nSurvival Time2\n\n\n0.052*** (0.004)\n\n\n\n\nSurvival Time3\n\n\n-0.001*** (0.0001)\n\n\n\n\nConstant\n\n\n-0.950*** (0.048)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n5,742.615\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\n\ncode\natmean &lt;- data.frame(stime=seq(1,34,1), stime2=seq(1,34,1)^2, stime3=seq(1,34,1)^3)\n\npredictions &lt;- data.frame(atmean, predict(dpm3, newdata=atmean, type=\"response\", se=TRUE)) \n\nggplot(predictions, aes(x=stime, y=fit)) +\n  geom_line() +\n  labs(x=\"Time since last dispute\", y=\"Pr(Dispute)\") +\n  geom_ribbon(aes(ymin=fit-1.96*se.fit, ymax=fit+1.96*se.fit), fill=\"#005A43\", alpha=0.4) +\n  theme_minimal()+\n  theme(panel.grid.major = element_line(color = \"black\",\n                                        size = 0.25,\n                                        linetype = 1)) +\n  theme(panel.grid.minor = element_line(color = \"black\",\n                                        size = 0.15,\n                                        linetype = 1))\n\n\n\n\n\n\n\n\n\nThis last is a close approximation of a Cox proportional hazards model. The hazard is non monotonic; it nests the exponential (constant hazard), and the monotonic (Weibull) hazard, and is very general. Besides, it’s very easy to estimate and interpret.\n\n\nUnderstanding substantive variables in the hazard context\nThe survival variables now permit the baseline hazard to vary. The effects of \\(x\\) variables can be thought of as deviations from those baseline hazards. For example, think about the models presented above, but with democracy. The estimates on democracy will shift the baseline hazard up or down.\n\n\ncode\nsurvivaldp &lt;- survivaldp %&gt;% mutate(stime2=stime^2, stime3=stime^3)\n\ndpm4 &lt;-glm(dispute ~ stime+stime2+stime3+deml, family=binomial(link=\"logit\"), data=survivaldp )\n\nstargazer(dpm4, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Survival Time\", \"Survival Time2\", \"Survival Time3\", \"Democracy\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Time\n\n\n-0.823*** (0.035)\n\n\n\n\nSurvival Time2\n\n\n0.051*** (0.004)\n\n\n\n\nSurvival Time3\n\n\n-0.001*** (0.0001)\n\n\n\n\nDemocracy\n\n\n-0.065*** (0.007)\n\n\n\n\nConstant\n\n\n-1.298*** (0.065)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n5,650.709\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\n\ncode\ndpm4 &lt;-glm(dispute ~ stime+stime2+stime3+deml+caprat, family=binomial(link=\"logit\"), data=survivaldp )\n\n# copy estimation data for avg effects\ndppred &lt;- survivaldp\n\n# df for output\ndf &lt;- data.frame(time=seq(1,34,1))\nfor (d in seq(-10,10,10)) {\n    df[paste0(\"p\", d+10)] &lt;- NA\n  df[paste0(\"Se\", d+10)] &lt;- NA\n}\n\n# predictions\nfor (d in seq(-10,10,10)) {\n  dppred$deml &lt;- d\n for (t in seq(1,34,1)) {  \n  dppred$stime &lt;- t\n  dppred$stime2 &lt;- t^2 \n  dppred$stime3 &lt;- t^3\n  pred &lt;- predict(dpm4, newdata=dppred, type=\"response\", se=TRUE)\n  df[t, paste0(\"p\", d+10)] &lt;- mean(pred$fit, na.rm=TRUE)\n  df[t, paste0(\"Se\", d+10)] &lt;- mean(pred$se.fit, na.rm=TRUE)\n  df$time[t] &lt;- t\n}\n}\n\n# plot\nggplot(df, aes(x=time, y=p0)) +\n  geom_line(color=\"#005A43\") +\n  geom_ribbon(aes(ymin=p0-1.96*Se0, ymax=p0+1.96*Se0), fill=\"#6CC24A\", alpha=0.4) +\n  geom_line(aes(y=p10), color=\"#005A43\") +\n  geom_ribbon(aes(ymin=p10-1.96*Se10, ymax=p10+1.96*Se10), fill=\"#A7DA92\", alpha=0.4) +\n  geom_line(aes(y=p20), color=\"#005A43\") +\n  geom_ribbon(aes(ymin=p20-1.96*Se20, ymax=p20+1.96*Se20), fill=\"#005A43\", alpha=0.4) +\n  labs(x=\"Time since last dispute\", y=\"Pr(Dispute)\") +\n  theme_minimal()+\n  theme(panel.grid.major = element_line(color = \"black\",\n                                        size = 0.25,\n                                        linetype = 1)) +\n  theme(panel.grid.minor = element_line(color = \"black\",\n                                        size = 0.15,\n                                        linetype = 1))"
  },
  {
    "objectID": "discretehazards24.html#an-alternative---transitions",
    "href": "discretehazards24.html#an-alternative---transitions",
    "title": "Discrete Time Hazard Models",
    "section": "An alternative - transitions",
    "text": "An alternative - transitions\nWhat happens if we lag \\(y\\) as we might with a continuous variable (say, in OLS), such that the estimation model is\n\\[y_t = \\beta_0 + \\beta_1(x_1) + \\ldots + \\gamma(y_{t-1}) + \\varepsilon\\]\nWith binary time series data, lagging \\(y\\) would measure changes of state - these are a class known as transition models (there are a variety of these).\nIn general, these are interactive models where\n\\[Pr(y_i=1) = F(x_{i,t}\\beta + y_{i,t-1}*x_{i,t} \\gamma)\\]\nand \\(\\gamma\\) measures the difference in effect when \\(y_{i,t-1} = 0\\) (this is just \\(\\beta\\)), and when \\(y_{i,t-1}  = 1\\); denote this \\(\\alpha\\). So \\(\\gamma = \\beta - \\alpha\\). That difference indicates the conditional probability of state transitions from the state where \\(y\\) takes on one value, to the state where it takes on the other value.\nTransition models are useful, but measure something fundamentally different from the latent hazard rate, or the chances of failure given a history of survival. Put differently, lagging \\(y\\) in a binary variable model does not measure memory or persistence; it does not measure the extent to which the observed value today depends on the value yesterday; it does not measure how the latent probability of failure today depends on surviving through yesterday."
  },
  {
    "objectID": "binaryextensions224.html",
    "href": "binaryextensions224.html",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "",
    "text": "The binary model serves as the root, or really a special case, of many other models. More generally, the binomial distribution informs many models beyond the binary variable ones.\n\ninstead of a binary choice, we can consider multiple choices simply by extending the utility model logic, and expanding the on/off switches in the LLF (weeks 8, 9).\nwe can consider multiple ordered choices (we’ll look at this today).\nwe can relax the assumption observations are independent over time, that the DGP has no memory (discrete hazards - last time).\nwe can relax the constant variance assumption (today).\nwe can relax the symmetry assumption in the link function (2 weeks ago).\nwe can take binary events, binomially distributed, count them up over time, and treat them as discrete (Poisson) event counts (week 10).\n\nToday, we’ll discuss models for ordered \\(y\\) variables, and how to address non-constant variance in the probit model."
  },
  {
    "objectID": "binaryextensions224.html#symmetry-and-asymmetry",
    "href": "binaryextensions224.html#symmetry-and-asymmetry",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Symmetry and Asymmetry",
    "text": "Symmetry and Asymmetry\nCompare three CDFs: the logistic, the clog-log, and a skewed logit function. The logistic is symmetric, the clog-log and skewed logit are not. You should notice the probability associated with x=0 for each function - the logistic is .5, the clog-log is about .64, and the skewed logit is about than .71.\nFor skewed binary \\(y\\) variables, it could be that one of these CDFs is a more appropriate link function than the symmetric logistic or normal. Implementing these merely requires substituting the appropriate CDF into the log-likelihood function.\n\n\ncode\n# Load required libraries\nlibrary(highcharter)\nlibrary(dplyr)\n\n# Binghamton University colors\nbinghamton_green &lt;- \"#005A43\"\nbinghamton_gray &lt;- \"#707070\"\nbinghamton_yellow &lt;- \"#FFC726\"\n\n# Generate data\nx &lt;- seq(-5, 5, length.out = 1000)\nlogistic_cdf &lt;- plogis(x)\ncloglog_cdf &lt;- 1 - exp(-exp(x))\n\n# Skewed logit function (shape parameter = 0.5)\n\nskewed_logit_cdf &lt;- 1 / (1 + exp(-x)) ^ 0.5 \n\n# Create data frame\ndf &lt;- data.frame(x = x, logistic = logistic_cdf, cloglog = cloglog_cdf, skewed_logit = skewed_logit_cdf)\n\n# Create the highchart\nhc &lt;- highchart() %&gt;%\n  hc_chart(type = \"line\") %&gt;%\n  hc_title(text = \"Comparison of CDFs: Logistic, Clog-log, and Skewed Logit\") %&gt;%\n  hc_xAxis(\n    title = list(text = \"x\"),\n    plotLines = list(\n      list(\n        color = \"#999\",\n        width = 2,\n        value = 0,\n        zIndex = 3,\n        label = list(text = \"x = 0\")\n      )\n    )\n  ) %&gt;%\n  hc_yAxis(\n    title = list(text = \"Cumulative Probability\"),\n    plotLines = list(\n      list(\n        color = \"#999\",\n        width = 2,\n        value = 0.5,\n        zIndex = 3,\n        label = list(text = \"y = 0.5\")\n      )\n    )\n  ) %&gt;%\n  hc_tooltip(\n    shared = TRUE,\n    formatter = JS(\"function() {\n      return 'x: ' + this.x.toFixed(4) + '&lt;br&gt;' +\n             'Logistic: ' + this.points[0].y.toFixed(4) + '&lt;br&gt;' +\n             'Clog-log: ' + this.points[1].y.toFixed(4) + '&lt;br&gt;' +\n             'Skewed Logit: ' + this.points[2].y.toFixed(4);\n    }\")\n  ) %&gt;%\n  hc_plotOptions(series = list(marker = list(enabled = FALSE))) %&gt;%\n  \n  # Add logistic CDF\n  hc_add_series(\n    data = df,\n    type = \"line\",\n    name = \"Logistic\",\n    color = binghamton_green,\n    hcaes(x = x, y = logistic)\n  ) %&gt;%\n  \n  # Add clog-log CDF\n  hc_add_series(\n    data = df,\n    type = \"line\",\n    name = \"Clog-log\",\n    color = binghamton_gray,\n    hcaes(x = x, y = cloglog)\n  ) %&gt;%\n  \n  # Add skewed logit CDF\n  hc_add_series(\n    data = df,\n    type = \"line\",\n    name = \"Skewed Logit\",\n    color = binghamton_yellow,\n    hcaes(x = x, y = skewed_logit)\n  )\n\n# Display the chart\nhc"
  },
  {
    "objectID": "binaryextensions224.html#skewed-logit",
    "href": "binaryextensions224.html#skewed-logit",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Skewed Logit",
    "text": "Skewed Logit\nNagler (1994) proposes the skewed logit (scobit) model - it’s a binary response model, the usual LLF, with a different link (the Burr-10):\n\\[ Pr(y=1) = \\frac{1}{(1+e^{-x\\beta})^\\alpha}\\]\nNote that if \\(\\alpha=1\\) this is the logistic CDF. If it is less than 1, the fastest rate of change is at \\(Pr(y =1 &lt; .5)\\); when greater than 1, the fastest rate of change, is at \\(Pr(y=1 &gt; .5)\\)\nNagler’s logic is that symmetric links require the assumption that individuals in the model are most sensitive to the effects of the \\(X\\) variables at or around \\(Pr(y=1) = .5\\). Looking at the (symmetric) logit curve above, you can see that’s where the derivative with respect to changes in \\(x\\) is greatest. If \\(y\\) is about half ones, half zeros, this may make sense - but often, we have \\(y\\) variables that are not symmetrically distributed like this. It makes sense in such cases not to assume the fastest rate of change, and the transition point from zero to one, is at \\(Pr(y=1) = .5\\).\nThe scobit model allows us to estimate the \\(\\alpha\\) parameter, which tells us where the fastest rate of change is in the CDF - that is, the transition point is an empirical question, not an assumption.\nThe model appears rarely in the political science literature; a Google Scholar search indicates most of its use is transportation analysis. A cursory survey also indicates the scobit estimates are often not that different from logit estimates. Estimation sometimes is funky insofar as we cannot always tell if changes in the likelihood are due to changes in \\(\\beta\\) or in \\(\\alpha\\)."
  },
  {
    "objectID": "binaryextensions224.html#symmetry-1",
    "href": "binaryextensions224.html#symmetry-1",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Symmetry",
    "text": "Symmetry\nThe big point here is not that we should or should not use the scobit, but that we need to be very aware that the assumption in models with symmetric links is that the biggest effect of an \\(x\\) variable is at \\(Pr(y=1) = 0.5\\) which is where \\(x\\beta=0\\)."
  },
  {
    "objectID": "binaryextensions224.html#why-does-symmetry-matter",
    "href": "binaryextensions224.html#why-does-symmetry-matter",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Why does symmetry matter?",
    "text": "Why does symmetry matter?\n\nsymmetry determines where the greatest effect of \\(x\\) is.\nsymmetry ensures rates of change above and below \\(x=0\\) are the same as they approach the limits.\nsymmetry implies the theoretical threshold, \\(\\tau\\), separating observed zeros and ones is \\(\\tau=0.5\\).\nif we want to use the model to generate predicted values of \\(y\\) (rather than \\(y^*\\)), we need some threshold for classification.\n\nSome (very few) questions naturally link to a clear threshold like 0.5 …election outcomes? But …are we measuring the correct outcome variable?"
  },
  {
    "objectID": "binaryextensions224.html#confusion-matrix",
    "href": "binaryextensions224.html#confusion-matrix",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nThe “confusion matrix” (I didn’t make this up) illustrates that intersection and identifies where our classification is “confused.”\n\n\n\n\ncode\nlibrary(kableExtra)\n# opts &lt;- options(knitr.kable.NA = \"\")\ndf &lt;- data.frame(\"Predicted Positive\" = c(\"True Positive\", \"False Positive\"), \"Predicted Negative\" = c(\"False Positive\", \"True Negative\"), \"Rate\" = c(\"TPR=TP/P\", \"FPR=FP/N\"))\nrow.names(df) &lt;- c(\"Observed Positive\", \"Observed Negative\")\n\ntbl &lt;- kbl(df, align = rep('c', 4)) |&gt; \n  column_spec(2:4, border_left = T) |&gt; \n  column_spec(1,  bold=T) |&gt;\n  column_spec(1:4, width = \"2cm\", color = 'white', background = '#005A43', include_thead = TRUE) |&gt;\n  column_spec(2:4, background = \"inherit\", color=\"inherit\") |&gt;\n  row_spec(1, extra_css = \"border-bottom: 1px solid\") |&gt;\n  kable_paper(\"hover\", bootstrap_options = c(\"condensed\", \"responsive\"), full_width = F, font_size = 20)\ntbl\n\n\n\n\n\n\nPredicted.Positive\nPredicted.Negative\nRate\n\n\n\n\nObserved Positive\nTrue Positive\nFalse Positive\nTPR=TP/P\n\n\nObserved Negative\nFalse Positive\nTrue Negative\nFPR=FP/N\n\n\n\n\n\n\n\n\n\n\n\nTrue Positive Rate: correctly classify positive outcomes. This is often called “sensitivity.”\nFalse Positive Rate: we incorrectly classify negative outcomes (\\(y=0\\)) as positive (\\(y=1\\)). This is often called “1-specificity.” Specificity is the True Negative Rate, or the probability of correctly classifying a negative outcome (\\(y=0\\))."
  },
  {
    "objectID": "binaryextensions224.html#using-the-confusion-matrix-to-measure-model-fit",
    "href": "binaryextensions224.html#using-the-confusion-matrix-to-measure-model-fit",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Using the Confusion Matrix to Measure Model Fit",
    "text": "Using the Confusion Matrix to Measure Model Fit\nSo here’s the deal:\n\nestimate the model.\ngenerate the predicted probability \\(y=1\\) for each observation.\nassume a threshold separating zeros and ones; usually \\(\\tau=0.5\\).\nif \\(Pr(y=1 \\geq 0.5)\\), predict a positive outcome (predict \\(y=1\\)).\nif \\(Pr(y=0 &lt; 0.5)\\), predict a negative outcome (predict \\(y=0\\)).\nusing the observed and predicted outcomes, generate a confusion table, and compute measures of fit like “percent correctly predicted” (PCP) and “proportional reduction of error” (PRE).\n\n\nPercent Correctly Predicted (PCP)\nOne thing we can do is sum the main diagonal and divide by the estimation sample: \\((TP+TN)/N\\). This gives us the Percent Correctly Predicted (PCP).\n\n\nProportional Reduction of Error (PRE)}\nA second thing is to compute the Proportional Reduction of Error (PRE) - the difference between the correct predictions of the model versus a null model usually the unconditional frequency of \\(y\\) - in other words, we use the Percent in the Modal Category (PCM) of the \\(y\\) variable.\n\\[\\frac{correct_{\\text{informed}} - correct_{\\text{null}}}{N - correct_{\\text{null}}} \\]\nor\n\\[\\frac{PCP- PMC}{1-PMC} \\]\n\n\nExample: NAFTA vote, 1993\nSo here’s a model predicting members of the US House votes for or against the NAFTA treaty in 1993: 1 is a “yes” vote; the \\(x\\) variables are party (Democrat) and an ideology score.\n\n\ncode\nnafta &lt;- read.csv(\"NAFTA.csv\")\n\nnaftamodel &lt;- glm(vote ~ democrat + cope93, data=nafta, family=binomial(link=\"logit\"))\n\nstargazer(naftamodel, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: NAFTA vote\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Party (Democrat=1)\", \"Ideology (COPE score)\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: NAFTA vote\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParty (Democrat=1)\n\n\n1.990*** (0.504)\n\n\n\n\nIdeology (COPE score)\n\n\n-0.054*** (0.007)\n\n\n\n\nConstant\n\n\n2.380*** (0.272)\n\n\n\n\n\n\n\n\nObservations\n\n\n434\n\n\n\n\nAkaike Inf. Crit.\n\n\n465.904\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\nAnd here’s the confusion matrix from that model assuming a threshold of \\(\\tau=.5\\) - it compares the observed vote frequency against how we classify our predictions based on a predicted probability greater than or less than 0.5. This is generated using the caret package in R.\n\n\ncode\n# Load required libraries\nlibrary(caret)\n\n# Assuming you have a fitted GLM object called 'glm_model'\n# and test data 'test_data' with actual outcomes in 'test_data$actual'\n\ntest_data &lt;- nafta %&gt;% dplyr::select(vote, democrat, cope93) %&gt;% mutate(actual = vote)\n# Make predictions on the test data\npredictions &lt;- predict(naftamodel, newdata = test_data, type = \"response\")\n\n# Create confusion matrix\n# You may need to adjust the threshold (default is 0.5)\npredicted_classes &lt;- ifelse(predictions &gt; 0.5, 1, 0)\nconf_matrix &lt;- confusionMatrix(factor(predicted_classes), factor(test_data$actual))\n\n# Print confusion matrix\nprint(conf_matrix)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 160  77\n         1  40 157\n                                         \n               Accuracy : 0.7304         \n                 95% CI : (0.686, 0.7716)\n    No Information Rate : 0.5392         \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      \n                                         \n                  Kappa : 0.4647         \n                                         \n Mcnemar's Test P-Value : 0.0008741      \n                                         \n            Sensitivity : 0.8000         \n            Specificity : 0.6709         \n         Pos Pred Value : 0.6751         \n         Neg Pred Value : 0.7970         \n             Prevalence : 0.4608         \n         Detection Rate : 0.3687         \n   Detection Prevalence : 0.5461         \n      Balanced Accuracy : 0.7355         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\nYou should see the main diagonal presents the number of correct predictions - the off-diagonal elements are the incorrect predictions. If we sum the main diagonal and divide by \\(N\\), we get the Percent Correctly Predicted (PCP). In this case, the PCP is 0.735 - 73.5% of the votes are correctly predicted.\nThis all depends on the threshold (.5) - in the case of a Congressional vote, especially a relatively close vote like this one, the threshold might not be crazy. But it might be in other cases, and arbitrarily choosing a value for \\(\\tau\\) is problematic. So another approach is to compute the ROC curve.\nWhat makes this work relatively well in the NAFTA context? As you’ll see below, it works less well in the democratic peace models.\n\n\nExample: Democratic Peace\nHere’s a basic democratic peace model:\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\ndpm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n\nstargazer(dpm1, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\nLet’s generate the confusion matrix for the democratic peace model, threshold set at \\(\\tau =0.5\\):\n\n\ncode\nlibrary(kableExtra)\n# opts &lt;- options(knitr.kable.NA = \"\")\ndf &lt;- data.frame(\"Predicted Positive\" = c(\"0\", \"0\"), \"Predicted Negative\" = c(\"897\", \"19,245\"), \"Rate\" = c(\"TPR=0/897=0\", \"FPR=0/19,245=0\"))\nrow.names(df) &lt;- c(\"Observed Positive\", \"Observed Negative\")\n\ntbl &lt;- kbl(df, align = rep('c', 4)) |&gt; \n  column_spec(2:4, border_left = T) |&gt; \n  column_spec(1,  bold=T) |&gt;\n  column_spec(1:4, width = \"2cm\", color = 'white', background = '#005A43', include_thead = TRUE) |&gt;\n  column_spec(2:4, background = \"inherit\", color=\"inherit\") |&gt;\n  row_spec(1, extra_css = \"border-bottom: 1px solid\") |&gt;\n  kable_paper(\"hover\", bootstrap_options = c(\"condensed\", \"responsive\"), full_width = F, font_size = 20)\ntbl\n\n\n\n\n\n\nPredicted.Positive\nPredicted.Negative\nRate\n\n\n\n\nObserved Positive\n0\n897\nTPR=0/897=0\n\n\nObserved Negative\n0\n19,245\nFPR=0/19,245=0\n\n\n\n\n\n\n\nThe problem is \\(\\tau\\); at \\(\\tau=.5\\) we get none of the ones correct. Here’s the democratic peace at \\(\\tau = 0.1\\):\n\n\ncode\nlibrary(kableExtra)\n# opts &lt;- options(knitr.kable.NA = \"\")\ndf &lt;- data.frame(\"Predicted Positive\" = c(\"303\", \"1,764\"), \"Predicted Negative\" = c(\"594\", \"17,481\"), \"Rate\" = c(\"TPR=303/897=.34\", \"FPR=1,764/19,245=0.092\"))\nrow.names(df) &lt;- c(\"Observed Positive\", \"Observed Negative\")\n\ntbl &lt;- kbl(df, align = rep('c', 4)) |&gt; \n  column_spec(2:4, border_left = T) |&gt; \n  column_spec(1,  bold=T) |&gt;\n  column_spec(1:4, width = \"2cm\", color = 'white', background = '#005A43', include_thead = TRUE) |&gt;\n  column_spec(2:4, background = \"inherit\", color=\"inherit\") |&gt;\n  row_spec(1, extra_css = \"border-bottom: 1px solid\") |&gt;\n  kable_paper(\"hover\", bootstrap_options = c(\"condensed\", \"responsive\"), full_width = F, font_size = 20)\ntbl\n\n\n\n\n\n\nPredicted.Positive\nPredicted.Negative\nRate\n\n\n\n\nObserved Positive\n303\n594\nTPR=303/897=.34\n\n\nObserved Negative\n1,764\n17,481\nFPR=1,764/19,245=0.092\n\n\n\n\n\n\n\nAt \\(\\tau = 0.1\\), we get a TPR = 0.34 and FPR = 0.092. The PCP = (303+17481)/20142 = 0.88 …which is lower, but we get about 1/3 of the ones correct."
  },
  {
    "objectID": "binaryextensions224.html#receiver-operator-characteristic-roc-curves",
    "href": "binaryextensions224.html#receiver-operator-characteristic-roc-curves",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Receiver-Operator Characteristic (ROC) Curves",
    "text": "Receiver-Operator Characteristic (ROC) Curves\nThe problem is choosing the threshold - imagine that we compute a confusion matrix for all possible thresholds, \\(\\tau=.01, .02, .03 \\ldots 1\\), then compute TPR and FPR, and plot them against one another. This is an ROC curve.\nROCs originate in efforts to distinguish signal from noise in radar returns - the British built a radar system before WWII to detect German air attacks; they had the problem of distinguishing planes (signal) from flocks of geese (noise). As the turned up the sensitivity of the radar, they more often correctly detected planes, but they also lacked specificity and detected more geese too. So there was a tradeoff between sensitivity (correctly identifying positive signals as positive) and specificity (incorrectly identify negative signals as positive).\nROCs measure these two dimensions and graph them against one another:\n\nsensitivity - true positive rate at every possible latent threshold between zero and one.\n1- specificity - false positive rate at every possible latent threshold between zero and one. This is 1 minus the True Negative Rate\n\nHere’s the ROC for the NAFTA model - we’ll use the pROC package in R to compute the ROC and plot it:\n\n\ncode\nlibrary(pROC)\n\n# NAFTA\n\nnaftaroc &lt;- roc(test_data$actual, predictions, plot=TRUE, grid=TRUE, partial.auc.correct=TRUE,\n         print.auc=TRUE)"
  },
  {
    "objectID": "binaryextensions224.html#roc-intepretation",
    "href": "binaryextensions224.html#roc-intepretation",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "ROC Intepretation",
    "text": "ROC Intepretation\n\nthe diagonal is a model guessing randomly zero or one - no better than a coin toss.\nabove that line, the model is improving our classification over random guesses.\nthe top left corner would indicate a model that classifies perfectly - 100% sensitivity (TPR), and 0% FPR.\nbelow the diagonal line, the model is classifying worse than a coin toss would.\nwe can compute the Area Under the Curve (AUC) as a percentage - AUC is often reported to indicate model fit. In the NAFTA model, the AUC is .843. AUC closer to one indicates better fit; closer to .5 indicates worse fit, similar to random guessing.\nwe can plot ROCs from different models on the same space and compare their fits.\nThe x-axis is 1-specificity, or the False Positive Rate."
  },
  {
    "objectID": "binaryextensions224.html#roc-democratic-peace",
    "href": "binaryextensions224.html#roc-democratic-peace",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "ROC Democratic Peace",
    "text": "ROC Democratic Peace\nHere’s we compare fit for two models, one including “borders,” the other excluding it. Here are the two models :\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\ndpm1 &lt;-glm(dispute ~ border+deml+caprat+ally+border, family=binomial(link=\"logit\"), data=dp )\ndpm2 &lt;-glm(dispute ~ deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n  \nstargazer(list(dpm1,dpm2), type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n-0.078*** (0.007)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n-0.005*** (0.0005)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n-0.374*** (0.076)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n-2.979*** (0.064)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n7,262.635\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\n\n\nAnd compute the ROC for each model - here Claude.ai and I have written a function to compute the ROC and AUC for each model, and then plot them on the same space.\n\n\ncode\n# part written by Claude.ai\n# compute ROC curve\ncompute_roc &lt;- function(actual, predictions) {\n  # Sort actual and predictions in descending order of predicted probabilities\n  sorted_data &lt;- data.frame(actual, predictions)\n  sorted_data &lt;- sorted_data[order(sorted_data$predictions, decreasing = TRUE), ]\n\n  # Initialize variables\n  n_positive &lt;- sum(actual)\n  n_negative &lt;- length(actual) - n_positive\n  tp &lt;- 0\n  fp &lt;- 0\n  tpr &lt;- c()\n  fpr &lt;- c()\n\n  # Iterate over sorted data\n  for (i in 1:nrow(sorted_data)) {\n    if (sorted_data$actual[i] == 1) {\n      tp &lt;- tp + 1\n    } else {\n      fp &lt;- fp + 1\n    }\n\n    # Calculate true positive rate (TPR) and false positive rate (FPR)\n    tpr &lt;- c(tpr, tp / n_positive)\n    fpr &lt;- c(fpr, fp / n_negative)\n  }\n\n  # Create ROC curve\n  roc_curve &lt;- data.frame(fpr, tpr)\n  return(roc_curve)\n}\n\n# Function to compute AUC\ncompute_auc &lt;- function(fpr, tpr) {\n  # Sort FPR and TPR\n  ord &lt;- order(fpr)\n  fpr &lt;- fpr[ord]\n  tpr &lt;- tpr[ord]\n  \n  # Compute AUC using trapezoidal rule\n  auc &lt;- sum((fpr[-1] - fpr[-length(fpr)]) * (tpr[-1] + tpr[-length(tpr)]) / 2)\n  return(auc)\n}\n\n# Democratic Peace models\n\ntest_dataB &lt;- dp %&gt;% dplyr::select(border,deml,caprat,ally) %&gt;% \n  mutate(actual = dp$dispute)\n\ntest_dataNB &lt;- dp %&gt;% dplyr::select(deml,caprat,ally) %&gt;% \n  mutate(actual = dp$dispute)\n\n# Make predictions on the test data\npredictionsB &lt;- predict(dpm1, newdata = test_dataB, type = \"response\")\npredictionsNB &lt;- predict(dpm2, newdata = test_dataNB, type = \"response\")\n\n\nroc_curveB &lt;- compute_roc(test_dataB$actual, predictionsB)\n#AUC\nauc_border &lt;- compute_auc(roc_curveB$fpr, roc_curveB$tpr)\nroc_curveNB &lt;- compute_roc(test_dataNB$actual, predictionsNB)\nauc_noborder &lt;- compute_auc(roc_curveNB$fpr, roc_curveNB$tpr)\n\n# ROC Plot, pasting auc_border and auc_noborder on plot\n\nggplot() +\n  geom_line(data = roc_curveB, aes(x = fpr, y = tpr, color = \"Borders\")) +\n  geom_line(data = roc_curveNB, aes(x = fpr, y = tpr, color = \"No Borders\")) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"solid\", color = \"red\") +\n  labs(title = \"ROC Curve: Democratic Peace\", x = \"False Positive Rate\", y = \"True Positive Rate\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"Borders\" = \"#005A43\", \"No Borders\" = \"#6CC24A\"))+\n  annotate(\"text\", x = 0.75, y = 0.5, label = paste(\"AUC Model 1: \", round(auc_border, 2)), color = \"#005A43\") +\n  annotate(\"text\", x = 0.75, y = 0.4, label = paste(\"AUC Model 2: \", round(auc_noborder, 2)), color = \"#6CC24A\")\n\n\n\n\n\n\n\n\n\ncode\n#bucolors&lt;-list(\"#005A43\",\"#6CC24A\", \"#A7DA92\", \"#BDBEBD\", \"#000000\" )\n\n\nAlso, note the measure Area Under the Curve (AUC) for each model - the AUC is often reported to indicate model fit. The AUC for the model including borders is 0.75, while the AUC for the model excluding borders is 0.72. A model with an AUC of 0.5 is no better than a coin toss, while a model with an AUC of 1 is perfect."
  },
  {
    "objectID": "binaryextensions224.html#single-coefficient-estimates",
    "href": "binaryextensions224.html#single-coefficient-estimates",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Single Coefficient Estimates",
    "text": "Single Coefficient Estimates\nOne property of MLEs is they are asymptotically multivariate normal; inference is straightforward because the variances are also normal so the ratio of \\(\\beta /se\\) is a z-score."
  },
  {
    "objectID": "binaryextensions224.html#model-evaluation",
    "href": "binaryextensions224.html#model-evaluation",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nMost commonly, we evaluate model fit using one of the “trinity” of tests:\n\nlog-likelihood ratio tests (LLR)\nWald tests\nLagrange Multiplier tests (LM)\n\nThe first two are the most common, and it’s not clear one is better than the other.\n\nLog-Likelihood Ratio Test\nThe LLR test requires estimating two models - a null or constrained model, (\\(M_0\\)), and informed (unconstrained) model (\\(M_1\\)) - it compares the heights of the log-likelihood functions of the two models:\n\\[ \\chi^2 = -2 (ln\\mathcal{L}(M_0) - ln\\mathcal{L}(M_1))  \\]\nThe log-likelihoods here are literally the values of the \\(ln\\mathcal{L}\\) at the estimated maxima of the functions. Their difference is distributed \\(\\chi^2\\) with \\(k_1-k_0\\) degrees of freedom.\nThe LLR is simple to compute (you can do it in your head), but requires estimating two nested models. Recall, two models are nested iff the regressors in the constrained model are a strict subset of those in the unconstrained model, and the samples are identical.\n\n\nWald \\(\\chi^2\\)\nThe Wald test is similar, but only requires the unconstrained or informed model. During maximization, it examines the distance between \\(M_0\\) and \\(M_1\\), and weights that distance by the rate of change between the two (second derivative). If the distance is large and the rate of change is fast, the informed model improves a good bit on the uninformed one. You can imagine other combinations of distance and curvature. Long (p. 88) has a great illustration of this.\n\n\nIn Practice\nThese two tests are asymptotically equivalent. In practice, it makes little difference in most cases which you choose, provided the models are nested.\nStata reports LLR for most models, but reports Wald if you request robust standard errors.\n\n\nLimits\nThe limits of these tests is they apply only to nested models - models where the regressors in the constrained model are a strict subset of those in the unconstrained model and where the samples are identical.\n\n\nInformation Criterion Tests\nAlternatively, we can use information criterion tests - the two most common are the Akaike and Bayesian Information Criteria tests (AIC, BIC). These are both formulated to penalize likelihoods for the number of parameters estimated; this in effect rewards better specification (good variables, but few) and penalizes “garbage can” approaches (including lots of poor predictors).\nIC tests are useful for either nested or nonnested models. This is a significant though under-appreciated virtue of such tests.\n\n\nAkaike and Bayesian Information Criterion tests\n\\[AIC =  -2 ln(\\mathcal{L}) + 2k \\]\n\\[BIC =  -2 ln(\\mathcal{L}) + ln(N) k \\]\nwhere \\(k\\) is the number of parameters.\n\nProcess\nAIC: Estimate model 1; generate the AIC. Estimate model 2; generate the AIC. The model with the smaller AIC is the preferred model (see Long 1997: 110).\nBIC: Estimate model 1; generate the BIC. Estimate model 2; generate the BIC. Compute \\(BIC_1 - BIC_2\\) - the smaller BIC value is the preferable model. The strength of the test statistic is given by Rafferty (1996): absolute value of this difference 0-2 = weak; 2-6= Positive; 6-10= Strong; greater than 10 = Very Strong (see Long 1997: 112)."
  },
  {
    "objectID": "binaryextensions224.html#rare-events-logit",
    "href": "binaryextensions224.html#rare-events-logit",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Rare Events Logit",
    "text": "Rare Events Logit\nWhat they propose is:\n\nSelect all the cases with events (failures).\nRandomly choose a sample of the non-event (censored) cases (they say 2-5 times the size of the failure group).\nThis smaller sample makes data collection possible (compared to the gigantic number of zeros in some event data).\nEstimate a logit on the new, smaller sample, and adjust the estimates for the sample.\n\nThe Rare Events Logit doesn’t appear in the literature much, though it’s not uncommon for reviewers to ask for it. In my experience inferences from this model don’t vary much from the usual logit. The model does present a major opportunity for data collection efforts."
  },
  {
    "objectID": "binaryextensions224.html#footnotes",
    "href": "binaryextensions224.html#footnotes",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nsee what I did there?↩︎"
  },
  {
    "objectID": "ex22024answers.html",
    "href": "ex22024answers.html",
    "title": "Exercise #2 Answers",
    "section": "",
    "text": "Please answer the following questions. All answers should be coded in R and submitted in PDF format, created either in LaTeX or in R Markdown (Quarto). Please turn in both R scripts and PDFs on Brightspace.\nFor this assignment, the only R libraries you need (should use) are\nThis is the hardest assignment of the semester. I want you to know that, and to know that I know it’s hard. Work hard to get this stuff - it’ll pay off.\nThe assignment asks you to write a program to estimate binomial regression models with different link functions. The aim here is to emphasize the parts of the log-likelihood function, and how straightforwardly we can change the link function. The assignment also is designed to push on the programming skill of writing functions.\nShould you pull parts of my code from the maximization slides? Yes, of course. Should you consult AI bots? As a secondary source, sure, but please don’t start there. Please do not rely on bots to write this because I’ll ask you to explain your work, and things will devolve accordingly."
  },
  {
    "objectID": "ex22024answers.html#logit",
    "href": "ex22024answers.html#logit",
    "title": "Exercise #2 Answers",
    "section": "Logit",
    "text": "Logit\n\n\ncode\nces &lt;- read.csv(\"/users/dave/documents/teaching/606J-mle/2024/exercises/ex1/ces.csv\", header=TRUE)\n\ncesanalysis &lt;- ces %&gt;% mutate(pro = ifelse(prochoice == \"Support\", 1, 0), concealw = ifelse(conceal == \"Support\", 1, 0), buildwall = ifelse(wall == \"Support\", 1, 0), repealaca=ifelse(aca == \"Support\", 1, 0), white=ifelse(race == \"White\", 1, 0), vote=ifelse(votechoice == \"Joe Biden (Democrat)\", 1, ifelse(votechoice == \"Donald J. Trump (Republican)\", 0, NA))) \n\n# remove all na values\n\ncesanalysis &lt;- na.omit(cesanalysis)\n\n# llf\nllf &lt;- function(beta, y, X) {\n  probs &lt;- plogis(X %*% beta)\n  loglik &lt;- sum(y * log(probs) + (1 - y) * log(1 - probs))\n  return(loglik)\n}\n\n# logit set up - enter data\nlogit&lt;- function(data, y_var, x_vars) {\n  \n  # Prepare the data, add constant\n  y &lt;- data[[y_var]]\n  X &lt;- as.matrix(cbind(1, data[, x_vars]))\n\n  # starting values\n  start_b &lt;- rep(0, ncol(X))\n\n#Maximize the log-likelihood\n  result &lt;- maxLik(logLik = llf,\n                   start = start_b,\n                   method = \"BFGS\",\n                   y = y,\n                   X = X)\n\n  # Return the results\n  return(result)\n}\n\n\nm1 &lt;- logit(cesanalysis, y_var = \"vote\", x_vars = c(\"age\", \"white\", \"pro\"))\nsummary(m1)\n\n\n--------------------------------------------\nMaximum Likelihood estimation\nBFGS maximization, 47 iterations\nReturn code 0: successful convergence \nLog-Likelihood: -18186.29 \n4  free parameters\nEstimates:\n      Estimate Std. error t value Pr(&gt; t)    \n[1,]  0.716538   0.044679   16.04  &lt;2e-16 ***\n[2,] -0.015078   0.000482  -31.29  &lt;2e-16 ***\n[3,] -1.372134   0.035957  -38.16  &lt;2e-16 ***\n[4,]  2.899563   0.027328  106.10  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\ncode\nmglm &lt;- glm(vote ~ age + white + pro, data = cesanalysis, family = binomial(link = \"logit\"))\nsummary(mglm)\n\n\n\nCall:\nglm(formula = vote ~ age + white + pro, family = binomial(link = \"logit\"), \n    data = cesanalysis)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.7165192  0.0578847   12.38   &lt;2e-16 ***\nage         -0.0150772  0.0008272  -18.23   &lt;2e-16 ***\nwhite       -1.3721731  0.0408534  -33.59   &lt;2e-16 ***\npro          2.8995397  0.0269491  107.59   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 53824  on 39751  degrees of freedom\nResidual deviance: 36373  on 39748  degrees of freedom\nAIC: 36381\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nUsing the same program structure, alter the program to estimate a probit model.\nEstimate the CES model using your probit program, and report the results in a professional table. Compare them to the GLM model from last week - are they the same? [no need to report the GLM, just comment on the comparison.]"
  },
  {
    "objectID": "ex22024answers.html#probit",
    "href": "ex22024answers.html#probit",
    "title": "Exercise #2 Answers",
    "section": "Probit",
    "text": "Probit\n\n\ncode\nces &lt;- read.csv(\"/users/dave/documents/teaching/606J-mle/2024/exercises/ex1/ces.csv\", header=TRUE)\n\ncesanalysis &lt;- ces %&gt;% mutate(pro = ifelse(prochoice == \"Support\", 1, 0), concealw = ifelse(conceal == \"Support\", 1, 0), buildwall = ifelse(wall == \"Support\", 1, 0), repealaca=ifelse(aca == \"Support\", 1, 0), white=ifelse(race == \"White\", 1, 0), vote=ifelse(votechoice == \"Joe Biden (Democrat)\", 1, ifelse(votechoice == \"Donald J. Trump (Republican)\", 0, NA))) \n\n# remove all na values\n\ncesanalysis &lt;- na.omit(cesanalysis)\n\n# llf\nllf &lt;- function(beta, y, X) {\n  probs &lt;- pnorm(X %*% beta)\n  loglik &lt;- sum(y * log(probs) + (1 - y) * log(1 - probs))\n  return(loglik)\n}\n\n# logit set up - enter data\nprobit&lt;- function(data, y_var, x_vars) {\n  \n  # Prepare the data, add constant\n  y &lt;- data[[y_var]]\n  X &lt;- as.matrix(cbind(1, data[, x_vars]))\n\n  # starting values\n  start_b &lt;- rep(0, ncol(X))\n\n#Maximize the log-likelihood\n  result &lt;- maxLik(logLik = llf,\n                   start = start_b,\n                   method = \"BFGS\",\n                   y = y,\n                   X = X)\n\n  # Return the results\n  return(result)\n}\n\n\nm1 &lt;- probit(cesanalysis, y_var = \"vote\", x_vars = c(\"age\", \"white\", \"pro\"))\nsummary(m1)\n\n\n--------------------------------------------\nMaximum Likelihood estimation\nBFGS maximization, 71 iterations\nReturn code 0: successful convergence \nLog-Likelihood: -18208.16 \n4  free parameters\nEstimates:\n       Estimate Std. error t value Pr(&gt; t)    \n[1,]  0.3303225  0.0264991   12.46  &lt;2e-16 ***\n[2,] -0.0084557  0.0004428  -19.09  &lt;2e-16 ***\n[3,] -0.7513117  0.0216933  -34.63  &lt;2e-16 ***\n[4,]  1.7341769  0.0153122  113.25  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\ncode\nmglm &lt;- glm(vote ~ age + white + pro, data = cesanalysis, family = binomial(link = \"probit\"))\nsummary(mglm)\n\n\n\nCall:\nglm(formula = vote ~ age + white + pro, family = binomial(link = \"probit\"), \n    data = cesanalysis)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.3301610  0.0331127   9.971   &lt;2e-16 ***\nage         -0.0084563  0.0004656 -18.162   &lt;2e-16 ***\nwhite       -0.7510290  0.0227045 -33.078   &lt;2e-16 ***\npro          1.7340770  0.0151102 114.762   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 53824  on 39751  degrees of freedom\nResidual deviance: 36416  on 39748  degrees of freedom\nAIC: 36424\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nUsing the same program structure, please write a program to estimate a cloglog regression (so the link function is the cloglog).\nEstimate the CES model using your cloglog program, and report the results in a professional table."
  },
  {
    "objectID": "ex22024answers.html#cloglog",
    "href": "ex22024answers.html#cloglog",
    "title": "Exercise #2 Answers",
    "section": "Cloglog",
    "text": "Cloglog\n\n\ncode\nces &lt;- read.csv(\"/users/dave/documents/teaching/606J-mle/2024/exercises/ex1/ces.csv\", header=TRUE)\n\ncesanalysis &lt;- ces %&gt;% mutate(pro = ifelse(prochoice == \"Support\", 1, 0), concealw = ifelse(conceal == \"Support\", 1, 0), buildwall = ifelse(wall == \"Support\", 1, 0), repealaca=ifelse(aca == \"Support\", 1, 0), white=ifelse(race == \"White\", 1, 0), vote=ifelse(votechoice == \"Joe Biden (Democrat)\", 1, ifelse(votechoice == \"Donald J. Trump (Republican)\", 0, NA))) \n\n# remove all na values\n\ncesanalysis &lt;- na.omit(cesanalysis)\n\n# llf\nllf &lt;- function(beta, y, X) {\n  probs &lt;- 1-exp(-exp(X %*% beta))\n  loglik &lt;- sum(y * log(probs) + (1 - y) * log(1 - probs))\n  return(loglik)\n}\n\n# logit set up - enter data\ncloglog&lt;- function(data, y_var, x_vars) {\n  \n  # Prepare the data, add constant\n  y &lt;- data[[y_var]]\n  X &lt;- as.matrix(cbind(1, data[, x_vars]))\n\n  # starting values\n  start_b &lt;- rep(0, ncol(X))\n\n#Maximize the log-likelihood\n  result &lt;- maxLik(logLik = llf,\n                   start = start_b,\n                   method = \"BFGS\",\n                   y = y,\n                   X = X)\n\n  # Return the results\n  return(result)\n}\n\n\nm1 &lt;- cloglog(cesanalysis, y_var = \"vote\", x_vars = c(\"age\", \"white\", \"pro\"))\nsummary(m1)\n\n\n--------------------------------------------\nMaximum Likelihood estimation\nBFGS maximization, 48 iterations\nReturn code 0: successful convergence \nLog-Likelihood: -18368.19 \n4  free parameters\nEstimates:\n       Estimate Std. error t value Pr(&gt; t)    \n[1,] -0.4177767  0.0307841  -13.57  &lt;2e-16 ***\n[2,] -0.0077632  0.0003905  -19.88  &lt;2e-16 ***\n[3,] -0.6305224  0.0197149  -31.98  &lt;2e-16 ***\n[4,]  1.9395862  0.0187080  103.68  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\ncode\nmglm &lt;- glm(vote ~ age + white + pro, data = cesanalysis, family = binomial(link = \"cloglog\"))\nsummary(mglm)\n\n\n\nCall:\nglm(formula = vote ~ age + white + pro, family = binomial(link = \"cloglog\"), \n    data = cesanalysis)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.4178023  0.0342270  -12.21   &lt;2e-16 ***\nage         -0.0077630  0.0004625  -16.78   &lt;2e-16 ***\nwhite       -0.6307838  0.0207458  -30.41   &lt;2e-16 ***\npro          1.9398906  0.0189437  102.40   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 53824  on 39751  degrees of freedom\nResidual deviance: 36736  on 39748  degrees of freedom\nAIC: 36744\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nUsing the cloglog estimates, please produce average effects and present them in a professional plot.\n\n\n\ncode\nmcloglog &lt;- glm(vote ~ age + white + pro, data = cesanalysis, family = binomial(link = \"cloglog\"))\n\n# Average effects\n\n\n# average effects\n#pro=0\ncesplotdata &lt;- cesanalysis\nmedianpred &lt;-numeric(length(95))\nage &lt;- 0\nmedianse &lt;- numeric(length(95))\nfor (i in seq(18, 95, 1)) {\n  cesplotdata$pro &lt;- 0\n  cesplotdata$age &lt;- i-17\n  predictions &lt;- data.frame(predict(mcloglog, type = \"response\", se=TRUE, newdata=cesplotdata))\n  medianpred[i-17]&lt;- median(predictions$fit, na.rm=TRUE)\n  medianse[i-17] &lt;- median(predictions$se.fit, na.rm=TRUE)\n  age[i-17] &lt;- i\n  pro &lt;-0\n}\n  p1&lt;- data.frame(age= age, pro=pro, xb = medianpred, se=medianse)\n# pro = 1\nfor (i in seq(18, 95, 1)) {\n  cesplotdata$pro &lt;- 1\n  cesplotdata$age &lt;- i-17\n  predictions &lt;- data.frame(predict(mcloglog, type = \"response\", se=TRUE, newdata=cesplotdata))\n  medianpred[i-17]&lt;- median(predictions$fit, na.rm=TRUE)\n  medianse[i-17] &lt;- median(predictions$se.fit, na.rm=TRUE)\n  age[i-17] &lt;- i\n  pro &lt;- 1\n}\n  p2&lt;- data.frame(age= age, pro= pro, xb = medianpred ,se=medianse)\nallpredictions &lt;- rbind(p1, p2)  \n  \n## predictions by prochoice using color=\"#005A43\" and \"#6CC24A\" for ribbon fills\n\nggplot(data=allpredictions, aes(x=age, y=xb, color=factor(pro))) +\n  geom_line() +\n  geom_ribbon(aes(ymin=xb-se, ymax=xb+se, fill=factor(pro)), alpha=.2) +\n  labs(y=\"Predicted Probability\", x=\"Age\") +\n  ggtitle(\"Predicted Probabilities of Voting for Biden by Age and Pro-Choice Stance\") +\n  scale_color_manual(values=c(\"#005A43\", \"#6CC24A\")) +\n  scale_fill_manual(values=c(\"#005A43\", \"#6CC24A\")) +\n  theme_minimal()"
  },
  {
    "objectID": "ex12024answers.html",
    "href": "ex12024answers.html",
    "title": "Exercise #1 Answers",
    "section": "",
    "text": "Please answer the following questions. All answers should be coded in R and submitted in PDF format, created either in LaTeX or in R Markdown (Quarto). Please turn in both R scripts and PDFs on Brightspace.\nFor this assignment, the only R libraries you need (should use) are\nThis assignment is aimed at developing coding skills/habits, an understanding of probability distributions, their relationship to data, and some basic data management.\nFor this section, please write out all PDF/CDF equations - that is, do not use built-in R functions (e.g. plogis, etc.).\ncode\nxb &lt;- runif(1000, min=-4, max=4)\nlogitcdf &lt;- 1/(1+exp(-xb))\ncauchycdf &lt;- 1/2 + atan(xb)/pi\nlogitpdf &lt;- exp(xb)/(1+exp(xb))^2\ncauchypdf &lt;- 1/(pi*(1+xb^2))\n\ndf &lt;- data.frame(xb, logitcdf, cauchycdf, logitpdf, cauchypdf)\n\n#cdf  \ncdf &lt;- ggplot(data=df, aes(x=xb, y=logitcdf)) +\n  geom_line() +\n  geom_line(aes(y=cauchycdf),  linetype=\"dotted\"  ) +\n  annotate(\"text\", x = 0, y = .9, label = \"Logistic\") +\n  annotate(\"text\", x = -3, y = .15, label = \"Cauchy\") +\n  labs(y=\"Pr(Y=1)\",  x=expression(x*beta)) +\n  ggtitle(\"Cauchy and Logistic CDFs\")\n            \npdf &lt;- ggplot(data=df, aes(x=xb, y=logitpdf)) +\n  geom_line() +\n  geom_line(aes(y=cauchypdf), linetype=\"dotted\" ) +\n  annotate(\"text\", x = 0, y = .35, label = \"Cauchy\") +\n  annotate(\"text\", x = -2.8, y = .2, label = \"Logistic\") +\n  labs(y=\"Pr(Y=1)\", x=expression(x*beta)) +\n  ggtitle(\"Cauchy and Logistic PDFs\") +\n  theme_minimal()\n\npdf-cdf\nComment on the differences among the three.\ncode\n# plot CDFs for logistic, cloglog, and loglog distributions\n\nxb &lt;- seq(-4, 4, length.out=1000)\nlogistic &lt;- 1/(1+exp(-xb))\ncloglog &lt;- 1-exp(-exp(xb))\nloglog &lt;- exp(-exp(-xb)) \n\ndf &lt;- data.frame(xb, logistic, cloglog, loglog)\n\nggplot(data=df, aes(x=xb, y=logistic)) +\n  geom_line() +\n  geom_line(aes(y=cloglog), linetype=\"dotted\") +\n  geom_line(aes(y=loglog), linetype=\"twodash\") +\n  annotate(\"text\", x = 1.25, y = .85, label = \"Logistic\") +\n  annotate(\"text\", x = -.5, y = .8, label = \"Cloglog\") +\n  annotate(\"text\", x = 0, y = .2, label = \"Loglog\") +\n  labs(y=\"Pr(Y=1)\", x=expression(x*beta)) +\n  ggtitle(\"Logistic, Cloglog, and Loglog CDFs\")\nPlease describe what you see happening at the intercept changes. Compare these intercept shifts to intercept shifts in the linear model.\ncode\n#logit cdf at -2, -1, 0, 1, 2\n\nxb &lt;- seq(-4, 4, length.out=1000)\n\nlogitcdfm1 &lt;- 1/(1+exp(-xb-1))\nlogitcdf0 &lt;- 1/(1+exp(-xb))\nlogitcdf1 &lt;- 1/(1+exp(-xb+1))\n\ndf &lt;- data.frame(xb, logitcdfm1, logitcdf0, logitcdf1)\n\nggplot(data=df, aes(x=xb, y=logitcdf0)) +\n  geom_line() +\n  geom_line(aes(y=logitcdfm1), linetype=\"longdash\") +\n  geom_line(aes(y=logitcdf1), linetype=\"dashed\") +\n  annotate(\"text\", x = 1.5, y = .4, label = \"Intercept=-1\") +\n  annotate(\"text\", x = 0, y = .5, label = \"Intercept=0\") +\n  annotate(\"text\", x = -1.7, y = .6, label = \"Intercept=+1\") +\n  labs(y=\"Pr(Y=1)\", x=expression(x*beta)) +\n  ggtitle(\"Logistic CDFs with Intercept Shifts\")\nPlease describe what you see happening at the shape parameter changes. How are these similar/different from the logistic CDF?\ncode\n#overlay skewed logit cdfs\n\nxb &lt;- seq(-4, 4, length.out=1000)\n\nscobit1 &lt;- 1/((1+exp(-xb))^1)\nscobit0 &lt;- 1/((1+exp(-xb))^2)\nscobitp5 &lt;- 1/((1+exp(-xb))^.5)\n\ndf &lt;- data.frame(xb, scobit1, scobit0, scobitp5)\n\nggplot(data=df, aes(x=xb, y=scobit1)) +\n  geom_line() +\n  geom_line(aes(y=scobit0), linetype=\"dotted\") +\n  geom_line(aes(y=scobitp5), linetype=\"longdash\") +\n  annotate(\"text\", x = 1.5, y = .4, label = \"shape=1\") +\n  annotate(\"text\", x = 0, y = .5, label = \"shape=0\") +\n  annotate(\"text\", x = -1.7, y = .6, label = \"shape=-1\") +\n  labs(y=\"Pr(Y=1)\", x=expression(x*beta)) +\n  ggtitle(\"Skewed Logit CDFs\")\ncode\nset.seed(8675309)\nn &lt;- 10000\nX &lt;- matrix(rnorm(n * 2), ncol = 2)\ntrue_beta &lt;- c(-1, 0.5, -0.5)\nz &lt;- cbind(1, X) %*% true_beta\nprob &lt;- 1 / (1 + exp(-z))\ny &lt;- rbinom(n, 1, prob)\ncloglog link function:\n\\[ g(p) = 1 - \\exp(-\\exp(x \\beta)) \\] cloglog likelihood function:\n\\[ L(\\beta | y) = \\prod_{i=1}^n \\left[ \\left(1 - \\exp(-\\exp(x \\beta)) \\right)^{y_i} \\left( 1 - 1 - \\exp(-\\exp(x \\beta)) \\right)^{1-y_i} \\right] \\]\ncloglog log-likelihood function:\n\\[ \\ln L(\\beta | y) = \\sum_{i=1}^n \\left[ y_i \\log \\left(1 - \\exp(-\\exp(x \\beta)) \\right) + (1 - y_i) \\log \\left(1 - 1 - \\exp(-\\exp(x \\beta)) \\right) \\right] \\]\ncode\nclogloglik &lt;- function(beta, y, X) {\n  xb &lt;- cbind(1, X) %*% beta\n  prob &lt;- 1 - exp(- exp(xb))\n  -sum(y * log(prob) + (1 - y) * log(1 - prob))\n}\nYou’ll find a dataset alongside this assignment called ces.csv. It contains data from the Cooperative Election Study for the 2020 election via the “American Voter Bot” on Twitter. Variable descriptions are in the data. More information is in the notes section below.\nYou’ll need to look carefully at the variables, recode/clean as necessary prior to estimation - please include all code for this so it can be replicated.\ncode\n# average effects\n#pro=0\ncesplotdata &lt;- cesanalysis\nmedianpred &lt;-numeric(length(95))\nage &lt;- 0\nmedianse &lt;- numeric(length(95))\nfor (i in seq(18, 95, 1)) {\n  cesplotdata$pro &lt;- 0\n  cesplotdata$age &lt;- i-17\n  predictions &lt;- data.frame(predict(m3, type = \"response\", se=TRUE, newdata=cesplotdata))\n  medianpred[i-17]&lt;- median(predictions$fit, na.rm=TRUE)\n  medianse[i-17] &lt;- median(predictions$se.fit, na.rm=TRUE)\n  age[i-17] &lt;- i\n  pro &lt;-0\n}\n  p1&lt;- data.frame(age= age, pro=pro, xb = medianpred, se=medianse)\n# pro = 1\nfor (i in seq(18, 95, 1)) {\n  cesplotdata$pro &lt;- 1\n  cesplotdata$age &lt;- i-17\n  predictions &lt;- data.frame(predict(m3, type = \"response\", se=TRUE, newdata=cesplotdata))\n  medianpred[i-17]&lt;- median(predictions$fit, na.rm=TRUE)\n  medianse[i-17] &lt;- median(predictions$se.fit, na.rm=TRUE)\n  age[i-17] &lt;- i\n  pro &lt;- 1\n}\n  p2&lt;- data.frame(age= age, pro= pro, xb = medianpred ,se=medianse)\nallpredictions &lt;- rbind(p1, p2)  \n  \n## predictions by prochoice using color=\"#005A43\" and \"#6CC24A\" for ribbon fills\n\nggplot(data=allpredictions, aes(x=age, y=xb, color=factor(pro))) +\n  geom_line() +\n  geom_ribbon(aes(ymin=xb-se, ymax=xb+se, fill=factor(pro)), alpha=.2) +\n  labs(y=\"Predicted Probability\", x=\"Age\") +\n  ggtitle(\"Predicted Probabilities of Voting for Biden by Age and Pro-Choice Stance\") +\n  scale_color_manual(values=c(\"#005A43\", \"#6CC24A\")) +\n  scale_fill_manual(values=c(\"#005A43\", \"#6CC24A\")) +\n  theme_minimal()"
  },
  {
    "objectID": "ex12024answers.html#notes",
    "href": "ex12024answers.html#notes",
    "title": "Exercise #1 Answers",
    "section": "Notes",
    "text": "Notes\nThe CES project site is https://cces.gov.harvard.edu. Here’s an article about the “American Voter Bot” project: https://www.nytimes.com/2020/01/20/opinion/twitter-democratic-debate.html?smtyp=cur&smid=tw-nytopinion"
  },
  {
    "objectID": "binaryextensions224.html#revisiting-continuous-v.-discrete-measures",
    "href": "binaryextensions224.html#revisiting-continuous-v.-discrete-measures",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Revisiting continuous v. discrete measures",
    "text": "Revisiting continuous v. discrete measures\n\nContinuous variables (interval, ratio level) have meaningful distances between discrete observations and can be infinitely divided.\n\nWe could, for instance, infinitely divide a measure of income into finer and finer units. The distances between these units would be equal and meaningful in some fashion.\n\nDiscrete variables, on the other hand, only take on certain values.\nThose values may or may not indicate order, value or magnitude, and the intervals between values generally are not meaningful nor are they equal.\n\nDiscrete indicators cannot reasonably be subdivided in meaningful ways as a general rule.\n\nIn spite of this distinction, it is not uncommon for scholars to use a continuous data model (e.g. OLS) to examine data that are by their nature discrete. While the results of doing so are not always awful, they are suboptimal. If we treat a discrete ordered variable as if it is continuous, we are explicitly assuming that the distances between categories are equal. If this assumption is met, our estimates of \\(\\beta\\) might be unbiased (though the errors will be heteroskedastic and nonnormal); if this assumption is not met, then even the \\(\\beta\\)s are biased."
  },
  {
    "objectID": "binaryextensions224.html#discrete-ordered-variables",
    "href": "binaryextensions224.html#discrete-ordered-variables",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Discrete Ordered Variables",
    "text": "Discrete Ordered Variables\nWhen we are interested in predicting an ordered (discrete) variable, we can turn to the ordered logit and probit models - these are natural extensions of the binary models we’ve already discussed and are not that much more complicated. I will focus on the ordered probit because it is more common in the literature, and because it can be easily extended to accommodate non constant errors.\nWhat sorts of measures are ordinal? Likert scales, ranks, survey responses, eg. how much do you trust government to protect you from terrorists?\n\nnot at all.\nsomewhat.\nsubstantially.\ncompletely.\n\nWe can receive 4 meaningful responses (the fifth less meaningful response would be “no opinion, don’t know”) - we’d need this category for exhaustiveness.\nThese observed responses represent some latent variable, trust in government. We cannot directly observe this variable but can measure and observe these discrete manifestations of the unobserved variable.\nWhen we estimate and interpret the discrete variable model, we are interested in the effect of \\(x_i\\) on the underlying latent variable \\(y^{*}\\), though we only measure \\(y_{i}\\). We are really interested in the probability of any given level of trust in government, not in whether the mean of \\(y\\) is 2.3 or 2.6; in fact, is this mean even meaningful? Given the latent variable motivation, no. This is part of the problem with linear regression on such variables."
  },
  {
    "objectID": "binaryextensions224.html#latent-variable-motivation",
    "href": "binaryextensions224.html#latent-variable-motivation",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Latent Variable Motivation",
    "text": "Latent Variable Motivation\nBegin with the binary case - assume a latent quantity we’re interested, denoted \\(y^*\\), but our observations of \\(y\\) are limited to successes (\\(y_i=1\\)) and failures (\\(y_i=0\\)).\n\\[\\begin{aligned}\ny^{*}=\\mathbf{x_i \\beta}+\\epsilon_i \\nonumber\n\\end{aligned}\\]\nfor \\(y^{*}\\), the latent variable,\n\\[ y_{i} = \\left\\{ \\begin{array}{ll}\n         0, & \\mbox{if $-\\infty &lt; y^{*}_{i} \\leq \\tau_1 $} \\\\\n         1, & \\mbox{if $\\tau_1 &lt; y^{*}_{i} \\leq \\infty$}\n         \\end{array}\n     \\right. \\]\nwhere \\(\\tau_1\\) is an unobserved threshold.\nMake probabilities statements, and let \\(\\tau_1=0\\),\n\\[\\begin{aligned}\nPr(y_i=1) = Pr(y^{*}_{1}&gt;0) \\nonumber \\\\ \\nonumber \\\\\n=Pr(\\mathbf{x_i \\beta}+\\epsilon_i&gt;0) \\nonumber \\\\ \\nonumber \\\\\n=Pr(\\epsilon_i&gt;-\\mathbf{x_i \\beta}) \\nonumber\n\\end{aligned}\\]\nIf \\(F(\\cdot)\\) is symmetric, then\n\\[\\begin{aligned}\nPr(y_i=1)= Pr(\\epsilon_i&lt;\\mathbf{x_i \\beta}) \\nonumber\n\\end{aligned}\\]\n\\[\\begin{aligned}\nPr(y_i=1)=Pr(\\epsilon_i&lt;\\mathbf{x_i \\beta}) \\nonumber \\\\ \\nonumber \\\\\n=F(\\mathbf{x_i \\beta}) \\nonumber\n\\end{aligned}\\]\nRelate to the binomial:\n\\[\\begin{aligned}\n\\pi_i= Pr(y_i=1)=  F(\\mathbf{x_i \\beta})    \\nonumber \\\\\n1-\\pi_i=Pr(y_i=0)= 1-F(\\mathbf{x_i \\beta}) \\nonumber\n\\end{aligned}\\]\nLet \\(\\tau_1=0\\) (as above); let \\(-\\infty = \\tau_0\\); let \\(\\infty=\\tau_2\\). We now have \\(j\\) categories of \\(y\\) (\\(j=2\\) in the binary case above), and we have \\(j+1=3\\) unobserved thresholds, \\(\\tau_0, \\tau_1, \\tau_2\\):\n\\[ y_{i} = \\left\\{ \\begin{array}{ll}\n         0, & \\mbox{if $\\tau_0 &lt; y^{*}_{i} \\leq \\tau_1 $} \\\\\n         1, & \\mbox{if $\\tau_1 &lt; y^{*}_{i} \\leq \\tau_2$}\n         \\end{array}\n     \\right. \\]\nand extending,\n\\[ y_{i} = \\left\\{ \\begin{array}{ll}\n         0, & \\mbox{if $\\tau_0 &lt; y^{*}_{i} \\leq \\tau_1 $} \\\\\n         1, & \\mbox{if $\\tau_1 &lt; y^{*}_{i} \\leq \\tau_2$} \\\\\n         2, & \\mbox{if $\\tau_2 &lt; y^{*}_{i} \\leq \\tau_3$} \\\\\n         3, & \\mbox{if $\\tau_3 &lt; y^{*}_{i} \\leq \\tau_4 $} \\\\\n          &$\\ldots$ \\\\\n          j, & \\mbox{if $\\tau_{j-1} &lt; y^{*}_{i} \\leq \\tau_j $} \\\\\n         \\end{array}\n     \\right. \\]\nAs before, let \\(\\tau_0= -\\infty\\), \\(\\tau_j=\\infty\\), and \\(\\tau_1=0\\); so we need to estimate \\(j-3\\) thresholds or values of \\(\\tau\\).\nLet’s relate the formal statement of what we observe (\\(y_i=  j, \\mbox{if }  \\tau_{j-1} \\leq y^{*}\\) etc.) to the survey example.\nWhat are the \\(\\tau\\)s? These are the cutpoints or dividing thresholds between categories of our observed variable, \\(y\\). Thus, \\(\\tau_{1}\\) is the threshold between respondents that fall in the zero category and those that fall in the one category; \\(\\tau_{2}\\) is the threshold between one and two, etc.\nThese thresholds represent an important link between our underlying latent variable and the observed variable insofar as the thresholds measure the probability on the normal curve where we make the transition from one category to the other.\nRecall where we started with the latent variable as a linear function of systematic and random components:\n\\[\\begin{aligned}\ny^{*}=\\mathbf{x_i \\beta}+\\epsilon_i \\nonumber\n\\end{aligned}\\]\nSubstitute:\n\\[y_{i} = \\left\\{ \\begin{array}{ll}\n         0, & \\mbox{if $\\tau_0 &lt; x_i \\beta+\\epsilon_i \\leq \\tau_1 $} \\\\\n         1, & \\mbox{if $\\tau_1 &lt; x_i \\beta+\\epsilon_i \\leq \\tau_2$} \\\\\n         2, & \\mbox{if $\\tau_2 &lt; x_i \\beta+\\epsilon_i \\leq \\tau_3$} \\\\\n         3, & \\mbox{if $\\tau_3 &lt; x_i \\beta+\\epsilon_i \\leq \\tau_4 $} \\\\\n          &$\\ldots$ \\\\\n          j, & \\mbox{if $\\tau_{j-1} &lt; x_i \\beta+\\epsilon_i\\leq \\tau_j $} \\\\\n         \\end{array}\n     \\right. \\]\nIf we had a survey question asking “How secure do you feel personally given the Department of Homeland Security’s handling of terrorist threats?” with the following results,\n\nnot secure at all, I hide under the bed most days - 32%\nsomewhat secure, i’ve got lots of duct tape - 12%\nvery secure, i like the color charts a lot - 22%\nmore secure than I’ve ever felt, bring it on big daddy - 30%\n\nthen these percentages represent the probability at which we shift from one category to the next. So these probabilities divide the normal (in the probit model) into 4 regions each containing the correct mass.\n\\[\\begin{aligned}\n\\tau_1 = \\Phi^{-1} (.32) = z(.32) = -.47  \\nonumber\n\\end{aligned}\\]\nso the cutpoint or threshold is at -.47. The second threshold (\\(\\tau_2\\)) would be at\n\\[\\begin{aligned}\n\\tau_2 = \\Phi^{-1} (.32 + .12) =  -.15 \\nonumber \\\\\n\\tau_3 = \\Phi^{-1} (.32 + .12 + .22) =  .52  \\nonumber\n\\end{aligned}\\]"
  },
  {
    "objectID": "binaryextensions224.html#cut-points",
    "href": "binaryextensions224.html#cut-points",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Cut points",
    "text": "Cut points\n\n\ncode\n# normal pdf with vertical lines at -0.47, -.15, .52\n\nz &lt;- runif(100, -3, 3 )\npz &lt;- dnorm(z)\n\nggplot() + \n  geom_line(aes(x=z, y=pz), color=\"blue\") +\n  geom_vline(xintercept = -0.47, color=\"red\") +\n  geom_vline(xintercept = -0.15, color=\"red\") +\n  geom_vline(xintercept = 0.52, color=\"red\") +\n  labs(title=\"Normal Distribution with Cutpoints\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nAs a general rule, the probability \\(y_i=j\\) is given by:\n\\[\\begin{aligned}\nPr(y_i=j)= F(\\tau_{j} -x_i\\beta) - F(\\tau_{j-1} - x_i\\beta) \\nonumber\n\\end{aligned}\\]\nso in the survey, the probability \\(y_i=1\\) is the area between the two thresholds that delineate \\(y_i=1\\); so the area beneath \\(\\tau_1\\) and \\(\\tau_0=-\\infty=0\\)\n\\[\\begin{aligned}\nPr(y_i=1)= F(\\tau_{1} - x_i\\beta) - 0 \\nonumber \\\\\n\\mbox{or in the probit model} \\nonumber \\\\\nPr(y_i=1)= \\Phi(\\tau_{1} - x_i\\beta)  \\nonumber\n\\end{aligned}\\]\nThis formulation gives us the probability any observation falls in the area between thresholds, and thus provides us estimates of the probability of observing any particular value of the dependent variable.\n\\[\\begin{aligned}\nPr(y_i=1)= \\Phi(\\tau_{1} - x_i\\beta)  \\nonumber \\\\\nPr(y_i=2)= \\Phi(\\tau_{2} - x_i\\beta) - \\Phi(\\tau_{1} - x_i\\beta)  \\nonumber \\\\\nPr(y_i=3)= \\Phi(\\tau_{3} - x_i\\beta) -\\Phi(\\tau_{2} - x_i\\beta)  \\nonumber \\\\\nPr(y_i=4)= 1- \\Phi(\\tau_{4} - x_i\\beta) \\nonumber\n\\end{aligned}\\]\nbecause \\(\\tau_4 = \\infty = 1\\).\n\nAside on Link Functions\nI’ve only written this with a Normal (probit) link but the logit is just as easy:\n\\[\\begin{aligned}\nPr(y_i=1)= F(\\tau_{1} - x_i\\beta) - 0 \\nonumber \\\\\nPr(y_i=1)= \\Lambda(\\tau_{1} - x_i\\beta) \\\\ \\nonumber\nPr(y_i=1)= 1/(1+exp(-(\\tau_{1} - x_i\\beta)))  \\nonumber\n\\end{aligned}\\]"
  },
  {
    "objectID": "binaryextensions224.html#example",
    "href": "binaryextensions224.html#example",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Example",
    "text": "Example\nRecall our survey questions:\n\nnot secure at all.\nsomewhat secure.\nvery secure.\nmore secure than I’ve ever felt, bring it on big daddy.\n\nAnd imagine we also have data on respondents’\n\nyears of education (0-16)\ngender (0 = female; 1 = male)\nparty id (0 = democrat; 1 = republican)\n\nSuppose we use an ordered probit to regress survey response (increasing in security), on education, gender, and party:\n\n\n\nFake Estimates: Perceptions of Security\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX\n\\(\\widehat{\\beta}\\)\ns.e.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngender\n-1.0\n(.22)\n\n\n\n\n\n\n\n\n\nparty id\n+2.0\n(.034)\n\n\n\n\n\n\n\n\n\neducation\n+0.6\n(.11)\n\n\n\n\n\n\n\n\n\nconstant\n-4.0\n(1.2)\n\n\n\n\n\n\n\n\n\n\\(\\tau_2\\)\n+1.5\n(.51)\n\n\n\n\n\n\n\n\n\n\\(\\tau_3\\)\n+3\n(.42)\n\n\n\n\n\n\n\n\n\n\n\n\nRecall that \\(\\tau_0=-\\infty\\); \\(\\tau_1=0\\); and \\(\\tau_4=+\\infty =1\\); the constant shifts \\(\\tau_1\\).\nShifting \\(x\\beta\\) shifts the curve left and right …\n\n\ncode\n# Load required libraries\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Create data\nn &lt;- 1000\nz &lt;- seq(-5, 10, length.out = n)\n\n# Calculate normal densities\na &lt;- dnorm(z, mean = 4.2, sd = sqrt(12))\nb &lt;- dnorm(z, mean = 5.2, sd = sqrt(12))\n\n\n# Define thresholds\nt1 &lt;- 0\nt2 &lt;- 1.5\nt3 &lt;- 4\n\n# Create data frame\ndf &lt;- data.frame(z = z, a = a, b = b)\n\n# Function to create plots\ncreate_plot &lt;- function(data, y_var, title, x_beta) {\n  ggplot(data, aes(x = z)) +\n    geom_line(aes_string(y = y_var)) +\n    geom_vline(xintercept = c(t1, t2, t3), linetype = \"dotted\") +\n    geom_text(aes(x = 0, y = 0.014, label = \"τ[1]\"), parse = TRUE, hjust = 0) +\n    geom_text(aes(x = 1.5, y = 0.014, label = \"τ[2]\"), parse = TRUE, hjust = 0) +\n    geom_text(aes(x = 4, y = 0.014, label = \"τ[3]\"), parse = TRUE, hjust = 0) +\n    geom_text(aes(x = -3, y = 0.057, label = paste0(\"xβ=\", x_beta)), hjust = 0) +\n    geom_text(aes(x = -4, y = 0.072, label = title), hjust = 0) +\n    #theme_minimal() +\n    theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(),\n          axis.title = element_blank(), legend.position = \"none\")  \n}\n\n# Create plots\np1 &lt;- create_plot(df, \"a\", \"Male, Republican, HS grad\", 4.2)\np2 &lt;- create_plot(df, \"b\", \"Female, Republican, HS grad\", 5.2)\n\n# Combined plot\np3 &lt;- ggplot(df, aes(x = z)) +\n  geom_line(aes(y = a), color = \"blue\") +\n  geom_line(aes(y = b), color = \"red\") +\n  geom_vline(xintercept = c(t1, t2, t3), linetype = \"dotted\") +\n  geom_text(aes(x = 0, y = 0.014, label = \"τ[1]\"), parse = TRUE, hjust = 0) +\n  geom_text(aes(x = 1.5, y = 0.014, label = \"τ[2]\"), parse = TRUE, hjust = 0) +\n  geom_text(aes(x = 4, y = 0.014, label = \"τ[3]\"), parse = TRUE, hjust = 0) +\n  geom_text(aes(x = -3, y = 0.05, label = \"xβ=4.2\"), hjust = 0) +\n  geom_text(aes(x = 6.0, y = 0.099, label = \"Female, Republican, HS grad\"), hjust = 0) +\n  geom_text(aes(x = 6.5, y = 0.08, label = \"xβ=5.2\"), hjust = 0) +\n  geom_text(aes(x = -4, y = 0.067, label = \"Male, Republican, HS grad\"), hjust = 0) +\n  #geom_text(aes(x = -4, y = 0.072, label = \"Shift in latent variable given change in xβ. Difference is estimate on Gender (β=-1)\"), hjust = 0) +\n  theme_minimal() +\n  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(),\n        axis.title = element_blank(), legend.position = \"none\") +\n   labs(caption=\"Shift in latent variable given change in xβ. Difference is estimate on Gender (β=-1)\")\n \n\n\np1/p2/p3\n\n\n\n\n\n\n\n\n\n\n\ncode\n# Load required libraries\nlibrary(plotly)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Create data\nn &lt;- 1000\nz &lt;- seq(-5, 10, length.out = n)\n\n# Calculate normal densities\na &lt;- dnorm(z, mean = 4.2, sd = sqrt(12))\nb &lt;- dnorm(z, mean = 5.2, sd = sqrt(12))\n\n# Define thresholds\nt1 &lt;- 0\nt2 &lt;- 1.5\nt3 &lt;- 4\n\n# Create data frame\ndf &lt;- data.frame(z = z, a = a, b = b)\n\n# Create the interactive plot\nplot &lt;- plot_ly() %&gt;%\n  add_lines(x = ~z, y = ~a, data = df, name = \"Male\", line = list(color = \"#005A43\"), visible = TRUE) %&gt;%\n  add_lines(x = ~z, y = ~b, data = df, name = \"Female\", line = list(color = \"#6CC24A\"), visible = FALSE) %&gt;%\n  add_segments(x = t1, xend = t1, y = 0, yend = 0.045, line = list(dash = \"solid\",color = \"#6CC24A\"), showlegend = FALSE) %&gt;%\n  add_segments(x = t2, xend = t2, y = 0, yend = 0.045, line = list(dash = \"solid\",color = \"#6CC24A\"), showlegend = FALSE) %&gt;%\n  add_segments(x = t3, xend = t3, y = 0, yend = 0.045, line = list(dash = \"solid\", color = \"#6CC24A\"), showlegend = FALSE) %&gt;%\n  add_annotations(x = c(0, 1.5, 4), y = 0.014, text = c(\"τ[1]\", \"τ[2]\", \"τ[3]\"), showarrow = FALSE) %&gt;%\n  layout(\n    showlegend = FALSE,\n    xaxis = list(title = \"\", range = c(-5, 10)),\n    yaxis = list(title = \"\", showticklabels = FALSE),\n    annotations = list(\n      list(x = -3, y = 0.05, text = \"xβ=4.2\", showarrow = FALSE, visible = TRUE),\n      list(x = -4, y = 0.059, text = \"Male, Republican, HS grad\", showarrow = FALSE, visible = TRUE),\n      list(x = -3, y = 0.01, text = \"xβ=5.2\", showarrow = FALSE, visible = FALSE),\n      list(x = -4, y = 0.059, text = \"Female, Republican, HS grad\", showarrow = FALSE, visible = FALSE),\n      list(x = -4, y = 0.028, text = \"Shift in latent variable given change in xβ.&lt;br&gt;Difference is estimate on Gender (β=-1)\", showarrow = FALSE, visible = FALSE)\n    ),\n    updatemenus = list(\n      list(\n        type = \"buttons\",\n        direction = \"right\",\n        x = 0.1,\n        y = 1.2,\n        buttons = list(\n          list(method = \"update\",\n               args = list(\n                 list(visible = c(TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE)),\n                 list(title = \"Male, Republican, HS grad\",\n                      annotations = list(\n                        list(x = -3, y = 0.05, text = \"xβ=4.2\", showarrow = FALSE, visible = TRUE),\n                        list(x = -4, y = 0.059, text = \"Male, Republican, HS grad\", showarrow = FALSE, visible = TRUE)\n                      ))\n               ),\n               label = \"Figure 1\"),\n          list(method = \"update\",\n               args = list(\n                 list(visible = c(FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE)),\n                 list(title = \"Female, Republican, HS grad\",\n                      annotations = list(\n                        list(x = -3, y = 0.05, text = \"xβ=5.2\", showarrow = FALSE, visible = TRUE),\n                        list(x = -4, y = 0.059, text = \"Female, Republican, HS grad\", showarrow = FALSE, visible = TRUE)\n                      ))\n               ),\n               label = \"Figure 2\"),\n          list(method = \"update\",\n               args = list(\n                 list(visible = c(TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE)),\n                 list(title = \"Combined Plot\",\n                      annotations = list(\n                        list(x = -3, y = 0.05, text = \"xβ=4.2\", showarrow = FALSE, visible = TRUE),\n                        list(x = -4, y = 0.059, text = \"Male, Republican, HS grad\", showarrow = FALSE, visible = TRUE),\n                        list(x = 6.5, y = 0.059, text = \"Female, Republican, HS grad\", showarrow = FALSE, visible = TRUE),\n                        list(x = 6.5, y = 0.05, text = \"xβ=5.2\", showarrow = FALSE, visible = TRUE),\n                        list(x = -4, y = 0.028, text = \"Shift in latent variable given change in xβ.&lt;br&gt;Difference is estimate on Gender (β=-1)\", showarrow = FALSE, visible = TRUE)\n                      ))\n               ),\n               label = \"Figure 3\")\n        )\n      )\n    )\n  )\n\n# Save the plot as an HTML file\nhtmlwidgets::saveWidget(plot, \"interactive_ordered_probit_plot.html\")\n\n\nplot"
  },
  {
    "objectID": "binaryextensions224.html#estimation",
    "href": "binaryextensions224.html#estimation",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Estimation",
    "text": "Estimation\nBecause the ordered probit and logit models are straightforward extensions of the binary models, estimation is relatively simple. Let’s recall the probit likelihood function:\n\\[\\begin{aligned}\nL(\\beta|Y,X) = \\prod\\limits_{i=1}^{n} [\\Phi(X \\beta)]^{y_{i}} [1-\\Phi(X \\beta)]^{1-y_{i}}  \\nonumber\n\\end{aligned}\\] and the log likelihood as \\[\\begin{aligned}\n\\ln L(\\beta|Y,X) = \\sum\\limits_{i=1}^{n} y_{i} \\ln(\\Phi(X \\beta)) + (1-y_{i}) \\ln(1-\\Phi(X \\beta))  \\nonumber\n\\end{aligned}\\]\n\nProbit …\n\\[\\begin{aligned}\nL(\\beta, \\tau |Y,X) = \\prod\\limits_{i=1}^{n} \\prod\\limits_{j=1}^{m} \\left[ \\Phi(\\tau_{j} - \\beta'X)- \\Phi(\\tau_{j-1}- \\beta'X) \\right]^{y_{ij}}  \\nonumber\n\\end{aligned}\\] and the log likelihood is \\[\\begin{aligned}\n\\ln L(\\beta, \\tau |Y,X) = \\sum\\limits_{i=1}^{n} \\sum\\limits_{j=1}^{m} y_{ij} \\ln \\left[ \\Phi(\\tau_{j}- \\beta'X) - \\Phi(\\tau_{j-1} - \\beta'X) \\right]  \\nonumber\n\\end{aligned}\\]\nNote the LLF will turn on for \\(y=j\\) and off for \\(y\\neq j\\) just as in the binary case.\nand as is the case in the binary probit model, we assume \\(\\sigma^{2}\\) to be one (and in the ordered logit model to be \\(\\frac{\\pi^{2}}{3}\\)); put another way, we assume the model is homoskedastic. Relaxing this assumption is just as easy in the ordered probit model as it is in the binary model since:\n\\[\\begin{aligned}\n\\ln L(\\beta, \\tau |Y,X) = \\sum\\limits_{i=1}^{n} \\sum\\limits_{j=1}^{m} y_{ij} \\ln \\left[ \\Phi \\left(\\frac{\\tau_{j}- \\beta'X}{\\sigma^{2}} \\right) - \\Phi \\left( \\frac{\\tau_{j-1} - \\beta'X)}{\\sigma^{2}} \\right) \\right]  \\nonumber\n\\end{aligned}\\]\nand we can parameterize \\(\\sigma^{2}\\) as \\(\\exp(z \\gamma)\\) just as in the binary model."
  },
  {
    "objectID": "binaryextensions224.html#parallel-regression-assumption",
    "href": "binaryextensions224.html#parallel-regression-assumption",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Parallel Regression Assumption",
    "text": "Parallel Regression Assumption\nOrdered models rest on the parallel regression assumption (in the ordered logit, this is sometimes called the proportional odds assumption). The parallel regression assumption requires that the effect of \\(X_{i}\\) is \\(\\beta\\) for all categories of \\(Y\\). That is, the regression of \\(y_i\\) on \\(x\\) is parallel to the regression of \\(y_j\\) on \\(x\\) and so forth.\nSuppose we have some reason to expect that \\(X_{i}\\) increases the probability of \\(Y=1\\), but decreases the probability of \\(Y=2\\). First, it seems likely we should revisit whether or not \\(Y\\) is ordinal. Second, since we’re only estimating one value of \\(\\beta\\), it cannot simultaneously represent our expectations that \\(X_{i}\\) increases one probability while decreasing another. So, if we run the ordered model, we will only get one value of \\(\\beta\\) and its effect will be the same on all categories of \\(Y\\).\nHere’s what non-parallel regressions might look like:\n\n\ncode\n# Load required libraries\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Define Binghamton University colors\nbinghamton_green &lt;- \"#005A43\"\nbinghamton_gray &lt;- \"#8C8C8C\"\nbinghamton_black &lt;- \"#000000\"\nbinghamton_light_green &lt;- \"#4C7C6F\"\n\n# Define education data\neducation &lt;- seq(0, 20, by = 0.1)\n\n# Function to calculate cumulative probabilities\ncalc_cum_probs &lt;- function(education, beta, thresholds) {\n  xb &lt;- beta * education\n  probs &lt;- lapply(thresholds, function(t) pnorm(t - xb))\n  do.call(cbind, probs)\n}\n\n# Parameters for parallel case\nbeta_parallel &lt;- 0.2\nthresholds_parallel &lt;- c(-1, 0, 1)\n\n# Parameters for non-parallel case\nbeta_non_parallel &lt;- c(0.1, 0.2, 0.3)\nthresholds_non_parallel &lt;- c(-1, 0, 1)\n\n# Calculate probabilities\nprobs_parallel &lt;- calc_cum_probs(education, beta_parallel, thresholds_parallel)\nprobs_non_parallel &lt;- cbind(\n  pnorm(thresholds_non_parallel[1] - beta_non_parallel[1] * education),\n  pnorm(thresholds_non_parallel[2] - beta_non_parallel[2] * education),\n  pnorm(thresholds_non_parallel[3] - beta_non_parallel[3] * education)\n)\n\n# Create data frames\ndf_parallel &lt;- data.frame(education = education, probs_parallel)\ndf_non_parallel &lt;- data.frame(education = education, probs_non_parallel)\n\n# Function to create plot\ncreate_plot &lt;- function(df, title) {\n  ggplot(df, aes(x = education)) +\n    geom_line(aes(y = X1, color = \"Y ≤ 1\"), size = 1) +\n    geom_line(aes(y = X2, color = \"Y ≤ 2\"), size = 1) +\n    geom_line(aes(y = X3, color = \"Y ≤ 3\"), size = 1) +\n    scale_color_manual(values = c(\"Y ≤ 1\" = binghamton_green,\n                                  \"Y ≤ 2\" = binghamton_gray,\n                                  \"Y ≤ 3\" = binghamton_black)) +\n    labs(x = \"Years of Education\", y = \"Cumulative Probability\", title = title, color = \"Category\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\",\n          axis.title = element_text(color = binghamton_black),\n          axis.text = element_text(color = binghamton_black),\n          panel.grid = element_line(color = \"#E0E0E0\"),\n          plot.title = element_text(hjust = 0.5, face = \"bold\"))\n}\n\n# Create plots\np1 &lt;- create_plot(df_parallel, \"Parallel Regression (Assumption Satisfied)\")\np2 &lt;- create_plot(df_non_parallel, \"Non-Parallel Regression (Assumption Violated)\")\n\n# Combine plots\ncombined_plot &lt;- grid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\nEvaluating the parallel regression assumption:\n\nEstimate the ordered logit model and a multinomial model and compare how well they fit the data (compare the log-likelihood \\(\\chi^{2}\\) values). Also examine the MNL estimates of \\(\\beta\\) for \\(x_i\\) and see if they are the same across categories. MNL relaxes the parallel regression assumption. - A second informal way is to estimate \\(j-1\\) individual probit or logit models, one for each additional value of \\(Y\\). Compare the estimates of \\(\\beta_{j}\\) with the ordered probit/logit estimate of \\(\\beta\\). If the estimates are roughly the same, the parallel regression assumption is likely met. This is equivalent to the ordered logit/MNL comparison above.\nformal tests exist as well comparing models where parallel regressions is relaxed and where it’s not.\n\nThe parallel lines assumption is, in my experience, difficult to satisfy. Long & Freese (p. 168) seem to have the same experience. The assumption is extremely restrictive, and perhaps difficult to meet for two reasons:\n\n\\(y\\) may not actually be ordered.\n\\(y | \\mathbf{X}\\) is very likely not ordered."
  },
  {
    "objectID": "binaryextensions224.html#cumulative-probabilities",
    "href": "binaryextensions224.html#cumulative-probabilities",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Cumulative Probabilities",
    "text": "Cumulative Probabilities\n\n\ncode\nt1 &lt;- 0\nt2 &lt;- 1.5\nt3 &lt;- 4\n\neducation &lt;- 1:16\nxb_RM &lt;- (2 - 1 - 4) + 0.4 * education\n\n# probabilities\nc0 &lt;- pnorm(t1 - xb_RM)\nc1 &lt;- pnorm((t2 - xb_RM) + (t1 - xb_RM))\nc2 &lt;- pnorm((t3 - xb_RM) + (t2 - xb_RM))\nc3 &lt;- 1 - pnorm(t3 - xb_RM)\n\n# Create data frame\ndf &lt;- data.frame(education = education, c0 = c0, c1 = c1, c2 = c2, c3 = c3)\n\nbinghamton_green &lt;- \"#005A43\"\nbinghamton_gray &lt;- \"#6CC24A\"\nbinghamton_black &lt;- \"#000000\"\n\nggplot(df, aes(x = education)) +\n  geom_line(aes(y = c0, color = \"Not Secure\"), size = .5) +\n  geom_line(aes(y = c1, color = \"Somewhat Secure\"), size = .5) +\n  geom_line(aes(y = c2, color = \"Very Secure\"), size = .5) +\n  #geom_line(aes(y = c3, color = \"Extremely Secure\"), size = 1) +\n  scale_color_manual(values = c(\"Not Secure\" = binghamton_green,\n                                \"Somewhat Secure\" = binghamton_gray,\n                                \"Very Secure\" = binghamton_black)) +\n  annotate(\"text\", x = 1, y = 0.7, label = \"Not Secure\", hjust = 0, color = binghamton_green) +\n  annotate(\"text\", x = 5, y = 0.9, label = \"Somewhat Secure\", hjust = 0, color = binghamton_gray) +\n  annotate(\"text\", x = 10, y = 0.7, label = \"Very Secure\", hjust = 0, color = binghamton_black) +\n  labs(x = \"Years of Education\", y = \"Pr(y=j)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "binaryextensions224.html#example-1",
    "href": "binaryextensions224.html#example-1",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Example",
    "text": "Example\nThis example uses a dataset on beer quality and price. The dataset contains the following variables:\n\nquality: quality of the beer (1 = fair, 2 = good, 3 = very good, 4 = excellent)\nprice: price of the beer\ncalories: calories per serving\ncraftbeer: indicator for craft beer\nbitter: bitterness score\nmalty: maltiness score\n\nThe \\(y\\) variable here is the four category quality score.\n\n\ncode\n# Load required libraries\nlibrary(haven)\nlibrary(dplyr)\nlibrary(MASS)\nlibrary(ggplot2)\n\n# Read the beer dataset\nbeer &lt;- read_dta(\"/Users/dave/Documents/teaching/606J-mle/2024/topics/ordered-variance/beer.dta\")\n\n# Create quality4 variable\nbeer &lt;- beer %&gt;%\n  mutate(quality4 = ntile(quality, 4))\n\n# Fit ordered logistic regression model\nmodel &lt;- polr(factor(quality4) ~ price + calories + craftbeer + bitter + malty, data = beer)\n\nstargazer::stargazer(model, type = \"html\")\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nquality4\n\n\n\n\n\n\n\n\nprice\n\n\n-0.521*\n\n\n\n\n\n\n(0.297)\n\n\n\n\n\n\n\n\n\n\ncalories\n\n\n0.045***\n\n\n\n\n\n\n(0.012)\n\n\n\n\n\n\n\n\n\n\ncraftbeer\n\n\n-1.738*\n\n\n\n\n\n\n(0.942)\n\n\n\n\n\n\n\n\n\n\nbitter\n\n\n-0.027\n\n\n\n\n\n\n(0.042)\n\n\n\n\n\n\n\n\n\n\nmalty\n\n\n0.054**\n\n\n\n\n\n\n(0.025)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n69\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\nLet’s make predictions of quality over the values of price (at-means):\n\n\ncode\n# at means data\nbeersim &lt;- data.frame(\n  calories = 60:200,\n  price = 4.96,\n  craftbeer = 0,\n  bitter = 35.44,\n  malty = 33.13\n)\n\n# Predict probabilities\nprobs &lt;- predict(model, newdata = beersim, type = \"probs\")\nbeersim &lt;- cbind(beersim, probs)\nnames(beersim)[6:9] &lt;- c(\"ProbFair\", \"ProbGood\", \"ProbVG\", \"ProbExc\")\n\n#  plot\nggplot(beersim, aes(x = calories)) +\n  geom_line(aes(y = ProbFair, color = \"Fair\"), size = 1) +\n  geom_line(aes(y = ProbGood, color = \"Good\"), size = 1, linetype = \"dashed\") +\n  geom_line(aes(y = ProbVG, color = \"Very Good\"), size = 1, linetype = \"longdash\") +\n  geom_line(aes(y = ProbExc, color = \"Excellent\"), size = 1, linetype = \"dotdash\") +\n  scale_color_manual(values = c(\"Fair\" = \"black\", \"Good\" = \"red\", \"Very Good\" = \"navy\", \"Excellent\" = \"darkgreen\")) +\n  labs(y = \"Predicted Probabilities\", x = \"Calories per Serving\", color = \"Quality\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nAnd here, let’s plot the cumulative probabilities:\n\n\ncode\n# Calculate cumulative probabilities\nbeersim &lt;- beersim %&gt;%\n  mutate(\n    CDzero = 0,\n    CDFair = ProbFair,\n    CDGood = ProbFair + ProbGood,\n    CDVG = ProbFair + ProbGood + ProbVG,\n    CDExcellent = ProbFair + ProbGood + ProbVG + ProbExc\n  )\n\n# plot\nggplot(beersim, aes(x = calories)) +\n  geom_area(aes(y = CDExcellent), fill = \"gray20\") +\n  geom_area(aes(y = CDVG), fill = \"gray50\") +\n  geom_area(aes(y = CDGood), fill = \"gray80\") +\n  geom_area(aes(y = CDFair), fill = \"white\") +\n  geom_line(aes(y = CDExcellent), color = \"black\") +\n  geom_line(aes(y = CDVG), color = \"black\") +\n  geom_line(aes(y = CDGood), color = \"black\") +\n  geom_line(aes(y = CDFair), color = \"black\") +\n  labs(y = \"Cumulative Probabilities\", x = \"Calories per Serving\") +\n  scale_x_continuous(limits = c(60, 200), breaks = seq(60, 200, by = 20)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.2)) +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "binaryextensions224.html#parameterizing-sigma2",
    "href": "binaryextensions224.html#parameterizing-sigma2",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Parameterizing \\(\\sigma^2\\)",
    "text": "Parameterizing \\(\\sigma^2\\)\nThe notion here is that the variance is neither constant nor random - it potentially arises as a function of variables, so the goal is to write the variance as a systematic function of variables and coefficients. For the probit model, let\n\\[\\begin{aligned}\nVar[\\epsilon] = \\sigma^{2}=[e^{z\\gamma}]^{2} \\nonumber \\\\\n\\mbox{so} ~~~\\sigma = e^{z\\gamma} \\nonumber\n\\end{aligned}\\]\nand\n\\[\\begin{aligned}\n\\mathcal{L} = \\prod\\limits_{i=1}^{n} \\left[\\Phi \\frac{(X \\beta)}{e^{z\\gamma}} \\right ]^{y_{i}} \\left[1-\\Phi \\frac{(X \\beta)}{e^{z\\gamma}} \\right ]^{1-y_{i}}  \\nonumber\n\\end{aligned}\\]"
  },
  {
    "objectID": "binaryextensions224.html#heteroskedastic-probit-llf",
    "href": "binaryextensions224.html#heteroskedastic-probit-llf",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Heteroskedastic Probit LLF",
    "text": "Heteroskedastic Probit LLF\n\\[\\begin{aligned}\n\\ln \\mathcal{L} = \\sum\\limits_{i=1}^{n} y_{i} \\ln \\Phi \\left( \\frac{X \\beta}{e^{z\\gamma}} \\right) + (1-y_{i}) \\ln \\left[1- \\Phi \\left (\\frac{X \\beta}{e^{z\\gamma}} \\right) \\right] \\nonumber\n\\end{aligned}\\]\nThe LLF now has two unknowns, \\(\\widehat{\\beta}\\) and \\(\\widehat{\\gamma}\\), where \\(\\widehat{\\beta}\\) represents the effects of \\(X\\) on the mean probability of \\(y\\), and \\(\\widehat{\\gamma}\\) represents the effects of \\(Z\\) on the variance of \\(y\\). \\(X\\) and \\(Z\\) can be the same - we can anticipate that the same variables (or some of the same variables) influence the mean of \\(y\\) and the variance of \\(y\\)."
  },
  {
    "objectID": "binaryextensions224.html#link-distribution-for-sigma2",
    "href": "binaryextensions224.html#link-distribution-for-sigma2",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Link Distribution for \\(\\sigma^2\\)",
    "text": "Link Distribution for \\(\\sigma^2\\)\nWhy is\n\\[\\begin{aligned}\n\\sigma^{2}=[e^{z\\gamma}]^{2} \\nonumber\n\\end{aligned}\\]\nThe variance must:\n\nmust be non-negative.\nif the effect of \\(Z\\) on the variance is zero, then \\(\\sigma^{2}\\) must revert to one.\n\nExponentiating $z $ accomplishes both of these goals: it will always be positive and if \\(z\\gamma\\) equals 0, then \\(e^{z\\gamma}\\) will equal one and the model is homoskedastic."
  },
  {
    "objectID": "binaryextensions224.html#what-if-variance-is-not-constant",
    "href": "binaryextensions224.html#what-if-variance-is-not-constant",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "What if variance is not constant?",
    "text": "What if variance is not constant?\nYou notice that dividing the estimate by the variance presents a significant problem if the variance is larger for some groups in the data, smaller for others, but we restrict it to 1:\n\nfor a group with larger variance, but restricted to 1, we over estimate \\(\\beta\\). - for a group with smaller variance, but restricted to 1, we under estimate \\(\\beta\\).\n\nSo the estimates are inconsistent and the standard errors are incorrect. The bottom line is the heteroskedasticity is a bigger deal in binary response models than in the linear model."
  },
  {
    "objectID": "binaryextensions224.html#thinking-about-the-variance",
    "href": "binaryextensions224.html#thinking-about-the-variance",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Thinking about the variance",
    "text": "Thinking about the variance\nWhat does it mean for the variance to be different for different groups in the data? We are accustomed to thinking of groups in the data having different means - this is not so different.\n\none group in the data, given by some \\(x\\) variable, is more diffuse or variant in its behavior on \\(y\\) than another group.\nthose groups may or may not share the same mean behavior."
  },
  {
    "objectID": "binaryextensions224.html#variances",
    "href": "binaryextensions224.html#variances",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Variances",
    "text": "Variances\n\n\ncode\n# plot normal PDFs with mean 0, var 1; mean -1, var 2; mean 1, var 0.5\n\nz &lt;- seq(-5, 5, length.out = 1000)\na &lt;- dnorm(z, mean = 0, sd = 1)\nb &lt;- dnorm(z, mean = -1, sd = sqrt(2))\nc &lt;- dnorm(z, mean = 1, sd = sqrt(0.5))\n\ndf &lt;- data.frame(z = z, a = a, b = b, c = c)\n\n#plot using binghamton colors and annotate in the plot; exclude legend\nggplot(df, aes(x = z)) +\n  geom_line(aes(y = a, color = \"Mean 0, Var 1\"), size = 1) +\n  geom_line(aes(y = b, color = \"Mean -1, Var 2\"), size = 1) +\n  geom_line(aes(y = c, color = \"Mean 1, Var 0.5\"), size = 1) +\n  scale_color_manual(values = c(\"Mean 0, Var 1\" = binghamton_green,\n                                \"Mean -1, Var 2\" = binghamton_gray,\n                                \"Mean 1, Var 0.5\" = binghamton_black)) +\n  labs(x = \"z\", y = \"Density\", color = \"Group\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n##Variance of \\(\\epsilon\\)\nThe variance of \\(\\epsilon\\) in any model can be thought of like this:\n\\[\\begin{aligned}\nvar(\\epsilon_i)=var(\\epsilon_j) \\forall i,j \\ldots n \\nonumber\n\\end{aligned}\\]\nThis is explicitly why we write the variance of the errors without a subscript - var(\\(\\epsilon\\)) - it is constant across all \\(i\\).\nPut slightly differently, the distribution of \\(\\epsilon\\) is the same for all \\(i\\). If this does not hold, then the errors are not independent and identically distributed (i.i.d.) - their distributions are different. This is just another way to state the problem of nonconstant variance."
  },
  {
    "objectID": "binaryextensions224.html#varepsilon_ivarepsilon_j",
    "href": "binaryextensions224.html#varepsilon_ivarepsilon_j",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "\\(var(\\epsilon_i)=var(\\epsilon_j)\\)",
    "text": "\\(var(\\epsilon_i)=var(\\epsilon_j)\\)\n\n\ncode\nz &lt;- seq(-5, 5, length.out = 1000)\na &lt;- dnorm(z, mean = -1, sd = .5)\nb &lt;- dnorm(z, mean = 1, sd = .5)\nc &lt;- dnorm(z, mean = -1, sd = .5)\nd &lt;- dnorm(z, mean = 1, sd = sqrt(2))\n\ndf1 &lt;- data.frame(z = z, a = a, b = b)\n\ndf2 &lt;- data.frame(z = z, c = c, d = d)\n\n#plot using binghamton colors and annotate in the plot; exclude legend\n\np1 &lt;- ggplot(df1, aes(x = z)) +\n  geom_line(aes(y = a, color = \"Mean -1, Var 1\"), size = 1) +\n  geom_line(aes(y = b, color = \"Mean 1, Var 1\"), size = 1) +\n  scale_color_manual(values = c(\"Mean -1, Var 1\" = binghamton_green,\n                                \"Mean 1, Var 1\" = binghamton_gray)) +\n  labs(x = \"z\", y = \"Density\", color = \"Group\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")+\n  ggtitle(\"Constant Variance\")\n\np2 &lt;- ggplot(df2, aes(x = z)) +\n  geom_line(aes(y = c, color = \"Mean -1, Var 1\"), size = 1) +\n  geom_line(aes(y = d, color = \"Mean 1, Var 2\"), size = 1) +\n  scale_color_manual(values = c(\"Mean -1, Var 1\" = binghamton_green,\n                                \"Mean 1, Var 2\" = binghamton_gray)) +\n  labs(x = \"z\", y = \"Density\", color = \"Group\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")+\n  ggtitle(\"Non-Constant Variance\")\n\np1/p2"
  },
  {
    "objectID": "binaryextensions224.html#a-framework-for-theory-about-variance",
    "href": "binaryextensions224.html#a-framework-for-theory-about-variance",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "A Framework for Theory about Variance",
    "text": "A Framework for Theory about Variance\nWe’ve spent a lot of time fretting about the information in our data w.r.t. maximizing the LLF. A good bit of that information is related to the variability in the data. It makes sense to think about the sources of that variability. In building arguments, quantitative social scientists tend to obsess over central tendency, but to neglect thinking about what the dispersion in the data means.\nHere’s an attempt at a basic framework for thinking about variance:"
  },
  {
    "objectID": "binaryextensions224.html#variance-framework",
    "href": "binaryextensions224.html#variance-framework",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Variance Framework",
    "text": "Variance Framework\nSubstantively, what can variance represent?\n\namount of information (certainty, uncertainty)\nprecision, accuracy\nuniformity, diversity, heterogeneity\nchoice, constraint\nability, inability\nambivalence\n\nImagine a data set of \\(y\\) and \\(x\\) – suppose \\(x\\) is binary - it might relate to the mean of \\(y\\) and to the variance of \\(y\\), such that:\n\nan increase in \\(x\\) is related to an increase (decrease) in \\(y\\).\nan increase in \\(x\\) is related to an increase (decrease) in the variance of \\(y\\).\n\nThis might be because:\n\nthere are two groups of observations in the data w.r.t. \\(x\\)\none group has a higher/lower mean of \\(y\\) than the other.\none group is more/less heterogeneous in \\(y\\) than the other.\n\nPerhaps this is because:\n\nas individuals become more informed, they prefer more \\(y\\). This is an expectation about the mean of \\(y\\) - as \\(x\\) increases, the mean of \\(y\\) increases.\nas individuals become more informed, they behave more uniformly in preferring \\(y\\). This is an expectation about the variance of \\(y\\) - as \\(x\\) increases, the variance surrounding \\(y\\) decreases.\nless informed individuals prefer less \\(y\\), but choose more diffusely.\n\n\\(x\\) has two effects - increasing the mean and decreasing the variance of \\(y\\)."
  },
  {
    "objectID": "binaryextensions224.html#variance-framework-1",
    "href": "binaryextensions224.html#variance-framework-1",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Variance Framework",
    "text": "Variance Framework\nImagine a data set of \\(y\\) and \\(x\\) – suppose \\(x\\) is binary - it might relate to the mean of \\(y\\) and to the variance of \\(y\\), such that:\n\nan increase in \\(x\\) is related to an increase (decrease) in \\(y\\).\nan increase in \\(x\\) is related to an increase (decrease) in the variance of \\(y\\)."
  },
  {
    "objectID": "binaryextensions224.html#relaxing-parallel-regression",
    "href": "binaryextensions224.html#relaxing-parallel-regression",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Relaxing Parallel Regression",
    "text": "Relaxing Parallel Regression\nThe Generalized ordered logit/probit is one pathway to relaxing the parallel regression assumption. The generalized ordered logit/probit allows for the effects of \\(X\\) to vary across categories of \\(y\\), such that the model estimates \\(k-1\\) values of \\(\\beta\\). The model can be unstable and produce predictions out of bounds. Here’s the same model predicting beer quality, but we’re relaxing the parallel regression assumption in the variables “price” and “calories” - you’ll note each of these has \\(k-1\\) coefficients.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nlibrary(readr)\nlibrary(dplyr)\nlibrary(VGAM)\nlibrary(ggplot2)\n\nbeer &lt;- read_dta(\"/Users/dave/Documents/teaching/606J-mle/2024/topics/ordered-variance/beer.dta\")\n\n\nbeer &lt;- beer %&gt;%\n  mutate(quality4 = ntile(quality, 4))\n\n# Fit the generalized ordered logit model\ngologit_model &lt;- vglm(quality4 ~ price + calories + craftbeer + bitter + malty,\n      family = cumulative(parallel = FALSE~price+calories, reverse = TRUE), data = beer)\n\nmodelsummary::modelsummary(gologit_model)\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept) × 1\n                  -1.586 \n                \n                \n                                 \n                  (1.703)\n                \n                \n                  (Intercept) × 2\n                  -5.784 \n                \n                \n                                 \n                  (2.474)\n                \n                \n                  (Intercept) × 3\n                  -12.013\n                \n                \n                                 \n                  (2.919)\n                \n                \n                  price × 1      \n                  -0.599 \n                \n                \n                                 \n                  (0.357)\n                \n                \n                  price × 2      \n                  -0.447 \n                \n                \n                                 \n                  (0.324)\n                \n                \n                  price × 3      \n                  -0.399 \n                \n                \n                                 \n                  (0.346)\n                \n                \n                  calories × 1   \n                  0.043  \n                \n                \n                                 \n                  (0.013)\n                \n                \n                  calories × 2   \n                  0.057  \n                \n                \n                                 \n                  (0.018)\n                \n                \n                  calories × 3   \n                  0.087  \n                \n                \n                                 \n                  (0.020)\n                \n                \n                  craftbeer      \n                  -1.668 \n                \n                \n                                 \n                  (0.956)\n                \n                \n                  bitter         \n                  -0.029 \n                \n                \n                                 \n                  (0.042)\n                \n                \n                  malty          \n                  0.046  \n                \n                \n                                 \n                  (0.024)\n                \n                \n                  Num.Obs.       \n                  69     \n                \n                \n                  AIC            \n                  182.3  \n                \n                \n                  BIC            \n                  209.1  \n                \n                \n                  RMSE           \n                  2.40   \n                \n        \n      \n    \n\n\nPrice and Calories can exert effects in different directions across categories of \\(y\\), hence relaxing the parallel regression assumption."
  },
  {
    "objectID": "binaryextensions224.html#use-ordered-models-with-caution",
    "href": "binaryextensions224.html#use-ordered-models-with-caution",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Use Ordered Models with Caution …",
    "text": "Use Ordered Models with Caution …\nMy own view is ordered models rarely fit the data - our outcome variables are rarely conditionally ordered, i.e, the expected value of \\(y\\) ordered given the variables in the model. Other models (choice models in particular) are better suited for much of our data - and we are better able to satisfy their assumptions. As the wise man says, just because something can be ordered doesn’t mean it should be. Or, as the econometrician Amemyia (1985) says,\n“A model is unordered if it is not ordered.” (Amemyia 1985, 292)."
  },
  {
    "objectID": "binaryextensions224.html#variance-framework-2",
    "href": "binaryextensions224.html#variance-framework-2",
    "title": "Symmetry, Classification, and Model Fit",
    "section": "Variance Framework",
    "text": "Variance Framework\nThis might be because:\n\nthere are two groups of observations in the data w.r.t. \\(x\\)\none group has a higher/lower mean of \\(y\\) than the other.\none group is more/less heterogeneous in \\(y\\) than the other.\n\nPerhaps this is because:\n\nas individuals become more informed, they prefer more \\(y\\). This is an expectation about the mean of \\(y\\) - as \\(x\\) increases, the mean of \\(y\\) increases.\nas individuals become more informed, they behave more uniformly in preferring \\(y\\). This is an expectation about the variance of \\(y\\) - as \\(x\\) increases, the variance surrounding \\(y\\) decreases.\nless informed individuals prefer less \\(y\\), but choose more diffusely.\n\n\\(x\\) has two effects - increasing the mean and decreasing the variance of \\(y\\).\n\n\n\nExample\nAs an illustratio, let’s estimate a model predicting whether respondents believe Barack Obama is a secret Muslim. This analysis uses data from the ANES 2016 Pilot. We’ll specify two models. The probit model regresses responses to a question about whether Obama is a secret Muslim on a set of demographic and political variables; the heteroskedastic probit model posits age affects both the mean (as in the first model) and the variance. The expectation is that older voters are more likely to believe Obama is a secret Muslim, and that older voters believe this less uniformly or more diffusely than do younger voters. So we expect the effect of age on the mean to be positive, and the effect of age on the variance to be negative. Results are below - the first model is the standard probit model, the second is the heteroskedastic probit model.\n\n\ncode\nlibrary(Rchoice)\n\n# ANES 2016 data\nanes &lt;- read_csv(\"/Users/dave/Documents/teaching/606J-mle/2020/slides/L3_binaryextensions/code/anes_pilot_2016.csv\")\n\n# Select variables\nvars_to_keep &lt;- c(\"bo_muslim\", \"pid7\", \"disc_wo\", \"lazyb\", \"disc_b\", \"faminc\", \"birthyr\", \"race\", \"autism\", \"disc_selfsex\", \"gender\", \"vaccine\")\nanes &lt;- anes[, vars_to_keep]\n\n# Recode variables\nanes &lt;- anes %&gt;%\n  mutate(\n    pid7 = as.numeric(pid7),\n    bo_muslim = case_when(\n      bo_muslim == 2 ~ 0,\n      bo_muslim == 1 ~ 1,\n      bo_muslim == 8 ~ NA_real_,\n      TRUE ~ bo_muslim\n    ),\n    pid7 = ifelse(pid7 &gt; 7, NA, pid7),\n    disc_wo = ifelse(disc_wo &gt; 7, NA, disc_wo),\n    lazyb = ifelse(lazyb &gt; 7, NA, lazyb),\n    disc_b = ifelse(disc_b &gt; 5, NA, disc_b),\n    faminc = ifelse(faminc &gt; 16, NA, faminc),\n    age = 2016 - birthyr,\n    white = ifelse(race == 1, 1, 0),\n    # Reverse coding\n    disc_wo = -1 * disc_wo + 6,\n    disc_selfsex = -1 * disc_selfsex + 6,\n    autism = -1 * autism + 7,\n    disc_b = -1 * disc_b + 6\n  )\n# Ensure disc_wo is binary for probit model\nanes &lt;- anes %&gt;%\n  mutate(disc_wo_binary = ifelse(disc_wo &gt; median(disc_wo, na.rm = TRUE), 1, 0))\n\n# model \nhetprob_model &lt;- hetprob(bo_muslim ~ white+disc_b+pid7+age+ faminc+autism | age, data = anes, link=\"probit\")\n\n# Compare with standard probit model\nprobit_model &lt;- glm(bo_muslim ~ white + disc_b + pid7 + age + faminc + autism, data = anes, family = binomial(link = \"probit\"))\n\n\n# Load required libraries\nlibrary(Rchoice)\nlibrary(kableExtra)\nlibrary(dplyr)\n\n# Function to extract and format Rchoice model results\nextract_rchoice_results &lt;- function(model) {\n  coef &lt;- tryCatch(coef(model), error = function(e) NULL)\n  vcov_matrix &lt;- tryCatch(vcov(model), error = function(e) NULL)\n  \n  if (is.null(coef) || is.null(vcov_matrix)) {\n    stop(\"Unable to extract coefficients or variance-covariance matrix from the model.\")\n  }\n  \n  se &lt;- sqrt(diag(vcov_matrix))\n  p_value &lt;- 2 * (1 - pnorm(abs(coef / se)))\n  \n  results_df &lt;- data.frame(\n    Estimate = coef,\n    `Std. Error` = se,\n    `Pr(&gt;|z|)` = p_value\n  )\n  \n  return(results_df)\n}\n\n# Function to create a formatted HTML table for two models\ncreate_two_model_table &lt;- function(model1, model2, model1_name = \"Model 1\", model2_name = \"Model 2\") {\n  tryCatch({\n    # Extract results for both models\n    results1 &lt;- extract_rchoice_results(model1)\n    results2 &lt;- extract_rchoice_results(model2)\n    \n    # Combine results\n    combined_results &lt;- full_join(\n      results1 %&gt;% mutate(Variable = rownames(results1)),\n      results2 %&gt;% mutate(Variable = rownames(results2)),\n      by = \"Variable\",\n      suffix = c(\".1\", \".2\")\n    ) %&gt;%\n      select(Variable, everything()) %&gt;%\n      arrange(Variable)\n    \n    # Rename columns\n    names(combined_results) &lt;- c(\"Variable\",\n                                 \"Estimate.1\", \"Std. Error.1\", \"Pr(&gt;|z|).1\",\n                                 \"Estimate.2\", \"Std. Error.2\", \"Pr(&gt;|z|).2\")\n    \n    # Create HTML table\n    html_table &lt;- kable(combined_results, \n                        format = \"html\",\n                        digits = 3,\n                        caption = \"Comparison of  Probit Models\") %&gt;%\n      kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) %&gt;%\n      add_header_above(c(\" \" = 1, \n                         model1_name = 3, \n                         model2_name = 3)) \n    \n    # Save the HTML table to a file\n    #writeLines(as.character(html_table), \"two_model_results.html\")\n    \n    # Print the HTML code\n    print(html_table)\n  }, error = function(e) {\n    cat(\"An error occurred:\", conditionMessage(e), \"\\n\")\n  })\n}\n\n\n# Usage example (replace with your actual models):\ncreate_two_model_table(probit_model, hetprob_model, model1_name =\"Base Model\", model2_name=\"Extended Model\")\n\n\n\nComparison of Probit Models\n\n\n\n\n\n\n\nmodel1_name\n\n\n\n\nmodel2_name\n\n\n\n\n\nVariable\n\n\nEstimate.1\n\n\nStd. Error.1\n\n\nPr(&gt;|z|).1\n\n\nEstimate.2\n\n\nStd. Error.2\n\n\nPr(&gt;|z|).2\n\n\n\n\n\n\n(Intercept)\n\n\n-1.808\n\n\n0.278\n\n\n0.000\n\n\n-1.253\n\n\n0.284\n\n\n0.000\n\n\n\n\nage\n\n\n0.013\n\n\n0.003\n\n\n0.000\n\n\n0.010\n\n\n0.002\n\n\n0.000\n\n\n\n\nautism\n\n\n0.199\n\n\n0.030\n\n\n0.000\n\n\n0.128\n\n\n0.033\n\n\n0.000\n\n\n\n\ndisc_b\n\n\n-0.145\n\n\n0.043\n\n\n0.001\n\n\n-0.101\n\n\n0.033\n\n\n0.002\n\n\n\n\nfaminc\n\n\n-0.055\n\n\n0.015\n\n\n0.000\n\n\n-0.035\n\n\n0.012\n\n\n0.004\n\n\n\n\nhet.age\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n-0.009\n\n\n0.004\n\n\n0.028\n\n\n\n\npid7\n\n\n0.231\n\n\n0.024\n\n\n0.000\n\n\n0.148\n\n\n0.035\n\n\n0.000\n\n\n\n\nwhite\n\n\n0.128\n\n\n0.116\n\n\n0.271\n\n\n0.074\n\n\n0.078\n\n\n0.341\n\n\n\n\n\nHere are predictions from the two models. The main takeaway is that if you change things, things change - so there’s not much substantively to take away from the differences between the predictions. Strong theory expectations would potentially drive expectations about these differences.\n\n\ncode\npred_data &lt;- expand.grid(\n  disc_b = median(anes$disc_selfsex, na.rm = TRUE),\n  age = seq(min(anes$age, na.rm = TRUE), max(anes$age, na.rm = TRUE), length.out = 100),\n  faminc = median(anes$faminc, na.rm = TRUE),\n  pid7 = median(anes$pid7, na.rm = TRUE),\n  autism = median(anes$autism, na.rm = TRUE),\n  white = 1\n)\n\n# Predict probabilities\npred_probs_het &lt;- predict(hetprob_model, newdata = pred_data, type = \"pr\")\npred_probs_std &lt;- predict(probit_model, newdata = pred_data, type = \"response\")\n\n# Combine predictions\nplot_data &lt;- cbind(pred_data, \n                   Het_Prob = pred_probs_het,\n                   Std_Prob = pred_probs_std)\n\n# Create the plot\nggplot(plot_data, aes(x = age)) +\n  geom_line(aes(y = Het_Prob, color = \"Heteroskedastic Probit\")) +\n  geom_line(aes(y = Std_Prob, color = \"Standard Probit\")) +\n  labs(x = \"Age\", y = \"Pr(Secret Muslim)\", color = \"Model\") +\n  theme_minimal() +\n  ggtitle(\"Comparison of Heteroskedastic and Standard Probit Models\")"
  },
  {
    "objectID": "binaryextensions224.html#example-2",
    "href": "binaryextensions224.html#example-2",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Example",
    "text": "Example\nAs an illustratio, let’s estimate a model predicting whether respondents believe Barack Obama is a secret Muslim. This analysis uses data from the ANES 2016 Pilot. We’ll specify two models. The probit model regresses responses to a question about whether Obama is a secret Muslim on a set of demographic and political variables; the heteroskedastic probit model posits age affects both the mean (as in the first model) and the variance. The expectation is that older voters are more likely to believe Obama is a secret Muslim, and that older voters believe this less uniformly or more diffusely than do younger voters. So we expect the effect of age on the mean to be positive, and the effect of age on the variance to be negative. Results are below - the first model is the standard probit model, the second is the heteroskedastic probit model.\n\n\ncode\nlibrary(Rchoice)\n\n# ANES 2016 data\nanes &lt;- read_csv(\"/Users/dave/Documents/teaching/606J-mle/2020/slides/L3_binaryextensions/code/anes_pilot_2016.csv\")\n\n# Select variables\nvars_to_keep &lt;- c(\"bo_muslim\", \"pid7\", \"disc_wo\", \"lazyb\", \"disc_b\", \"faminc\", \"birthyr\", \"race\", \"autism\", \"disc_selfsex\", \"gender\", \"vaccine\")\nanes &lt;- anes[, vars_to_keep]\n\n# Recode variables\nanes &lt;- anes %&gt;%\n  mutate(\n    pid7 = as.numeric(pid7),\n    bo_muslim = case_when(\n      bo_muslim == 2 ~ 0,\n      bo_muslim == 1 ~ 1,\n      bo_muslim == 8 ~ NA_real_,\n      TRUE ~ bo_muslim\n    ),\n    pid7 = ifelse(pid7 &gt; 7, NA, pid7),\n    disc_wo = ifelse(disc_wo &gt; 7, NA, disc_wo),\n    lazyb = ifelse(lazyb &gt; 7, NA, lazyb),\n    disc_b = ifelse(disc_b &gt; 5, NA, disc_b),\n    faminc = ifelse(faminc &gt; 16, NA, faminc),\n    age = 2016 - birthyr,\n    white = ifelse(race == 1, 1, 0),\n    # Reverse coding\n    disc_wo = -1 * disc_wo + 6,\n    disc_selfsex = -1 * disc_selfsex + 6,\n    autism = -1 * autism + 7,\n    disc_b = -1 * disc_b + 6\n  )\n# Ensure disc_wo is binary for probit model\nanes &lt;- anes %&gt;%\n  mutate(disc_wo_binary = ifelse(disc_wo &gt; median(disc_wo, na.rm = TRUE), 1, 0))\n\n# model \nhetprob_model &lt;- hetprob(bo_muslim ~ white+disc_b+pid7+age+ faminc+autism | age, data = anes, link=\"probit\")\n\n# Compare with standard probit model\nprobit_model &lt;- glm(bo_muslim ~ white + disc_b + pid7 + age + faminc + autism, data = anes, family = binomial(link = \"probit\"))\n\n\n# Load required libraries\nlibrary(Rchoice)\nlibrary(kableExtra)\nlibrary(dplyr)\n\n# Function to extract and format Rchoice model results\nextract_rchoice_results &lt;- function(model) {\n  coef &lt;- tryCatch(coef(model), error = function(e) NULL)\n  vcov_matrix &lt;- tryCatch(vcov(model), error = function(e) NULL)\n  \n  if (is.null(coef) || is.null(vcov_matrix)) {\n    stop(\"Unable to extract coefficients or variance-covariance matrix from the model.\")\n  }\n  \n  se &lt;- sqrt(diag(vcov_matrix))\n  p_value &lt;- 2 * (1 - pnorm(abs(coef / se)))\n  \n  results_df &lt;- data.frame(\n    Estimate = coef,\n    `Std. Error` = se,\n    `Pr(&gt;|z|)` = p_value\n  )\n  \n  return(results_df)\n}\n\n# Function to create a formatted HTML table for two models\ncreate_two_model_table &lt;- function(model1, model2, model1_name = \"Model 1\", model2_name = \"Model 2\") {\n  tryCatch({\n    # Extract results for both models\n    results1 &lt;- extract_rchoice_results(model1)\n    results2 &lt;- extract_rchoice_results(model2)\n    \n    # Combine results\n    combined_results &lt;- full_join(\n      results1 %&gt;% mutate(Variable = rownames(results1)),\n      results2 %&gt;% mutate(Variable = rownames(results2)),\n      by = \"Variable\",\n      suffix = c(\".1\", \".2\")\n    ) %&gt;%\n      select(Variable, everything()) %&gt;%\n      arrange(Variable)\n    \n    # Rename columns\n    names(combined_results) &lt;- c(\"Variable\",\n                                 \"Estimate.1\", \"Std. Error.1\", \"Pr(&gt;|z|).1\",\n                                 \"Estimate.2\", \"Std. Error.2\", \"Pr(&gt;|z|).2\")\n    \n    # Create HTML table\n    html_table &lt;- kable(combined_results, \n                        format = \"html\",\n                        digits = 3,\n                        caption = \"Comparison of  Probit Models\") %&gt;%\n      kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) %&gt;%\n      add_header_above(c(\" \" = 1, \n                         model1_name = 3, \n                         model2_name = 3)) \n    \n    # Save the HTML table to a file\n    #writeLines(as.character(html_table), \"two_model_results.html\")\n    \n    # Print the HTML code\n    print(html_table)\n  }, error = function(e) {\n    cat(\"An error occurred:\", conditionMessage(e), \"\\n\")\n  })\n}\n\n\n# Usage example (replace with your actual models):\ncreate_two_model_table(probit_model, hetprob_model, model1_name =\"Base Model\", model2_name=\"Extended Model\")\n\n\n\nComparison of Probit Models\n\n\n\n\n\n\n\nmodel1_name\n\n\n\n\nmodel2_name\n\n\n\n\n\nVariable\n\n\nEstimate.1\n\n\nStd. Error.1\n\n\nPr(&gt;|z|).1\n\n\nEstimate.2\n\n\nStd. Error.2\n\n\nPr(&gt;|z|).2\n\n\n\n\n\n\n(Intercept)\n\n\n-1.808\n\n\n0.278\n\n\n0.000\n\n\n-1.253\n\n\n0.284\n\n\n0.000\n\n\n\n\nage\n\n\n0.013\n\n\n0.003\n\n\n0.000\n\n\n0.010\n\n\n0.002\n\n\n0.000\n\n\n\n\nautism\n\n\n0.199\n\n\n0.030\n\n\n0.000\n\n\n0.128\n\n\n0.033\n\n\n0.000\n\n\n\n\ndisc_b\n\n\n-0.145\n\n\n0.043\n\n\n0.001\n\n\n-0.101\n\n\n0.033\n\n\n0.002\n\n\n\n\nfaminc\n\n\n-0.055\n\n\n0.015\n\n\n0.000\n\n\n-0.035\n\n\n0.012\n\n\n0.004\n\n\n\n\nhet.age\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n-0.009\n\n\n0.004\n\n\n0.028\n\n\n\n\npid7\n\n\n0.231\n\n\n0.024\n\n\n0.000\n\n\n0.148\n\n\n0.035\n\n\n0.000\n\n\n\n\nwhite\n\n\n0.128\n\n\n0.116\n\n\n0.271\n\n\n0.074\n\n\n0.078\n\n\n0.341\n\n\n\n\n\nHere are predictions from the two models. The main takeaway is that if you change things, things change - so there’s not much substantively to take away from the differences between the predictions. Strong theory expectations would potentially drive expectations about these differences.\n\n\ncode\npred_data &lt;- expand.grid(\n  disc_b = median(anes$disc_selfsex, na.rm = TRUE),\n  age = seq(min(anes$age, na.rm = TRUE), max(anes$age, na.rm = TRUE), length.out = 100),\n  faminc = median(anes$faminc, na.rm = TRUE),\n  pid7 = median(anes$pid7, na.rm = TRUE),\n  autism = median(anes$autism, na.rm = TRUE),\n  white = 1\n)\n\n# Predict probabilities\npred_probs_het &lt;- predict(hetprob_model, newdata = pred_data, type = \"pr\")\npred_probs_std &lt;- predict(probit_model, newdata = pred_data, type = \"response\")\n\n# Combine predictions\nplot_data &lt;- cbind(pred_data, \n                   Het_Prob = pred_probs_het,\n                   Std_Prob = pred_probs_std)\n\n# Create the plot\nggplot(plot_data, aes(x = age)) +\n  geom_line(aes(y = Het_Prob, color = \"Heteroskedastic Probit\")) +\n  geom_line(aes(y = Std_Prob, color = \"Standard Probit\")) +\n  labs(x = \"Age\", y = \"Pr(Secret Muslim)\", color = \"Model\") +\n  theme_minimal() +\n  ggtitle(\"Comparison of Heteroskedastic and Standard Probit Models\")"
  },
  {
    "objectID": "binaryextensions224.html#variance-of-epsilon",
    "href": "binaryextensions224.html#variance-of-epsilon",
    "title": "Binomial Topics: Ordered \\(y\\), non-constant variance",
    "section": "Variance of \\(\\epsilon\\)",
    "text": "Variance of \\(\\epsilon\\)\nThe variance of \\(\\epsilon\\) in any model can be thought of like this:\n\\[\\begin{aligned}\nvar(\\epsilon_i)=var(\\epsilon_j) \\forall i,j \\ldots n \\nonumber\n\\end{aligned}\\]\nThis is explicitly why we write the variance of the errors without a subscript - var(\\(\\epsilon\\)) - it is constant across all \\(i\\).\nPut slightly differently, the distribution of \\(\\epsilon\\) is the same for all \\(i\\). If this does not hold, then the errors are not independent and identically distributed (i.i.d.) - their distributions are different. This is just another way to state the problem of nonconstant variance.\nThis figure illustrates what it means for the variance to be different for different groups in the data. In the top panel, the means differ, but variances are the same. This approximates the homoskedastic case. In the bottom panel, the means differ, as do the variances. This is the heteroskedastic case.\n\n\ncode\nz &lt;- seq(-5, 5, length.out = 1000)\na &lt;- dnorm(z, mean = -1, sd = .5)\nb &lt;- dnorm(z, mean = 1, sd = .5)\nc &lt;- dnorm(z, mean = -1, sd = .5)\nd &lt;- dnorm(z, mean = 1, sd = sqrt(2))\n\ndf1 &lt;- data.frame(z = z, a = a, b = b)\n\ndf2 &lt;- data.frame(z = z, c = c, d = d)\n\n#plot using binghamton colors and annotate in the plot; exclude legend\n\np1 &lt;- ggplot(df1, aes(x = z)) +\n  geom_line(aes(y = a, color = \"Mean -1, Var 1\"), size = 1) +\n  geom_line(aes(y = b, color = \"Mean 1, Var 1\"), size = 1) +\n  scale_color_manual(values = c(\"Mean -1, Var 1\" = binghamton_green,\n                                \"Mean 1, Var 1\" = binghamton_gray)) +\n  labs(x = \"z\", y = \"Density\", color = \"Group\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")+\n  ggtitle(\"Constant Variance\")\n\np2 &lt;- ggplot(df2, aes(x = z)) +\n  geom_line(aes(y = c, color = \"Mean -1, Var 1\"), size = 1) +\n  geom_line(aes(y = d, color = \"Mean 1, Var 2\"), size = 1) +\n  scale_color_manual(values = c(\"Mean -1, Var 1\" = binghamton_green,\n                                \"Mean 1, Var 2\" = binghamton_gray)) +\n  labs(x = \"z\", y = \"Density\", color = \"Group\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")+\n  ggtitle(\"Non-Constant Variance\")\n\np1/p2"
  },
  {
    "objectID": "mechanismpaper24.html",
    "href": "mechanismpaper24.html",
    "title": "Mechanism Paper Assignment",
    "section": "",
    "text": "The assignment: During the semester, write three short papers.The goal is to identify a causal mechanism you find interesting, discuss its mechanics, its use in the literature, and then to propose a new place to apply it, or a modification of the mechanism applied in an existing location.\nWhat is a mechanism? For our purposes, a mechanism is a set of rules and incentives that point actors toward some behavior they might not otherwise choose. A syllabus in a class can be thought of as a set of mechanisms working together to get students to do three things they otherwise would not do: show up; pay attention; read. A cumulative exam incentivizes keeping up during the semester. Requiring a topic page for a paper assignment incentivizes starting the paper earlier than would otherwise happen. Pop quizzes incentivize attendance and reading.\nHere’s another perhaps less trivial example. 17th century pirate ships needed crew members to be willing to fight hard, not to hold back in fear of injury. If pirates were cautious, they would be less effective at taking ships, and would not develop the reputations pirates needed - those fearsome reputations actually made violence less necessary and pirating somewhat safer. To get the crew to fight, captains often made a practice of providing social benefits including health care to those injured. Instead of throwing incapacitated pirates over the side, they provided social insurance, thereby encouraging others to fight hard knowing that injury did not guarantee death. The result was pirate crews willing to take risks and fight aggressively, success in taking target ships, and reputations as fearsome fighters. Captains who guaranteed social benefits created moral hazard, induced risk-taking behavior that benefited the ship.1\nHere’s another, even less trivial example, but also a puzzle. Governments of all types extract revenue from citizens via taxation - the more productive citizens are, the more they collect in taxes (holding the rate constant). So governments need to encourage productivity. Productivity requires investment (rather than consumption) - investment pays off in the future while consumption pays off now. For investment to make sense, citizens have to believe the future is valuable and safe. One way governments might try to encourage investment and persuade them the future is safe is to reassure citizens government will not steal from them - if citizens believe government might “change the deal” and seize assets at any time, citizens will neither save, nor invest - they will consume. How can governments maintain the monopoly on violence, but persuade citizens government won’t use that monopoly on violence to seize everything, thereby encouraging investment, and thereby increasing tax revenue? More succinctly, how can a government powerful enough to protect private property guarantee it won’t confiscate private property?2"
  },
  {
    "objectID": "mechanismpaper24.html#footnotes",
    "href": "mechanismpaper24.html#footnotes",
    "title": "Mechanism Paper Assignment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFrom Peter Leeson’s The Invisible Hook, available online through BU’s library.↩︎\nThis puzzle appears in a number of places, due in part to Douglas North and Barry Weingast; a form of this is what Weingast calls the fundamental political dilemma of an economic system.↩︎"
  },
  {
    "objectID": "interactions24.html",
    "href": "interactions24.html",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "Everything you know about multiplicative interactions in linear models applies in the nonlinear context. It would very much be worth reviewing the slides on interactions in the linear model before proceeding.\n\n\n\ncoefficients on dummy variables represent intercept or level shifts; differential intercepts between groups on the dummy.\nstructural stability (same slope across groups) - relaxed with multiplicative interactions.\ninclude constituents variables in the model.\nconstituent interpretation is always conditional.\ninteraction coefficient interpretation is always conditional.\ninference is always conditional. To evaluate whether \\(\\beta_1 + \\beta_3 \\neq 0\\), we need to compute a standard error on \\(\\beta_1 + \\beta_3\\). Matt Golder’s web site is an excellent reference for anything interaction-related, and particularly for guidance on computing standard errors.\n\n\n\n\nIn nonlinear models, the effects of \\(x\\) variables on \\(Pr(y=1)\\) are increasingly compressed as \\(x\\beta\\) moves away from zero, and as \\(Pr(y=1)\\) moves towards its limits. Berry, DeMeritt, and Esarey (2010) seem to originate the term “compression” referring to the changing effects of \\(x\\) on \\(y\\) as \\(x\\beta\\) changes in monotonic link functions (e.g. logit, probit, etc.). Compression is describing what Nagler (1994) calls “inherent” interaction in nonlinear models; it describes why the effects of \\(x\\) depend on the values of the other \\(x\\) variables in the model.\nIn the linear model, the effects of \\(x\\) are unconditional unless we include an interaction; in the nonlinear model, the effects are always conditional due to compression, even without a multiplicative interaction. So we need to answer two questions:\n\nhow can we take advantage of “inherent interaction” due to compression?\nif models are inherently interactive, when or why do we need multiplicative interactions?\n\nFor the second question, this is a debate in the literature. Berry, DeMeritt, and Esarey (2010) suggest interaction terms are not always necessary; Rainey (2016) suggests otherwise.\nLet’s think about the probability space and quantities of interest as a way to understand compression.\n\n\n\nPredicted probability: \\[Pr(y=1) = F(\\beta_0 + \\beta_1x_1+ \\beta_2x_2)\\]\n\n\n\n\n\\[\\frac{\\partial y}{\\partial x_1} =  \\beta_1\\]\n\n\n\n\\[\\frac{\\partial y}{\\partial x_k} = \\frac{\\partial F(x\\beta)}{\\partial x_k} =   \\frac{\\partial F(x\\beta)}{\\partial x\\beta} \\cdot  \\frac{\\partial x\\beta}{\\partial x_k}  = f(x\\beta)\\beta_k\\]\n\n\n\n\n\n\n\\[\\frac{\\partial y}{\\partial x_k} = \\frac{\\partial \\Lambda(x\\beta)}{\\partial x_k} =\\lambda(x\\beta)\\beta_k\\] since the derivative of the CDF is the PDF, and where the logit pdf is\n\\[\\lambda =\\frac{e^{x\\beta}}{(1+e^{x\\beta})^2}\\]\n\n\n\n\nThe logit ME can be rewritten:\n\\[\\begin{eqnarray}\n\\frac{\\partial \\Lambda(x\\beta)}{\\partial x_k} =\\lambda(x\\beta) \\cdot \\beta_k \\nonumber \\\\\n=\\left[\\frac{e^{x\\beta}}{(1+e^{x\\beta})^2}\\right] \\cdot \\beta_k \\nonumber \\\\\n= \\left[\\frac{e^{x\\beta}}{(1+e^{x\\beta})} \\frac{1}{(1+e^{x\\beta})}\\right] \\cdot \\beta_k \\nonumber \\\\\n= \\Lambda(x\\beta) \\left(1-\\frac{e^{x\\beta}}{1+e^{x\\beta}}\\right) \\cdot \\beta_k \\nonumber \\\\\n= \\Lambda(x\\beta)\\cdot (1-\\Lambda(x\\beta)) \\cdot\\beta_k \\nonumber \\\\\n= Pr(y=0) \\cdot Pr(y=1) \\cdot  \\beta_k \\nonumber\n\\end{eqnarray}\\]\n\n\n\n\\[\\frac{\\partial y}{\\partial x_k} = \\frac{\\partial \\Phi(x\\beta)}{\\partial x_k} =\\phi(x\\beta)\\beta_k\\]\nIn both cases, you can see the effect of \\(x_k\\) depends explicitly on the value of \\(x\\beta\\). Put differently, the marginal effect of \\(x_k\\) is {} on \\(x\\beta\\), or where we are on the \\(x\\) axis.\n\n\n\n\nCompression refers to the different rates of change in the \\(Pr(Y=1)\\) across the probability range.\n\nthe effects of changes in \\(x\\) are greatest at \\(Pr(Y=1)=.5\\).\nthe effects of changes in \\(x\\) decline as \\(Pr(Y=1)\\) approaches the limits.\nthe same change in \\(x\\) has an increasingly “compressed” smaller effect as \\(Pr(Y=1)\\Rightarrow(0,1)\\).\n\n\n\n\nIn the simulation below notice how the effects of \\(x\\) on \\(y\\) are different due to compression effects - that is, the effect of \\(x\\) is greatest at \\(x=0, y=.5\\), and declines as \\(abs{x}\\) increases. Also, notice how additive changes to the intercept shift the curve left-right, but do not change the slope at \\(Pr(Y=1)\\). Multiplicative (interactive) changes change the derivative of the curve. Notice that compression exists in both the interactive and non-interactive settings.\n\n\n\n\n\n\n\nThis helps answer when/why we’d use an interaction. The derivatives at \\(Pr(Y=1)\\) shift in the interactive model, remain constant in the non-interactive model. Note several things:\n\nCompression effects are present regardless of the presence or absence of interactions. So the interactive model has compression and multiplicative interaction effects at work.\nThe CDF shifts left-right based on intercept changes in \\(x_i \\beta\\), but derivatives remain the same for values of \\(y\\).\nWith interactions, the derivatives changes for values of \\(y\\); the derivative at \\(Pr(Y=1)\\) changes.\nChoosing the multiplicative model implies our theory tells us the slope at \\(Pr(Y=1)\\) is different due to \\(x \\dot z\\).\nIn the non-interactive model, \\(\\frac{\\partial Pr(Y=1)}{\\partial x_i\\beta}=\\frac{\\partial Pr(Y=1)}{\\partial x_j\\beta}\\); in the interactive model, this is not true.\n\n\n\n\n\nwhen we believe that the changes in \\(y\\) are a function of \\(x|z=i\\ldots j\\).\nwhen we specifically think the slope (derivative) at a particular value of \\(y\\) given \\(x\\) is not structurally stable; i.e. it varies by groups on some variable \\(z\\).\nRainey (2016) suggests we err on the side of including interaction terms.\n\n\n\n\n\nalways explore compression effects. Postestimation, examine how changes in variable values shift the curve in interesting ways. Explore interesting combinations of variables.\nnever rely on compression to handle a conditional expectation - if you have a conditional expectation, use a multiplicative interaction.\nthere’s continuing interest in how to think about compression and interaction in the literature, so read what folks are saying.\n\n\n\n\nComputing QIs for interactions is not difficult, but has a number of moving parts. The go-to technique is laid out by Matt Golder as part of the materials accompanying Brambor, Clark, and Golder (2006). That technique unfolds as follows:\n\nestimate the model\nsimulate the parameter distribution\ngenerate \\(pr(y=j)\\) for different values of \\(x_1\\), holding \\(x_2\\) constant, moving the interaction term.\nfrom the distribution, plot percentiles.\n\nAnother technique, illustrated below, is to compute “average effects” just as we have in other settings. The example below compares average effects from interaction models and non-interactive models just (so just compression effects).\n\n\n\nUsing the democratic peace data, let’s look at two models. In the first, lets think about how democracy and shared borders intersect. We’ll examine a model including both variables but no interaction, and so examine compression effects. Then, let’s interact democracy and border and look at the effects. In the second case, we’ll do the same thing with democracy and the balance of power under the idea that differences in power may affect conflict differently for democrats and non democrats.\n\n\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n \ndp$deml_border &lt;- dp$deml*dp$border\n\ndemborderint &lt;- glm(dispute~deml+border+deml_border+caprat+jio, data=dp, family=binomial(link=\"logit\"))\n\ndembordercomp &lt;- glm(dispute~deml+border+caprat+jio, data=dp, family=binomial(link=\"logit\"))\n\nstargazer(dembordercomp, demborderint, type=\"html\", title=\"Democracy and Shared Borders\")\n\n\n\nDemocracy and Shared Borders\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\ndispute\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\ndeml\n\n\n-0.048***\n\n\n-0.067***\n\n\n\n\n\n\n(0.008)\n\n\n(0.012)\n\n\n\n\n\n\n\n\n\n\n\n\nborder\n\n\n1.143***\n\n\n1.338***\n\n\n\n\n\n\n(0.078)\n\n\n(0.118)\n\n\n\n\n\n\n\n\n\n\n\n\ndeml_border\n\n\n\n\n0.033**\n\n\n\n\n\n\n\n\n(0.015)\n\n\n\n\n\n\n\n\n\n\n\n\ncaprat\n\n\n-0.003***\n\n\n-0.003***\n\n\n\n\n\n\n(0.0004)\n\n\n(0.0004)\n\n\n\n\n\n\n\n\n\n\n\n\njio\n\n\n-0.024***\n\n\n-0.024***\n\n\n\n\n\n\n(0.003)\n\n\n(0.003)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-2.860***\n\n\n-2.966***\n\n\n\n\n\n\n(0.115)\n\n\n(0.128)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n20,142\n\n\n20,142\n\n\n\n\nLog Likelihood\n\n\n-3,341.355\n\n\n-3,338.786\n\n\n\n\nAkaike Inf. Crit.\n\n\n6,692.710\n\n\n6,689.571\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\nAnd let’s plot average effects for these two models.\n\n\ncode\nlibrary(averagemarginaleffects)\n\n# Compression\n# compute effects using the averagemarginaleffects package\n\ncompression &lt;- compute_average_effects(\n  model = dembordercomp,\n  data = dp,\n  x_variable = \"deml\",\n  pred_type = \"response\",\n  mediator = \"border\",\n  interaction = NULL,\n  quiet=TRUE\n)\n\n\n\n# plot the compression effect\n\ncomp_plot &lt;- ggplot(compression, aes(x = x, y = median_prediction, color = factor(z))) +\n  geom_line() +\n  geom_ribbon(aes(ymin = median_prediction - 1.96 * median_se, ymax = median_prediction + 1.96 * median_se, fill = factor(z)), alpha = 0.2) +\n  labs(title = \"Compression Effects\",\n       x = \"Polity\",\n       y = \"Pr(Dispute)\",\n       color = \"Shared Border\",\n       fill = \"Shared Border\") +\n  scale_color_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  scale_fill_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n# Interaction \n# compute effects using the averagemarginaleffects package\n\ninteraction &lt;- compute_average_effects(\n  model = demborderint,\n  data = dp,\n  x_variable = \"deml\",\n  interaction = list(vars = c(\"deml\", \"border\"), int_var = \"deml_border\"),\n  pred_type = \"response\",\n  mediator = NULL,\n  quiet=TRUE\n)\n\n# plot the interaction effect\n\nint_plot &lt;- ggplot(interaction, aes(x = x, y = median_prediction, color = factor(z))) +\n  geom_line() +\n  geom_ribbon(aes(ymin = median_prediction - 1.96 * median_se, ymax = median_prediction + 1.96 * median_se, fill = factor(z)), alpha = 0.2) +\n  labs(title = \"Interaction Effects\",\n       x = \"Polity\",\n       y = \"Pr(Dispute)\",\n       color = \"Shared Border\",\n       fill = \"Shared Border\") +\n  scale_color_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  scale_fill_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\ncomp_plot+int_plot\n\n\n\n\n\n\n\n\n\nThe predictions of the compression and interactive models are very similar in this case.\n\n\n\n\n\ncode\n#ln of caprat\ndp$lncaprat &lt;- log(dp$caprat)\n#interaction term, deml*lncaprat\ndp$deml_caprat &lt;- dp$deml*dp$lncaprat\n\n#model\n\nmcompression &lt;- glm(dispute~deml+lncaprat+border+jio, data=dp, family=binomial(link=\"logit\"))\n\nminteraction &lt;- glm(dispute~deml+lncaprat+deml_caprat+border+jio, data=dp, family=binomial(link=\"logit\"))\n\nstargazer(mcompression, minteraction, type=\"html\", title=\"Democracy and Capabilities\")\n\n\n\nDemocracy and Capabilities\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\ndispute\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\ndeml\n\n\n-0.045***\n\n\n-0.077***\n\n\n\n\n\n\n(0.008)\n\n\n(0.013)\n\n\n\n\n\n\n\n\n\n\n\n\nlncaprat\n\n\n-0.254***\n\n\n-0.178***\n\n\n\n\n\n\n(0.025)\n\n\n(0.033)\n\n\n\n\n\n\n\n\n\n\n\n\ndeml_caprat\n\n\n\n\n0.013***\n\n\n\n\n\n\n\n\n(0.004)\n\n\n\n\n\n\n\n\n\n\n\n\nborder\n\n\n0.919***\n\n\n0.893***\n\n\n\n\n\n\n(0.089)\n\n\n(0.090)\n\n\n\n\n\n\n\n\n\n\n\n\njio\n\n\n-0.026***\n\n\n-0.025***\n\n\n\n\n\n\n(0.003)\n\n\n(0.003)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-2.219***\n\n\n-2.425***\n\n\n\n\n\n\n(0.149)\n\n\n(0.165)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n20,142\n\n\n20,142\n\n\n\n\nLog Likelihood\n\n\n-3,344.418\n\n\n-3,339.020\n\n\n\n\nAkaike Inf. Crit.\n\n\n6,698.835\n\n\n6,690.039\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\n\n\ncode\nlibrary(averagemarginaleffects)\n\n\n# average effects\n\ncap_int &lt;- compute_average_effects(\n  model = minteraction,\n  data = dp,\n  x_variable = \"lncaprat\",\n  interaction = list(vars = c(\"lncaprat\", \"deml\"), int_var = \"deml_caprat\"),\n  pred_type = \"response\",\n  mediator = NULL,\n  z_values = c(-10,  10),\n  quiet=TRUE\n)\n\n#plot\n\nint &lt;- ggplot(cap_int, aes(x = x, y = median_prediction, color = factor(z))) +\n  geom_line() +\n  geom_ribbon(aes(ymin = median_prediction - 1.96 * median_se, ymax = median_prediction + 1.96 * median_se, fill = factor(z)), alpha = 0.2) +\n  labs(title = \"Interaction Effects\",\n       x = \"ln(Capabilities Ratio)\",\n       y = \"Pr(Dispute)\",\n       color = \"Polity\",\n       fill = \"Polity\") +\n  scale_color_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  scale_fill_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n\n\ncomp &lt;- compute_average_effects(\n  model = mcompression,\n  data = dp,\n  x_variable = \"lncaprat\",\n  pred_type = \"response\",\n  mediator = \"deml\",\n  z_values = c(-10,  10),\n  quiet=TRUE\n)\n\n\ncomp &lt;- ggplot(comp, aes(x = x, y = median_prediction, color = factor(z))) +\n  geom_line() +\n  geom_ribbon(aes(ymin = median_prediction - 1.96 * median_se, ymax = median_prediction + 1.96 * median_se, fill = factor(z)), alpha = 0.2) +\n  labs(title = \"Compression Effects\",\n       x = \"ln(Capabilities Ratio)\",\n       y = \"Pr(Dispute)\",\n       color = \"Polity\",\n       fill = \"Polity\") +\n  scale_color_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  scale_fill_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\ncomp+int\n\n\n\n\n\n\n\n\n\nIn this set of models, the predictions are substantially different. The slopes in the noninteractive model are very similar (unsurprisingly - see the simulation above), but those slopes over capabilities vary dramatically in the interactive model. In the interactive model, the chance of conflict in autocratic dyads declines precipitously as the capabilities ratio grows (as the pair becomes less balanced). In democratic pairs, the chances of conflict are lower and practically constant across the range of capabilities ratio, indicating democratic dyads fight less often than do autocratic pairs, and that the balance of capabilities matters little to their chances of conflict."
  },
  {
    "objectID": "interactions24.html#linear-models",
    "href": "interactions24.html#linear-models",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "coefficients on dummy variables represent intercept or level shifts; differential intercepts between groups on the dummy.\nstructural stability (same slope across groups) - relaxed with multiplicative interactions.\ninclude constituents.\nconstituent interpretation is always conditional.\ninteraction coefficient interpretation is always conditional.\ninference is always conditional."
  },
  {
    "objectID": "interactions24.html#whats-the-difference-compression",
    "href": "interactions24.html#whats-the-difference-compression",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "In nonlinear models, the effects of \\(x\\) variables on \\(Pr(y=1)\\) are increasingly compressed as \\(x\\beta\\) moves away from zero, and as \\(Pr(y=1)\\) moves towards its limits.\nIn the linear model, the effects of \\(x\\) are unconditional unless we include an interaction; in the nonlinear model, the effects are always conditional due to compression, even without a multiplicative interaction.\nCompression at larger absolute values of \\(x\\beta\\) creates what Nagler (1994) calls “inherent” interaction in nonlinear models. So we need to answer two questions:\n\nhow can we take advantage of “inherent interaction” due to compression?\nif models are inherently interactive, when or why do we need multiplicative interactions?\n\nLet’s think about the probability space and quantities of interest as a way to understand compression."
  },
  {
    "objectID": "interactions24.html#quantities-of-interest",
    "href": "interactions24.html#quantities-of-interest",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "Predicted probability: \\[Pr(y=1) = F(\\beta_0 + \\beta_1x_1+ \\beta_2x_2)\\]\n\n\n\n\n\\[\\frac{\\partial y}{\\partial x_1} =  \\beta_1\\]\n\n\n\n\\[\\frac{\\partial y}{\\partial x_k} = \\frac{\\partial F(x\\beta)}{\\partial x_k} =   \\frac{\\partial F(x\\beta)}{\\partial x\\beta} \\cdot  \\frac{\\partial x\\beta}{\\partial x_k}  = f(x\\beta)\\beta_k\\]\n\n\n\n\n\n\n\\[\\frac{\\partial y}{\\partial x_k} = \\frac{\\partial \\Lambda(x\\beta)}{\\partial x_k} =\\lambda(x\\beta)\\beta_k\\] since the derivative of the CDF is the PDF, and where the logit pdf is\n\\[\\lambda =\\frac{e^{x\\beta}}{(1+e^{x\\beta})^2}\\]\n\n\n\n\nThe logit ME can be rewritten:\n\\[\\begin{eqnarray}\n\\frac{\\partial \\Lambda(x\\beta)}{\\partial x_k} =\\lambda(x\\beta) \\cdot \\beta_k \\nonumber \\\\\n=\\left[\\frac{e^{x\\beta}}{(1+e^{x\\beta})^2}\\right] \\cdot \\beta_k \\nonumber \\\\\n= \\left[\\frac{e^{x\\beta}}{(1+e^{x\\beta})} \\frac{1}{(1+e^{x\\beta})}\\right] \\cdot \\beta_k \\nonumber \\\\\n= \\Lambda(x\\beta) \\left(1-\\frac{e^{x\\beta}}{1+e^{x\\beta}}\\right) \\cdot \\beta_k \\nonumber \\\\\n= \\Lambda(x\\beta)\\cdot (1-\\Lambda(x\\beta)) \\cdot\\beta_k \\nonumber \\\\\n= Pr(y=0) \\cdot Pr(y=1) \\cdot  \\beta_k \\nonumber\n\\end{eqnarray}\\]\n\n\n\n\\[\\frac{\\partial y}{\\partial x_k} = \\frac{\\partial \\Phi(x\\beta)}{\\partial x_k} =\\phi(x\\beta)\\beta_k\\]\nIn both cases, you can see the effect of \\(x_k\\) depends explicitly on the value of \\(x\\beta\\). Put differently, the marginal effect of \\(x_k\\) is {} on \\(x\\beta\\), or where we are on the \\(x\\) axis."
  },
  {
    "objectID": "interactions24.html#compression-effects",
    "href": "interactions24.html#compression-effects",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "Compression refers to the different rates of change in the \\(Pr(Y=1)\\) across the probability range.\n\nthe effects of changes in \\(x\\) are greatest at \\(Pr(Y=1)=.5\\).\nthe effects of changes in \\(x\\) decline as \\(Pr(Y=1)\\) approaches the limits.\nthe same change in \\(x\\) has an increasingly “compressed” smaller effect as \\(Pr(Y=1)\\Rightarrow(0,1)\\)."
  },
  {
    "objectID": "interactions24.html#compression-v.-interaction",
    "href": "interactions24.html#compression-v.-interaction",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "On the preceding slide, notice how the effects of \\(x\\) on \\(y\\) are different due to compression effects in all three curves. But also notice that the derivatives at \\(Pr(Y=1)\\) are the same for the non-interactive curves, different for the interactive one. Look at their slopes (derivatives) in the following inset:\n\n\ncode\nknitr::include_app(url = \"https://clavedark.shinyapps.io/interactions/\", height = \"1000px\")"
  },
  {
    "objectID": "interactions24.html#why-use-an-interaction",
    "href": "interactions24.html#why-use-an-interaction",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "This helps answer when/why we’d use an interaction. The derivatives at \\(Pr(Y=1)\\) shift in the interactive model, remain constant in the non-interactive model. Note several things:\n\nCompression effects are present regardless of the presence or absence of interactions. So the interactive model has compression and multiplicative interaction effects at work.\nThe CDF shifts left-right based on intercept changes in \\(x_i \\beta\\), but derivatives remain the same for values of \\(y\\).\nWith interactions, the derivatives changes for values of \\(y\\); the derivative at \\(Pr(Y=1)\\) changes.\nChoosing the multiplicative model implies our theory tells us the slope at \\(Pr(Y=1)\\) is different due to \\(x \\dot z\\).\nIn the non-interactive model, \\(\\frac{\\partial Pr(Y=1)}{\\partial x_i\\beta}=\\frac{\\partial Pr(Y=1)}{\\partial x_j\\beta}\\); in the interactive model, this is not true."
  },
  {
    "objectID": "interactions24.html#when-do-we-specify-a-multiplicative-interaction",
    "href": "interactions24.html#when-do-we-specify-a-multiplicative-interaction",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "when we believe that the changes in \\(y\\) are a function of \\(x|z=i\\ldots j\\).\nwhen we specifically think the slope (derivative) at a particular value of \\(y\\) given \\(x\\) is not structurally stable; i.e. it varies by groups on some variable \\(z\\).\nRainey (2016) suggests we err on the side of including interaction terms."
  },
  {
    "objectID": "interactions24.html#compression-interaction",
    "href": "interactions24.html#compression-interaction",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "always explore compression effects. Postestimation, examine how changes in variable values shift the curve in interesting ways. Explore interesting combinations of variables.\nnever rely on compression to handle a conditional expectation - if you have a conditional expectation, use a multiplicative interaction.\nthere’s continuing interest in how to think about compression and interaction in the literature, so read what folks are saying."
  },
  {
    "objectID": "interactions24.html#method",
    "href": "interactions24.html#method",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "Computing QIs for interactions is not difficult, but has a number of moving parts. The go-to technique is laid out by Matt Golder as part of the materials accompanying Brambor, Clark, and Golder (2006). That technique unfolds as follows:\n\nestimate the model\nsimulate the parameter distribution\ngenerate \\(pr(y=j)\\) for different values of \\(x_1\\), holding \\(x_2\\) constant, moving the interaction term.\nfrom the distribution, plot percentiles.\n\nAnother technique, illustrated below, is to compute “average effects” just as we have in other settings. The example below compares average effects from interaction models and non-interactive models just (so just compression effects)."
  },
  {
    "objectID": "interactions24.html#example",
    "href": "interactions24.html#example",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "Using the democratic peace data, let’s compare the usual regression to one where we interact democracy and the balance of power under the idea that differences in power may affect conflict differently for democrats and non democrats.\n\n\ncode\nlibrary(averagemarginaleffects)\n\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n \ndp$deml_border &lt;- dp$deml*dp$border\n\ndemborderint &lt;- glm(dispute~deml+border+deml_border+caprat+jio, data=dp, family=binomial(link=\"logit\"))\n\ndembordercomp &lt;- glm(dispute~deml+border+caprat+jio, data=dp, family=binomial(link=\"logit\"))\n\n# Compression\n# compute effects using the averagemarginaleffects package\n\ncompression &lt;- compute_average_effects(\n  model = dembordercomp,\n  data = dp,\n  x_variable = \"deml\",\n  pred_type = \"response\",\n  mediator = \"border\",\n  interaction = NULL,\n  quiet=TRUE\n)\n\n\n\n# plot the compression effect\n\ncomp_plot &lt;- ggplot(compression, aes(x = x, y = median_prediction, color = factor(z))) +\n  geom_line() +\n  geom_ribbon(aes(ymin = median_prediction - 1.96 * median_se, ymax = median_prediction + 1.96 * median_se, fill = factor(z)), alpha = 0.2) +\n  labs(title = \"Compression Effects\",\n       x = \"Polity\",\n       y = \"Pr(Dispute)\",\n       color = \"Shared Border\",\n       fill = \"Shared Border\") +\n  scale_color_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  scale_fill_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n# Interaction \n# compute effects using the averagemarginaleffects package\n\ninteraction &lt;- compute_average_effects(\n  model = demborderint,\n  data = dp,\n  x_variable = \"deml\",\n  interaction = list(vars = c(\"deml\", \"border\"), int_var = \"deml_border\"),\n  pred_type = \"response\",\n  mediator = NULL,\n  quiet=TRUE\n)\n\n# plot the interaction effect\n\nint_plot &lt;- ggplot(interaction, aes(x = x, y = median_prediction, color = factor(z))) +\n  geom_line() +\n  geom_ribbon(aes(ymin = median_prediction - 1.96 * median_se, ymax = median_prediction + 1.96 * median_se, fill = factor(z)), alpha = 0.2) +\n  labs(title = \"Interaction Effects\",\n       x = \"Polity\",\n       y = \"Pr(Dispute)\",\n       color = \"Shared Border\",\n       fill = \"Shared Border\") +\n  scale_color_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  scale_fill_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\ncomp_plot+int_plot\n\n\n\n\n\n\n\n\n\n\n\ncode\nlibrary(averagemarginaleffects)\n\n#ln of caprat\ndp$lncaprat &lt;- log(dp$caprat)\n#interaction term, deml*lncaprat\ndp$deml_caprat &lt;- dp$deml*dp$lncaprat\n\n#model\n\nmcompression &lt;- glm(dispute~deml+lncaprat+border+jio, data=dp, family=binomial(link=\"logit\"))\n\nminteraction &lt;- glm(dispute~deml+lncaprat+deml_caprat+border+jio, data=dp, family=binomial(link=\"logit\"))\n\n\n# average effects\n\ncap_int &lt;- compute_average_effects(\n  model = minteraction,\n  data = dp,\n  x_variable = \"lncaprat\",\n  interaction = list(vars = c(\"lncaprat\", \"deml\"), int_var = \"deml_caprat\"),\n  pred_type = \"response\",\n  mediator = NULL,\n  z_values = c(-10,  10),\n  quiet=TRUE\n)\n\n#plot\n\nint &lt;- ggplot(cap_int, aes(x = x, y = median_prediction, color = factor(z))) +\n  geom_line() +\n  geom_ribbon(aes(ymin = median_prediction - 1.96 * median_se, ymax = median_prediction + 1.96 * median_se, fill = factor(z)), alpha = 0.2) +\n  labs(title = \"Capabilities and Shared Border - Interaction\",\n       x = \"ln(Capabilities Ratio)\",\n       y = \"Pr(Dispute)\",\n       color = \"Polity\",\n       fill = \"Polity\") +\n  scale_color_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  scale_fill_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n\n\ncomp &lt;- compute_average_effects(\n  model = mcompression,\n  data = dp,\n  x_variable = \"lncaprat\",\n  pred_type = \"response\",\n  mediator = \"deml\",\n  z_values = c(-10,  10),\n  quiet=TRUE\n)\n\n\ncomp &lt;- ggplot(comp, aes(x = x, y = median_prediction, color = factor(z))) +\n  geom_line() +\n  geom_ribbon(aes(ymin = median_prediction - 1.96 * median_se, ymax = median_prediction + 1.96 * median_se, fill = factor(z)), alpha = 0.2) +\n  labs(title = \"Capabilities and Democracy - Compression\",\n       x = \"ln(Capabilities Ratio)\",\n       y = \"Pr(Dispute)\",\n       color = \"Polity\",\n       fill = \"Polity\") +\n  scale_color_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  scale_fill_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\ncomp+int"
  },
  {
    "objectID": "interactions24.html#multiplicative-interactions",
    "href": "interactions24.html#multiplicative-interactions",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "coefficients on dummy variables represent intercept or level shifts; differential intercepts between groups on the dummy.\nstructural stability (same slope across groups) - relaxed with multiplicative interactions.\ninclude constituents variables in the model.\nconstituent interpretation is always conditional.\ninteraction coefficient interpretation is always conditional.\ninference is always conditional. To evaluate whether \\(\\beta_1 + \\beta_3 \\neq 0\\), we need to compute a standard error on \\(\\beta_1 + \\beta_3\\). Matt Golder’s web site is an excellent reference for anything interaction-related, and particularly for guidance on computing standard errors."
  },
  {
    "objectID": "interactions24.html#compression-and-interaction",
    "href": "interactions24.html#compression-and-interaction",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "In the simulation below notice how the effects of \\(x\\) on \\(y\\) are different due to compression effects - that is, the effect of \\(x\\) is greatest at \\(x=0, y=.5\\), and declines as \\(abs{x}\\) increases. Also, notice how additive changes to the intercept shift the curve left-right, but do not change the slope at \\(Pr(Y=1)\\). Multiplicative (interactive) changes change the derivative of the curve. Notice that compression exists in both the interactive and non-interactive settings."
  },
  {
    "objectID": "interactions24.html#whats-the-difference-compression.",
    "href": "interactions24.html#whats-the-difference-compression.",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "In nonlinear models, the effects of \\(x\\) variables on \\(Pr(y=1)\\) are increasingly compressed as \\(x\\beta\\) moves away from zero, and as \\(Pr(y=1)\\) moves towards its limits. Berry, DeMeritt, and Esarey (2010) seem to originate the term “compression” referring to the changing effects of \\(x\\) on \\(y\\) as \\(x\\beta\\) changes in monotonic link functions (e.g. logit, probit, etc.). Compression is describing what Nagler (1994) calls “inherent” interaction in nonlinear models; it describes why the effects of \\(x\\) depend on the values of the other \\(x\\) variables in the model.\nIn the linear model, the effects of \\(x\\) are unconditional unless we include an interaction; in the nonlinear model, the effects are always conditional due to compression, even without a multiplicative interaction. So we need to answer two questions:\n\nhow can we take advantage of “inherent interaction” due to compression?\nif models are inherently interactive, when or why do we need multiplicative interactions?\n\nFor the second question, this is a debate in the literature. Berry, DeMeritt, and Esarey (2010) suggest interaction terms are not always necessary; Rainey (2016) suggests otherwise.\nLet’s think about the probability space and quantities of interest as a way to understand compression."
  },
  {
    "objectID": "interactions24.html#examples",
    "href": "interactions24.html#examples",
    "title": "Interactions in Nonlinear Models",
    "section": "",
    "text": "Using the democratic peace data, let’s look at two models. In the first, lets think about how democracy and shared borders intersect. We’ll examine a model including both variables but no interaction, and so examine compression effects. Then, let’s interact democracy and border and look at the effects. In the second case, we’ll do the same thing with democracy and the balance of power under the idea that differences in power may affect conflict differently for democrats and non democrats.\n\n\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n \ndp$deml_border &lt;- dp$deml*dp$border\n\ndemborderint &lt;- glm(dispute~deml+border+deml_border+caprat+jio, data=dp, family=binomial(link=\"logit\"))\n\ndembordercomp &lt;- glm(dispute~deml+border+caprat+jio, data=dp, family=binomial(link=\"logit\"))\n\nstargazer(dembordercomp, demborderint, type=\"html\", title=\"Democracy and Shared Borders\")\n\n\n\nDemocracy and Shared Borders\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\ndispute\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\ndeml\n\n\n-0.048***\n\n\n-0.067***\n\n\n\n\n\n\n(0.008)\n\n\n(0.012)\n\n\n\n\n\n\n\n\n\n\n\n\nborder\n\n\n1.143***\n\n\n1.338***\n\n\n\n\n\n\n(0.078)\n\n\n(0.118)\n\n\n\n\n\n\n\n\n\n\n\n\ndeml_border\n\n\n\n\n0.033**\n\n\n\n\n\n\n\n\n(0.015)\n\n\n\n\n\n\n\n\n\n\n\n\ncaprat\n\n\n-0.003***\n\n\n-0.003***\n\n\n\n\n\n\n(0.0004)\n\n\n(0.0004)\n\n\n\n\n\n\n\n\n\n\n\n\njio\n\n\n-0.024***\n\n\n-0.024***\n\n\n\n\n\n\n(0.003)\n\n\n(0.003)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-2.860***\n\n\n-2.966***\n\n\n\n\n\n\n(0.115)\n\n\n(0.128)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n20,142\n\n\n20,142\n\n\n\n\nLog Likelihood\n\n\n-3,341.355\n\n\n-3,338.786\n\n\n\n\nAkaike Inf. Crit.\n\n\n6,692.710\n\n\n6,689.571\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\nAnd let’s plot average effects for these two models.\n\n\ncode\nlibrary(averagemarginaleffects)\n\n# Compression\n# compute effects using the averagemarginaleffects package\n\ncompression &lt;- compute_average_effects(\n  model = dembordercomp,\n  data = dp,\n  x_variable = \"deml\",\n  pred_type = \"response\",\n  mediator = \"border\",\n  interaction = NULL,\n  quiet=TRUE\n)\n\n\n\n# plot the compression effect\n\ncomp_plot &lt;- ggplot(compression, aes(x = x, y = median_prediction, color = factor(z))) +\n  geom_line() +\n  geom_ribbon(aes(ymin = median_prediction - 1.96 * median_se, ymax = median_prediction + 1.96 * median_se, fill = factor(z)), alpha = 0.2) +\n  labs(title = \"Compression Effects\",\n       x = \"Polity\",\n       y = \"Pr(Dispute)\",\n       color = \"Shared Border\",\n       fill = \"Shared Border\") +\n  scale_color_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  scale_fill_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n# Interaction \n# compute effects using the averagemarginaleffects package\n\ninteraction &lt;- compute_average_effects(\n  model = demborderint,\n  data = dp,\n  x_variable = \"deml\",\n  interaction = list(vars = c(\"deml\", \"border\"), int_var = \"deml_border\"),\n  pred_type = \"response\",\n  mediator = NULL,\n  quiet=TRUE\n)\n\n# plot the interaction effect\n\nint_plot &lt;- ggplot(interaction, aes(x = x, y = median_prediction, color = factor(z))) +\n  geom_line() +\n  geom_ribbon(aes(ymin = median_prediction - 1.96 * median_se, ymax = median_prediction + 1.96 * median_se, fill = factor(z)), alpha = 0.2) +\n  labs(title = \"Interaction Effects\",\n       x = \"Polity\",\n       y = \"Pr(Dispute)\",\n       color = \"Shared Border\",\n       fill = \"Shared Border\") +\n  scale_color_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  scale_fill_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\ncomp_plot+int_plot\n\n\n\n\n\n\n\n\n\nThe predictions of the compression and interactive models are very similar in this case.\n\n\n\n\n\ncode\n#ln of caprat\ndp$lncaprat &lt;- log(dp$caprat)\n#interaction term, deml*lncaprat\ndp$deml_caprat &lt;- dp$deml*dp$lncaprat\n\n#model\n\nmcompression &lt;- glm(dispute~deml+lncaprat+border+jio, data=dp, family=binomial(link=\"logit\"))\n\nminteraction &lt;- glm(dispute~deml+lncaprat+deml_caprat+border+jio, data=dp, family=binomial(link=\"logit\"))\n\nstargazer(mcompression, minteraction, type=\"html\", title=\"Democracy and Capabilities\")\n\n\n\nDemocracy and Capabilities\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\ndispute\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\ndeml\n\n\n-0.045***\n\n\n-0.077***\n\n\n\n\n\n\n(0.008)\n\n\n(0.013)\n\n\n\n\n\n\n\n\n\n\n\n\nlncaprat\n\n\n-0.254***\n\n\n-0.178***\n\n\n\n\n\n\n(0.025)\n\n\n(0.033)\n\n\n\n\n\n\n\n\n\n\n\n\ndeml_caprat\n\n\n\n\n0.013***\n\n\n\n\n\n\n\n\n(0.004)\n\n\n\n\n\n\n\n\n\n\n\n\nborder\n\n\n0.919***\n\n\n0.893***\n\n\n\n\n\n\n(0.089)\n\n\n(0.090)\n\n\n\n\n\n\n\n\n\n\n\n\njio\n\n\n-0.026***\n\n\n-0.025***\n\n\n\n\n\n\n(0.003)\n\n\n(0.003)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-2.219***\n\n\n-2.425***\n\n\n\n\n\n\n(0.149)\n\n\n(0.165)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n20,142\n\n\n20,142\n\n\n\n\nLog Likelihood\n\n\n-3,344.418\n\n\n-3,339.020\n\n\n\n\nAkaike Inf. Crit.\n\n\n6,698.835\n\n\n6,690.039\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\n\n\ncode\nlibrary(averagemarginaleffects)\n\n\n# average effects\n\ncap_int &lt;- compute_average_effects(\n  model = minteraction,\n  data = dp,\n  x_variable = \"lncaprat\",\n  interaction = list(vars = c(\"lncaprat\", \"deml\"), int_var = \"deml_caprat\"),\n  pred_type = \"response\",\n  mediator = NULL,\n  z_values = c(-10,  10),\n  quiet=TRUE\n)\n\n#plot\n\nint &lt;- ggplot(cap_int, aes(x = x, y = median_prediction, color = factor(z))) +\n  geom_line() +\n  geom_ribbon(aes(ymin = median_prediction - 1.96 * median_se, ymax = median_prediction + 1.96 * median_se, fill = factor(z)), alpha = 0.2) +\n  labs(title = \"Interaction Effects\",\n       x = \"ln(Capabilities Ratio)\",\n       y = \"Pr(Dispute)\",\n       color = \"Polity\",\n       fill = \"Polity\") +\n  scale_color_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  scale_fill_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n\n\ncomp &lt;- compute_average_effects(\n  model = mcompression,\n  data = dp,\n  x_variable = \"lncaprat\",\n  pred_type = \"response\",\n  mediator = \"deml\",\n  z_values = c(-10,  10),\n  quiet=TRUE\n)\n\n\ncomp &lt;- ggplot(comp, aes(x = x, y = median_prediction, color = factor(z))) +\n  geom_line() +\n  geom_ribbon(aes(ymin = median_prediction - 1.96 * median_se, ymax = median_prediction + 1.96 * median_se, fill = factor(z)), alpha = 0.2) +\n  labs(title = \"Compression Effects\",\n       x = \"ln(Capabilities Ratio)\",\n       y = \"Pr(Dispute)\",\n       color = \"Polity\",\n       fill = \"Polity\") +\n  scale_color_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  scale_fill_manual(values = c(\"#005A43\", \"#8C8C8C\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\ncomp+int\n\n\n\n\n\n\n\n\n\nIn this set of models, the predictions are substantially different. The slopes in the noninteractive model are very similar (unsurprisingly - see the simulation above), but those slopes over capabilities vary dramatically in the interactive model. In the interactive model, the chance of conflict in autocratic dyads declines precipitously as the capabilities ratio grows (as the pair becomes less balanced). In democratic pairs, the chances of conflict are lower and practically constant across the range of capabilities ratio, indicating democratic dyads fight less often than do autocratic pairs, and that the balance of capabilities matters little to their chances of conflict."
  }
]