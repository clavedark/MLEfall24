[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site:\nCreated with Quarto.\nAbout me:\nProfessor of political science, PhD Florida State 1999. I’ve taught this class for a long time, but never the same way twice. I study models of political violence, collect data on protests and repression.\n\n\n\n Back to top"
  },
  {
    "objectID": "likelihood24.html#how-would-you-characterize-the-variable-measuring-deaths-by-mule-kick",
    "href": "likelihood24.html#how-would-you-characterize-the-variable-measuring-deaths-by-mule-kick",
    "title": "Likelihood",
    "section": "How would you characterize the variable measuring deaths by mule kick?",
    "text": "How would you characterize the variable measuring deaths by mule kick?\n\nVariable measures events.\nEvents are discrete, not continuous.\nAre events correlated or independent?\nVariable is bounded.\nWould OLS be appropriate?\nWhat specific problems do we need to accommodate here?\n\nGiven what we know about the dependent variable itself, we can begin to think about an appropriate model:\n\nThe model must accommodate a discrete dependent variable.\nWe might have two quantities of interest - the expected number of events (\\(E[Y]\\)), and the probability of any given number of events (\\(Pr(Y=j)\\)).\nWe need to make sure that the \\(X\\) variables we think cause increases or decreases in the number of deaths cannot produce nonsensical effects (predictions less than zero, for instance).\n\nSo we need to think about two different things here - the distribution of \\(\\epsilon\\) based on the observed distribution of \\(y\\), and the link between the \\(X\\) variables and \\(\\widetilde{y}\\), the latent quantity of interest.\n\nWhat distribution might describe the frequency of mule kick deaths?\nNeeds to be discrete.\nNeeds to characterize rare events - at most we see about four per period, so relatively rare.\nNeeds to have a lower bound at zero (since we can’t observe negative numbers of deaths), and upper bound at \\(+\\infty\\)\n\n\n\\[Pr(Y=y_{i})=\\frac{e^{-\\lambda}\\lambda^{y_{i}}}{y_{i}!}\\]"
  },
  {
    "objectID": "likelihood24.html#the-poisson-distribution",
    "href": "likelihood24.html#the-poisson-distribution",
    "title": "Likelihood",
    "section": "The Poisson Distribution",
    "text": "The Poisson Distribution\n\n\ncode\n#simulate and plot the poisson distribution at 3 different values of the mean\n\nset.seed(123)\nn &lt;- 1000\nlambda &lt;- c(1, 5, 10)\npoisson &lt;- rpois(n, lambda[1])\npoisson2 &lt;- rpois(n, lambda[2])\npoisson3 &lt;- rpois(n, lambda[3])\n\ndf &lt;- data.frame(poisson, poisson2, poisson3)\n\nggplot(df, aes(x=poisson)) + \n  geom_histogram(aes(y=..density..), bins=20, fill=\"blue\", alpha=0.5) + \n  geom_density(aes(y=..density..), color=\"blue\") + \n  geom_histogram(data=df, aes(x=poisson2, y=..density..), bins=20, fill=\"red\", alpha=0.5) + \n  geom_density(data=df, aes(x=poisson2, y=..density..), color=\"red\") + \n  geom_histogram(data=df, aes(x=poisson3, y=..density..), bins=20, fill=\"green\", alpha=0.5) + \n  geom_density(data=df, aes(x=poisson3, y=..density..), color=\"green\") + \n  labs(title=\"Poisson Distribution\", x=\"Deaths\", y=\"Density\") + \n  theme_minimal() + \n  theme(legend.position=\"none\")\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\nThinking in terms of the data, let’s write a likelihood function, the joint probability for all \\(i\\) observations in the sample, \\(n\\):\n\\[\nL(\\lambda)= \\prod_{i=1}^{n} \\left[\\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!} \\right]\n\\]\nTake the natural log of the likelihood function:\n\\[\n\\ln L(\\lambda)= \\ln \\left\\{\\prod_{i=1}^{n} \\left[\\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!} \\right] \\right\\}\\nonumber \\\\\n\\ln L(\\lambda)= \\sum_{i=1}^{n} \\left[-\\lambda + y_i \\ln(\\lambda) - \\ln(y_i!) \\right]\n\\]\nWhat about the \\(X\\) variables? Parameterize the model with respect to those variables such that they influence the mean, \\(\\lambda\\). So let’s make \\(\\lambda\\) a function of the \\(X\\) variables and their effects, \\(\\beta\\), such that,\n\\[\nE[Y|X]=\\lambda =F(\\beta X)\n\\]\nwhere \\(F(\\beta X)\\) is some link function, \\(F\\) correctly scaling \\(\\beta X\\) to \\(y\\), and \\(\\widetilde{y}\\). Remembering that we cannot have negative predictions, suppose we let \\(F\\) be the exponential distribution which is nonnegative, unbounded to the right, and nonlinear with respect to changes at the limit.\nPutting all this together using the exponential distribution as the link, we have:\n\\[\nE[Y|X]=\\lambda =e^{X\\beta} \\\\\n\\ln L(\\lambda)= \\ln \\left\\{\\prod_{i=1}^{n} \\left[\\frac{e^{-e^{\\beta X}}  e^{(\\beta X)^{y_i}}}{y_i!} \\right] \\right\\} \\\\\n\\ln L(\\lambda)= \\sum_{i=1}^{n} \\left[-e^{\\beta X} + y_i \\ln(\\beta X) - \\ln(y_i!) \\right]\n\\]\nLet’s derive another LLF - note that we start with the data and move toward the model. Suppose we have data on the number of civil wars in Africa over a ten year period, and the data are as follows:\n\\(Y\\) = {5 0 1 1 0 3 2 3 4 1}\n\\[\nY \\sim f_{poisson}(\\lambda)=  \\frac{e^{-\\lambda}\\lambda^{y_{i}}}{y_{i}!}\\nonumber\n\\]\nThe likelihood is given by the joint density\n(\\(L(y_{1},y_{2},y_{3}\\ldots y_{10})\\)):\n\\[\nL(\\lambda|Y) = \\prod\\limits_{i=1}^{10} f(y_{i},\\lambda)=  \\prod\\limits_{i=1}^{10} \\frac{e^{-\\lambda}\\lambda^{y_{i}}}{y_{i}!}\\nonumber \\\\ \\nonumber \\\\\n= \\frac{e^{-10\\lambda}\\lambda^{\\sum{y_{i}}}}{\\prod y_{i}!}\\nonumber \\\\ \\nonumber \\\\\n= \\frac{e^{-10\\lambda}\\lambda^{20}}{207360}\\nonumber\n\\]\nHere’s what we know so far:\nwe begin with some data, \\(Y\\). We want to find the model most likely responsible for generating these data. consider the distribution of \\(y\\) - describe that distribution. write a log-likelihood function for the sample; it should characterize the distribution of \\(y\\), and should link the linear prediction (\\(x\\beta\\)) with the quantity of interest, \\(\\tilde{y}\\).\nSo, we have data, and we have a likelihood function - what do we do with them?"
  },
  {
    "objectID": "likelihood24.html#estimation-technology-ols",
    "href": "likelihood24.html#estimation-technology-ols",
    "title": "Likelihood",
    "section": "Estimation Technology: OLS",
    "text": "Estimation Technology: OLS\nRecall that the technology of OLS is to assume a normally distributed error term, minimize the sum of those squared errors analytically using calculus."
  },
  {
    "objectID": "likelihood24.html#estimation-technology-mle",
    "href": "likelihood24.html#estimation-technology-mle",
    "title": "Likelihood",
    "section": "Estimation Technology: MLE",
    "text": "Estimation Technology: MLE\nThe technology of ML is to maximize the LLF with respect to \\(\\beta\\). We can do this in several different ways:\n\nanalytic methods - use calculus. Some/many models do not have analytical or closed form solutions.\nnumerical methods - use an algorithm to estimate starting values for \\(\\theta\\), then hill climb until the first derivative is zero, and the second derivative is negative. This is iterative, trying values, looking at the derivatives. This is what nearly all ML estimation uses - there are many different algorithms for doing this. \n\n\nAnalytic Methods\nWith some functions, we can solve for the unknowns directly:\n\\[\\begin{aligned}\n\\ln L(\\lambda|Y) =\\ln \\left[ \\prod\\limits_{i=1}^{N} \\frac{e^{-\\lambda}\\lambda^{y_{i}}}{y_{i}!}\\right]\\nonumber \\\\ \\nonumber \\\\\n=-N \\lambda+ \\sum(y_{i}) \\ln(\\lambda) - \\sum(\\ln(y_{i}!)) \\nonumber\n\\end{aligned}\\]\nTaking the derivative with respect to \\(\\lambda\\) and setting equal to zero:\n\\[\n\\frac{\\partial \\ln L}{\\partial \\lambda}=-N \\lambda+ \\sum(y_{i}) \\ln(\\lambda) - \\sum(\\ln(y_{i}!)) \\nonumber \\\\ \\nonumber  \\\\\n0=-N + \\frac{\\sum y_{i}}{\\lambda} \\nonumber\\\\ \\nonumber \\\\\n\\widehat{\\lambda}= \\frac{\\sum y_{i}}{N} \\nonumber\n\\]\nThis is just the sample mean of course:\n\\[\n\\ln L(\\lambda|Y) = \\ln \\left[ \\frac{e^{-10\\lambda}\\lambda^{20}}{207360}\\right] \\nonumber \\\\\n= -10 \\lambda+ 20 \\ln(\\lambda) - \\ln(207360) \\nonumber\n\\]\nand now find the derivative of the log-likelihood, again with respect to \\(\\lambda\\):\n\\[\n\\frac{\\partial \\ln L}{\\partial \\lambda}= -10 \\lambda+ 20 \\ln(\\lambda) - \\ln(207360) \\nonumber\n=-10 + \\frac{20}{\\lambda} \\nonumber\\\\ \\nonumber \\\\\n\\widehat{\\lambda}= \\frac{20}{10} \\nonumber \\\\\n\\widehat{\\lambda}= 2 \\nonumber\n\\]\nso the value of \\(\\lambda\\) that maximizes the likelihood of observing the civil war data is 2.\nWith some functions, we can solve for the unknowns directly. Stick with me these next couple slides b/c they end in an important point about MLE-OLS."
  },
  {
    "objectID": "likelihood24.html#normal-linear-llf",
    "href": "likelihood24.html#normal-linear-llf",
    "title": "Likelihood",
    "section": "Normal (linear) LLF:",
    "text": "Normal (linear) LLF:\n\\[\n= -\\frac{N}{2}(\\ln(2\\pi)) -\\frac{N}{2}(\\ln(\\sigma^{2})) -\\frac{1}{2\\sigma^{2}}\\left[\\sum\\limits_{i=1}^{n}(y_{i}-\\mu)^{2}\\right] \\nonumber\n\\]\nNotice \\(N\\) in the numerator; recall the rule of summation that \\(\\sum\\limits_{i=1}^{n}a= n\\cdot a\\).\nNow, take the derivative of the log-likelihood with respect to each of the parameters in turn (ignoring constant terms and terms that pertain exclusively to the other parameter).\n\\[\n\\frac{\\partial \\ln L}{\\partial \\mu}= \\frac{1}{\\sigma^{2}}\\sum(y_{i}-\\mu)=0 \\nonumber\\\\\n=\\sum(y_{i}-\\mu) = \\sum y_{i}- \\sum \\mu  \\nonumber\\\\\n=\\sum y_{i}- N \\mu = 0 \\nonumber \\\\\n\\mu=\\frac{\\sum y_{i}}{N} = \\widehat{y}\\nonumber\n\\]\nwe can also solve for \\(\\sigma^{2}\\), getting\n\\[\n\\frac{\\partial \\ln L}{\\partial \\sigma^{2}}= -\\frac{N}{2 \\sigma^{2}}+\\frac{1}{2 \\sigma^{4}} +\\sum(y_{i}-\\mu)=0 \\nonumber\\\\ \\nonumber\\\\\n=-\\frac{N}{2}\\sigma^{2}+\\frac{1}{2}\\sum(y_{i}-\\bar{y})^{2}= 0 \\nonumber\\\\ \\nonumber\\\\\n\\ldots\n\\widehat{\\sigma^{2}}=\\frac{\\sum(y_{i}-\\bar{y})^{2}}{N} \\nonumber\n\\]\nThis is a biased estimator of \\(\\sigma^{2}\\); \\(\\sigma^{2}\\) is underestimated because the denominator should be \\(N-1\\).\nThe same thing in matrix notation:\n\\[ln\\mathcal{L}(y | X, \\beta, \\sigma^2) = -\\frac{N}{2} ln(2\\pi) - \\frac{N}{2} ln(\\sigma^2) -\\frac{1}{2} \\left[ \\frac{(y-X\\beta)'(y-X\\beta)}{\\sigma^2} \\right] \\]\nrewriting to isolate the parameters:\n\\[\nln\\mathcal{L}(y | X, \\beta, \\sigma^2) = -\\frac{N}{2} ln(2\\pi) - \\frac{N}{2} ln(\\sigma^2) -\\frac{1}{2\\sigma^2} \\left[ yy'- 2y' X\\beta +\\beta' X' X\\beta) \\right]\n\\]\nTake derivatives of \\(\\ln \\mathcal{L}\\) w.r.t. \\(\\beta\\) and \\(\\sigma^2\\) (and skipping a lot here):\n\\[\\frac{\\partial ln \\mathcal{L}}{\\partial \\beta} = \\frac{1}{\\sigma^2} (X'y - X'X \\beta)\\]\nsetting equal to zero …\n\\[ \\frac{1}{\\sigma^2} (X'y - X'X \\beta) = 0\\] \\[X'X \\beta = X'y\\] \\[\\widehat{\\beta} = (X'X)^{-1} X' y \\]\n…going through the same thing for \\(\\sigma^2\\) gives us:\n\\[\\widehat{\\sigma^2} = \\frac{e'e}{N}\\]\nSo aside from seeing how analytic methods work, we have also seen that the BLUE OLS estimator is the ML estimator for \\(\\beta\\), and that the variance estimate in ML is biased downward (the denominator is always too large by \\(k-1\\)). This difference disappears in large samples.\nWhy do we leave OLS if these are the same? Because this is the rare case defined by normal data which both satisfies the OLS requirement for a normal disturbance, and permits MLE estimation with a normal LLF. With non-normal data, OLS and ML estimators diverge quite a lot."
  },
  {
    "objectID": "likelihood24.html#numerical-methods",
    "href": "likelihood24.html#numerical-methods",
    "title": "Likelihood",
    "section": "Numerical Methods",
    "text": "Numerical Methods\nNumerical methods are computationally intensive ways to plug in possible parameter values, generate a log likelihood, and then use calculus to evaluate whether the that value is a maximum. We use numerical methods when no analytic or ``closed form’’ solution exists.\nDo this by evaluating:\n\nthe first derivative of the LLF - by finding the point on the function where a tangent line has a slope equal to zero, we know we’ve found an inflection point.\nthe second derivative of the LLF - if the rate of change in the function at the very next point is increasing, it’s a minimum; decreasing, it’s a maximum.\n\nthe Hessian matrix - the matrix of second derivatives - tells us the curvature of the LLF, or the rate of change.\n\nSuppose that we have the event count data reported above representing civil wars in Africa, and that we want to compute the likelihood of \\(\\lambda|Y\\). We can compute the likelihood using numerical methods; one specific technique is a grid search procedure. Just as we might try different values for \\(x\\) when graphing a function, \\(f(x)\\) in algebra, we will insert possible values for \\(\\lambda\\) into the log-likelihood function in such a way that we can identify an apparent maximum (a value for \\(\\lambda\\) for which the log-likelihood is at its largest compared to contiguous values of \\(\\lambda\\)). Put another way, we take a guess at the value of \\(\\lambda\\), compute the log-likelihood, and take another guess at \\(\\lambda\\), compute the log-likelihood and compare the two estimates of the likelihood; we repeat this process until a pattern emerges such that we can discern a maximum value.\nThe log-likelihood function for the poisson distributed data on civil wars is\n\\[\n\\ln L(\\lambda|Y)= \\ln \\left[\\frac{e^{-10\\lambda}\\lambda^{20}}{207360}\\right] \\nonumber  \\\\ \\nonumber \\\\\n= -10 \\lambda+ 20 \\ln(\\lambda) - \\ln(207360) \\nonumber\n\\]\nSuppose we make some guesses regarding the value of \\(\\lambda\\), plug them into the function and compare the resulting values of the log-likelihood:\n\n\ncode\n#iterate over lambda, create data frame of lambda and log-likelihood\nlambda &lt;- seq(0.1, 3.5, by=0.1)\nllf &lt;- NULL\nfor (i in 1:length(lambda)){\n  L &lt;- -10*lambda[i] + 20*log(lambda[i]) - log(207360)\n  llf &lt;- data.frame(rbind(llf, c(lambda=lambda[i], ll=L)))\n}\n\n#highchart with reference line at maximum value of the log-likelihood\nhighchart() %&gt;% \n  hc_add_series(llf, \"line\", hcaes(x=lambda, y=ll)) %&gt;% \n  hc_title(text=\"Log-Likelihood Estimates\") %&gt;% \n  hc_subtitle(text=\"Civil Wars in Africa\") %&gt;% \n hc_xAxis(title = list(text = \"Lambda\"), plotLines = list(list(value = 2, color=\"red\"))) %&gt;%\n  hc_yAxis(title = list(text = \"log-likelihood\"), plotLines = list(list(value = max(llf$ll), color=\"red\"))) %&gt;%\n  hc_tooltip(pointFormat = \"Lambda: {point.x}&lt;br&gt;Log-Likelihood: {point.y}\") %&gt;% \n  hc_colors(\"#005A43\") \n\n\n\n\n\n\nWe can see that the largest value of the likelihood is where \\(\\lambda\\) = 2; similarly, 2 is the value of \\(\\lambda\\) that maximizes the log-likelihood as well. And not surprisingly, notice that we have arrived at the same solution we produced in the analytic example above."
  },
  {
    "objectID": "likelihood24.html#how-numerical-methods-work",
    "href": "likelihood24.html#how-numerical-methods-work",
    "title": "Likelihood",
    "section": "How numerical methods work",
    "text": "How numerical methods work\n\nChoose starting values of \\(\\beta\\) (sometimes from OLS) to estimate the log-likelihood.\nTake the derivative of the log-likelihood with respect to the parameters to find the gradient}. The gradient (or the gradient matrix, a \\(kxk\\) matrix) tells us the direction of the slope of a line tangent to the curve at the point of the log-likelihood estimate.\nIf the gradient is positive (if the matrix is positive definite), then \\(ln \\mathcal{L}\\) is increasing in \\(\\beta\\) - the slope is increasing, so increase our estimate of \\(\\beta\\) and try again.\nIf the gradient is negative (if the matrix is negative definite), the \\(ln \\mathcal{L}\\) is decreasing in \\(\\beta\\) - the slope is decreasing, so we’ve passed the maximum; choose a smaller value for \\(\\beta\\) and try again.\nAs the log-likelihood approaches the maximum, the gradient approaches zero - the slope of the line tangent to the curve at the point of the log-likelihood estimate is approaching zero, indicating we’re reaching the maximum of the function. Stop the search and evaluate the estimates of \\(\\beta\\) that produced the zero gradient.\nThroughout this process, we need to evaluate the second derivatives in order to figure out the rate at which the slope is changing; this helps us tell how close or far we are from the maximum. The second derivative describes the curvature of the LLF, or the rate of change.\nThe matrix of second derivatives (the Hessian matrix) or its approximation also provide the source of our estimates of the variance, and thus the standard errors.\n\nThe first derivative tells us the direction in which the function is changing. This is obviously important since we’re trying to find the maximum.\nThink of this as trying to figure out when you’re exactly at the top of a hill. The slope (the grade, the gradient) is positive while you’re climbing to the top, it’s zero at the top, and it’s negative on the way down the other side.\nBut is the hill flat or steep? If it’s flat, then the change in the slope between point A and point B is likely to be very small - this, of course, can make it difficult to know exactly when we’re at the top (the maximum). On the other hand, if the hill is very steep, the change in the slope between two points is pretty substantial. Put another way, the rate of change in the slope is larger (faster) the steeper the slope; it’s smaller (slower) the flatter the slope.\nThis matters to maximization because the second derivatives tell us how big (or small) a step we should take up the hill as we try to find the top. Suppose that the function is very flat; as indicated above, the change in the slope between two points would be small, so we can take larger steps in order to try to find the maximum. The second derivatives would tell us that the rate of change is very small, so we should take larger steps.\nThe software performing the estimation will choose the next value of \\(\\beta\\) a bit further away from the last value it tried. On the other hand, if the second derivatives are large so the rate of change is fast, we want to take relatively small steps so we don’t step right over the maximum. In any case, that’s the intuition for why we need to know the matrix of second derivatives."
  },
  {
    "objectID": "likelihood24.html#variance-covariance-matrix",
    "href": "likelihood24.html#variance-covariance-matrix",
    "title": "Likelihood",
    "section": "Variance-Covariance matrix",
    "text": "Variance-Covariance matrix\nEstimating the second derivatives can be a real nightmare in estimation, but it’s important not only for finding the maximum of the function (and therefore in estimating the \\(\\beta\\)s), but for computing the variance-covariance matrix as well. Here are our options:\n\nFind the Hessian. The Hessian is a \\(kxk\\) matrix of the second derivatives of the log-likelihood function with respect to \\(\\beta\\), where the second derivatives are on the main diagonal. Commonly estimated using the Newton-Raphson algorithm.\nFind the information matrix. This is the negative of the expected value of the Hessian matrix, computed using the method of scoring.\n\nOuter product approximation, where we sum the squares of the first derivatives (thus avoiding the second derivatives all together). This is computed using the Berndt, Hall, Hall, and Hausman} or BHHH algorithm."
  },
  {
    "objectID": "likelihood24.html#grid-search",
    "href": "likelihood24.html#grid-search",
    "title": "Likelihood",
    "section": "Grid Search",
    "text": "Grid Search\nGrid search is another method for maximization. The process is to iteratively try values for the parameters of interest, refining those values as the log-likelihood gets larger and larger. In general, we plug in values for the parameters, compute the likelihood, then identify the largest LL value - the parameters that produce that value are our answer.\nThis method is instructive for how numerical methods work, but not practical in most applications with more than a couple of unknowns."
  },
  {
    "objectID": "likelihood24.html#latent-variable-motivation",
    "href": "likelihood24.html#latent-variable-motivation",
    "title": "Likelihood",
    "section": "Latent Variable Motivation",
    "text": "Latent Variable Motivation\nThere are a couple of (related) ways to motivate the model. Let’s assume a latent quantity we’re interested, denoted \\(y^*\\), but our observations of \\(y\\) are limited to successes (\\(y_i=1\\)) and failures (\\(y_i=0\\)).\n\\[\ny^{*}=\\mathbf{x_i \\beta}+\\epsilon_i \\nonumber\n\\]\nfor \\(y^{*}\\), the latent variable,\n\\[\ny_{i} = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{if $y^{*}_{1}&gt;\\kappa$} \\\\\n         0, & \\mbox{if $y^{*}_{1} \\leq \\kappa$}\n         \\end{array}\n     \\right.\n\\]\nwhere \\(\\kappa\\) is an unobserved threshold.\nMake probabilities statements,\n\\[\nPr(y_i=1) = Pr(y^{*}_{1}&gt;\\kappa) \\nonumber \\\\ \\nonumber \\\\\n=Pr(\\mathbf{x_i \\beta}+\\epsilon_i&gt;\\kappa) \\nonumber \\\\ \\nonumber \\\\\n=Pr(\\epsilon_i&gt;\\kappa-\\mathbf{x_i \\beta}) \\nonumber\n\\]\nNormalizing \\(\\kappa=0\\),\n\\[\nPr(y_i=1)=Pr(\\epsilon_i&gt;-\\mathbf{x_i \\beta}) \\nonumber\n\\]\n\\[\nPr(y_i=1)=Pr(\\epsilon_i&gt;-\\mathbf{x_i \\beta}) \\nonumber \\\\ \\nonumber \\\\\n=1-F(-\\mathbf{x_i \\beta}) \\nonumber\n\\]\nassuming \\(F\\) is symmetric, with unit variance,\n\\[\n\\pi_i= Pr(y_i=1)=1-F(-\\mathbf{x_i \\beta}) = F(\\mathbf{x_i \\beta}) \\\\   \\nonumber \\\\\n1-\\pi_i=Pr(y_i=0)= 1-F(\\mathbf{x_i \\beta})\n\\]"
  },
  {
    "objectID": "likelihood24.html#binomial-likelihood-function",
    "href": "likelihood24.html#binomial-likelihood-function",
    "title": "Likelihood",
    "section": "Binomial Likelihood Function",
    "text": "Binomial Likelihood Function\nThe observed data are binary, assumed binomial (\\(\\pi\\)), \\(y_i=0,1\\). The likelihood function must have two parts, one for cases where \\(y_i=0\\), the other for cases where \\(y_i=1\\). Recalling that \\(\\pi=F(\\mathbf{x_i \\beta})\\), and \\(1-\\pi= 1-F(\\mathbf{x_i \\beta})\\),\n\\[\nPr(y_1,y_2,y_3 \\ldots y_n) =  \\prod_{y=1}F(\\mathbf{x_i \\beta}) \\prod_{y=0}[1-F(\\mathbf{x_i \\beta})]\\nonumber\n\\]\nThis is the joint probability we observe all the data, \\(Y\\), simultaneously. We can rewrite this as the likelihood of observing the data given \\(\\beta\\),\n\\[\n\\mathcal{L} (Y|\\beta) =  \\prod_{i=1}^{N} [F(\\mathbf{x_i \\beta})]^{y_i} [1-F(\\mathbf{x_i \\beta})]^{1-y_i}\\nonumber\n\\]\nAnd take the natural log\n\\[\n\\ln(\\mathcal{L} (Y|\\beta)) = \\ln( \\prod_{i=1}^{N} [F(\\mathbf{x_i \\beta})]^{y_i} [1-F(\\mathbf{x_i \\beta})]^{1-y_i})\\nonumber \\\\ \\nonumber \\\\\n= \\sum_{i=1}^{N} y_i \\ln F(\\mathbf{x_i \\beta})+ (1-y_i) \\ln[1-F(\\mathbf{x_i \\beta})] \\nonumber\n\\]\nNote the two parts of the LLF corresponding to the limited observations in the data, 0,1."
  },
  {
    "objectID": "likelihood24.html#choosing-a-link",
    "href": "likelihood24.html#choosing-a-link",
    "title": "Likelihood",
    "section": "Choosing a Link",
    "text": "Choosing a Link\nLet’s make this a probit model by assuming the link to the latent variable is standard normal, so \\(F(\\cdot)\\sim N_{i.i.d.}(0,1)\\):\n\\[\n\\ln \\mathcal{L} (Y|\\beta) = \\sum_{i=1}^{N} y_i \\ln \\Phi(\\mathbf{x_i \\beta})+ (1-y_i) \\ln[1-\\Phi(\\mathbf{x_i \\beta})] \\nonumber\n\\]\nLogit would look like this:\n\\[\n\\ln \\mathcal{L} (Y|\\beta) = \\sum_{i=1}^{N} \\left\\{ y_i \\ln \\left(\\frac{1}{1+e^{-\\mathbf{x_i \\beta}}}\\right)+ (1-y_i) \\ln \\left[1-\\left(\\frac{1}{1+e^{-\\mathbf{x_i \\beta}}}\\right)\\right] \\right\\}\\nonumber\n\\]"
  },
  {
    "objectID": "likelihood24.html#estimation",
    "href": "likelihood24.html#estimation",
    "title": "Likelihood",
    "section": "Estimation",
    "text": "Estimation\nIdeally, we’d like just to estimate by finding the values of \\(\\beta\\) that maximize the log-likelihood function, and do so analytically (i.e., using calculus). This is what we do in OLS, though with respect to minimizing the sum of the squared residuals. But because the solution is non-linear in \\(\\beta\\), there is no closed form or simple analytic solution.\nAs a result, ML models produce estimates of \\(\\beta\\) by using numerical optimization methods. These are generally iterative attempts to narrow down the range in which the maximum lies by plugging in different values of \\(\\beta\\) until the range is so small, we can safely say we’ve maximized the function using those values of \\(\\beta\\)."
  },
  {
    "objectID": "likelihood24.html#maximization",
    "href": "likelihood24.html#maximization",
    "title": "Likelihood",
    "section": "Maximization",
    "text": "Maximization\nSuppose we have binary data that look like this:\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\nOur question is what is the distribution parameter most likely responsible for having generated these observed data.\nWe need to plug a hypothetical value for the distribution parameter into the log-likelihood function, compute log-likelihoods for each observation, then do the same thing with other hypothetical values. Whichever value produces the biggest log-likelihoods is the value most likely responsible for producing the data we have.\nWhat do we mean by distribution parameter? Well, in the LLF below, we’ve referred to our unknown as \\(F(\\mathbf{x_i \\beta})\\), but we really mean we need an estimate of the parameter \\(\\Theta\\) which represents the effects of the \\(X\\)s via the functional form we’ve imposed by assuming a distribution of \\(\\epsilon\\). In this particular case (for simplicity) we don’t have any \\(X\\) variables.\n\\[\n\\ln(\\mathcal{L} (Y|\\Theta)) = \\sum_{i=1}^{N} y_i \\ln F(\\mathbf{\\Theta})+ (1-y_i) \\ln[1-F(\\mathbf{\\Theta})] \\nonumber\n\\]\nHere’s what we’ll do:\n\nchoose some hypothetical values of \\(\\Theta\\); since this is binary, and our latent variable a probability, let’s choose values from .2 to .8.\ncompute \\(N\\) log-likelihoods for each value of \\(\\Theta\\); N=20, and we have 7 values of \\(\\Theta\\).\nsum the \\(N\\) log-likelihoods for each value of \\(\\Theta\\); so we’ll end up with 7 summed log-likelihoods.\nevaluate the summed log-likelihoods, and see which is largest.\ndeclare the value of \\(\\Theta\\) that produced that largest summed log-likelihood as the parameter most likely to have generated the data.\n\nLet \\(\\Theta=.2\\):\n\\[\n\\ln(\\mathcal{L}_{i=1} (Y|\\Theta)) = 0 *\\ln (.2)+ (1-0) \\ln[1-.2] =-0.2231\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=2} (Y|\\Theta)) = 0 *\\ln (.2)+ (1-0) \\ln[1-.2]=-0.2231 \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=3} (Y|\\Theta)) = 0 *\\ln (.2)+ (1-0) \\ln[1-.2]=-0.2231 \\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=11} (Y|\\Theta)) = 1 *\\ln (.2)+ (1-1) \\ln[1-.2] =-1.609\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=12} (Y|\\Theta)) = 1 *\\ln (.2)+ (1-1) \\ln[1-.2]=-1.609 \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=13} (Y|\\Theta)) = 1 *\\ln (.2)+ (1-1) \\ln[1-.2] =-1.609\\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\sum_{i=1}^{20}=-18.32100 \\nonumber\n\\]\nLet \\(\\Theta=.3\\):\n\\[\n\\ln(\\mathcal{L}_{i=1} (Y|\\Theta)) = 0 *\\ln (.3)+ (1-0) \\ln[1-.3] =-0.3567\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=2} (Y|\\Theta)) = 0 *\\ln (.3)+ (1-0) \\ln[1-.3]=-0.3567 \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=3} (Y|\\Theta)) = 0 *\\ln (.3)+ (1-0) \\ln[1-.3]=-0.3567 \\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=11} (Y|\\Theta)) = 1 *\\ln (.3)+ (1-1) \\ln[1-.3] =-1.204\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=12} (Y|\\Theta)) = 1 *\\ln (.3)+ (1-1) \\ln[1-.3]=-1.204\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=13} (Y|\\Theta)) = 1 *\\ln (.3)+ (1-1) \\ln[1-.3] =-1.204\\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\sum_{i=1}^{20}=-15.60700 \\nonumber\n\\]\nLet \\(\\Theta=.4\\):\n\\[\n\\ln(\\mathcal{L}_{i=1} (Y|\\Theta)) = 0 *\\ln (.4)+ (1-0) \\ln[1-.5] =-0.5108\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=2} (Y|\\Theta)) = 0 *\\ln (.4)+ (1-0) \\ln[1-.5]=-0.5108 \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=3} (Y|\\Theta)) = 0 *\\ln (.4)+ (1-0) \\ln[1-.5]=-0.5108 \\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=11} (Y|\\Theta)) = 1 *\\ln (.4)+ (1-1) \\ln[1-.4] =-0.9163\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=12} (Y|\\Theta)) = 1 *\\ln (.4)+ (1-1) \\ln[1-.4]=-0.9163\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=13} (Y|\\Theta)) = 1 *\\ln (.4)+ (1-1) \\ln[1-.4] =-0.9163\\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\sum_{i=1}^{20}=-14.2711636 \\nonumber\n\\]\nLet \\(\\Theta=.5\\):\n\\[\n\\ln(\\mathcal{L}_{i=1} (Y|\\Theta)) = 0 *\\ln (.5)+ (1-0) \\ln[1-.5] =-0.6931\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=2} (Y|\\Theta)) = 0 *\\ln (.5)+ (1-0) \\ln[1-.5]=-0.6931 \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=3} (Y|\\Theta)) = 0 *\\ln (.5)+ (1-0) \\ln[1-.5]=-0.6931 \\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\ln(\\mathcal{L}_{i=11} (Y|\\Theta)) = 1 *\\ln (.5)+ (1-1) \\ln[1-.5] =-0.6931\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=12} (Y|\\Theta)) = 1 *\\ln (.5)+ (1-1) \\ln[1-.5]=-0.6931\\nonumber \\\\\n\\ln(\\mathcal{L}_{i=13} (Y|\\Theta)) = 1 *\\ln (.5)+ (1-1) \\ln[1-.5] =-0.6931\\nonumber \\\\\n\\vdots \\nonumber \\\\\n\\sum_{i=1}^{20}=-13.8629436 \\nonumber\n\\]\nLet’s compare what we have so far:\n\\[llf_{.2}&lt;llf_{.3}&lt;llf_{.4}&lt;llf_{.5}\\]\nYou can probably see some symmetry here due to the fact that half the data are ones, half zeros, so completing:\\~\\\n\\(llf_{.2}&lt;llf_{.3}&lt;llf_{.4}&lt;llf_{.5}&gt;llf_{.6}&gt;llf_{.7}&gt;llf_{.8}\\) \\~\\\nSo \\(\\Theta=.5\\) produces the largest log-likelihood (-13.86) and thus is the parameter most likely to have produced the observed data."
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "Binomial Models - video\nPrediction Methods - spring 2024 prediction video\n\n\n\n Back to top"
  },
  {
    "objectID": "binarymodels24.html",
    "href": "binarymodels24.html",
    "title": "Binary Response Models",
    "section": "",
    "text": "How can we model a binary \\(y\\) variable?\nDoes OLS (the linear probability model) work sufficiently well?\nHow can we build a maximum likelihood model?"
  },
  {
    "objectID": "binarymodels24.html#questions",
    "href": "binarymodels24.html#questions",
    "title": "Binary Response Models",
    "section": "",
    "text": "How can we model a binary \\(y\\) variable?\nDoes OLS (the linear probability model) work sufficiently well?\nHow can we build a maximum likelihood model?"
  },
  {
    "objectID": "binarymodels24.html#example---democratic-peace-data",
    "href": "binarymodels24.html#example---democratic-peace-data",
    "title": "Binary Response Models",
    "section": "Example - Democratic Peace data",
    "text": "Example - Democratic Peace data\nAs a running example, I’ll use the Democratic Peace data to estimate logit and probit models. These come from Oneal and Russett (1997)’s well-known study in ISQ. The units are dyad-years; the \\(y\\) variable is the presence or absence of a militarized dispute, and the \\(x\\) variables include a measure of democracy (the lowest of the two Polity scores in the dyad), and a set of controls. The principle expectation here is that as the lowest democracy score in the dyad increases, the probability of a militarized dispute decreases.\n\nPredictions out of bounds\nThis figured plots the predictions from a logit and OLS model. Unsurprisingly, the logit predictions are probabilities, so are in the \\([0,1]\\) interval. The OLS predictions are not, and are often out of bounds.\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\nlogitpreds &lt;- predict(m1, type=\"response\")\n\nm2 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"probit\"), data=dp )\nmols &lt;-lm(dispute ~ border+deml+caprat+ally, data=dp )\nolspreds &lt;- predict(mols)\n\ndf &lt;- data.frame(logitpreds, olspreds, dispute=as.factor(dp$dispute))\n\nggplot(df, aes(x=logitpreds, y=olspreds, color=dispute)) + \n  geom_point()+\n  labs(title=\"Predictions from Logit and OLS\", x=\"Logit Predictions\", y=\"OLS Predictions\")+\n  geom_hline(yintercept=0)+\n  theme_minimal() +\n  scale_color_manual(values=c(\"#005A43\", \"#6CC24A\")) +\n  annotate(\"text\", x=.05, y=-.05, label=\"2,147 Predictions out of bounds\", color=\"red\")\n\n\n\n\n\n\n\n\n\nHere’s the distribution of predictions from the OLS model - you’ll note the modal density is around .04 (which is the sample frequency of \\(y\\).), but that a substantial and long tail are negative, so out of probability bounds.\n\n\ncode\nggplot(df, aes(x=olspreds)) + \n  geom_density(alpha=.5)+\n  labs(title=\"Density of OLS Predictions\", x=\"Predictions\", y=\"Density\")+\n  theme_minimal()+\ngeom_vline(xintercept=0, linetype=\"dashed\")\n\n\n\n\n\n\n\n\n\n\n\nHeteroskedastic Residuals\nThe residuals from the OLS model appear heteroskedastic, and the distribution is not normal. In fact, the distribution appears more binomial, clustered around zero and one. This shouldn’t be surprising since the \\(y\\) variable only takes on values of zero and one, and since we compute the residuals by \\(u = y - \\hat{y}\\).\n\n\ncode\ndf &lt;- data.frame(df, mols$residuals)\n \nggplot(df, aes(x=mols.residuals, color=dispute)) + \n  geom_density()+\n  labs(title=\"Density of OLS Residuals\", x=\"Residuals\", y=\"Density\")+\n  theme_minimal()+\n    scale_color_manual(values=c(\"#005A43\", \"#6CC24A\")) +\n  geom_vline(xintercept=0, linetype=\"dashed\")"
  },
  {
    "objectID": "binarymodels24.html#when-is-the-lpm-appropriate",
    "href": "binarymodels24.html#when-is-the-lpm-appropriate",
    "title": "Binary Response Models",
    "section": "When is the LPM Appropriate?",
    "text": "When is the LPM Appropriate?\nThe best answer is never.\n\nThere seems to be a mild trend in the discipline to rehabilitate the LPM though it’s not clear why - that is, it’s hard to find statements about the advantages of doing so in any particular setting, or about the disadvantages of estimating a logit or probit model that would lead us to prefer the LPM.\n\nOLS is a rockin’ estimator, but it’s just not well suited to limited \\(y\\) variables. Efforts to rehabilitate the LPM are like putting lipstick on a pig."
  },
  {
    "objectID": "binarymodels24.html#examples-of-limited-dvs",
    "href": "binarymodels24.html#examples-of-limited-dvs",
    "title": "Binary Response Models",
    "section": "Examples of Limited DVs",
    "text": "Examples of Limited DVs\n\nbinary variables: 0=peace, 1=war; 0=vote, 1=don’t vote.\nunordered or nominal categorical variables: type of car you prefer: Honda, Toyota, Ford, Buick; policy choices; party or candidate choices.\nordered variables that take on few values: some survey responses.\ndiscrete count variables; number of episodes of scarring torture in a country-year, 0, 1, 2, 3, …, \\(\\infty\\); the number of flawed computer chips produced in a factory in a shift; the number of times a person has been arrested; the number of self-reported extramarital affairs; number of visits to your primary care doctor.\ntime to failure; how long a civil war lasts; how long a patient survives disease; how long a leader survives in office."
  },
  {
    "objectID": "binarymodels24.html#binary-y-variables-1",
    "href": "binarymodels24.html#binary-y-variables-1",
    "title": "Binary Response Models",
    "section": "Binary \\(y\\) variables",
    "text": "Binary \\(y\\) variables\nGenerally, we think of a binary variable as being the observable manifestation of some latent, unobserved continuous variable.\nIf we could adequately observe (and measure) the underlying continuous variable, we’d use some form of OLS regression to analyze that variable. But because we have limited observation, we turn to maximum likelihood methods to estimate a model that allows to use \\(y\\), but generate estimates of \\(y^*\\), the variable we wish we could measure."
  },
  {
    "objectID": "binarymodels24.html#a-nonlinear-model-for-binary-data",
    "href": "binarymodels24.html#a-nonlinear-model-for-binary-data",
    "title": "Binary Response Models",
    "section": "A nonlinear model for binary data",
    "text": "A nonlinear model for binary data\nSo \\(y\\) is binary, and we’ve established the linear model is not appropriate. The observed variable, \\(y\\), appears to be binomial (iid):\n\\[ y \\sim f_{binomial}(\\pi_i)\\]\n\\[ y = \\left\\{ \\begin{array}{ll}\n         1, & \\mbox{} \\pi_{i}\\\\\n         0, & \\mbox{} 1-\\pi_{i}\n         \\end{array}\n     \\right. \\]\n\\[ \\pi_i = F(x_i\\widehat{\\beta}) \\] \\[1- \\pi_i=1-F(x_i\\widehat{\\beta})\\]"
  },
  {
    "objectID": "binarymodels24.html#binomial-likelihood",
    "href": "binarymodels24.html#binomial-likelihood",
    "title": "Binary Response Models",
    "section": "Binomial Likelihood",
    "text": "Binomial Likelihood\nWrite the binomial density:\n\\[ Pr(y=1| \\pi) = \\pi_i^{y_i} (1-\\pi_i)^{1-y_i} \\]\nWrite the joint probability as a likelihood:\n\\[\\mathcal{L} (\\pi |\\ y) = \\prod \\limits_{i=1}^{n} \\left[ \\pi_i^{y_i} (1-\\pi_i)^{1-y_i}\\right]\\]\nTake the log of that likelihood:\n\\[\\ln \\mathcal{L} (\\pi| \\ y) = \\sum \\limits_{i=1}^{n}  \\left[ y_i \\ln ( \\pi_i) +  (1-y_i) \\ln(1-\\pi_i)\\right]\\]"
  },
  {
    "objectID": "binarymodels24.html#parameterize-the-model",
    "href": "binarymodels24.html#parameterize-the-model",
    "title": "Binary Response Models",
    "section": "Parameterize the model",
    "text": "Parameterize the model\nParameterize \\(\\pi_i\\) - make \\(\\pi_i\\) a function of some variables and their slope effects, \\(x\\beta\\) - this is the systematic component of the model:\n\\[\\pi_i= F(x \\beta)\\]\nThis is the binomial log-likelihood function.\n\\[\\ln \\mathcal{L} (\\pi| \\ y) = \\sum \\limits_{i=1}^{n}  \\left[ y_i \\ln (F(x_i\\widehat{\\beta})) +  (1-y_i) \\ln(1-F(x_i\\widehat{\\beta}))\\right]\\]\nBut we need to fill in \\(F\\), the link function."
  },
  {
    "objectID": "binarymodels24.html#link-function",
    "href": "binarymodels24.html#link-function",
    "title": "Binary Response Models",
    "section": "Link Function",
    "text": "Link Function\nWe parameterized \\(\\pi_i\\):\n\\[\\pi_i= F(x \\beta)\\]\nand now need to choose an appropriate link function for \\(F\\) such that:\n\nour prediction of \\(\\widehat{\\pi_i}\\) is bounded [0,1].\n\\(x_i \\widehat{\\beta}\\) can range over the interval \\([-\\infty, +\\infty]\\) and map onto the [0,1] interval.\n\nThere’s a large number of sigmoid shaped probability functions that will satisfy these needs.\n\n\n\n\n\n\nImportant Concept\n\n\n\nThe link function maps or transforms the linear prediction on the sigmoid probability space, and obeys the bounds of 0,1.\n\n\nThe most commonly used link functions are the standard normal (probit)}\n\\[Pr(y_i=1 | X) = \\Phi(x_i\\widehat{\\beta}) \\]\nand the logistic (logit) CDFs.\n\\[Pr(y_i=1 | X) = \\frac{1}{1+exp^{-(x_i\\widehat{\\beta})}} \\]\nHere are the logistic and Normal CDFs:\n\n\ncode\nz &lt;- seq(-5,5,.1)\nl &lt;- seq(0,1,.01)\ns1 &lt;- 1/(1+exp(-z))\ns2 &lt;- pnorm(z)\n\ndf &lt;- data.frame(z=z, l=l, s1=s1, s2=s2)\n\nggplot(df, aes(x=z, y=l)) + \n  geom_line(aes(x=z, y=l), color=\"black\", linetype=\"dashed\" )+\n  geom_line(aes(x=z, y=s1), color=\"#005A43\")+\n  geom_line(aes(x=z, y=s2), color=\"#6CC24A\")+\n  labs(title=\"Logistic and Normal CDFs\", x=expression(x*beta), y=\"Pr(y=1)\")+\n  theme_minimal() +\n  annotate(\"text\", x=1.3, y=.7, label=\"logistic\", color=\"black\")+\n  annotate(\"text\", x=-.2, y=.15, label=\"normal\", color=\"black\")\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nNote the sigmoid functions approach the limits at decreasing rates; the fastest rate of change is at \\(y=.5\\), a point around which the curves are symmetric. The point \\(y=.5\\) is the transition point below which we’d predict a zero, above which we’d predict a one if we were interested in classifying cases into zeros and ones. Classification is a common use for models like these, say distinguishing spam from non-spam emails, or predicting the presence or absence of a disease. More on this later."
  },
  {
    "objectID": "binarymodels24.html#probit-and-logit-llfs",
    "href": "binarymodels24.html#probit-and-logit-llfs",
    "title": "Binary Response Models",
    "section": "Probit and Logit LLFs",
    "text": "Probit and Logit LLFs\nProbit - link between \\(x\\hat{\\beta}\\) and \\(Pr(y=1)\\) is standard normal CDF: \\[\n\\ln \\mathcal{L} (Y|\\beta) = \\sum_{i=1}^{N} y_i \\ln \\Phi(\\mathbf{x_i \\beta})+ (1-y_i) \\ln[1-\\Phi(\\mathbf{x_i \\beta})] \\nonumber\n\\]\nLogit (logistic CDF):\n\\[\n\\ln \\mathcal{L} (Y|\\beta) = \\sum_{i=1}^{N} \\left\\{ y_i \\ln \\left(\\frac{1}{1+e^{-\\mathbf{x_i \\beta}}}\\right)+ (1-y_i) \\ln \\left[1-\\left(\\frac{1}{1+e^{-\\mathbf{x_i \\beta}}}\\right)\\right] \\right\\}\\nonumber\n\\]"
  },
  {
    "objectID": "binarymodels24.html#predicted-probabilities",
    "href": "binarymodels24.html#predicted-probabilities",
    "title": "Binary Response Models",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\nIn the nonlinear model, the most basic quantity is\n\\[F(x\\widehat{\\beta})\\]\nwhere \\(F\\) is the link function, mapping the linear prediction onto the probability space.\nFor the logit, the predicted probability is\n\\[Pr(y=1) = \\frac{1}{1+exp(-x\\widehat{\\beta})}\\]\nFor the probit, the predicted probability is\n\\[Pr(y=1) = \\Phi(x\\widehat{\\beta})\\]\nAgain, simply using the link function to map the linear prediction onto the probability space."
  },
  {
    "objectID": "binarymodels24.html#marginal-effects",
    "href": "binarymodels24.html#marginal-effects",
    "title": "Binary Response Models",
    "section": "Marginal Effects",
    "text": "Marginal Effects\nIn the linear model, the marginal effect of \\(x\\) is \\(\\widehat{\\beta}\\). That is, the effect of a one unit change in \\(x\\) on \\(y\\) is \\(\\widehat{\\beta}\\).\n\\[\n\\frac{\\partial \\widehat{y}}{\\partial x_k}= \\frac{\\partial x \\widehat{\\beta}}{\\partial x_k} \\nonumber \\\\  \\nonumber \\\\\n= \\widehat{\\beta} \\nonumber\n\\]\nThe marginal effect is constant with respect to \\(x_k\\). Take a look:\n\n\ncode\nx &lt;- seq(0,10,.1)\ny &lt;- 2*x\ndf &lt;- data.frame(x=x, y=y)\n\nggplot(df, aes(x=x, y=y)) + \n  geom_line()+\n  labs(title=\"Marginal Effect of x on y\", x=\"x\", y=\"y\")+\n  theme_minimal() +\n  annotate(\"text\", x=5, y=15, label=\"y = 2x\", color=\"black\")+\n  geom_segment(aes(x = 5, xend = 5, y = 0, yend = 10), color = \"red\")+\n  geom_segment(aes(x = 10, xend = 10, y = 0, yend = 20), color = \"red\")\n\n\n\n\n\n\n\n\n\nThe effect of \\(x\\) on \\(y\\) is 2 - it’s the same at \\(x=5\\) and at \\(x=10\\).\nIn the nonlinear model, the marginal effect of \\(x_k\\) depends on where \\(x\\widehat{\\beta}\\) lies with respect to the probability distribution \\(F(\\cdot)\\).\n\\[\n\\frac{\\partial Pr(y=1)}{\\partial x_k}= \\frac{\\partial F(x\\widehat{\\beta})}{\\partial x_k} \\nonumber \\\\  \\nonumber \\\\\n=  \\frac{\\partial F(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} \\cdot \\frac{\\partial (x\\widehat{\\beta})}{\\partial x_k}  \\nonumber\n\\]\nBoth of these terms simplify …\nRemember that\n\\[\n\\frac{\\partial (x\\widehat{\\beta})}{\\partial x} = \\widehat{\\beta} \\nonumber\n\\]\nand \\[\n\\frac{\\partial F(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} = f(x\\widehat{\\beta}) \\nonumber\n\\]\nwhere the derivative of the CDF is the PDF.\nPutting these together gives us:\n\\[\n\\frac{\\partial F(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} = f(x\\widehat{\\beta}) \\widehat{\\beta} \\nonumber\n\\]\nThis is \\(\\widehat{\\beta}\\) weighted by or measured at the ordinate on the PDF - the ordinate is the height of the PDF associated with a value of the \\(x\\) axis.\n\n\n\n\n\n\nImportant Concept\n\n\n\nThe effect of \\(x\\) on \\(Pr(y=1)\\) is not constant; it will be large for some values of \\(x\\) and small for others. This makes sense if we think about the sigmoid functions - the slope of the curve is steepest at \\(y=.5\\), and flattens as we move away from that point toward either limit. Take another look at Figure 1\n\n\n\nLogit Marginal Effects\n\n\nRecall \\(\\Lambda\\) is the logistic CDF = \\[1/(1+exp(-x_i\\widehat{\\beta}))\\].\n\\(\\lambda\\) is the logit PDF \\[1/(1+exp(-x_i\\widehat{\\beta}))^2\\]\nAlso, remember that\n\\[\\frac{e^{x_i\\widehat{\\beta}}}{1+e^{x_i\\widehat{\\beta}}} = \\frac{1}{1+e^{-x_i\\widehat{\\beta}}}\\]\n\\[\n\\begin{align}\n\\frac{\\partial \\Lambda(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} = \\lambda(x\\widehat{\\beta}) \\widehat{\\beta} \\\\\n= \\frac{e^{x_i\\widehat{\\beta}}}{(1+e^{x_i\\widehat{\\beta}})^2} \\widehat{\\beta}  \\\\\n=\\frac{e^{x_i\\widehat{\\beta}}}{1+e^{x_i\\widehat{\\beta}}} \\frac{1}{1+e^{x_i\\widehat{\\beta}}} \\widehat{\\beta}  \\\\\n=\\Lambda(x_i\\widehat{\\beta}) \\frac{1+e^{x_i\\widehat{\\beta}}-e^{x_i\\widehat{\\beta}}}{1+e^{x_i\\widehat{\\beta}}} \\widehat{\\beta}   \\\\\n=\\Lambda(x_i\\widehat{\\beta}) 1-\\frac{e^{x_i\\widehat{\\beta}}}{1+e^{x_i\\widehat{\\beta}}} \\widehat{\\beta}  \\\\\n=\\Lambda(x_i\\widehat{\\beta}) (1-\\Lambda(x_i\\widehat{\\beta})) \\widehat{\\beta}  \n\\end{align}\n\\]\nSo this last line indicates the marginal effect of \\(x\\) is the probability of a one times the probability of a zero times \\(\\widehat{\\beta}\\).\nThis is useful because the largest value this can take on is .25 \\((Pr(y_i=1)=0.5 \\cdot Pr(y_i=0)=0.5= 0.25)\\) - therefore, the maximum marginal effect any \\(x\\) can have is \\(0.25 \\widehat{\\beta}\\).\nLooking at the democratic peace model below, the coefficient on democracy is -.071, so the largest effect democracy can have on the probability of a militarized dispute is \\(0.25 \\cdot -.071 = -.01775\\).\n\ncode\nlibrary(stargazer)\n\nstargazer(m1,m2, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\nlogistic\n\n\nprobit\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n0.587*** (0.037)\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n-0.031*** (0.003)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n-0.001*** (0.0001)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n-0.350*** (0.038)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n-1.903*** (0.032)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n7,032.985\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\nIn the probit model, the marginal effect is:\n\\[\n\\frac{\\partial \\Phi(x\\widehat{\\beta})}{\\partial x\\widehat{\\beta}} = \\phi(x\\widehat{\\beta}) \\widehat{\\beta} \\nonumber\n\\]\nThe ordinate at the maximum of the standard normal PDF is 0.3989 - rounding to 0.4, we can say that the maximum marginal effect of any \\(\\widehat{\\beta}\\) in the probit model is \\(0.4\\widehat{\\beta}\\).\nThe ordinate is at the maximum where \\(z=0\\); recall this is the standard normal, so \\(x_i\\widehat{\\beta}=z\\). When \\(z=0\\),\n\\[Pr(z)=\\frac{1}{\\sqrt{2 \\pi}} \\exp \\left[\\frac{-(z)^{2}}{2}\\right] \\nonumber \\\\ \\nonumber\\\\\n=\\frac{1}{\\sqrt{2 \\pi}} \\nonumber\\\\\n\\approx .4 \\nonumber \\]\nSo the maximum marginal effect of any \\(x\\) in the probit model is \\(0.4\\widehat{\\beta}\\)."
  },
  {
    "objectID": "binarymodels24.html#logit-odds-interpretation",
    "href": "binarymodels24.html#logit-odds-interpretation",
    "title": "Binary Response Models",
    "section": "Logit Odds Interpretation",
    "text": "Logit Odds Interpretation\nThe odds are given by the probability an event occurs divided by the probability it does not:\n\\[\n\\Omega(X) = \\frac{Pr(y=1)}{1-Pr(y=1)} \\nonumber\n= \\frac{\\Lambda(X\\widehat{\\beta})}{(1-\\Lambda(X\\widehat{\\beta}))} \\nonumber\n\\]"
  },
  {
    "objectID": "binarymodels24.html#logit-log-odds",
    "href": "binarymodels24.html#logit-log-odds",
    "title": "Binary Response Models",
    "section": "Logit Log-odds",
    "text": "Logit Log-odds\nLogging …\n\\[\\ln \\Omega(X) = \\ln \\left(\\frac{\\Lambda(X\\widehat{\\beta})}{(1-\\Lambda(X\\widehat{\\beta}))}\\right) =X\\widehat{\\beta} \\]\n\\[\n\\frac{\\partial \\ln \\Omega}{\\partial X} = \\widehat{\\beta} \\nonumber\n\\]\nWhich shows the change in the log-odds given a change in \\(X\\) is constant (and therefore linear). This quantity is sometimes called “the logit.”"
  },
  {
    "objectID": "binarymodels24.html#logit-odds-ratios",
    "href": "binarymodels24.html#logit-odds-ratios",
    "title": "Binary Response Models",
    "section": "Logit Odds Ratios",
    "text": "Logit Odds Ratios\nOdds ratios are very useful:\n\\[\n\\frac{ \\Omega x_k + 1}{\\Omega x_k} =exp(\\widehat{\\beta_k}) \\nonumber\n\\]\ncomparing the difference in odds between two values of \\(x_k\\); note the change in value does not have to be 1.\n\\[\n\\frac{ \\Omega x_k + \\iota}{\\Omega x_k} =exp(\\widehat{\\beta_k}* \\iota) \\nonumber\n\\]\nNot only is it simple to exponentiate \\(\\widehat{\\beta_k}\\), but the interpretation is that \\(x\\) increases/decreases \\(Pr(y=1)\\) by that factor, \\(exp(\\widehat{\\beta_k})\\), and more usefully, that:\n\\[\n100*(exp(\\widehat{\\beta_k})-1) \\nonumber\n\\]\nis the percentage change in the odds given a one unit change in \\(x_k\\).\nSo a logit coefficient of .226\n\\[\n100*(exp(.226)-1) =25.36 \\nonumber\n\\]\nProduces a 25.36% increase in the odds of \\(y\\) occurring."
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "mlesyllabus24.html#seminar-description",
    "href": "mlesyllabus24.html#seminar-description",
    "title": "MLE Syllabus",
    "section": "Seminar Description",
    "text": "Seminar Description\nThis 4 credit hour seminar is a survey of maximum likelihood methods and their applications to empirical political questions. It presumes students have a detailed and intuitive knowledge of least squares, probability theory, basic skills in scalar and matrix algebra, and a basic understanding of calculus. The course will deal mainly in understanding the principles of maximum likelihood estimation, under what conditions we move away from least squares, and what particular models are appropriate given observed data. The seminar will focus on application and interpretation of ML models and linking theory to statistical models. The course emphasizes coding and data viz in R and Stata.\nThe class meets one time per week for three hours. My office hours are designed to be homework help hours where I’ll work in the grad lab with any of you who are working on the exercises. The most productive pathway for this class is for you to get in the habit of working together, and those office hours are a good time for this."
  },
  {
    "objectID": "mlesyllabus24.html#course-purpose",
    "href": "mlesyllabus24.html#course-purpose",
    "title": "MLE Syllabus",
    "section": "Course Purpose",
    "text": "Course Purpose\nThis seminar fulfills the advanced quantitative methods requirement in the Ph.D. curriculum. The method of maximum likelihood underlies a majority of quantitative models in Political Science; this class teaches students to be astute consumers of such models, and how to implement and interpret ML models. These are crucial skills for dissertations in Political Science, and for producing publishable quantitative research."
  },
  {
    "objectID": "mlesyllabus24.html#learning-objectives",
    "href": "mlesyllabus24.html#learning-objectives",
    "title": "MLE Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nStudents will encounter an array of maximum likelihood models in this course. By the end of the course, students will have mastered the theory of maximum likelihood sufficient to write and program likelihood functions in ; they will be able to choose, estimate, and interpret appropriate models, model specifications, and model evaluation tools given their data; and they will be able to produce sophisticated quantities of interest (e.g. predicted probabilities, expected values, confidence intervals) via a variety of techniques including simulation and end point transformation. Students will also be able to present model findings verbally and graphically."
  },
  {
    "objectID": "mlesyllabus24.html#class-meetings-office-hours-assignments",
    "href": "mlesyllabus24.html#class-meetings-office-hours-assignments",
    "title": "MLE Syllabus",
    "section": "Class Meetings, Office Hours, Assignments",
    "text": "Class Meetings, Office Hours, Assignments\nThe course will meet this fall entirely in-person in the Social Science Experiment Lab on Wednesdays 9:40am-12:40pm.\nOffice hours are Mondays 1:30pm-3:30pm. I’ll likely hold these in the grad work room to help with your assignments. For an appointment, email me and we’ll sort out a time.\nAll assignments should be turned in on Brightspace - please submit ::\n\nPDFs generated from LaTeX or R Markdown (Quarto).\nannotated R scripts.\nwhere necessary, data.\n\nAssignments should be instantly replicable - running the code file should produce all models, tables, plots, etc."
  },
  {
    "objectID": "mlesyllabus24.html#reading",
    "href": "mlesyllabus24.html#reading",
    "title": "MLE Syllabus",
    "section": "Reading",
    "text": "Reading\nThe reading material for the course is important because it often demonstrates application of various MLE models; seeing how folks apply these and how they motivate their applications is really informative, something you cannot miss. We often won’t directly discuss the readings, but don’t let that imply they’re not important. If I get the sense we’re not keeping up with reading, expect the syllabus to change to incorporate quizzes or other accountability measures.\nReading for the course will consist of several books and articles (listed by week below). The books listed below also have Amazon links - you’ll find most of these cheaper used online.\n\nRequired\n\nBox-Steffensmeier, Janet and Jones, Brad. 2004. Event History Modeling. Cambridge. ISBN 0521546737\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Sage Publications Inc. ISBN 0803973748\nWard, Michael D. and John S. Ahlquist. 2018 Maximum Likelihood for Social Science. Cambridge. ISBN 978-1316636824.\n\n\n\nRecommended\nUseful, but not required (though some required reading in the first one):\n\nGary King. 1998. Unifying Political Methodology. University of Michigan Press. ISBN 0472085549\nJ. Scott Long. 2014. Regression Models for Categorical Dependent Variables Using Stata. 3rd Ed. Stata Press. ISBN 1597181110 (this book is good for practical/applied examples even if R is your primary language)\n\nGary King’s book is regarded as seminal in developing ML applications in political science. Scott Long’s is a similarlyaccessible treatment of a host of ML models and applications (and the Stata book is a great applied companion). Together, these two books are probably the most important on the syllabus as they are both accessible, but comprehensive and technical enough to be useful. Ward and Ahlquist’s book is a new overview of applied ML in a political science setting. Box-Steffensmeier and Jones is a thorough and accessible treatment of hazard models in a variety of empirical settings.\n\n\nAdditional Resources\nOther useful books include:\n\nCameron, A. Colin and Trivedi, Pravin K. 1998. Regression Analysis of Count Data. Cambridge. ISBN 0521635675\nMaddala, Gregory. 1983. Limited Dependent and Qualitative Variables in Econometrics. Cambridge. ISBN 0521338255\nPaul D Allison - Event History Analysis : Regression for Longitudinal Event Data. Sage Publications Inc. ISBN 0803920555\nTim Futing Liao - Interpreting Probability Models : Logit, Probit, and Other Generalized Linear Models. Sage Publications Inc. ISBN 0803949995\nJohn H Aldrich and Forrest D Nelson - Linear Probability, Logit, and Probit Models. Sage Publications Inc. ISBN 0803921330\nFred C Pampel - Logistic Regression : A Primer. Sage Publications Inc. ISBN 0761920102\nVani Kant Borooah - Logit and Probit : Ordered and Multinomial Models. Sage Publications Inc. ISBN 0761922423\nScott R Eliason - Maximum Likelihood Estimation : Logic and Practice. Sage Publications Inc. ISBN 0803941072\nRichard Breen - Regression Models : Censored, Sample Selected, or Truncated Data. Sage Publications Inc. ISBN 0803957106\nKrishnan Namboodiri - Matrix Algebra : An Introduction. ISBN 0803920520"
  },
  {
    "objectID": "mlesyllabus24.html#course-requirements-and-grades",
    "href": "mlesyllabus24.html#course-requirements-and-grades",
    "title": "MLE Syllabus",
    "section": "Course Requirements and Grades",
    "text": "Course Requirements and Grades\nThe seminar requires the following:\n\nProblem sets - 60% total\nMechanism papers - 40%\n\nPlease note that all written assignments must be submitted as PDFs either compiled in LaTeX or in R markdown (Quarto).\nYou’ll complete a series of problem sets, mostly applied. How many will depend on how things move along during the term. Regarding the problem sets - the work you turn in for the problem sets should clearly be your own, but I urge you to work together - doing so is a great way to learn and to overcome problems.\nThe mechanism papers are a series of three short papers you’ll write during the semester aimed at learning to identify and describe causal mechanisms, then at producing a causal mechanism. More on these early in the term.\nA word about completeness - attempt everything. To receive a passing grade in the course, you must finish all elements of the course, so all problem sets, all exams, papers, etc. To complete an element, you must at least attempt all parts of the element - so if a problem set has 10 problems, you must attempt all 10 or the assignment is incomplete, you’ve not completed every element of the course, and you cannot pass. I realize there may be problems you have trouble with and even get wrong, but you must try - the bottom line is don’t turn in incomplete work.\nGrades will be assigned on the following scale:\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\nRange\nGrade\nRange\n\n\n\n\nA\n94-100%\nC+\n77-79%\n\n\nA-\n90–93%\nC\n73-76%\n\n\nB+\n87–89%\nC-\n70-72%\n\n\nB\n83-86%\nD\n60-69%\n\n\nB-\n80-82%\nF\n&lt;60%"
  },
  {
    "objectID": "mlesyllabus24.html#course-policies",
    "href": "mlesyllabus24.html#course-policies",
    "title": "MLE Syllabus",
    "section": "Course Policies",
    "text": "Course Policies\n\nAttendance\nAttendance is expected, and is essential if you’re to succeed in this class.\n\n\nAcademic Integrity\nIdeas are the currency in academic exchange, so acknowledging where ideas come from is important. Acknowledging the sources of ideas also helps us identify an idea’s lineage which can be important for understanding how that line of thought has developed, and toward promoting future growth. As graduate students, you should have a good understanding of academic honesty and best practices. Here are details of Binghamton’s honesty policy."
  },
  {
    "objectID": "mlesyllabus24.html#course-schedule",
    "href": "mlesyllabus24.html#course-schedule",
    "title": "MLE Syllabus",
    "section": "Course Schedule",
    "text": "Course Schedule\nWeek 1, Aug 21 – Binary \\(y\\) Variables I - probit/logit, QI\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 1, 2, 4\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 3.    \n\nWeek 2, Aug 28 – Likelihood Theory and ML Estimation\n\nGary King. 1998. Unifying Political Methodology. Chapter 1-4\nJ. Scott Long. 1997. *Regression Models for Categorical and Limited Dependent Variables}. Chapters 1-2.\n\nWeek 3, Sept 4 – Binary \\(y\\) Variables II - symmetry, fit, diagnostics, prediction\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 3, 5, 6, 7\n\n\n\nNagler (1994)\nKing and Zeng (2001)\nFranklin and Kosaki (1989)\nC. Zorn (2005)\n\nWeek 4, Sept 11 – Binary \\(y\\) Variables III (discrete hazards)\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 11 \nBeck, Katz, and Tucker (1998)\nCarter and Signorino (2010)\n\nWeek 5, Sept 18 – Binary \\(y\\) Variables IV - variance, order\n\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 5.\nFranklin (1991)\nAlvarez and Brehm (1995)\nClark and Nordstrom (2005)\n\nWeek 6, Sept 25 – Assumptions and Specification - interactions, functional form, measurement of \\(y\\)\n\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 5.\nClark, Nordstrom, and Reed (2008)\nClarke and Stone (2008)\nBerry, Golder, and Milton (2012)\nBrambor, Clark, and Golder (2006)\n\nWeek 7, Oct 2 – No class, Yom Kippur\nWeek 8, Oct 9 – Choice Models I (Unordered \\(y\\) Variables) - MNL, MNP, CL (IIA)\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 9\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 6.\nAlvarez and Nagler (1998)\nLacy and Burden (1999)\nC. J. W. Zorn (1996)\n\nWeek 9, Oct 16– Choice Models II (Unordered Dependent Variables continued, and systems of eqs, ordered)\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 8\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 5.\nFranklin and Kosaki (1989)\n\nWeek 10, Oct 23 – Event Count Models I - poisson, dispersion\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 10\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 8.1, 8.2.\nGowa (1998)\nFordham (1998)\n\nWeek 11, Oct 30 – Event Count Models II - negative binomial, zero-altered\n\nJ. Scott Long. 1997. Regression Models for Categorical and Limited Dependent Variables. Chapter 8.3-8.7.\nC. J. W. Zorn (1998)\nClark (2003)\n\nWeek 12, Nov 6 – Continuous Time Hazard Models I - parametric, semi-parametric models\n\nWard & Alhlquist, 2018. Maximum Likelihood for Social Science. Chapter 11\nJanet Box-Steffensmeier and Brad Jones. 2004. Event History Modeling. Chs. 1-4\nJ. M. Box-Steffensmeier, Arnold, and Zorn (1997)\n\nWeek 13, Nov 13 – Continuous Time Hazard Models II - parametric models, special topics\n\nJanet Box-Steffensmeier and Brad Jones. 2004. Event History Modeling. Chs. 5-11\nC. J. W. Zorn (2000)\nBennett and Stam (1996)\nJ. Box-Steffensmeier, Reiter, and Zorn (2003)\n\nWeek 14, Nov 20 – Censored/Truncated Variables, Samples - selection models - J. Scott Long. 1997. *Regression Models for Categorical and Limited Dependent Variables}. Chapter 7\n\nReed (2000)\nSignorino (1999)\nTimpone (1998)\n\nWeek 15, Nov 27 – no class, Thanksgiving\nWeek 16, Dec 4 – Review"
  },
  {
    "objectID": "prediction24.html",
    "href": "prediction24.html",
    "title": "Prediction Methods for MLE Models",
    "section": "",
    "text": "Most MLE models are nonlinear, so their coefficients are not their marginal effects. As a result, most MLE models require a transformation of the linear prediction to generate quantities of interest. The methods outlined here apply to most MLE applications; the immediate interest and examples here use binary response models. These slides will form a foundation for prediction in other types of models we encounter.\n\n\nProbit and logit coefficients are directly interpretable in the senses that\n\nWe can interpret direction.\nWe can interpret statistical difference from zero.\nWe can say the largest marginal effect of \\(x \\approx 0.4\\cdot\\widehat{\\beta}\\) for the probit model.\nWe can say the largest marginal effect of \\(x \\approx 0.25\\cdot\\widehat{\\beta}\\) for the logit model.\nWe can say that \\(exp(\\widehat{\\beta_k})-1\\) is the percentage change in the odds that \\(y=1\\), for the logit model.\n\nIt’s still the case that we often want other quantities of interest like probabilities, and that requires the straightforward transformations of the linear prediction, \\(F(x_i\\widehat{\\beta})\\).\nLet’s look briefly at the intuition of the “maximum marginal effect” in the logit model.\n\n\ncode\nz &lt;- seq(-5,5,.1)\nncdf &lt;- pnorm(z)\nnpdf &lt;- dnorm(z)\nlcdf &lt;- plogis(z)\nlpdf &lt;- dlogis(z)\n\ndf &lt;- data.frame(ncdf=ncdf, npdf=npdf, lcdf=lcdf, lpdf=lpdf, z=z)\n\nggplot(df, aes(x=z, y=ncdf), color=\"black\") +\n  geom_line() +\n  geom_line(aes(x=z, y=lcdf), color=\"green\") +\n  geom_line(aes(x=z, y=npdf), color=\"black\") +\n  geom_line(aes(x=z, y=lpdf), color=\"green\") +\n  geom_hline(yintercept = .3989, linetype=\"dashed\") +\n  geom_hline(yintercept = .25, linetype=\"dashed\") +\n  labs(x=\"z\", y=\"Pr(y=1)\") +\n  theme_minimal() +\n  annotate(\"text\", x = 2.5, y = .36, label = \"Normal\", color = \"black\") +\n  annotate(\"text\", x = 2.5, y = .22, label = \"Logistic\", color = \"black\") \n\n\n\n\n\n\n\n\n\nThe highest points on the PDFs indicate the maximum marginal effect of \\(x\\) on \\(Pr(y=1)\\) in the logit and probit models."
  },
  {
    "objectID": "prediction24.html#binary-response-models",
    "href": "prediction24.html#binary-response-models",
    "title": "Prediction Methods for MLE Models",
    "section": "",
    "text": "Probit and logit coefficients are directly interpretable in the senses that\n\nWe can interpret direction.\nWe can interpret statistical difference from zero.\nWe can say the largest marginal effect of \\(x \\approx 0.4\\cdot\\widehat{\\beta}\\) for the probit model.\nWe can say the largest marginal effect of \\(x \\approx 0.25\\cdot\\widehat{\\beta}\\) for the logit model.\nWe can say that \\(exp(\\widehat{\\beta_k})-1\\) is the percentage change in the odds that \\(y=1\\), for the logit model.\n\nIt’s still the case that we often want other quantities of interest like probabilities, and that requires the straightforward transformations of the linear prediction, \\(F(x_i\\widehat{\\beta})\\).\nLet’s look briefly at the intuition of the “maximum marginal effect” in the logit model.\n\n\ncode\nz &lt;- seq(-5,5,.1)\nncdf &lt;- pnorm(z)\nnpdf &lt;- dnorm(z)\nlcdf &lt;- plogis(z)\nlpdf &lt;- dlogis(z)\n\ndf &lt;- data.frame(ncdf=ncdf, npdf=npdf, lcdf=lcdf, lpdf=lpdf, z=z)\n\nggplot(df, aes(x=z, y=ncdf), color=\"black\") +\n  geom_line() +\n  geom_line(aes(x=z, y=lcdf), color=\"green\") +\n  geom_line(aes(x=z, y=npdf), color=\"black\") +\n  geom_line(aes(x=z, y=lpdf), color=\"green\") +\n  geom_hline(yintercept = .3989, linetype=\"dashed\") +\n  geom_hline(yintercept = .25, linetype=\"dashed\") +\n  labs(x=\"z\", y=\"Pr(y=1)\") +\n  theme_minimal() +\n  annotate(\"text\", x = 2.5, y = .36, label = \"Normal\", color = \"black\") +\n  annotate(\"text\", x = 2.5, y = .22, label = \"Logistic\", color = \"black\") \n\n\n\n\n\n\n\n\n\nThe highest points on the PDFs indicate the maximum marginal effect of \\(x\\) on \\(Pr(y=1)\\) in the logit and probit models."
  },
  {
    "objectID": "prediction24.html#at-means-predictions",
    "href": "prediction24.html#at-means-predictions",
    "title": "Prediction Methods for MLE Models",
    "section": "At-Means Predictions",
    "text": "At-Means Predictions\nAt-means predictions are what they sound like - effects with independent variables set at central tendencies. These are sometimes called “adjusted predictions.”\n\nestimate model.\ncreate out of sample data.\nvary \\(x\\) of interest; set all other \\(x\\) variables to appropriate central tendencies - hence the “at Means.”\ngenerate QIs in out of sample data."
  },
  {
    "objectID": "prediction24.html#average-effects",
    "href": "prediction24.html#average-effects",
    "title": "Prediction Methods for MLE Models",
    "section": "Average Effects",
    "text": "Average Effects\nAverage Marginal Effects are in-sample but create a counterfactual for a variable of interest, assuming the entire sample looks like that case.\nFor instance, suppose a model of wages with covariates for education and gender. We might ask the question what would the predictions look like if the entire sample were male, but otherwise looked as it does? Alternatively, what would the predictions look like if the entire sample were female, but all other variables the same as they appear in the estimation data?\nTo answer these, we’d change the gender variable to male, generate \\(x{\\widehat{\\beta}}\\) for the entire sample, and take the average, then repeat with the gender variable set to female.\nTo generate Average Effects,\n\nestimate model.\nin estimation data, set variable of interest to a particular value for the entire estimation sample.\ngenerate QIs (expected values, standard errors).\ntake average of QIs, and save.\nrepeat for all values of variable of interest, and plot."
  },
  {
    "objectID": "prediction24.html#at-means-predictions-logit",
    "href": "prediction24.html#at-means-predictions-logit",
    "title": "Prediction Methods for MLE Models",
    "section": "At-means predictions (logit)",
    "text": "At-means predictions (logit)\nHere’s an example of at-means predictions for a logit model of the democratic peace. FIrst, let’s look at the model estimates:\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n\nstargazer(m1, type=\"html\",  single.row=TRUE, header=FALSE, digits=3,  omit.stat=c(\"LL\",\"ser\"),  star.cutoffs=c(0.05,0.01,0.001),    dep.var.caption=\"Dependent Variable: Dispute\", dep.var.labels.include=FALSE,  covariate.labels=c(\"Shared Border\", \"Democracy\", \"Capabilities Ratio\", \"Allies\"),  notes=c(\"Standard errors in parentheses\", \"Significance levels:  *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\"), notes.append = FALSE,  align=TRUE,  font.size=\"small\")\n\n\n\n\n\n\n\n\n\n\nDependent Variable: Dispute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShared Border\n\n\n1.221*** (0.078)\n\n\n\n\nDemocracy\n\n\n-0.071*** (0.007)\n\n\n\n\nCapabilities Ratio\n\n\n-0.003*** (0.0004)\n\n\n\n\nAllies\n\n\n-0.806*** (0.080)\n\n\n\n\nConstant\n\n\n-3.492*** (0.075)\n\n\n\n\n\n\n\n\nObservations\n\n\n20,990\n\n\n\n\nAkaike Inf. Crit.\n\n\n7,011.947\n\n\n\n\n\n\n\n\nNote:\n\n\nStandard errors in parentheses\n\n\n\n\n\n\nSignificance levels: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n\n\n\nAs with any nonlinear model, we need to compute a linear prediction, \\(x\\widehat{\\beta}\\), and then transform that to a probability. For at-means predictions, we’ll vary democracy across its range, holding the remaining variables at appropriate central tendency (e.g, mode for dummy variables, median for categorical or skewed variables, etc.) Take a look at the code:\n\n\ncode\ndp &lt;- read_dta(\"/Users/dave/Documents/teaching/501/2023/slides/L7_limiteddv/code/dp.dta\")\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\nlogitpreds &lt;- predict(m1, type=\"response\")\n\nm2 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"probit\"), data=dp )\n\n#new data frame for MEM prediction\nmem &lt;- data.frame(deml= c(seq(-10,10,1)), \n                  border=0, caprat=median(dp$caprat), ally=0)\n\n# type=\"link\" produces the linear predictions; transform by hand below w/EPT\nmem  &lt;-data.frame(mem, predict(m1, type=\"link\", newdata=mem, se=TRUE))\n\nmem &lt;- cbind(mem,lb=plogis(mem$fit-1.96*mem$se.fit),\n             ub=plogis(mem$fit+1.96*mem$se.fit), \n             p=plogis(mem$fit))\n\nggplot(mem, aes(x=deml, y=p)) +\n  geom_line() +\n  geom_ribbon(data=mem, aes(x=deml, ymin=lb, ymax=ub),fill = \"grey30\", alpha = .4, ) +\n  labs(x=\"Polity Score\", y=\"Pr(Dispute) (95% confidence interval)\")"
  },
  {
    "objectID": "prediction24.html#average-effects-logit",
    "href": "prediction24.html#average-effects-logit",
    "title": "Prediction Methods for MLE Models",
    "section": "Average effects (logit)",
    "text": "Average effects (logit)\nAverage effects are often a better choice because they represent the data more completely than central tendency can (in the at-mean effects). Here are average effects (using the logit estimates) across the range of polity, and for pairs of states that share borders and those that do not.\n\n\ncode\n#avg effects\n\n#identify the estimation sample\ndp$used &lt;- TRUE\ndp$used[na.action(m1)] &lt;- FALSE\ndpesample &lt;- dp %&gt;%  filter(used==\"TRUE\")\n\npolity &lt;- 0\nmedxbd0 &lt;- 0\nubxbd0 &lt;- 0\nlbxbd0 &lt;- 0\n# medse &lt;- 0\n# medxbd1 &lt;- 0\n# ubxbd1 &lt;- 0\n# lbxbd1 &lt;- 0\n\nfor(i in seq(1,21,1)){\n  dpesample$border&lt;- 0\n  dpesample$deml &lt;- i-11\n  polity[i] &lt;- i-11\n  allpreds &lt;- data.frame(predict(m1, type= \"response\", se.fit=TRUE, newdata = dpesample))  \n  medxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)\n  ubxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)+1.96*(median(allpreds$se.fit, na.rm=TRUE))\n  lbxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)-1.96*(median(allpreds$se.fit, na.rm=TRUE))\n}\n  \nnoborder &lt;- data.frame(polity, medxbd0, ubxbd0, lbxbd0)\n  \nfor(i in seq(1,21,1)){\n  dpesample$border&lt;- 1\n  dpesample$deml &lt;- i-11\n  polity[i] &lt;- i-11\n  allpreds &lt;- data.frame(predict(m1, type= \"response\", se.fit=TRUE, newdata = dpesample))  \n  medxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)\n  ubxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)+1.96*(median(allpreds$se.fit, na.rm=TRUE))\n  lbxbd0[i] &lt;- median(allpreds$fit, na.rm=TRUE)-1.96*(median(allpreds$se.fit, na.rm=TRUE))\n}\n  \nborder &lt;- data.frame(polity, medxbd0, ubxbd0, lbxbd0)\n  \n\n\nggplot() +\n  geom_ribbon(data=noborder, aes(x=polity, ymin=lbxbd0, ymax=ubxbd0),fill = \"grey30\", alpha = .4, ) +\n  geom_line(data=noborder, aes(x=polity, y=medxbd0))+\n  geom_ribbon(data=border, aes(x=polity, ymin=lbxbd0, ymax=ubxbd0),fill = \"grey30\", alpha = .4, ) +\n  geom_line(data=border, aes(x=polity, y=medxbd0))+\n  labs ( colour = NULL, x = \"Polity Score\", y =  \"Pr(Dispute)\" )+\n  theme_minimal()+\n  ggtitle(\"Average Effects\")"
  },
  {
    "objectID": "prediction24.html#simulation",
    "href": "prediction24.html#simulation",
    "title": "Prediction Methods for MLE Models",
    "section": "Simulation",
    "text": "Simulation\nSimulation is an especially good approach for nonlinear models:\n\nestimate the model.\n\\(m\\) times (say, 1000 times), simulate the distribution of \\(\\widehat{\\beta}\\).\ngenerate the \\(m\\) linear predictions, \\(x\\widehat{\\beta}\\).\ntransform by the appropriate link function (logistic, standard normal CDF).\nidentify the 2.5, 50, and 97.5 percentiles.\nplot against \\(x\\).\n\n\n\ncode\n#draws from multivariate normal using logit model estimates\nsimL &lt;- data.frame(MASS::mvrnorm(1000, coef(m1), vcov(m1)))\n#draws from multivariate normal using probit model estimates\nsimP &lt;- data.frame(MASS::mvrnorm(1000, coef(m2), vcov(m2)))\n\n#Logit predictions\nlogitprobs &lt;- data.frame(dem= numeric(0) , lb=numeric(0), med= numeric(0), ub=numeric(0))\nfor (i in seq(1,21,1)) {\nsimpreds &lt;- quantile(simL$X.Intercept.+ simL$border*0+simL$deml*(i-11)+simL$caprat*median(dp$caprat)+simL$ally*0, probs=c(.025, .5, .975))\nlogitprobs[i,] &lt;- data.frame(dem=i, lb=plogis(simpreds[1]), med=plogis(simpreds[2]), ub=plogis(simpreds[3]))\n}\n\n#Probit predictions\nprobitprobs &lt;- data.frame(dem= numeric(0) , lb=numeric(0), med= numeric(0), ub=numeric(0))\nfor (i in seq(1,21,1)) {\nsimpreds &lt;- quantile(simP$X.Intercept.+ simP$border*0+simP$deml*(i-11)+simP$caprat*median(dp$caprat)+simP$ally*0, probs=c(.025, .5, .975))\nprobitprobs[i,] &lt;- data.frame(dem=i, lb=pnorm(simpreds[1]), med=pnorm(simpreds[2]), ub=pnorm(simpreds[3]))\n}\n\nlogit &lt;- ggplot() +\n  geom_ribbon(data=logitprobs, aes(x=dem, ymin=lb, ymax=ub),fill = \"grey30\", alpha = .4, ) +\n  geom_line(data=logitprobs, aes(x=dem, y=med))+\n  labs ( colour = NULL, x = \"Polity Score\", y =  \"Pr(Dispute)\" )+\n  theme_minimal()+\n  ggtitle(\"Logit Predictions\")\n\nprobit &lt;- ggplot() +\n  geom_ribbon(data=probitprobs, aes(x=dem, ymin=lb, ymax=ub),fill = \"grey30\", alpha = .4, ) +\n  geom_line(data=probitprobs, aes(x=dem, y=med))+\n  labs ( colour = NULL, x = \"Polity Score\", y =  \"Pr(Dispute)\" )+\n  theme_minimal()+\n  ggtitle(\"Probit Predictions\")\n\nlogit+probit"
  },
  {
    "objectID": "prediction24.html#simulating-combinations-of-binary-variables",
    "href": "prediction24.html#simulating-combinations-of-binary-variables",
    "title": "Prediction Methods for MLE Models",
    "section": "Simulating combinations of binary variables",
    "text": "Simulating combinations of binary variables\nLet’s look at the differences among the four combinations of the binary variables, border and ally.\n\n\ncode\n## simulating for binary combinations ----\n\nm1 &lt;-glm(dispute ~ border+deml+caprat+ally, family=binomial(link=\"logit\"), data=dp )\n\n#draws from multivariate normal using logit model estimates\nsimL &lt;- data.frame(MASS::mvrnorm(1000, coef(m1), vcov(m1)))\n\n\nlogitprobs &lt;- data.frame(b0a0= numeric(0) , b1a0=numeric(0), b0a1= numeric(0), b1a1=numeric(0))\n\n  b0a0 &lt;- plogis(simL$X.Intercept.+ simL$border*0+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*0)\nb1a0 &lt;- plogis(simL$X.Intercept.+ simL$border*1+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*0)\nb0a1 &lt;- plogis(simL$X.Intercept.+ simL$border*0+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*1)\nb1a1 &lt;- plogis(simL$X.Intercept.+ simL$border*1+simL$deml*-7+simL$caprat*median(dp$caprat)+simL$ally*1)\n\nlogitprobs &lt;- data.frame(b0a0, b1a0, b0a1, b1a1)\n\nggplot() +\n  geom_density(data=logitprobs, aes(x=b0a0), fill=\"grey30\", alpha = .4, ) +\n  geom_density(data=logitprobs, aes(x=b1a0), fill=\"grey30\", alpha = .4, ) +\n  geom_density(data=logitprobs, aes(x=b0a1), fill=\"grey30\", alpha = .4, ) +\n  geom_density(data=logitprobs, aes(x=b1a1), fill=\"grey30\", alpha = .4, ) +\n  labs ( colour = NULL, x = \"Pr(Dispute)\", y =  \"Density\" ) +\n  theme_minimal()+\n  annotate(\"text\", x = .07, y = 150, label = \"No border, not allies\", color = \"black\") +\n  annotate(\"text\", x = .13, y = 70, label = \"Border, not allies\", color = \"black\") +\n  annotate(\"text\", x = .04, y = 200, label = \"No border, allies\", color = \"black\") +\n  annotate(\"text\", x = .09, y = 50, label = \"Border, allies\", color = \"black\") +\n  ggtitle(\"Logit Predictions\")"
  },
  {
    "objectID": "prediction24.html#ml-standard-errors-of-linear-predictions",
    "href": "prediction24.html#ml-standard-errors-of-linear-predictions",
    "title": "Prediction Methods for MLE Models",
    "section": "ML Standard Errors of Linear Predictions",
    "text": "ML Standard Errors of Linear Predictions\nOne commonly used measure of uncertainty is the standard error of the linear prediction, \\(X\\widehat{\\beta}\\).\nConsider the linear prediction\n\\[X \\widehat{\\beta} \\]\nunder maximum likelihood theory:\n\\[var(X \\widehat{\\beta}) = \\mathbf{X V X'} \\]\nan \\(N x N\\) matrix, where \\(V\\) is the var-cov matrix of \\({\\widehat{\\beta}}\\). The main diagonal contains the variances of the \\(N\\) predictions. The standard errors are:\n\\[se(X \\widehat{\\beta}) = \\sqrt{diag(\\mathbf{X V X'})} \\]\nwhich is an \\(N x 1\\) vector. So now we have a column vector of standard errors for the linear prediction, \\(X\\widehat{\\beta}\\). Like the linear predictions, these are not transformed into probabilities, so when we compute confidence intervals, we need to map the upper and lower bounds onto the probability space.\n\\[F(X \\widehat{\\beta} - c*s.e.) \\leq F(X \\widehat{\\beta}) \\leq F(X \\widehat{\\beta} + c* s.e.)\\]"
  },
  {
    "objectID": "prediction24.html#delta-method-standard-errors",
    "href": "prediction24.html#delta-method-standard-errors",
    "title": "Prediction Methods for MLE Models",
    "section": "Delta Method standard errors",
    "text": "Delta Method standard errors\nThe maximum likelihood method is appropriate for monotonic functions of \\(X \\widehat{\\beta}\\), e.g. logit, probit. In other models (e.g., multinomial logit), the function is not monotonic in \\(X \\widehat{\\beta}\\) so we use the Delta Method - this creates a linear approximation of the function. Greene (2012) (693ff) gives this as a general derivation of the variance:\n\\[Var[F(X \\widehat{\\beta})] = f(\\mathbf{x'\\widehat{\\beta}})^2 \\mathbf{x' V x} \\]\nwhere this would generate variances for whatever \\(F(X \\widehat{\\beta})\\) is, perhaps a predicted probability.\n\nDelta method standard errors for Logit\nFor the logit, the delta standard errors are given by:\n\\[F(X \\widehat{\\beta}) * (1-F(X \\widehat{\\beta}) * \\mathbf(X V X')\\]\n\\[ = f(X \\widehat{\\beta})  *  \\mathbf{\\sqrt{X V X'}}\\]\nor\n\\[ p * (1-p) * stdp\\]\nwhere \\(stdp\\) is the standard error of the linear prediction."
  },
  {
    "objectID": "prediction24.html#ses-of-predictions-for-linear-combinations",
    "href": "prediction24.html#ses-of-predictions-for-linear-combinations",
    "title": "Prediction Methods for MLE Models",
    "section": "SEs of Predictions for linear combinations",
    "text": "SEs of Predictions for linear combinations\nA common circumstance that requires joint hypothesis tests is the case of polynomials (which are themselves interactions):\n\\[y = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_1 + \\widehat{\\beta}_2 x_{1}^2  + \\varepsilon \\]\nThe question is whether \\(\\widehat{\\beta}_1 = \\widehat{\\beta}_2  = 0\\) - the marginal effect is:\n\\[ \\widehat{\\beta}_1 + 2 \\widehat{\\beta}_2x_1\\]\nand requires the standard error for \\(\\widehat{\\beta}_1+\\widehat{\\beta}_2\\), which is:\n\\[ \\sqrt{var(\\widehat{\\beta}_1) + 4x_{1}^{2}var(\\widehat{\\beta}_2) +  4x_1 cov(\\widehat{\\beta}_1, \\widehat{\\beta}_2)  }\\]"
  },
  {
    "objectID": "prediction24.html#cis---end-point-transformation",
    "href": "prediction24.html#cis---end-point-transformation",
    "title": "Prediction Methods for MLE Models",
    "section": "CIs - End Point Transformation",
    "text": "CIs - End Point Transformation\nGenerate upper and lower bounds using either ML or Delta standard errors, such that\n\\[F(X \\widehat{\\beta} - c*s.e.) \\leq F(X \\widehat{\\beta}) \\leq F(X \\widehat{\\beta} + c* s.e.)\\]\n\nestimate the model, generate the linear prediction, and the standard error of the linear prediction using the either ML or Delta.\ngenerate linear boundary predictions, \\(x{\\widehat{\\beta}} \\pm c * \\text{st. err.}\\) where \\(c\\) is a critical value on the normal, eg. \\(z=1.96\\).\ntransform the linear prediction and the upper and lower boundary predictions by \\(F(\\cdot)\\).\nWith ML standard errors, EPT boundaries will obey distributional boundaries (ie, won’t fall outside 0-1 interval for probabilities); the linear end point predictions are symmetric, though they will not be symmetric in nonlinear models.\nWith delta standard errors, bounds may not obey distributional boundaries."
  },
  {
    "objectID": "prediction24.html#simulating-confidence-intervals-i",
    "href": "prediction24.html#simulating-confidence-intervals-i",
    "title": "Prediction Methods for MLE Models",
    "section": "Simulating confidence intervals, I",
    "text": "Simulating confidence intervals, I\n\ndraw a sample with replacement of size \\(\\tilde{N}\\) from the estimation sample.\nestimate the model parameters in that bootstrap sample.\nusing the bootstrap estimates, generate quantities of interest (e.g. \\(x\\widehat{\\beta}\\)) repeat \\(j\\) times.\ncollect all these bootstrap QIs and use either percentiles or standard deviations to measure uncertainty."
  },
  {
    "objectID": "prediction24.html#uncertainty-simulating-confidence-intervals-ii",
    "href": "prediction24.html#uncertainty-simulating-confidence-intervals-ii",
    "title": "Prediction Methods for MLE Models",
    "section": "Uncertainty: Simulating confidence intervals, II",
    "text": "Uncertainty: Simulating confidence intervals, II\n\nestimate the model.\ngenerate a large sample distribution of parameters (e.g. using drawnorm).\ngenerate quantities of interest for the distribution of parameters.\nuse either percentiles or standard deviations of the QI to measure uncertainty."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Maximum Likelihood, Fall 2024",
    "section": "",
    "text": "This is the course website for PLSC 606J, Maximum Likelihood Estimation, Fall 2024. The course is an advanced course in data science focusing on maximum likelihood methods and their applications in political science.\n\nSyllabus\nSlides\nCode\n\n\n\n\n Back to top"
  }
]